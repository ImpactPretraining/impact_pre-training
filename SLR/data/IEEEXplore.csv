,id,title,author,year,issn,url,doi,abstract,keyword,pages,num_pages,content_type,publication_venue,publisher
0,,Better Data Labelling With EMBLEM (and how that Impacts Defect Prediction),H. Tu; Z. Yu; T. Menzies,2022,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9064604,10.1109/TSE.2020.2986415,"Standard automatic methods for recognizing problematic development commits can be greatly improved via the incremental application of human+artificial expertise. In this approach, called EMBLEM, an AI tool first explore the software development process to label commits that are most problematic. Humans then apply their expertise to check those labels (perhaps resulting in the AI updating the support vectors within their SVM learner). We recommend this human+AI partnership, for several reasons. When a new domain is encountered, EMBLEM can learn better ways to label which comments refer to real problems. Also, in studies with 9 open source software projects, labelling via EMBLEM's incremental application of human+AI is at least an order of magnitude cheaper than existing methods (<inline-formula><tex-math notation=""LaTeX"">$\approx$</tex-math><alternatives><mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""><mml:mo>≈</mml:mo></mml:math><inline-graphic xlink:href=""menzies-ieq1-2986415.gif"" xmlns:xlink=""http://www.w3.org/1999/xlink""/></alternatives></inline-formula> eight times). Further, EMBLEM is very effective. For the data sets explored here, EMBLEM better labelling methods significantly improved <inline-formula><tex-math notation=""LaTeX"">$P_{opt}20$</tex-math><alternatives><mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mn>20</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href=""menzies-ieq2-2986415.gif"" xmlns:xlink=""http://www.w3.org/1999/xlink""/></alternatives></inline-formula> and G-scores performance in nearly all the projects studied here.",Human-in-the-loop AI;data labelling;defect prediction;software analytics,"278, 294",,IEEE Journals,IEEE Transactions on Software Engineering,IEEE
1,,Reinforcement-Learning-Guided Source Code Summarization Using Hierarchical Attention,W. Wang; Y. Zhang; Y. Sui; Y. Wan; Z. Zhao; J. Wu; P. S. Yu; G. Xu,2022,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9031440,10.1109/TSE.2020.2979701,"Code summarization (aka comment generation) provides a high-level natural language description of the function performed by code, which can benefit the software maintenance, code categorization and retrieval. To the best of our knowledge, the state-of-the-art approaches follow an encoder-decoder framework which encodes source code into a hidden space and later decodes it into a natural language space. Such approaches suffer from the following drawbacks: (a) they are mainly input by representing code as a sequence of tokens while ignoring code hierarchy; (b) most of the encoders only input simple features (e.g., tokens) while ignoring the features that can help capture the correlations between comments and code; (c) the decoders are typically trained to predict subsequent words by maximizing the likelihood of subsequent ground truth words, while in real world, they are excepted to generate the entire word sequence from scratch. As a result, such drawbacks lead to inferior and inconsistent comment generation accuracy. To address the above limitations, this paper presents a new code summarization approach using hierarchical attention network by incorporating multiple code features, including type-augmented abstract syntax trees and program control flows. Such features, along with plain code sequences, are injected into a deep reinforcement learning (DRL) framework (e.g., actor-critic network) for comment generation. Our approach assigns weights (pays “attention”) to tokens and statements when constructing the code representation to reflect the hierarchical code structure under different contexts regarding code features (e.g., control flows and abstract syntax trees). Our reinforcement learning mechanism further strengthens the prediction results through the actor network and the critic network, where the actor network provides the confidence of predicting subsequent words based on the current state, and the critic network computes the reward values of all the possible extensions of the current state to provide global guidance for explorations. Eventually, we employ an advantage reward to train both networks and conduct a set of experiments on a real-world dataset. The experimental results demonstrate that our approach outperforms the baselines by around 22 to 45 percent in BLEU-1 and outperforms the state-of-the-art approaches by around 5 to 60 percent in terms of S-BLEU and C-BLEU.",Code summarization;hierarchical attention;reinforcement learning,"102, 119",,IEEE Journals,IEEE Transactions on Software Engineering,IEEE
2,,"Machine Learning Testing: Survey, Landscapes and Horizons",J. M. Zhang; M. Harman; L. Ma; Y. Liu,2022,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9000651,10.1109/TSE.2019.2962027,"This paper provides a comprehensive survey of techniques for testing machine learning systems; Machine Learning Testing (ML testing) research. It covers 144 papers on testing properties (e.g., correctness, robustness, and fairness), testing components (e.g., the data, learning program, and framework), testing workflow (e.g., test generation and test evaluation), and application scenarios (e.g., autonomous driving, machine translation). The paper also analyses trends concerning datasets, research trends, and research focus, concluding with research challenges and promising research directions in ML testing.",Machine learning;software testing;deep neural network,"1, 36",,IEEE Journals,IEEE Transactions on Software Engineering,IEEE
3,,Pre-Trained Neural Language Models for Automatic Mobile App User Feedback Answer Generation,Y. Cao; F. H. Fard,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9680321,10.1109/ASEW52652.2021.00033,"Studies show that developers’ answers to the mobile app users’ feedbacks on app stores can increase the apps’ star rating. To help app developers generate answers that are related to the users’ issues, recent studies develop models to generate the answers automatically. Aims: The app response generation models use deep neural networks and require training data. Pre-Trained neural language Models (PTM) used in Natural Language Processing (NLP) take advantage of the information they learned from a large corpora in an unsupervised manner, and can reduce the amount of required training data. In this paper, we evaluate PTMs to generate replies to the mobile app user feedbacks. Method: We train a Transformer model from scratch and fine tune two PTMs to evaluate the generated responses, which are compared to RRGEN, a current app response model. We also evaluate the models with different portions of the training data. Results: The results on a large dataset evaluated by automatic metrics show that PTMs obtain lower scores than the baselines. However, our human evaluation confirm that PTMs can generate more relevant and meaningful responses to the posted feedbacks. Moreover, the performance of PTMs has less drop compared to other model when the amount of training data is reduced to 1/3. Conclusion: PTMs are useful in generating responses to app reviews and are more robust models to the amount of training data provided. However, the prediction time is 19X than RRGEN. This study can provide new avenues for research in adapting the PTMs for analyzing mobile app user feedbacks.",mobile app user feedback analysis;neural pre-trained language models;automatic answer generation,"120, 125",,IEEE Conferences,2021 36th IEEE/ACM International Conference on Automated Software Engineering Workshops (ASEW),IEEE
4,,Classification of UML Diagrams to Support Software Engineering Education,J. F. Tavares; Y. M. G. Costa; T. E. Colanzi,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9680314,10.1109/ASEW52652.2021.00030,"There is a huge necessity for tools that implement accessibility in Software Engineering (SE) education. The use of diagrams to teach software development is a very common practice, and there are a lot of UML diagrams represented as images in didactic materials that need an accessible version for visually impaired or blind students. Machine learning techniques, such as deep learning, can be used to automate this task. The practical application of deep learning in many classification problems in the context of SE is problematic due to the large volumes of labeled data required for training. Transfer learning techniques can help in this type of task by taking advantage of pre-trained models based on Convolutional Neural Networks (CNN), so that better results may be achieved even with few images. In this work, we applied transfer learning and data augmentation for UML diagrams classification on a dataset specially created for the development of this work, containing six types of UML diagrams. The dataset was also made available as a contribution of this work. We experimented three widely-known CNN architectures: VGG16, RestNet50, and InceptionV3. The results demonstrated that the use of transfer learning contributes for achieving good results even using scarce data. However, there is still a room for improvement regarding the successful classification of the UML diagrams addressed in this work.",Software Engineering Education;UML diagrams;Deep Learning;Transfer Learning;Assistive Technologies,"102, 107",,IEEE Conferences,2021 36th IEEE/ACM International Conference on Automated Software Engineering Workshops (ASEW),IEEE
5,,JavaBERT: Training a Transformer-Based Model for the Java Programming Language,N. T. De Sousa; W. Hasselbring,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9680322,10.1109/ASEW52652.2021.00028,"Code quality is and will be a crucial factor while developing new software code, requiring appropriate tools to ensure functional and reliable code. Machine learning techniques are still rarely used for software engineering tools, missing out the potential benefits of its application. Natural language processing has shown the potential to process text data regarding a variety of tasks. We argue, that such models can also show similar benefits for software code processing. In this paper, we investigate how models used for natural language processing can be trained upon software code. We introduce a data retrieval pipeline for software code and train a model upon Java software code. The resulting model, JavaBERT, shows a high accuracy on the masked language modeling task showing its potential for software engineering tools.",,"90, 95",,IEEE Conferences,2021 36th IEEE/ACM International Conference on Automated Software Engineering Workshops (ASEW),IEEE
6,,Identifying non-natural language artifacts in bug reports,T. Hirsch; B. Hofer,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9680274,10.1109/ASEW52652.2021.00046,"Bug reports are a popular target for natural language processing (NLP). However, bug reports often contain artifacts such as code snippets, log outputs and stack traces. These artifacts not only inflate the bug reports with noise, but often constitute a real problem for the NLP approach at hand and have to be removed. In this paper, we present a machine learning based approach to classify content into natural language and artifacts at line level implemented in Python. We show how data from GitHub issue trackers can be used for automated training set generation, and present a custom preprocessing approach for bug reports. Our model scores at 0.95 ROC-AUC and 0.93 F1 against our manually annotated validation set, and classifies 10k lines in 0.72 seconds. We cross evaluated our model against a foreign dataset and a foreign R model for the same task. The Python implementation of our model and our datasets are made publicly available under an open source license.",NLP;bug reports;data cleaning;artifact removal,"191, 197",,IEEE Conferences,2021 36th IEEE/ACM International Conference on Automated Software Engineering Workshops (ASEW),IEEE
7,,Sustainable AI in the Cloud: Exploring Machine Learning Energy Use in the Cloud,P. Walsh; J. Bera; V. S. Sharma; V. Kaulgud; R. M. Rao; O. Ross,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9680315,10.1109/ASEW52652.2021.00058,"In light of the increasing urgency regarding climate change due to man-made greenhouse gas emissions, focus is now being brought to bear on the amount of energy that artificial intelligence (AI) applications consume. Research has highlighted the immense carbon footprint of machine learning (ML) driven applications, due to the extraordinary growth in the size of deep learning models, which are estimated to have grown by a factor of 300,000 over the last six years. This is a concern, so we wish to add our voice to a growing community of responsible AI researchers and practitioners and help highlight how energy awareness and responsible best practices can be used to enhance the environmental sustainability of AI. Hence, we provide a preliminary exploration of the energy use profile of ML training in the cloud and demonstrate how transfer learning can be used to reduce this energy consumption.",AI;Cloud;Energy;Machine Learning;Transfer Learning,"265, 266",,IEEE Conferences,2021 36th IEEE/ACM International Conference on Automated Software Engineering Workshops (ASEW),IEEE
8,,Merging Datasets for Emotion Analysis,A. De Arriba; M. Oriol; X. Franch,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9680305,10.1109/ASEW52652.2021.00051,"Context. Applying sentiment analysis is in general a laborious task. Furthermore, if we add the task of getting a good quality dataset with balanced distribution and enough samples, the job becomes more complicated. Objective. We want to find out whether merging compatible datasets improves emotion analysis based on machine learning (ML) techniques, compared to the original, individual datasets. Method. We obtained two datasets with Covid-19-related tweets written in Spanish, and then built from them two new datasets combining the original ones with different consolidation of balance. We analyzed the results according to precision, recall, F1-score and accuracy. Results. The results obtained show that merging two datasets can improve the performance of ML models, particularly the F1-score, when the merging process follows a strategy that optimizes the balance of the resulting dataset. Conclusions. Merging two datasets can improve the performance of ML models for emotion analysis, whilst saving resources for labeling training data. This might be especially useful for several software engineering activities that leverage on ML-based emotion analysis techniques.",Sentiment Analysis;Emotion Classification;Machine Learning;Merging Datasets;Social Media;Twitter;BETO,"227, 231",,IEEE Conferences,2021 36th IEEE/ACM International Conference on Automated Software Engineering Workshops (ASEW),IEEE
9,,AWARE: Aspect-Based Sentiment Analysis Dataset of Apps Reviews for Requirements Elicitation,N. Alturaief; H. Aljamaan; M. Baslyman,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9679823,10.1109/ASEW52652.2021.00049,"The smartphone apps market is growing rapidly which challenges apps owners to continue improving their products and to compete in the market. The analysis of users feedback is a key enabler for improvements as stakeholders can utilize it to gain a broad understanding of the successes and failures of their products as well as those of competitors. That leads to generating evidence-based requirements and enhancing the requirements elicitation activities. Aspect-Based Sentiment Analysis (ABSA) is a branch of Sentiment Analysis that identifies aspects and assigns a sentiment to each aspect. Having the aspect information adds a more accurate understanding of opinions and addresses the limited use of the overall sentiment. However, the ABSA task has not yet been investigated in the context of smartphone apps reviews and requirements elicitation. In this paper, we introduce AWARE as a benchmark dataset of 11323 apps reviews that are annotated with aspect terms, categories, and sentiment. Reviews were collected from three domains: productivity, social networking, and games. We derived the aspect categories for each domain using content analysis and validated them with domain experts in terms of importance, comprehensiveness, overlapping, and granularity level. We crowdsourced the annotations of aspect categories and sentiment polarities and performed quality control procedures. The aspect terms were annotated using a partially automated Natural Language Processing (NLP) approach and validated by annotators, which resulted in 98% correct aspect terms. Lastly, we built machine learning baselines for three tasks, namely (i) aspect term extraction using a POS tagger, (ii) aspect category classification, and (iii) aspect sentiment classification, using both Support Vector Machine (SVM) and Multi-layer Perceptron (MLP) classifiers.",aspect-based sentiment analysis;apps reviews;dataset;requirements elicitation,"211, 218",,IEEE Conferences,2021 36th IEEE/ACM International Conference on Automated Software Engineering Workshops (ASEW),IEEE
10,,Towards Exploring the Limitations of Active Learning: An Empirical Study,Q. Hu; Y. Guo; M. Cordy; X. Xie; W. Ma; M. Papadakis; Y. L. Traon,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9678672,10.1109/ASE51524.2021.9678672,"Deep neural networks (DNNs) are increasingly deployed as integral parts of software systems. However, due to the complex interconnections among hidden layers and massive hyperparameters, DNNs must be trained using a large number of labeled inputs, which calls for extensive human effort for collecting and labeling data. Spontaneously, to alleviate this growing demand, multiple state-of-the-art studies have developed different metrics to select a small yet informative dataset for the model training. These research works have demonstrated that DNN models can achieve competitive performance using a carefully selected small set of data. However, the literature lacks proper investigation of the limitations of data selection metrics, which is crucial to apply them in practice. In this paper, we fill this gap and conduct an extensive empirical study to explore the limits of data selection metrics. Our study involves 15 data selection metrics evaluated over 5 datasets (2 image classification tasks and 3 text classification tasks), 10 DNN architectures, and 20 labeling budgets (ratio of training data being labeled). Our findings reveal that, while data selection metrics are usually effective in producing accurate models, they may induce a loss of model robustness (against adversarial examples) and resilience to compression. Overall, we demonstrate the existence of a trade-off between labeling effort and different model qualities. This paves the way for future research in devising data selection metrics considering multiple quality criteria.",deep learning;data selection;active learning;empirical study,"917, 929",,IEEE Conferences,2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
11,,DeepCVA: Automated Commit-level Vulnerability Assessment with Deep Multi-task Learning,T. H. Minh Le; D. Hin; R. Croft; M. Ali Babar,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9678622,10.1109/ASE51524.2021.9678622,"It is increasingly suggested to identify Software Vulnerabilities (SVs) in code commits to give early warnings about potential security risks. However, there is a lack of effort to assess vulnerability-contributing commits right after they are detected to provide timely information about the exploitability, impact and severity of SVs. Such information is important to plan and prioritize the mitigation for the identified SVs. We propose a novel Deep multi-task learning model, DeepCVA, to automate seven Commit-level Vulnerability Assessment tasks simultaneously based on Common Vulnerability Scoring System (CVSS) metrics. We conduct large-scale experiments on 1,229 vulnerability-contributing commits containing 542 different SVs in 246 real-world software projects to evaluate the effectiveness and efficiency of our model. We show that DeepCVA is the best-performing model with 38% to 59.8% higher Matthews Correlation Coefficient than many supervised and unsupervised baseline models. DeepCVA also requires 6.3 times less training and validation time than seven cumulative assessment models, leading to significantly less model maintenance cost as well. Over-all, DeepCVA presents the first effective and efficient solution to automatically assess SVs early in software systems.",Software vulnerability;Vulnerability assessment;Deep learning;Multi-task learning;Mining software repositories;Software security,"717, 729",,IEEE Conferences,2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
12,,On Multi-Modal Learning of Editing Source Code,S. Chakraborty; B. Ray,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9678559,10.1109/ASE51524.2021.9678559,"In recent years, Neural Machine Translator (NMT) has shown promise in automatically editing source code. Typical NMT based code editor only considers the code that needs to be changed as input and suggests developers with a ranked list of patched code to choose from - where the correct one may not always be at the top of the list. While NMT based code editing systems generate a broad spectrum of plausible patches, the correct one depends on the developers’ requirement and often on the context where the patch is applied. Thus, if developers provide some hints, using natural language, or providing patch context, NMT models can benefit from them.As a proof of concept, in this research, we leverage three modalities of information: edit location, edit code context, commit messages (as a proxy of developers’ hint in natural language) to automatically generate edits with NMT models. To that end, we build Modit, a multi-modal NMT based code editing engine. With in-depth investigation and analysis, we show that developers’ hint as an input modality can narrow the search space for patches and outperform state-of-the-art models to generate correctly patched code in top-1 position.",Source Code Edit;Neural Networks;Automated Programming;Neural Machine Translator;Pretraining;Transformers,"443, 455",,IEEE Conferences,2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
13,,Deep GUI: Black-box GUI Input Generation with Deep Learning,F. YazdaniBanafsheDaragh; S. Malek,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9678778,10.1109/ASE51524.2021.9678778,"Despite the proliferation of Android testing tools, Google Monkey has remained the de facto standard for practitioners. The popularity of Google Monkey is largely due to the fact that it is a black-box testing tool, making it widely applicable to all types of Android apps, regardless of their underlying implementation details. An important drawback of Google Monkey, however, is the fact that it uses the most naive form of test input generation technique, i.e., random testing. In this work, we present Deep GUI, an approach that aims to complement the benefits of black-box testing with a more intelligent form of GUI input generation. Given only screenshots of apps, Deep GUI first employs deep learning to construct a model of valid GUI interactions. It then uses this model to generate effective inputs for an app under test without the need to probe its implementation details. Moreover, since the data collection, training, and inference processes are performed independent of the platform, the model inferred by Deep GUI has application for testing apps in other platforms as well. We implemented a prototype of Deep GUI in a tool called Monkey++ by extending Google Monkey and evaluated it for its ability to crawl Android apps. We found that Monkey++ achieves significant improvements over Google Monkey in cases where an app’s UI is complex, requiring sophisticated inputs. Furthermore, our experimental results demonstrate the model inferred using Deep GUI can be reused for effective GUI input generation across platforms without the need for retraining.",Android;GUI Testing;Deep Learning,"905, 916",,IEEE Conferences,2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
14,,Unsupervised Labeling and Extraction of Phrase-based Concepts in Vulnerability Descriptions,S. Yitagesu; Z. Xing; X. Zhang; Z. Feng; X. Li; L. Han,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9678638,10.1109/ASE51524.2021.9678638,"People usually describe the key characteristics of software vulnerabilities in natural language mixed with domain-specific names and concepts. This textual nature poses a significant challenge for the automatic analysis of vulnerabilities. Automatic extraction of key vulnerability aspects is highly desirable but demands significant effort to manually label data for model training. In this paper, we propose an unsupervised approach to label and extract important vulnerability concepts in textural vulnerability descriptions (TVDs). We focus on three types of phrase-based vulnerability concepts (root cause, attack vector, and impact) as they are much more difficult to label and extract than name- or number-based entities (i.e., vendor, product, and version). Our approach is based on a key observation that the same-type of phrases, no matter how they differ in sentence structures and phrase expressions, usually share syntactically similar paths in the sentence parsing trees. Therefore, we propose two path representations (absolute paths and relative paths) and use an auto-encoder to encode such syntactic similarities. To address the discrete nature of our paths, we enhance traditional Variational Auto-encoder (VAE) with Gumble-Max trick for categorical data distribution, and thus creates a Categorical VAE (CaVAE). In the latent space of absolute and relative paths, we further use FIt-TSNE and clustering techniques to generate clusters of the same-type of concepts. Our evaluation confirms the effectiveness of our CaVAE for encoding path representations and the accuracy of vulnerability concepts in the resulting clusters. In a concept classification task, our unsupervisedly labeled vulnerability concepts outperform the two manually labeled datasets from previous work.",Textual Vulnerability Descriptions;Vulnerability Concepts;Unsupervised Representation Learning;Concept Labeling and Extraction,"943, 954",,IEEE Conferences,2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
15,,Assessing Robustness of ML-Based Program Analysis Tools using Metamorphic Program Transformations,L. Applis; A. Panichella; A. van Deursen,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9678706,10.1109/ASE51524.2021.9678706,"Metamorphic testing is a well-established testing technique that has been successfully applied in various domains, including testing deep learning models to assess their robustness against data noise or malicious input. Currently, metamorphic testing approaches for machine learning (ML) models focused on image processing and object recognition tasks. Hence, these approaches cannot be applied to ML targeting program analysis tasks. In this paper, we extend metamorphic testing approaches for ML models targeting software programs. We present LAMPION, a novel testing framework that applies (semantics preserving) metamorphic transformations on the test datasets. LAMPION produces new code snippets equivalent to the original test set but different in their identifiers or syntactic structure. We evaluate LAMPION against CodeBERT, a state-of-the-art ML model for Code-To-Text tasks that creates Javadoc summaries for given Java methods. Our results show that simple transformations significantly impact the target model behavior, providing additional information on the models reasoning apart from the classic performance metric.",metamorphic testing;machine learning in software engineering;documentation-generation,"1377, 1381",,IEEE Conferences,2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
16,,Can Neural Clone Detection Generalize to Unseen Functionalitiesƒ,C. Liu; Z. Lin; J. -G. Lou; L. Wen; D. Zhang,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9678907,10.1109/ASE51524.2021.9678907,"Many recently proposed code clone detectors exploit neural networks to capture latent semantics of source code, thus achieving impressive results for detecting semantic clones. These neural clone detectors rely on the availability of large amounts of labeled training data. We identify a key oversight in the current evaluation methodology for neural clone detection: cross-functionality generalization (i.e., detecting semantic clones of which the functionalities are unseen in training). Specifically, we focus on this question: do neural clone detectors truly learn the ability to detect semantic clones, or they just learn how to model specific functionalities in training data while cannot generalize to realistic unseen functionalitiesƒ This paper investigates how the generalizability can be evaluated and improved.Our contributions are 3-folds: (1) We propose an evaluation methodology that can systematically measure the cross-functionality generalizability of neural clone detection. Based on this evaluation methodology, an empirical study is conducted and the results indicate that current neural clone detectors cannot generalize well as expected. (2) We conduct empirical analysis to understand key factors that can impact the generalizability. We investigate 3 factors: training data diversity, vocabulary, and locality. Results show that the performance loss on unseen functionalities can be reduced through addressing the out-of-vocabulary problem and increasing training data diversity. (3) We propose a human-in-the-loop mechanism that help adapt neural clone detectors to new code repositories containing lots of unseen functionalities. It improves annotation efficiency with the combination of transfer learning and active learning. Experimental results show that it reduces the amount of annotations by about 88%. Our code and data are publicly available<sup>1</sup>.",Code Clone Detection;Generalization;Neural Network;Evaluation Methodology;Human-in-the-Loop,"617, 629",,IEEE Conferences,2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
17,,Using Version Control and Issue Tickets to detect Code Debt and Economical Cost,A. Al Maruf; N. Lambaria; A. S. Abdelfattah; T. Cerny,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9678532,10.1109/ASE51524.2021.9678532,"Despite the fact that there are numerous classifications of technical debt based on various criteria, Code Debt or code smells is a category that appears in the majority of current research. One of the primary causes of code debt is the urgency to deliver software quickly, as well as bad coding practices. Among many approaches, static code analysis has received the most attention in studies to detect code-smell/code debt. However, most of them examine the same programming language, although today’s software company utilizes many development stacks with various languages and tools. This problem can be resolved by detecting code debt with Issue/Ticket cards. This paper presents a method for detecting code debt leveraging natural language processing on issue tickets. It also proposes a method for calculating the average amount of time that a code debt was present in the software. This method is implemented utilizing git mining.",Code Debt;Code Smell;Architectural Degradation;Technical Debt;Economical-Cost;Version Control;Issue Ticket,"1208, 1209",,IEEE Conferences,2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
18,,Automating User Notice Generation for Smart Contract Functions,X. Hu; Z. Gao; X. Xia; D. Lo; X. Yang,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9678552,10.1109/ASE51524.2021.9678552,"Smart contracts have obtained much attention and are crucial for automatic financial and business transactions. For end-users who have never seen the source code, they can read the user notice shown in end-user client to understand what a transaction does of a smart contract function. However, due to time constraints or lack of motivation, user notice is often missing during the development of smart contracts. For end-users who lack the information of the user notices, there is no easy way for them to check the code semantics of the smart contracts. Thus, in this paper, we propose a new approach SMARTDOC to generate user notice for smart contract functions automatically. Our tool can help end-users better understand the smart contract and aware of the financial risks, improving the users’ confidence on the reliability of the smart contracts. SMARTDOC exploits the Transformer to learn the representation of source code and generates natural language descriptions from the learned representation. We also integrate the Pointer mechanism to copy words from the input source code instead of generating words during the prediction process. We extract 7,878 〈function, notice〉 pairs from 54,739 smart contracts written in Solidity. Due to the limited amount of collected smart contract functions (i.e., 7,878 functions), we exploit a transfer learning technique to utilize the learned knowledge to improve the performance of SMARTDOC. The learned knowledge obtained by the pre-training on a corpus of Java code, that has similar characteristics as Solidity code. The experimental results show that our approach can effectively generate user notice given the source code and significantly outperform the state-of-the-art approaches. To investigate human perspectives on our generated user notice, we also conduct a human evaluation and ask participants to score user notice generated by different approaches. Results show that SMARTDOC outperforms baselines from three aspects, naturalness, informativeness, and similarity.",Smart Contract;User Notice Generation;Deep Learning,"5, 17",,IEEE Conferences,2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
19,,Finding A Needle in a Haystack: Automated Mining of Silent Vulnerability Fixes,J. Zhou; M. Pacheco; Z. Wan; X. Xia; D. Lo; Y. Wang; A. E. Hassan,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9678720,10.1109/ASE51524.2021.9678720,"Following the coordinated vulnerability disclosure model, a vulnerability in open source software (OSS) is sug-gested to be fixed ""silently"", without disclosing the fix until the vulnerability is disclosed. Yet, it is crucial for OSS users to be aware of vulnerability fixes as early as possible, as once a vulnerability fix is pushed to the source code repository, a malicious party could probe for the corresponding vulnerability to exploit it. In practice, OSS users often rely on the vulnerability disclosure information from security advisories (e.g., National Vulnerability Database) to sense vulnerability fixes. However, the time between the availability of a vulnerability fix and its disclosure can vary from days to months, and in some cases, even years. Due to manpower constraints and the lack of expert knowledge, it is infeasible for OSS users to manually analyze all code changes for vulnerability fix detection. Therefore, it is essential to identify vulnerability fixes automatically and promptly. In a first-of-its-kind study, we propose VulFixMiner, a Transformer-based approach, capable of automatically extracting semantic meaning from commit-level code changes to identify silent vulnerability fixes. We construct our model using sampled commits from 204 projects, and evaluate using the full set of commits from 52 additional projects. The evaluation results show that VulFixMiner outperforms various state-of-the-art baselines in terms of AUC (i.e., 0.81 and 0.73 on Java and Python dataset, respectively) and two effort-aware performance metrics (i.e., EffortCost, P<inf>opt</inf>). Especially, with an effort of inspecting 5% of total LOC, VulFixMiner can identify 49% of total vulnerability fixes. Additionally, with manual verification of sampled commits that were identified as vulnerability fixes, but not marked as such in our dataset, we observe that 35% (29 out of 82) of the commits are for fixing vulnerabilities, indicating VulFixMiner is also capable of identifying unreported vulnerability fixes.",Software Security;Vulnerability Fix;Open Source Software;Deep Learning,"705, 716",,IEEE Conferences,2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
20,,Cross-Lingual Transfer Learning Framework for Program Analysis,Z. Li,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9678848,10.1109/ASE51524.2021.9678848,"Deep learning-based techniques have been widely applied to program analysis tasks, in fields such as type inference, fault localization, and code summarization. Hitherto deep learning-based software engineering systems rely thoroughly on supervised learning approaches, which require laborious manual effort to collect and label a prohibitively large amount of data. However, most Turing-complete imperative languages share similar control- and data-flow structures, which make it possible to transfer knowledge learned from one language to another. In this paper, we propose a general cross-lingual transfer learning framework PLATO for program analysis by using a series of techniques that are general to different downstream tasks. PLATO allows Bert-based models to leverage prior knowledge learned from the labeled dataset of one language and transfer it to the others. We evaluate our approaches on several downstream tasks such as type inference and code summarization to demonstrate its feasibility.",program analysis;transfer learning;domain adaptation;deep learning;graph kernel,"1074, 1078",,IEEE Conferences,2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
21,,A Prediction Model for Software Requirements Change Impact,K. Zamani,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9678582,10.1109/ASE51524.2021.9678582,"Software requirements Change Impact Analysis (CIA) is a pivotal process in requirements engineering (RE) since changes to requirements are inevitable. When a requirement change is requested, its impact on all software artefacts has to be investigated to accept or reject the request. Manually performed CIA in large-scale software development is time-consuming and error-prone so, automating this analysis can improve the process of requirements change management. The main goal of this research is to apply a combination of Machine Learning (ML) and Natural Language Processing (NLP) based approaches to develop a prediction model for forecasting the requirement change impact on other requirements in the specification document. The proposed prediction model will be evaluated using appropriate datasets for accuracy and performance. The resulting tool will support project managers to perform automated change impact analysis and make informed decisions on the acceptance or rejection of requirement change requests.",Change impact analysis;Software requirements change;Machine learning;RE,"1028, 1032",,IEEE Conferences,2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
22,,Thinking Like a Developer? Comparing the Attention of Humans with Neural Models of Code,M. Paltenghi; M. Pradel,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9678712,10.1109/ASE51524.2021.9678712,"Neural models of code are successfully tackling various prediction tasks, complementing and sometimes even outperforming traditional program analyses. While most work focuses on end-to-end evaluations of such models, it often remains unclear what the models actually learn, and to what extent their reasoning about code matches that of skilled humans. A poor understanding of the model reasoning risks deploying models that are right for the wrong reason, and taking decisions based on spurious correlations in the training dataset. This paper investigates to what extent the attention weights of effective neural models match the reasoning of skilled humans. To this end, we present a methodology for recording human attention and use it to gather 1,508 human attention maps from 91 participants, which is the largest such dataset we are aware of. Computing human-model correlations shows that the copy attention of neural models often matches the way humans reason about code (Spearman rank coefficients of 0.49 and 0.47), which gives an empirical justification for the intuition behind copy attention. In contrast, the regular attention of models is mostly uncorrelated with human attention. We find that models and humans sometimes focus on different kinds of tokens, e.g., strings are important to humans but mostly ignored by models. The results also show that human-model agreement positively correlates with accurate predictions by a model, which calls for neural models that even more closely mimic human reasoning. Beyond the insights from our study, we envision the release of our dataset of human attention maps to help understand future neural models of code and to foster work on human-inspired models.",Artificial intelligence for software engineering;Empirical software engineering;Program comprehension;Software analysis,"867, 879",,IEEE Conferences,2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
23,,Automatically Deciding on the Integration of Commits Based on Their Descriptions,S. C. Fonseca; M. C. Lucena; T. M. Reis; P. F. Cabral; W. A. Silva; F. de S. Santos; F. T. Giuntini; J. Sales,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9678906,10.1109/ASE51524.2021.9678906,"Continuous Integration is a critical problem for software maintenance in global projects, compromising companies’ performance, which tends to accumulate a high-resolution time due to the approval process, conflict resolution, tests, and validations. The process of the validation involves the commit description interpretation and can be automated by NLP-mechanisms. This paper presents an intelligent NLP-based approach to evaluate whether the commits can be integrated into a certain software release based only on their descriptions. Our experiments showed an accuracy of 92.9%.",Commit classification;continuous integration evaluation;global industry;natural language processing;software maintenance;software release,"1131, 1135",,IEEE Conferences,2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
24,,Interactive Cross-language Code Retrieval with Auto-Encoders,B. Chen; Z. Abedjan,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9678929,10.1109/ASE51524.2021.9678929,"Cross-language code retrieval is necessary in many real-world scenarios. A major application is program translation, e.g., porting codebases from an obsolete or deprecated language to a modern one or re-implementing existing projects in one’s preferred programming language. Existing approaches based on the translation model require large amounts of training data and extra information or neglects significant characteristics of programs. Leveraging cross-language code retrieval to assist automatic program translation can make use of Big Code. However, existing code retrieval systems have the barrier to finding the translation with only the features of the input program as the query. In this paper, we present BigPT for interactive cross-language retrieval from Big Code only based on raw code and reusing the retrieved code to assist program translation. We build on existing work on cross-language code representation and propose a novel predictive transformation model based on auto-encoders. The model is trained on Big Code to generate a target-language representation, which will be used as the query to retrieve the most relevant translations for a given program. Our query representation enables the user to easily update and correct the returned results to improve the retrieval process. Our experiments show that BigPT outperforms state-of-the-art baselines in terms of program accuracy. Using our novel querying and retrieving mechanism, BigPT can be scaled to the large dataset and efficiently retrieve the translation.",,"167, 178",,IEEE Conferences,2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
25,,Automating Developer Chat Mining,S. Pan; L. Bao; X. Ren; X. Xia; D. Lo; S. Li,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9678923,10.1109/ASE51524.2021.9678923,"Online chatrooms are gaining popularity as a communication channel between widely distributed developers of Open Source Software (OSS) projects. Most discussion threads in chatrooms follow a Q&A format, with some developers (askers) raising an initial question and others (respondents) joining in to provide answers. These discussion threads are embedded with rich information that can satisfy the diverse needs of various OSS stakeholders. However, retrieving information from threads is challenging as it requires a thread-level analysis to understand the context. Moreover, the chat data is transient and unstructured, consisting of entangled informal conversations. In this paper, we address this challenge by identifying the information types available in developer chats and further introducing an automated mining technique. Through manual examination of chat data from three chatrooms on Gitter, using card sorting, we build a thread-level taxonomy with nine information categories and create a labeled dataset with 2,959 threads. We propose a classification approach (named F2CHAT) to structure the vast amount of threads based on the information type automatically, helping stakeholders quickly acquire their desired information. F2CHAT effectively combines handcrafted non-textual features with deep textual features extracted by neural models. Specifically, it has two stages with the first one leveraging the siamese architecture to pretrain the textual feature encoder, and the second one facilitating an in-depth fusion of two types of features. Evaluation results suggest that our approach achieves an average F1-score of 0.628, which improves the baseline by 57%. Experiments also verify the effectiveness of our identified non-textual features under both intra-project and cross-project validations.",Developer Chatrooms;Information Mining;Deep Learning;Gitter,"854, 866",,IEEE Conferences,2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
26,,ISPY: Automatic Issue-Solution Pair Extraction from Community Live Chats,L. Shi; Z. Jiang; Y. Yang; X. Chen; Y. Zhang; F. Mu; H. Jiang; Q. Wang,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9678894,10.1109/ASE51524.2021.9678894,"Collaborative live chats are gaining popularity as a development communication tool. In community live chatting, developers are likely to post issues they encountered (e.g., setup issues and compile issues), and other developers respond with possible solutions. Therefore, community live chats contain rich sets of information for reported issues and their corresponding solutions, which can be quite useful for knowledge sharing and future reuse if extracted and restored in time. However, it remains challenging to accurately mine such knowledge due to the noisy nature of interleaved dialogs in live chat data. In this paper, we first formulate the problem of issue-solution pair extraction from developer live chat data, and propose an automated approach, named ISPY, based on natural language processing and deep learning techniques with customized enhancements, to address the problem. Specifically, ISPY automates three tasks: 1) Disentangle live chat logs, employing a feedforward neural network to disentangle a conversation history into separate dialogs automatically; 2) Detect dialogs discussing issues, using a novel convolutional neural network (CNN), which consists of a BERT-based utterance embedding layer, a context-aware dialog embedding layer, and an output layer; 3) Extract appropriate utterances and combine them as corresponding solutions, based on the same CNN structure but with different feeding inputs. To evaluate ISPY, we compare it with six baselines, utilizing a dataset with 750 dialogs including 171 issue-solution pairs and evaluate ISPY from eight open source communities. The results show that, for issue-detection, our approach achieves the F1 of 76%, and outperforms all baselines by 30%. Our approach achieves the F1 of 63% for solution-extraction and outperforms the baselines by 20%. Furthermore, we apply ISPY on three new communities to extensively evaluate ISPY’s practical usage. Moreover, we publish over 30K issue-solution pairs extracted from 11 communities. We believe that ISPY can facilitate community-based software development by promoting knowledge sharing and shortening the issue-resolving process.",,"142, 154",,IEEE Conferences,2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
27,,FIGCPS: Effective Failure-inducing Input Generation for Cyber-Physical Systems with Deep Reinforcement Learning,S. Zhang; S. Liu; J. Sun; Y. Chen; W. Huang; J. Liu; J. Liu; J. Hao,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9678832,10.1109/ASE51524.2021.9678832,"Cyber-Physical Systems (CPSs) are composed of computational control logic and physical processes, which intertwine with each other. CPSs are widely used in various domains of daily life, including those safety-critical systems and infrastructures, such as medical monitoring, autonomous vehicles, and water treatment systems. It is thus critical to effectively test them. However, it is not easy to obtain test cases which can fail the CPS. In this work, we propose a failure-inducing input generation approach FIGCPS, which requires no knowledge of the CPS under test or any history logs of the CPS which are usually hard to obtain. Our approach adopts deep reinforcement learning techniques to interact with the CPS under test and effectively searches for failure-inducing input guided by rewards. Our approach adaptively collects information from the CPS, which reduces the training time and is also able to explore different states. Moreover, our approach is the first attempt to generate failure-inducing input for CPSs with both continuous action space and high-dimensional discrete action space, which are common for some classes of CPSs. The evaluation results show that FIGCPS not only achieves a higher success rate than the state-of-the-art approaches but also finds two new attacks in a well-tested CPS.",Test Case Generation;CPS;Deep Reinforcement Learning,"555, 567",,IEEE Conferences,2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
28,,Evaluating Semantic Autocompletion of Business Processes with Domain Experts,M. Goldstein; C. González-Álvarez,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9678821,10.1109/ASE51524.2021.9678821,"Process modeling can benefit from automation using knowledge mined from collections of existing processes. One promising technique for such automation is the recommendation of the next elements to be added to the processes under construction. In this paper, we review an autocompletion engine that is based on the semantic similarity of business processes. To assess its efficiency in practical settings, we conduct a user study where domain experts are asked to rate the suggestions made by the engine for a commercial product. Their ratings are then compared to the engine’s accuracy measured by metrics from the natural language processing field. Our study shows a strong correlation between the expert ratings and some of these metrics. We confirm the usefulness of such an autocompletion engine, and enumerate potential improvements to any process autocompletion technique.",,"1116, 1120",,IEEE Conferences,2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
29,,Modeling Team Dynamics for the Characterization and Prediction of Delays in User Stories,E. Kula; A. v. Deursen; G. Gousios,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9678939,10.1109/ASE51524.2021.9678939,"In agile software development, proper team structures and effort estimates are crucial to ensure the on-time delivery of software projects. Delivery performance can vary due to the influence of changes in teams, resulting in team dynamics that remain largely unexplored. In this paper, we explore the effects of various aspects of teamwork on delays in software deliveries. We conducted a case study at ING and analyzed historical log data from 765,200 user stories and 571 teams to identify team factors characterizing delayed user stories. Based on these factors, we built models to predict the likelihood and duration of delays in user stories. The evaluation results show that the use of team-related features leads to a significant improvement in the predictions of delay, achieving on average 74%-82% precision, 78%-86% recall and 76%-84% F-measure. Moreover, our results show that team-related features can help improve the prediction of delay likelihood, while delay duration can be explained exclusively using them. Finally, training on recent user stories using a sliding window setting improves the predictive performance; our predictive models perform significantly better for teams that have been stable. Overall, our results indicate that planning in agile development settings can be significantly improved by incorporating team-related information and incremental learning methods into analysis/predictive models.",effort estimation;team dynamics;agile software development,"991, 1002",,IEEE Conferences,2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
30,,What do pre-trained code models know about code?,A. Karmakar; R. Robbes,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9678927,10.1109/ASE51524.2021.9678927,"Pre-trained models of code built on the transformer architecture have performed well on software engineering (SE) tasks such as predictive code generation, code summarization, among others. However, whether the vector representations from these pre-trained models comprehensively encode characteristics of source code well enough to be applicable to a broad spectrum of downstream tasks remains an open question.One way to investigate this is with diagnostic tasks called probes. In this paper, we construct four probing tasks (probing for surface-level, syntactic, structural, and semantic information) for pre-trained code models. We show how probes can be used to identify whether models are deficient in (understanding) certain code properties, characterize different model layers, and get insight into the model sample-efficiency.We probe four models that vary in their expected knowledge of code properties: BERT (pre-trained on English), CodeBERT and CodeBERTa (pre-trained on source code, and natural language documentation), and GraphCodeBERT (pre-trained on source code with dataflow). While GraphCodeBERT performs more consistently overall, we find that BERT performs surprisingly well on some code tasks, which calls for further investigation.",probing;source code models;transformers;software engineering tasks,"1332, 1336",,IEEE Conferences,2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
31,,Testing Your Question Answering Software via Asking Recursively,S. Chen; S. Jin; X. Xie,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9678670,10.1109/ASE51524.2021.9678670,"Question Answering (QA) is an attractive and challenging area in NLP community. There are diverse algorithms being proposed and various benchmark datasets with different topics and task formats being constructed. QA software has also been widely used in daily human life now. However, current QA software is mainly tested in a reference-based paradigm, in which the expected outputs (labels) of test cases need to be annotated with much human effort before testing. As a result, neither the just-in-time test during usage nor the extensible test on massive unlabeled real-life data is feasible, which keeps the current testing of QA software from being flexible and sufficient. In this paper, we propose a method, qaAskeR, with three novel Metamorphic Relations for testing QA software. qaAskeR does not require the annotated labels but tests QA software by checking its behaviors on multiple recursively asked questions that are related to the same knowledge. Experimental results show that qaAskeR can reveal violations at over 80% of valid cases without using any preannotated labels. Diverse answering issues, especially the limited generalization on question types across datasets, are revealed on a state-of-the-art QA algorithm.",question answering;testing and validation;recursive metamorphic testing;natural language processing,"104, 116",,IEEE Conferences,2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
32,,Log-based Anomaly Detection Without Log Parsing,V. -H. Le; H. Zhang,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9678773,10.1109/ASE51524.2021.9678773,"Software systems often record important runtime information in system logs for troubleshooting purposes. There have been many studies that use log data to construct machine learning models for detecting system anomalies. Through our empirical study, we find that existing log-based anomaly detection approaches are significantly affected by log parsing errors that are introduced by 1) OOV (out-of-vocabulary) words, and 2) semantic misunderstandings. The log parsing errors could cause the loss of important information for anomaly detection. To address the limitations of existing methods, we propose NeuralLog, a novel log-based anomaly detection approach that does not require log parsing. NeuralLog extracts the semantic meaning of raw log messages and represents them as semantic vectors. These representation vectors are then used to detect anomalies through a Transformer-based classification model, which can capture the contextual information from log sequences. Our experimental results show that the proposed approach can effectively understand the semantic meaning of log messages and achieve accurate anomaly detection results. Overall, NeuralLog achieves F1-scores greater than 0.95 on four public datasets, outperforming the existing approaches.",Anomaly Detection;Log Analysis;Log Parsing;Deep Learning,"492, 504",,IEEE Conferences,2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
33,,Cognitive Ledger Project: Towards Building Personal Digital Twins Through Cognitive Blockchain,A. R. Asadi,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9672433,10.1109/IISEC54230.2021.9672433,"The Cognitive Ledger Project is an effort to develop a modular system for turning users' personal data into structured information and machine learning models based on a blockchain-based infrastructure. In this work-in-progress paper, we propose a cognitive architecture for cognitive digital twins. The suggested design embraces a cognitive blockchain (Cognitive ledger) at its core. The architecture includes several modules that turn users' activities in the digital environment into reusable knowledge objects and artificial intelligence that one day can work together to form the cognitive digital twin of users.",Cognitive Computing;Knowledge Management;Artificial Intelligence;Blockchain;Tokenomics,"1, 5",,IEEE Conferences,2021 2nd International Informatics and Software Engineering Conference (IISEC),IEEE
34,,An Improved Transfer Learning-Based Model for Malaria Detection using Blood Smear of Microscopic Cell Images,M. Bilyaminu; A. Varol,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9672447,10.1109/IISEC54230.2021.9672447,"Because of insufficient medical specialists in some parts of the African and Asian continents, malaria patients' mortality rates have increased over the years. Since the people of regions generally suffer from malaria diseases, computer-aided detection (CAD) technology is required to decrease the number of casualties and reduce the waiting time for consulting by a Malaria specialist. This study shows the potential of transfer learning, a method of Deep Learning (DL) to classify the smeared blood of microscopic malaria cell images to determine whether it is parasitized or uninfected. This classification of malaria cell images will enhance the workflow of health practitioners at the frontline, especially microscopists, and provides them with a valuable alternative for malaria detection based on microscopic cell images. Although many technological advancements and evaluation techniques for identifying the infection exist, a microscopist at regions with limited resources faces challenges in improving diagnostic accuracy. We compared and evaluated a type of pre-trained CNN models, such as ResNet-50 and our appended Resnet-50+KNN. The experiment shows that our new model has the excellent capability and can perform better on malarial microscopic cell image classification with a higher accuracy rate of 98%.",malaria;deep learning;transfer learning;resnet-50;microscopic cell images,"1, 5",,IEEE Conferences,2021 2nd International Informatics and Software Engineering Conference (IISEC),IEEE
35,,Disease Diagnosis by Nadi Analysis Using Ayurvedic Methods with Portable Nadi Device & Web Application,A. W. M. H. K. Bandara; N. K. Madanayake; P. K. K. Devaka; P. L. Geethan Madurange; S. Silva; P. Abeygunawardhna,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9672342,10.1109/IISEC54230.2021.9672342,"As the world's population grows, so does the number of people who suffer from asthma, gastritis, and hypotension. In the future, there will be a need for the development of a system with early human health diagnostics. There are various methods to examine a patient's pulse in the medical field today. However, their fundamental concepts, clinical procedures are also evolved from our ayurvedic treatments. The purpose of this study is to develop a minimally invasive device and questionnaire to aid doctors in diagnosing diseases based on traditional Ayurvedic methods. Nadi Pariksha is an old medical technique that dates back to India and China. Ayurveda is the root of this approach. Ayurveda, in addition to examining the pulse, the patient is also diagnosed by asking questions and examining the cough, eyes, and face. Once the disease is diagnosed with these two methods, a prescription is generated for the diagnosed disease and the nearest hospitals and pharmacies. They are displayed to the patient for further convenience.",Nadi Pariksha;Vata;Pitta and Kapha;Machine Learning;Artificial neural network;Asthma;Gastritis;Hypertension,"1, 6",,IEEE Conferences,2021 2nd International Informatics and Software Engineering Conference (IISEC),IEEE
36,,Driving Through a Bend: Detection of Unsafe Driving Patterns and Prevention of Heavy Good Vehicle Rollovers,E. M. A. K. Siriwardana; S. K. D. Amila; S. G. L. D. H. Kaushalya; S. S. Chandrasiri; V. S. Piyawardana,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9672345,10.1109/IISEC54230.2021.9672345,"Road Traffic Crashes are simply ordinary within the present world. However, heavy goods vehicles (HGV) rollover has become a significant problem worldwide. Depending on the data collected, the sources used, and several key factors contribute to HGV overturning. Accidents overturn due to longer reaction time, shriveled driving performance, lack of driving experience, and driver carelessness. In further consideration, over-steering to turning over, not steering enough to stay in lane, over speed, high located center of gravity, weather condition, road condition, and the road's curves are the most contributing reasons to the overturning of a long vehicle. Thus, this paper proposes machine learning processes to overcome these problems and reduce the HGV rollovers. The proposed system includes a vehicle-equipped system and a ground-based operational surveillance camera. The Vehicle-equipped system can determine the safe speed at which the vehicle should travel according to the type of vehicle and curvature of the road and can detect road cracks and notify the driver by sending the notification to the vehicle dashboard screen. The ground-based driver support system can detect safe speed for HGVs and determine various other traffic parameters which can affect the HGV rollover accidents.",Artificial intelligence;Object detection;Machine learning;Road cracks detection;Vehicle-equipped system;Ground-based driver support system,"1, 6",,IEEE Conferences,2021 2nd International Informatics and Software Engineering Conference (IISEC),IEEE
37,,Application of Deep Convolution Neural Network in Breast Cancer Prediction using Digital Mammograms,R. A. Mamun; G. A. Rafin; A. Alam; M. A. I. Sefat,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9672368,10.1109/IISEC54230.2021.9672368,"Cancer, a diagnosis so dreaded and scary, that its fear alone can strike even the strongest of souls. The disease is often thought of as untreatable and unbearably painful, with usually, no cure available. Among all the cancers, breast cancer is the second most deadliest, especially among women. What decides the patients' fate is the early diagnosis of the cancer, facilitating subsequent clinical management. Mammography plays a vital role in the screening of breast cancers as it can detect any breast masses or calcifications early. However, the extremely dense breast tissues pose difficulty in the detection of cancer mass, thus, encouraging the use of machine learning (ML) techniques and artificial neural networks (ANN) to assist radiologists in faster cancer diagnosis. This paper explores the MIAS database, containing 332 digital mammograms from women, which were augmented and preprocessed, and fed into a custom and different pre-trained convolutional neural network (CNN) models, with the aim of differentiating healthy tissues from cancerous ones with high accuracy. Although the pre-trained CNN models produced splendid results, the custom CNN model came out on top, achieving test accuracy, AUC, precision, recall and <tex>$\mathbf{F}_{1}$</tex> scores of 0.9362, 0.9407, 0.9200, 0.8025 and 0.8572 respectively while having minimal to no overfitting. The paper, along with proposing a new custom CNN model for better breast cancer classification using raw mammograms, focuses on the significance of computer-aided detection (CAD) models overall in the early diagnosis of breast cancer. While a diagnosis of breast cancer may still leave patients dreaded, we believe our research can be a symbol of hope for all.",breast cancer;CAD model;convolutional neural network;mammogram;MIAS database,"1, 7",,IEEE Conferences,2021 2nd International Informatics and Software Engineering Conference (IISEC),IEEE
38,,Improving Impact and Dependency Analysis through Software Categorization Methods,E. Tanjong; D. Carver,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9653408,10.1109/CONISOFT52520.2021.00029,"Software requirements specifications serve as instructions for any software development engagement. These instructions are mostly written in natural language for ease of manual analysis and comprehension. Since natural language is inherently ambiguous, software requirements analysis plays a pivotal role in enhancing clarity during the software development life cycle. There are several methods of software requirements analysis. We focus on analysis methods which categorize requirements. We present a comparison of the performance of three common categorization techniques of software requirements documents, using three different datasets. We evaluate three bag of words models: count vectorization, term frequency - inverse document frequency (TF-IDF), and a word embeddings technique. We report the similarity of the categories obtained using cosine similarity as a measure of similarity between the requirements vectors produced by the different methods. Syntactic techniques outperformed semantic techniques for some datasets. These results suggest that syntactic techniques produce comparable categories to semantic techniques for some requirements categorization tasks.",software;requirements;categorization,"142, 151",,IEEE Conferences,2021 9th International Conference in Software Engineering Research and Innovation (CONISOFT),IEEE
39,,Geolocation of Tweets in Spanish with Transformer Encoders,A. -D. Ambrosio-Aguilar; E. Bárcenas; G. Molero-Castillo; R. Aldeco-Pérez,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9653208,10.1109/CONISOFT52520.2021.00038,"Tweet geolocation is very important in many contexts: disaster relief, opinion polling, recommendation systems, etc. There are some recent studies showing that tweets with geolocation tags are sparse in several settings. Current state of the art geolocation algorithms for tweets are based on natural language processing methods. Most of these algorithms have been tested in English.Transformers are machine learning models based on attention mechanisms. These models have been proven successful in many natural language processing and computer vision scenarios. In this paper, we propose a transformer model for tweet geolocation. We describe several experiments for tweets in Spanish located in the Mexican region.",Geolocation;Transformer Encoders;Social Media;Tweets in Spanish,"227, 231",,IEEE Conferences,2021 9th International Conference in Software Engineering Research and Innovation (CONISOFT),IEEE
40,,"QAOAKit: A Toolkit for Reproducible Study, Application, and Verification of the QAOA",R. Shaydulin; K. Marwaha; J. Wurtz; P. C. Lotshaw,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9651381,10.1109/QCS54837.2021.00011,"Understanding the best known parameters, performance, and systematic behavior of the Quantum Approximate Optimization Algorithm (QAOA) remain open research questions, even as the algorithm gains popularity. We introduce QAOAKit, a Python toolkit for the QAOA built for exploratory research. QAOAKit is a unified repository of preoptimized QAOA parameters and circuit generators for common quantum simulation frameworks. We combine, standardize, and cross-validate previously known parameters for the MaxCut problem, and incorporate this into QAOAKit. We also build conversion tools to use these parameters as inputs in several quantum simulation frameworks that can be used to reproduce, compare, and extend known results from various sources in the literature. We describe QAOAKit and provide examples of how it can be used to reproduce research results and tackle open problems in quantum optimization.",quantum approximate optimization algorithm;open quantum software,"64, 71",,IEEE Conferences,2021 IEEE/ACM Second International Workshop on Quantum Computing Software (QCS),IEEE
41,,Inter-personal Relation Extraction Model based on Dependency Parsing and Bidirectional Gating Recurrent Unit,B. Jin; S. Shang; M. Qin; Z. Li,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9626428,10.1109/ISSSR53171.2021.00022,"Relationship extraction is a fundamental component of various information extraction systems. Traditional relationship extraction methods are mainly rule-based methods and machine learning methods. Rule-based methods require induction and analysis of the corpus, followed by extraction of relationship extraction rules and finally pattern matching. The machine learning approach requires a large amount of manually annotated train data and manual extraction of features. However, these methods require a lot of statics and higher time costs. Considering these issues in the traditional relationship extraction methods and the linguistic characteristics of Chinese text, this paper proposes a new deep neural network structure. Firstly, the dependency relationships between sentence components are analyzed by using dependency parsing, which reveals the syntactic structure of the sentence and enhance the potential semantic information. Secondly, the important semantic information in the sentences is captured by using the sentence-level attention mechanism. Finally, the Bidirectional Gating Recurrent Unit model is used to simultaneously capture the contextual information of the text, and to improve the performance of relation extraction. The experimental results show that the model proposed in this paper is more effective than existing methods.",component;relationship extraction;dependency parsing;attention mechanism;deep neural network structure,"119, 125",,IEEE Conferences,2021 7th International Symposium on System and Software Reliability (ISSSR),IEEE
42,,Chinese Named Entity Recognition based on BERT-Transformer-BiLSTM-CRF Model,Y. Gan; R. Yang; C. Zhang; D. Jia,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9626407,10.1109/ISSSR53171.2021.00029,"Among many named entity recognition modes in natural languages, most of the processing in the text preprocessing stage only pays attention to the vector representation of single words and characters, and seldom pays attention to the semantic relationship in the text. In the language text information, there are many pronouns and polysemous words, which makes the problem of polysemous words appear in the text preprocessing stage. Based on this problem, this paper adopts a Chinese named entity recognition method based on the BERT-Transformer-BiLSTM-CRF model. First, use the pre-trained BERT model in a large-scale corpus to dynamically generate a sequence of word vectors according to its input context, then use the Transformer encoder to model the contextual long-distance semantic features of the text, and use the BiLSTM model to perform sentence context features Extract, and finally input the feature vector sequence into CRF (Conditional Random Field) to get the final prediction result. Tested on the public MSRA Chinese corpus. Experimental results on the corpus show that the model has improved accuracy, recall and F1 value than most models.",named entity recognition;BERT;Transformer;BiLSTM;CRF,"109, 118",,IEEE Conferences,2021 7th International Symposium on System and Software Reliability (ISSSR),IEEE
43,,Towards Understanding Developers’ Machine-Learning Challenges: A Multi-Language Study on Stack Overflow,A. Hamidi; G. Antoniol; F. Khomh; M. D. Penta; M. Hamidi,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9610631,10.1109/SCAM52516.2021.00016,"Machine Learning (ML) is increasingly being used as an essential component of modern software systems. Also, the maturity of the adopted techniques and the availability of frameworks have changed the way developers approach ML-related development problems. This paper aims at investigating, by analyzing Stack Overflow (SO) posts related to ML, how the questions about ML have been changing over the years, and across six different programming languages. We analyzed 43,950 SO posts in the period 2008-2020, studying (i) how the number of ML-related posts changes over time for each programming language, (ii) how the posts are distributed across different phases of a ML pipeline, and (iii) whether posts belonging to different languages or phases are more or less challenging to address. We found that some programming languages are fading while others are becoming more popular in ML development. While model-building questions are the most discussed in general, the level of challenges posed by the other phases of the ML pipeline appears to be language-dependent. Results of this work could be used to better understand ML challenges in different programming languages, and, possibly, to improve ML tutorials related to different languages.",stack overflow;machine learning;machine learning Life cycle;programming language,"58, 69",,IEEE Conferences,2021 IEEE 21st International Working Conference on Source Code Analysis and Manipulation (SCAM),IEEE
44,,Leveraging Unsupervised Learning to Summarize APIs Discussed in Stack Overflow,A. Naghshzan; L. Guerrouj; O. Baysal,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9610724,10.1109/SCAM52516.2021.00026,"Automated source code summarization is a task that generates summarized information about the purpose, usage, and–or implementation of methods and classes to support understanding of these code entities. Multiple approaches and techniques have been proposed for supervised and unsupervised learning in code summarization, however, they were mostly focused on generating a summary for a piece of code. In addition, very few works have leveraged unofficial documentation.This paper proposes an automatic and novel approach for summarizing Android API methods discussed in Stack Overflow that we consider as unofficial documentation in this research. Our approach takes the API method’s name as an input and generates a natural language summary based on Stack Overflow discussions of that API method. We have conducted a survey that involves 16 Android developers to evaluate the quality of our automatically generated summaries and compare them with the official Android documentation.Our results demonstrate that while developers find the official documentation more useful in general, the generated summaries are also competitive, in particular for offering implementation details, and can be used as a complementary source for guiding developers in software development and maintenance tasks.",code summarization;unsupervised learning;unofficial documentation;survey;professional developers,"142, 152",,IEEE Conferences,2021 IEEE 21st International Working Conference on Source Code Analysis and Manipulation (SCAM),IEEE
45,,NLP-assisted Web Element Identification Toward Script-free Testing,H. Kirinuki; S. Matsumoto; Y. Higo; S. Kusumoto,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9609160,10.1109/ICSME52107.2021.00072,"End-to-end test automation is important in modern web application development. However, existing test automation techniques have challenges in implementing and maintaining test scripts. It is difficult to keep correct locators, which test scripts require to identify web elements on web pages. The reason is that locators depend on the metadata in web elements or the structure of each web page. One efficient way to solve the problem of locators is to make test cases written in natural language executable without test scripts. As the first step of script-free testing, we propose a technique to identify web elements to be operated and to determine test procedures by interpreting test cases. The test cases are written in a domain-specific language without relying on the metadata of web elements or the structural information of web pages. We leverage natural language processing techniques to understand the semantics of web elements. We also create heuristic search algorithms to find promising test procedures. To evaluate our proposed technique, we applied it to two open-source web applications. The experimental results show that our technique successfully identified 94% of web elements to be operated in the test cases.",Script-free Testing;Web Testing;Locator,"639, 643",,IEEE Conferences,2021 IEEE International Conference on Software Maintenance and Evolution (ICSME),IEEE
46,,Duplicate Bug Report Detection by Using Sentence Embedding and Fine-tuning,H. Isotani; H. Washizaki; Y. Fukazawa; T. Nomoto; S. Ouji; S. Saito,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9609125,10.1109/ICSME52107.2021.00054,"Industrial software maintenance devotes much time and effort to find duplicate bug reports. In this paper, we propose an automated duplicate bug report detection system to improve software maintenance efficiency. Our system detects duplicate reports by vectorizing the contents of each report item by deep-learning-based sentence embedding and calculating the similarity of the whole report from those of the item vectors. The Sentence-BERT fine-tuned with report texts is used for sentence embedding. Finally, we verify that the combination of processing separately by item and Sentence-BERT fine-tuned with reports effectively detects duplicate bug reports in industrial experiments that compare the performance of existing methods.",Bug reports;duplicate detection;BERT;sentence embedding;natural language processing;information retrieval,"535, 544",,IEEE Conferences,2021 IEEE International Conference on Software Maintenance and Evolution (ICSME),IEEE
47,,Toward Less Hidden Cost of Code Completion with Acceptance and Ranking Models,J. Li; R. Huang; W. Li; K. Yao; W. Tan,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9609202,10.1109/ICSME52107.2021.00024,"Code completion is widely used by software developers to provide coding suggestions given a partially written code snippet. Apart from the traditional code completion methods, which only support single token completion at minimal positions, recent studies show the ability to provide longer code completion at more flexible positions. However, such frequently triggered and longer completion results reduce the overall precision as they generate more invalid results. Moreover, different studies are mostly incompatible with each other. Thus, it is vital to develop an ensemble framework that can combine results from multiple models to draw merits and offset defects of each model. This paper conducts a coding simulation to collect data from code context and different code completion models and then apply the data in two tasks. First, we introduce an acceptance model which can dynamically control whether to display completion results to the developer. It uses simulation features to predict whether correct results exist in the output of these models. Our best model reduces the percentage of false-positive completion from 55.09% to 17.44%. Second, we design a fusion ranking scheme that can automatically identify the priority of the completion results and reorder the candidates from multiple code completion models. This scheme is flexible in dealing with various models, regardless of the type or the length of their completion results. We integrate this ranking scheme with two frequency models and a GPT-2 styled language model, along with the acceptance model to yield 27.80% and 37.64% increase in TOP1 and TOP5 accuracy, respectively. In addition, we propose a new code completion evaluation metric, Benefit-Cost Ratio(BCR), taking into account the benefit of keystrokes saving and hidden cost of completion list browsing, which is closer to real coder experience scenario.",Code completion;neural networks;acceptance model;ranking model;evaluation metrics,"195, 205",,IEEE Conferences,2021 IEEE International Conference on Software Maintenance and Evolution (ICSME),IEEE
48,,Multimodal Representation for Neural Code Search,J. Gu; Z. Chen; M. Monperrus,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9609178,10.1109/ICSME52107.2021.00049,"Semantic code search is about finding semantically relevant code snippets for a given natural language query. In the state-of-the-art approaches, the semantic similarity between code and query is quantified as the distance of their representation in the shared vector space. In this paper, to improve the vector space, we introduce tree-serialization methods on a simplified form of AST and build the multimodal representation for the code data. We conduct extensive experiments using a single corpus that is large-scale and multi-language: CodeSearchNet. Our results show that both our tree-serialized representations and multimodal learning model improve the performance of code search. Last, we define intuitive quantification metrics oriented to the completeness of semantic and syntactic information of the code data, to help understand the experimental findings.",multimodal learning;program representation;information completeness;tree serialization;code search,"483, 494",,IEEE Conferences,2021 IEEE International Conference on Software Maintenance and Evolution (ICSME),IEEE
49,,An Empirical Study on Code Comment Completion,A. Mastropaolo; E. Aghajani; L. Pascarella; G. Bavota,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9609149,10.1109/ICSME52107.2021.00021,"Code comments play a prominent role in program comprehension activities. However, source code is not always documented and code and comments not always co-evolve. To deal with these issues, researchers have proposed techniques to automatically generate comments documenting a given code at hand. The most recent works in the area applied deep learning (DL) techniques to support such a task. Despite the achieved advances, the empirical evaluations of these approaches show that they are still far from a performance level that would make them valuable for developers. We tackle a simpler and related problem: Code comment completion. Instead of generating a comment for a given code from scratch, we investigate the extent to which state-of-the-art techniques can help developers in writing comments faster. We present a large-scale study in which we empirically assess how a simple n-gram model and the recently proposed Text-To-Text Transfer Transformer (T5) architecture can perform in autocompleting a code comment the developer is typing. The achieved results show the superiority of the T5 model, despite the n-gram model being a competitive solution.",Empirical Study;Code Comments,"159, 170",,IEEE Conferences,2021 IEEE International Conference on Software Maintenance and Evolution (ICSME),IEEE
50,,Design Smells in Deep Learning Programs: An Empirical Study,A. Nikanjam; F. Khomh,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9609188,10.1109/ICSME52107.2021.00036,"Nowadays, we are witnessing an increasing adoption of Deep Learning (DL) based software systems in many industries. Designing a DL program requires constructing a deep neural network (DNN) and then training it on a dataset. This process requires that developers make multiple architectural (e.g., type, size, number, and order of layers) and configuration (e.g., optimizer, regularization methods, and activation functions) choices that affect the quality of the DL models, and consequently software quality. An under-specified or poorly-designed DL model may train successfully but is likely to perform poorly when deployed in production. Design smells in DL programs are poor design and-or configuration decisions taken during the development of DL components, that are likely to have a negative impact on the performance (i.e., prediction accuracy) and then quality of DL based software systems. In this paper, we present a catalogue of 8 design smells for a popular DL architecture, namely deep Feedforward Neural Networks which is widely employed in industrial applications. The design smells were identified through a review of the existing literature on DL design and a manual inspection of 659 DL programs with performance issues and design inefficiencies. The smells are specified by describing their context, consequences, and recommended refactorings. To provide empirical evidence on the relevance and perceived impact of the proposed design smells, we conducted a survey with 81 DL developers. In general, the developers perceived the proposed design smells as reflective of design or implementation problems, with agreement levels varying between 47% and 68%.",Design smells;Deep Learning;Software Quality,"332, 342",,IEEE Conferences,2021 IEEE International Conference on Software Maintenance and Evolution (ICSME),IEEE
51,,Assessing Generalizability of CodeBERT,X. Zhou; D. Han; D. Lo,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9609166,10.1109/ICSME52107.2021.00044,"Pre-trained models like BERT have achieved strong improvements on many natural language processing (NLP) tasks, showing their great generalizability. The success of pre-trained models in NLP inspires pre-trained models for programming language. Recently, CodeBERT, a model for both natural language (NL) and programming language (PL), pre-trained on code search dataset, is proposed. Although promising, CodeBERT has not been evaluated beyond its pre-trained dataset for NL-PL tasks. Also, it has only been shown effective on two tasks that are close in nature to its pre-trained data. This raises two questions: Can CodeBERT generalize beyond its pre-trained data? Can it generalize to various software engineering tasks involving NL and PL? Our work answers these questions by performing an empirical investigation into the generalizability of CodeBERT. First, we assess the generalizability of CodeBERT to datasets other than its pre-training data. Specifically, considering the code search task, we conduct experiments on another dataset containing Python code snippets and their corresponding documentation. We also consider yet another dataset of questions and answers collected from Stack Overflow about Python programming. Second, to assess the generalizability of CodeBERT to various software engineering tasks, we apply CodeBERT to the just-in-time defect prediction task. Our empirical results support the generalizability of CodeBERT on the additional data and task. CodeBERT-based solutions can achieve higher or comparable performance than specialized solutions designed for the code search and just-in-time defect prediction tasks. However, the superior performance of the CodeBERT requires a tradeoff; for example, it requires much more computation resources as compared to specialized code search approaches.",pre-trained model;generalizability;CodeBERT,"425, 436",,IEEE Conferences,2021 IEEE International Conference on Software Maintenance and Evolution (ICSME),IEEE
52,,SmartGift: Learning to Generate Practical Inputs for Testing Smart Contracts,T. Zhou; K. Liu; L. Li; Z. Liu; J. Klein; T. F. Bissyandé,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9609227,10.1109/ICSME52107.2021.00009,"With the boom of Initial Coin Offerings (ICO) in the financial markets, smart contracts have gained rapid popularity among consumers. Smart contract vulnerabilities however made them a prime target to malicious attacks that are leading to huge losses. The research community is thus applying various software engineering technologies to smart contracts to address them. In general, to detect vulnerabilities in smart contracts, mutation and fuzz based testing approaches have been widely studied and indeed achieved promising performance on benchmark datasets. Generating test inputs with mutation approaches essentially relies on the available test cases in a smart contract program. In our preliminary study, however, we observed that 56.4% of 218 identified open-source smart contract project repositories do not provide any test case for validation. Fuzzing test inputs leads to random values and lacks practical usefulness. Our work addresses this problem: we propose an approach, Smartgift, which generates practical inputs for testing smart contracts by learning from the transaction records of real-world smart contracts. Leveraging a collected set of over 60 thousand transaction records, Smartgift is able to generate relevant test inputs for ~77% smart contract functions, largely outperforming the traditional fuzzing approach (successful for only 60% functions). We further demonstrate the practicality of the test inputs by using them to replace the test inputs of the ContractFuzzer state of the art smart contract vulnerability detector: with inputs by Smartgift, ContractFuzzer can now detect 131 of the 154 vulnerabilities in its benchmark.",Test Input Generation;Smart Contract;Deep Learning,"23, 34",,IEEE Conferences,2021 IEEE International Conference on Software Maintenance and Evolution (ICSME),IEEE
53,,Can Differential Testing Improve Automatic Speech Recognition Systems?,M. H. Asyrofi; Z. Yang; J. Shi; C. W. Quan; D. Lo,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9609154,10.1109/ICSME52107.2021.00079,"Due to the widespread adoption of Automatic Speech Recognition (ASR) systems in many critical domains, ensuring the quality of recognized transcriptions is of great importance. A recent work, CrossASR++, can automatically uncover many failures in ASR systems by taking advantage of the differential testing technique. It employs a Text-To-Speech (TTS) system to synthesize audios from texts and then reveals failed test cases by feeding them to multiple ASR systems for cross-referencing. However, no prior work tries to utilize the generated test cases to enhance the quality of ASR systems. In this paper, we explore the subsequent improvements brought by leveraging these test cases from two aspects, which we collectively refer to as a novel idea, evolutionary differential testing. On the one hand, we fine-tune a target ASR system on the corresponding test cases generated for it. On the other hand, we fine-tune a cross-referenced ASR system inside CrossASR++, with the hope to boost CrossASR++'s performance in uncovering more failed test cases. Our experiment results empirically show that the above methods to leverage the test cases can substantially improve both the target ASR system and CrossASR++ itself. After fine-tuning, the number of failed test cases uncovered decreases by 25.81% and the word error rate of the improved target ASR system drops by 45.81%. Moreover, by evolving just one cross-referenced ASR system, CrossASR++ can find 5.70%, 7.25%, 3.93%, and 1.52% more failed test cases for 4 target ASR systems, respectively.",Automatic Speech Recognition;Test Case Generation;Differential Testing,"674, 678",,IEEE Conferences,2021 IEEE International Conference on Software Maintenance and Evolution (ICSME),IEEE
54,,Improving Traceability Link Recovery Using Fine-grained Requirements-to-Code Relations,T. Hey; F. Chen; S. Weigelt; W. F. Tichy,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9609109,10.1109/ICSME52107.2021.00008,"Traceability information is a fundamental prerequisite for many essential software maintenance and evolution tasks, such as change impact and software reusability analyses. However, manually generating traceability information is costly and error-prone. Therefore, researchers have developed automated approaches that utilize textual similarities between artifacts to establish trace links. These approaches tend to achieve low precision at reasonable recall levels, as they are not able to bridge the semantic gap between high-level natural language requirements and code. We propose to overcome this limitation by leveraging fine-grained, method and sentence level, similarities between the artifacts for traceability link recovery. Our approach uses word embeddings and a Word Mover's Distance-based similarity to bridge the semantic gap. The fine-grained similarities are aggregated according to the artifacts structure and participate in a majority vote to retrieve coarse-grained, requirement-to-class, trace links. In a comprehensive empirical evaluation, we show that our approach is able to outperform state-of-the-art unsupervised traceability link recovery approaches. Additionally, we illustrate the benefits of fine-grained structural analyses to word embedding-based trace link generation.",Traceability;Traceability Link Recovery;Requirements Engineering;Word Embeddings;Natural Language Processing;Word Movers Distance,"12, 22",,IEEE Conferences,2021 IEEE International Conference on Software Maintenance and Evolution (ICSME),IEEE
55,,Serial vs. Parallel Turbo-Autoencoders and Accelerated Training for Learned Channel Codes,J. Clausius; S. Dörner; S. Cammerer; S. t. Brink,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9594130,10.1109/ISTC49272.2021.9594130,"Attracted by its scalability towards practical code-word lengths, we revisit the idea of Turbo-autoencoders for end-to-end learning of PHY-Layer communications. For this, we study the existing concepts of Turbo-autoencoders from the literature and compare the concept with state-of-the-art classical coding schemes. We propose a new component-wise training algorithm based on the idea of Gaussian a priori distributions that reduces the overall training time by almost a magnitude. Further, we propose a new serial architecture inspired by classical serially concatenated Turbo code structures and show that a carefully optimized interface between the two component autoencoders is required. To the best of our knowledge, these serial Turbo autoencoder structures are the best known neural network based learned sequences that can be trained from scratch without any required expert knowledge in the domain of channel codes.",,"1, 5",,IEEE Conferences,2021 11th International Symposium on Topics in Coding (ISTC),IEEE
56,,CSI-aided Robust Neural-based Decoders,M. Benammar; E. D. C. Gomes; P. Piantanida,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9594117,10.1109/ISTC49272.2021.9594117,"In this work <sup>1</sup>, we investigate the design of neural based channel decoders for the Binary Asymmetric Channel (BAC), which exhibits robustness issues related to training/testing channel parameters mismatch. Rather than enforcing the independence of the trained model to the channel parameter as in our previous work, we show that providing even a coarse (possibly imperfect) quantized CSI to the decoder, allows to build a single robust neural decoder for all values of channel parameters.",,"1, 5",,IEEE Conferences,2021 11th International Symposium on Topics in Coding (ISTC),IEEE
57,,A Structured Analysis of the Video Degradation Effects on the Performance of a Machine Learning-enabled Pedestrian Detector,C. Berger,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9582604,10.1109/SEAA53835.2021.00053,"Machine Learning (ML)-enabled software systems have been incorporated in many public demonstrations for automated driving (AD) systems. Such solutions have also been considered as a crucial approach to aim at SAE Level 5 systems, where the passengers in such vehicles do not have to interact with the system at all anymore. Already in 2016, Nvidia demonstrated a complete end-to-end approach for training the complete software stack covering perception, planning and decision making, and the actual vehicle control. While such approaches show the great potential of such ML-enabled systems, there have also been demonstrations where already changes to single pixels in a video frame can potentially lead to completely different decisions with dangerous consequences in the worst case. In this paper, a structured analysis has been conducted to explore video degradation effects on the performance of an ML-enabled pedestrian detector. Firstly, a baseline of applying “You only look once” (YOLO) to 1,026 frames with pedestrian annotations in the KITTI Vision Benchmark Suite has been established. Next, video degradation candidates for each of these frames were generated using the leading video compression codecs libx264, libx265, Nvidia HEVC, and AV1: 52 frames for the various compression presets for color frames, and 52 frames for gray-scale frames resulting in 104 degradation candidates per original KITTI frame and in 426,816 images in total. YOLO was applied to each image to compute the intersection-over-union (IoU) metric to compare the performance with the original baseline. While aggressively lossy compression settings result in significant performance drops as expected, it was also observed that some configurations actually result in slightly better IoU results compared to the baseline. Hence, while related work in literature demonstrated the potentially negative consequences of even simple modifications to video data when using ML-enabled systems, the findings from this work show that carefully chosen lossy video configurations preserve a decent performance of particular ML-enabled systems while allowing for substantial savings when storing or transmitting data. Such aspects are of crucial importance when, for example, video data needs to be collected from multiple vehicles wirelessly, where lossy video codecs are required to cope with bandwidth limitations for example.",video;machine learning;performance;degradation,"357, 362",,IEEE Conferences,2021 47th Euromicro Conference on Software Engineering and Advanced Applications (SEAA),IEEE
58,,NLP4IP: Natural Language Processing-based Recommendation Approach for Issues Prioritization,S. Shafiq; A. Mashkoor; C. Mayr-Dorn; A. Egyed,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9582582,10.1109/SEAA53835.2021.00022,"This paper proposes a recommendation approach for issues (e.g., a story, a bug, or a task) prioritization based on natural language processing, called NLP4IP. The proposed semi-automatic approach takes into account the priority and story points attributes of existing issues defined by the project stakeholders and devises a recommendation model capable of dynamically predicting the rank of newly added or modified issues. NLP4IP was evaluated on 19 projects from 6 repositories employing the JIRA issue tracking software with a total of 29,698 issues. A comprehensive benchmark study was also conducted to compare the performance of various machine learning models. The results of the study showed an average top@3 accuracy of 81% and a mean squared error of 2.2 when evaluated on the validation set. The applicability of the proposed approach is demonstrated in the form of a JIRA plug-in illustrating predictions made by the newly developed machine learning model. The dataset has also been made publicly available in order to support other researchers working in this domain.",Agile software development;natural language processing;issues prioritization,"99, 108",,IEEE Conferences,2021 47th Euromicro Conference on Software Engineering and Advanced Applications (SEAA),IEEE
59,,Self-adaptive K8S Cloud Controller for Time-sensitive Applications,L. Bulej; T. Bureš; P. Hnětynka; D. Khalyeyev,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9582587,10.1109/SEAA53835.2021.00029,"The paper presents a self-adaptive Kubernetes cloud controller for scheduling time-sensitive applications. The controller allows services to specify timing requirements (response time or throughput) and schedules services on shared cloud resources so as to meet the requirements. The controller builds and continuously updates an internal performance model of each service and uses it to determine the kind of resources needed by a service, as well as predict potential contention on shared resources, and (re-)deploys services accordingly. The controller is integrated with our highly-customizable data processing and visualization platform IVIS, which provides a web-based front-end for service deployment and visualization of results. The controller implementation is open-source and is intended to provide an easy-to-use testbed for experiments focusing on various aspects of adaptive scheduling and deployment in the cloud.",Self-adaptation;cloud;QoS;Kubernetes;visualizations,"166, 169",,IEEE Conferences,2021 47th Euromicro Conference on Software Engineering and Advanced Applications (SEAA),IEEE
60,,Head Movements of 3D Virtual Head in HMI Systems using Rigid Elements,M. Kocoń,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9559088,10.23919/SoftCOM52868.2021.9559088,"This paper focuses on rigid head motion synthesis for a virtual person in human–machine interaction systems. Head gestures transmit additional, nonverbal information in human communication and can be used to increase the emotional context of the facial animation authentically simulating human behaviour. The presented method includes the head motion of the expressive face. The proposed motion model was described using Lie algebra for four rigid objects, which reflect the character of the neck and head skeleton movement and jaw bone. The obtained dependencies were used to generate three-dimensional motion of the head model, which was made considering the anatomical structure of the human head. The description presented in this article has the main advantage that it can be extended with additionally rigid elements. It means that we can get a description of the movement of the entire human skeleton, which reflects human anatomy. The proposed motion description eliminates the requirement to use motion capture whenever we need avatar movement in a particular situation, and we use it only for measurement data acquisition.",head motion;the range of motion;motion model;Lie algebra elements;virtual human;human–machine interaction,"1, 6s",,IEEE Conferences,"2021 International Conference on Software, Telecommunications and Computer Networks (SoftCOM)",IEEE
61,,Multimodal Segmentation Neural Network to Determine the Cause of Damage to Grasslands,M. Johenneken; A. Drak; R. Herpers; A. Asteroth,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9559072,10.23919/SoftCOM52868.2021.9559072,"Agricultural fields suffer from damage due to changing climate conditions and wildlife foraging. Damage incurred by wild boar is classified as the main contributing factor to grasslands damage. Such damage results in losses for farmers due to reduced yield potential and repair costs. The extent of wild boar damage is typically estimated manually by using ground based approaches (e.g. GPS measurements), which is a time consuming task with questionable accuracy. Building upon our previous work, we present an autonomous approach to detect and classify the cause of damage to grasslands (wild boar, mole etc.). The approach entails utilizing convolutional neural networks for semantic segmentation of grasslands. An RGB baseline was established, in addition to evaluating multimodal architectures that incorporate different surface model feature representations, leading to a joint representation of spectral and elevation information. Testing and experimentation was performed in real-world grasslands around Bonn, Germany. The results show that incorporating elevation features with late fusion enhances the overall performance of the network over the RGB baselines.",convolutional neural networks;grasslands;remote sensing;unmanned aerial vehicles;semantic segmentation;multimodal,"1, 6",,IEEE Conferences,"2021 International Conference on Software, Telecommunications and Computer Networks (SoftCOM)",IEEE
62,,A Study on Text Classification for Applications in Special Education,L. C. Karathanasi; C. Bazinas; G. Iordanou; V. G. Kaburlasos,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9559128,10.23919/SoftCOM52868.2021.9559128,"Text classification, or text categorization, is the process that categorizes text into organized groups. Text classifiers can parse text and categorize it based on its content. The text analysis is done using Natural Language Processing (NLP). In recent years we have much more understanding to learning disabilities and how to diagnose and categorize their impact. In this paper we studied learning difficulties as a classification problem and use the NLP tool to solve it. In addition, the results of classifiers such as Naive Bayes, SVM and CNN are compared. The precision results show that the Naïve Bayes classifier had the lowest accuracy of 71.72%, compared to the other two, where the CNN classifier has a 82.79% accuracy and the SVM has the best accuracy in percentage of 93.03%.",text classification;NLP;Naïve Bayes;SVM;CNN,"1, 5",,IEEE Conferences,"2021 International Conference on Software, Telecommunications and Computer Networks (SoftCOM)",IEEE
63,,An Online Platform for Testing and Evaluating Random Number Generators,P. Kubczak; W. Wozniak; J. Nikonowicz; L. Matuszewski; M. Jessa,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9559127,10.23919/SoftCOM52868.2021.9559127,"This article introduces a new online platform for testing binary random number generators. The growing share of low-complex devices in IoT networks increases the demand for basic authorization and authentication tools, the critical block of which is a secure random number generator. Communication devices, therefore, require designers to carry out time-consuming tests and acquire specialist knowledge of statistical testing in evaluation of their results. To meet the current requirements, we have created a test platform to assess the quality of random strings produced by the generator. The presented solution, based on the proprietary evaluation metric, provides feedback on the properties of the uploaded random sequences. Clear interface provides ease of use and by machine learning in the platform’s backend, along with the increase of processed data, the improved quality of the interpretation delivered by the system is ensured. The operation of the platform has been confirmed experimentally, based on the analysis of hardware generators producing random strings with known properties.",random number generator;pattern recognition;sequence similarity;statistical test,"1, 6",,IEEE Conferences,"2021 International Conference on Software, Telecommunications and Computer Networks (SoftCOM)",IEEE
64,,A Character-Level Convolutional Neural Network for Predicting Exploitability of Vulnerability,J. Lyu; Y. Bai; Z. Xing; X. Li; W. Ge,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9546814,10.1109/TASE52547.2021.00014,"The continuous discovery of software vulnerabilities have brought great challenges to the cyber security, which will lead to severe systematical or individual losses after being exploited. But the harshly increasing of software vulnerabilities overwhelms the time consuming vulnerability analysis. Security experts must pay more attention to the ones which have the highest priority to be repaired. In general, both severity and exploitability determine the severity of a software vulnerability. Compared with the severity evaluated by the Common Vulnerability Scoring System (CVSS score), the exploitability is still lack of a well-accepted standard. Furthermore, based on the perspective of attack and defense, we found that the exploitability of vulnerabilities is more attractive to hackers so that system or individual is severely affected by the exploitability rather than the severity. In this paper, we propose a deep learning based approach to predict the exploitability of the vulnerability by using the correlated textual description and characteristics. Specifically, our approach takes character-level Convolutional Neural Network (charCNN) to fetch more fine-grained character-level features from the vulnerability description instead of the word-level features considered by the previous literatures. And we highlight the importance of vulnerability characteristics such as Confidentiality Impact, Integrity Impact, Attack Vector etc. during the determination of vulnerability exploitability. Extensive experiments are set to prove the effectiveness of the given charCNN approach through the comparison on both different levels of features and different neural network models. Our approach achieves the best F1 values 93.1% (at least 2.2% more than the baselines). And we also investigate the efficiency of charCNN trained by historical vulnerability when predicting the exploitability of the newly published vulnerabilities. Finally, we further explore the robustness of the proposed model by changing the scale of training sets. For the prediction of vulnerability exploitability, we recommend to adopt 40.0% to 50.0% vulnerabilities to train a robust charCNN model.",Vulnerability Exploitability Prediction;Vulnerability description;Deep Learning;Mining Software Repositories,"119, 126",,IEEE Conferences,2021 International Symposium on Theoretical Aspects of Software Engineering (TASE),IEEE
65,,Word Embedding based News Classification by using CNN,F. Ahmed; N. Akther; M. Hasan; K. Chowdhury; M. S. H. Mukta,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9537091,10.1109/ICSECS52883.2021.00117,"In this era of information technology, the number of online news portal is increasing day by day. These online news portals make a good profit by advertising different consumer products to their reader. However, due to the lack of intelligence, traditional news portals cannot identify what types of news are preferred by the users. As a consequence, these news portals most of the time show irrelevant advertisements to the readers and incur a great economic loss to the advertisers. If these news portals can identify what type of news a user is reading, then they can provide contextual advertisements (showing advertisements of news-related products) and gain more profit. Therefore, in this paper, we proposed a method integrating word embedding with Convolutional Neural Network (CNN) for the classification of English news into four different categories: Sports, Business, National and International. The performance of the proposed method is evaluated on our prepared dataset in terms of macro- f1 and micro-f1 scores. The experimental result shows that our proposed method achieved macro-f1 and micro-f1 scores of 0.90 and 0.89, respectively which are significantly higher than that of all the baseline methods.",News Classification;Word Embedding;CNN;BoW;Contextual Marketing;Machine learning,"609, 613",,IEEE Conferences,2021 International Conference on Software Engineering & Computer Systems and 4th International Conference on Computational Science and Information Management (ICSECS-ICOCSIM),IEEE
66,,Iris Recognition System Using Convolutional Neural Network,A. Sallam; H. A. Amery; S. Al-Qudasi; S. Al-Ghorbani; T. H. Rassem; N. M. Makbol,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9536996,10.1109/ICSECS52883.2021.00027,"Identification system is one of the important parts in security domains of the present time. The traditional protection methods considered to be inefficient and unreliable as they are subjected to the theft, imitation or forgetfulness. In contrast, biometrics such as facial recognition, fingerprints and the retina have emerged as modern protection methods, but still also suffer from some defects and violations. However, Iris recognition is an automated method that considered as a promising biometric identification due to the stability and the uniqueness of its patterns. In this paper, an iris recognition system based on Convolutional Neural Network (CNN) model was proposed. CNN is used to perform the required processes of feature extraction and classification. The proposed system was evaluated through CASIA-V1 and ATVS datasets, after the required pre-processing steps taken place, and achieved 98% and 97.83% as a result, respectively.",Fuzzy Operation;Deep Learning;Segmentation;Convolution Neural Network,"109, 114",,IEEE Conferences,2021 International Conference on Software Engineering & Computer Systems and 4th International Conference on Computational Science and Information Management (ICSECS-ICOCSIM),IEEE
67,,The Co-ChiLeRFE: Couple LBP and LTP Methods of Children-Learning Readiness Using Facial Expression,U. Mahsa Anandiwa; E. Rachmawati; R. Risnandar,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9536981,10.1109/ICSECS52883.2021.00039,"Children’s emotions can affect the learning process, especially positive emotions, making them more focused on learning. In addition, in terms of identifying someone’s emotions, we can represent them through facial expressions by combining the local binary pattern (LBP) and the local ternary patterns (LTP) method, known as Co-ChiLeRFE. The reasons for combining the two methods are that the LBP has proven to be very good at performing feature extraction, especially in describing textures. At the same time, LTP is adept at dealing with uniform motifs such as those on the face area. Subsequently, in this study, we used the NIMH child emotional faces picture set (NIMH-ChEFS), which has five-class expressions: sad, neutral, happy, angry, and afraid. To achieve optimal results in the Co-ChiLeRFE method, we set the LBP parameter as P = 8, R = 8, and the LTP parameter threshold value of one. The results we got from this experiment achieved a system performance superior accuracy of 92.51%.",children;learning;face expression;LBP;LTP,"177, 182",,IEEE Conferences,2021 International Conference on Software Engineering & Computer Systems and 4th International Conference on Computational Science and Information Management (ICSECS-ICOCSIM),IEEE
68,,Word Embedding based Event Identification and Labeling of Connected Events from Tweets,P. K. Das; M. Al Banna; M. A. Al Fahad; S. Islam; M. S. Hossain Mukta,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9537046,10.1109/ICSECS52883.2021.00105,"Events conceived as facts which are fine grained entities that happen around us such as completing graduation, birthday celebration, and death of an individual. Events are regarded as happening in a certain place, during a particular interval of time which can be occurred planned or unplanned way. Social media is a platform where users share their attending events with others. In this paper, we present a novel machine learning based approach to identify events from social media, i.e., Twitter, by using Bidirectional Encoder Representations from Transformers (BERT) based word embedding technique. Events might be connected with each other which might have real life implications such as identifying causal-effect, investigating criminal activities, etc. We also demonstrate a mechanism which can organize relevant events into a cluster based on their spatiotemporal properties. Later, we develop an unsupervised connected event labeling technique by using BERT word embedding approach by exploiting its semantic strength from the content of tweets. Our model shows an outstanding performance which has an accuracy of 91%. We also compare our approach with two competitive baseline techniques (i.e., word2vec and tf-idf) to identify events and our model shows better performance (on an average 5% better accuracy) than that of those baseline models.",Event;Word Embedding;BERT;Classification;Twitter,"541, 546",,IEEE Conferences,2021 International Conference on Software Engineering & Computer Systems and 4th International Conference on Computational Science and Information Management (ICSECS-ICOCSIM),IEEE
69,,Comparison of document similarity algorithms in extracting document keywords from an academic paper,M. S. U. Miah; J. Sulaiman; S. Azad; K. Z. Zamli; R. Jose,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9537092,10.1109/ICSECS52883.2021.00121,"The idea of this study is to validate a list of keywords derived from a scientific article by a domain expert from years of knowledge with prominent document similarity algorithms. For this study, a list of handcrafted keywords generated by Electric Double Layer Capacitor (EDLC) experts are chosen, and relevant documents to EDLC are considered for the comparison. Then, different similarity calculation algorithms were employed in different settings on the documents such as using the whole texts of the documents, selecting the positive sentences of the documents, and generating similarity score with automatically extracted keywords from the documents. The experiment’s outcome provides us with findings that the machine-generated keywords are mostly similar to the curated list by the domain experts. This study also suggests the preferable algorithms for similarity calculation and automated key-phrase extraction for the EDLC domain.",Document similarity calculation;Relevant Document Selection;keyword extraction comparison;keyword validation;Electric Double Layer Capacitor;EDLC;Keyword Based Recommendation System,"631, 636",,IEEE Conferences,2021 International Conference on Software Engineering & Computer Systems and 4th International Conference on Computational Science and Information Management (ICSECS-ICOCSIM),IEEE
70,,Dew Intelligence: Federated learning perspective,E. Guberović; T. Lipić; I. Čavrak,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9529852,10.1109/COMPSAC51774.2021.00274,"Newly emerging and evolving technologies such as Cloud, Fog and Edge Computing, as well as Internet of Things, Cyber-Physical Systems and Distributed Ledger Technology (such as blockchain) together with advances in Artificial Intelligence (AI) research are increasingly becoming a common and pervasive phenomenon in our everyday lives. Their co-evolution with society is driving the emergence of future socio-technical systems, which further promote ubiquitous entanglement between humans and machines. Fog, Edge and Dew computing as post-Cloud computing paradigms aim to relocate computing resources closer to end users in order to mitigate cloud-specific issues of highly centralized computation. Dew computing as the youngest of the post-cloud paradigms promotes human centered independence and collaboration between devices within scalable distributed computing infrastructures. Meanwhile, the field of artificial intelligence is adapting to recent challenges posed by user data privacy regulations as well as opportunities for applications on mobile devices based on their growing computational abilities. The usage of artificial intelligence in pervasive and scalable distributed computing systems is a natural step towards ubiquitous intelligent infrastructures and collaborative human and machine environments. Federated learning is an artificial intelligence technique enabling collaborative learning in distributed devices environment without sharing the training data sets, which are often private. This paper provides the overview of the federated learning paradigm showing that it inherently leverages both independence and collaboration, thus exemplifying implementation of dew intelligence within scalable distributed computing hierarchy.",dew computing;distributed computing;federated learning;collaborative learning;machine learning,"1819, 1824",,IEEE Conferences,"2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)",IEEE
71,,Breast Mass Detection and Classification Using Deep Convolutional Neural Networks for Radiologist Diagnosis Assistance,T. Mahmood; J. Li; Y. Pei; F. Akhtar; Y. Jia; Z. H. Khand,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9529375,10.1109/COMPSAC51774.2021.00291,"Several developments in computational image processing methods assist the radiologist in detecting abnormal breast tissue in recent years. Consequently, deep learning-based models have become crucial for early screening and interpretation of mammographic images for breast masses diagnosis, helping for successful treatment. Breast masses and calcification is an essential parameter for the prognosis of breast cancer. However, the mammographic image’s mass detection needs a deeper investigation due to the breast masses’ heterogeneity and anomalies’ characteristics that are easily confused with other objects present in the image. Hence, this study proposed a deep learning-based convolutional neural network (ConvNet) that will incorporate both mammography and clinical variables to predict and classify breast masses to assist the expert’s decision-making processes. We trained our proposed model with 322 scanned digital mammographic images of the MIAS (Mammogram Image Analysis Society) dataset and 580 images of the private dataset to evaluate the performance, which is highly imbalanced. This study aimed to perform an automatic and comprehensive characterization of breast masses using appropriate layers deep ConvNet model with high accuracy true-positive rate, decreased error rate and applying data-augmentation techniques. We obtained a classification accuracy of 97% applying the filtered deep features, which is the best performance from the existing approaches.",breast masses classification;data-augmentation;deep convolutional neural network;image classification;computer aid diagnosis,"1918, 1923",,IEEE Conferences,"2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)",IEEE
72,,A Fast Training Method using Bounded Continual Learning in Image Classification,S. Jang; Y. Kim,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9529795,10.1109/COMPSAC51774.2021.00036,"These days, Deep neural networks (DNNs) are showing good performance in image classification. They bring sufficient performance not only in specific fields such as medical images and meteorological observation images, but also in fields necessary for daily life. However, for them to be more useful practically, they should be able to add new tasks to suit a changing environment. This is the same as saying that they should be able to learn by adding new data. Unfortunately, since catastrophic forgetting is an inevitable feature of connectionist model, it is very difficult to capture both accuracy and computational efficiency in continuous task learning. In this paper, we propose Bounded Continual Learning (BCL) based on inductive transfer learning. BCL extracts feature values from sub-models created by separating base datasets and train a new classifier. BCL showed very good performance in training time efficiency to learn tasks in a sequential. We demonstrate our approach is flexible and efficient by various classification tasks based on the CIFAR dataset.",Continual learning;Lifelong learning;Image classification;Deep neural networks;Transfer learning,"186, 191",,IEEE Conferences,"2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)",IEEE
73,,A Self-enhanced Automatic Traceability Link Recovery via Structure Knowledge Mining for Small-scale Labeled Data,L. Chen; D. Wang; L. Shi; Q. Wang,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9529509,10.1109/COMPSAC51774.2021.00123,"Traceability links between requirements and source code are beneficial to the maintenance and evolution activities. Compared with the proposed unsupervised solutions, supervised solutions are more effective to generate trace links automatically and gaining more attention. However, supervised solutions often need to spend a lot of effort on labeling data. To overcome this limitation, we propose a self-enhanced automatic traceability link recovery approach based on structure knowledge mining for small-scale labeled data, named K2Trace, which not only enhances the semantic representations of artifacts by mining context information but also self-enhances the size of training set by exploring transitive relationships. Evaluation results show that K2Trace can outperform the state-of-the-art baseline approach. K2Trace proves the usefulness of mining knowledge from the structure information of software artifacts, as well as provides a new way to substantially reduce the amount of training data needed for training efficient classification models, which may pave the way for generating accurate trace links.",traceability link recovery;context information;context embedding;transitive relationship;inference rule,"904, 913",,IEEE Conferences,"2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)",IEEE
74,,A Comprehensive Qualitative and Quantitative Review of Current Research in GANs,J. Ma; P. Saxena; S. I. Ahamed,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9529432,10.1109/COMPSAC51774.2021.00250,"Generative Adversarial Networks (GANs) are among the most actively researched neural networks in today’s artificial intelligence research. Scientists across different domains, particularly in image processing, constantly utilize variants of GANs to conduct research. The topic has increasingly drawn attention and interest in recent years. Our survey paper reviews the current literature and applications of GANs from both qualitative and quantitative perspectives. This survey also summarizes the challenges and improvement techniques of training GANs. We hope this paper may help researchers interested in GANs and serve as an informative source for ongoing and future work in this field.",Generative Adversarial Networks;Neural Net-works;Artificial Intelligence;Machine Learning;Data Science,"1675, 1682",,IEEE Conferences,"2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)",IEEE
75,,Data Analysis Methods for Health Monitoring Sensors: A survey,S. Sobhan; S. Islam; M. Valero; H. Shahriar; S. I. Ahamed,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9529457,10.1109/COMPSAC51774.2021.00097,"Innovations in health monitoring systems are fundamental for the continuous improvement of remote healthcare. With the current presence of SARS-CoV-2, better known as COVID-19, in people’s daily lives, solutions for monitoring heart and especially respiration and pulmonary functions are more needed than ever. In this paper, we survey the current approaches that utilize the advantages of sensor technologies to sense, analyze, and estimate health data related to respiration, heart, and sleep monitoring. We focus on illustrating the signal processing and machine learning techniques used on each approach to facilitate researchers’ understanding of how data is processed nowadays. We have classified the reviewed papers into two main categories: contact and contactless sensors. In each category, we discuss the different types of used sensors, the data analysis technique, and the accuracy of those techniques.",Signal processing (SP);machine learning (ML);remote monitoring;internet of things (IoT);respiration and pulmonary function,"669, 676",,IEEE Conferences,"2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)",IEEE
76,,Medical named entity recognition of Chinese electronic medical records based on stacked Bidirectional Long Short-Term Memory,Z. Zhu; J. Li; Q. Zhao; Y. -C. Wei; Y. Jia,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9529670,10.1109/COMPSAC51774.2021.00293,"The wide adoption of electronic medical record (EMR) systems causes rapid growth of medical and clinical data. It makes the medical named entity recognition (NER) technologies become critical to find useful patient information in the medical dataset. However, the medical terminologies usually have the characteristics of inherent complexity and ambiguity, it is difficult to capture context-dependency representations by supervision signal from a simple single layer structure model. In order to address this problem, this paper proposes a hybrid model based on stacked Bidirectional Long Short-Term Memory (BILSTM) for medical named entity recognition, which we call BSBC (BERT combined with stacked BILSTM and CRF). First, we use Bidirectional Encoder Representation from Transformers (BERT) to perform unsupervised learning on an unlabeled dataset to obtain character-level embeddings. Then, stacked BILSTM is utilized to obtain context-dependency representations through the multi hidden layers structure. Finally, Conditional Random Field (CRF) is used to predict sequence tags. The experiment results show that our method significantly outperforms the baseline methods, it serves as a strong alternative approach compared with traditional methods.",Electronic medical record (EMR);Named entity recognition (NER);Bidirectional Encoder Representation from Transformers (BERT);Stacked Bidirectional Long Short-Term Memory (BILSTM),"1930, 1935",,IEEE Conferences,"2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)",IEEE
77,,Multi-faceted Classification for the Identification of Informative Communications during Crises: Case of COVID-19,Z. Xie; A. Jayanth; K. Yadav; G. Ye; L. Hong,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9529603,10.1109/COMPSAC51774.2021.00125,"Social media data are used to enhance crisis management, as people widely adopt social media to share and acquire information to cope with uncertainties in crises. Identification and extraction of informative communications out of large volumes of data is critical for accurate situational awareness and timely response. Existing studies use conditions of geolocations, keywords, and topics separately or jointly to retrieve data that can be crisis related, but are not enough to filter subsets of data for different crisis management tasks. We propose that the crisis communication purposes of users can be detected to enhance data selection and prioritization for different crisis management tasks. A classification framework was built to identify three facets of a message: content type, audience type, and information source. The definitions of these categories are not dependent on a specific type of crises. So the classification framework can be potentially applied to different crisis scenarios. Machine learning models were created for the automatic classification of messages. Results showed the CNN-based model achieved the best accuracy (88.5%) for the classification of content type. The proposed Naive Bayes and logistic repression with predetermined features can best differentiate audience types and information source with an accuracy of 72.7% and 72.2%, respectively.",Crisis;Social Media;COVID-19;Informative Communication;Machine Learning,"924, 933",,IEEE Conferences,"2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)",IEEE
78,,Exploiting Multi-aspect Interactions for God Class Detection with Dataset Fine-tuning,S. Ren; C. Shi; S. Zhao,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9529815,10.1109/COMPSAC51774.2021.00119,"God class refers to a class that undertakes too many responsibilities for tasks that should more appropriately be handled by multiple classes. The existence of god classes seriously affects the maintainability and understandability of software. To eliminate god class, we first need to identify them. Researchers have proposed traditional methods using code metrics and deep learning methods using code metrics and text information to detect god classes. However, the relationship existing in metrics and text information is often ignored; moreover, deep learning methods require a large number of reliable datasets, while authentic god class datasets are scarce. To solve the above problems, we propose a novel god class detection method based on multi-aspect interactions and dataset fine-tuning. First, we use proposed model to extract multi-aspect interaction information, including three parts: (i) the interaction information existing in code metrics; (ii) the interaction information existing in texts; (iii) the interaction information existing in texts and code metrics. In this way, we can not only make use of code metrics and text information, but also fully exploit the multi-aspect interaction information. Second, we train with large-scale synthetic datasets to obtain a pre-trained model, then fine-tune the pre-trained model parameters with high-quality authentic datasets. Using the training method of pre-training and fine-tuning, we can solve the problem of low-reliability synthetic datasets and scarce authentic datasets. Finally, evaluation results on open-source applications suggest that the proposed approach improves on the state-of-the-art.",Code Smells;Fine-tuning;Feature Interactions;God Class;Pre-training,"864, 873",,IEEE Conferences,"2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)",IEEE
79,,Deep Learning Applied to Automatic Reclosers Detection in Power Grid,F. Marques; A. Pinto; A. Bastos; A. Gonçalves; G. Pereira; F. Reis,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9529666,10.1109/COMPSAC51774.2021.00281,"Brazilian energy distribution companies are investing in automatic circuit reclosers (ACRs) to optimize their energy grids. These devices often require specialists to maintain. Training electricians can be problematic, as there are very similar models and provide different commands for the same tasks. Based on this problem, an application was developed in this work that allows generalists to carry out maintenance on reclosers and make training less confusing. This paper describes the object detection module of this application, which employs Deep Learning to identify four different recloser models. Were tested some state of art neural networks implementations in object detection field in the recognition of the ACRs supported models and in the tests was reached the best neural network obtained approximately 89% in Mean Average Precision (mAP). This work aim apply the object detection in energy area, focused in maintenance and training scenarios, showing in effective to detect and differentiate ACRs similar models, thus helping general electricians to provide a more accurate service and reducing company costs.",deep learning;reclosers;object detection,"1861, 1866",,IEEE Conferences,"2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)",IEEE
80,,A Bowel Sound Detection Method Based on a Novel Non-speech Body Sound Sensing Device,Y. Qiao; L. Wang; X. Tao,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9529515,10.1109/COMPSAC51774.2021.00111,"Bowel sounds are the sounds produced by intestinal peristalsis and can reflect intestinal activities. There is a long history of using bowel sounds as indicators for intestinal health in clinical practices, making bowel sound detection potentially useful as a key enabling technology for health care applications. Traditionally, bowel sounds are detected manually by medical staff. However, manual inspections are time consuming and prone to personal differences. In modern hospitals, advanced medical devices are used to detect the patients’ intestinal activities with rich sensing modalities. However, these devices are often invasive, expensive and large in size, making them only used in limited, critical scenarios. To facilitate bowel sound detection and further intestinal activity analysis in daily living, this paper presents a wearable system to capture and recognize users’ bowel sounds and other types of body sounds without restricting their daily activities. We propose to use dual-channel, stethoscope augmented microphones to capture users’ body sounds. With the captured audio stream, we extract both time- and frequency-domain features to represent signals in each frame. Beyond traditional audio features such as energy and MFCC, we further extract dual-channel features to augment the information obtained from a frame. We evaluate the system’s performance with four subjects in real-world settings, the results suggest our system is effective for achieving a detection accuracy of 85.7%.",Bowel Sound Detection;Wearable Acoustic Sensing;Non-Speech Body Sound;Acoustic Signal Processing,"785, 793",,IEEE Conferences,"2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)",IEEE
81,,Automatic Cataract Grading with Visual-semantic Interpretability,X. Xu; J. Li; Y. Guan; L. Zhao; L. Zhang; L. Li,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9529571,10.1109/COMPSAC51774.2021.00175,"Cataract is a chronic eye disease that causes irreversible vision loss. Automatic cataract detection can help people prevent visual impairment and decrease the possibility of blindness. To date, many studies utilize deep learning methods to grade cataract severity on fundus images. However, they mainly focus on the classification performance and ignore the model interpretability, which may lead to a semantic gap between networks and users. In this paper, we present a deep learning network to improve the model interpretability, which consists three main modules: deep feature extraction, visual saliency module and semantic description module. Visual and semantic interpretation jointly employed to provide cataract-grade oriented interpretation for the overall model. Experimental results on real clinical data set show that our method improves the interpretability for cataract grading while ensuring the high classification performance.",Cataract grading;deep learning;model inter-pretability,"1260, 1264",,IEEE Conferences,"2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)",IEEE
82,,Facial Image Classification for Obstructive Sleep Apnea Pre-Screening,W. Lin; Q. Zhang; J. Yang; Fangfang; Q. Wang; Q. Chen; Y. Lei,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9529861,10.1109/COMPSAC51774.2021.00290,"In order to effectively implement Obstructive Sleep Apnea (OSA) pre-screening, an OSA pre-recognition model based on Resnet50 network was proposed. The frontal and lateral faces of 1000 patients with OSA were collected, and a series of preprocessing operations were performed to augment the image data, which were then fed into the Resnet50 network. The experimental results show that compared with the frontal face data, the classification accuracy obtained by using the side face data input into the network is 17.4% higher on average, and the frontal and lateral face data were indeed helpful in the pre - screening of obstructive sleep apnea. The grad-cam method shows that the focus areas of the model presented in this paper basically coincide with the face areas diagnosed in medical clinic.",OSA;Side Face;Frontal Face;Resnet;Grad-Cam,"1913, 1917",,IEEE Conferences,"2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)",IEEE
83,,YOLOv4-object: an Efficient Model and Method for Object Discovery,M. Ning; Y. Lu; W. Hou; M. Matskin,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9529473,10.1109/COMPSAC51774.2021.00016,"Object discovery refers to recognising all unknown objects in images, which is crucial for robotic systems to explore the unseen environment. Recently, object detection models based on deep learning have shown remarkable achievements in object classification and localisation. However, these models have difficulties handling the unseen environment because it is infeasible to exhaustively predefine all types of objects. In this paper, we propose the model YOLOv4-object to recognise all objects in images by modifying the output space of YOLOv4 and related image labels. Experiments on COCO dataset demonstrate the effectiveness of our method by achieving 67.97% recall (6.49% higher than vanilla YOLOv4). We point out that the incomplete labels (COCO only labels for 80 categories) hurt the learning process of object discovery and a higher recall can be achieved by our method if the dataset is fully labelled. Moreover, our approach is transferable, extensible, and compressible, showing broad application scenarios. Finally, we conduct extensive experiments to illustrate the factors that affect the object discovery performance of our model and some suggestions on practical implementations are elaborated.",deep learning;object detection;object discovery;robotics system,"31, 36",,IEEE Conferences,"2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)",IEEE
84,,Diagnostic Imaging Support System for Rheumatoid Arthritis Using Ultrasound Images,K. Arai; C. Miura; S. -Y. Kawashiri; T. Imai; T. Kobayashi,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9529523,10.1109/COMPSAC51774.2021.00088,"In recent years, image inspections have become a useful tool in the early diagnosis of rheumatoid arthritis (RA). However, evaluations in RA ultrasound inspections are visual inspections by doctors, evaluation tends to become somewhat subjective, meaning that it is difficult for less-experienced doctors to make an assessment. In this study, therefore, we propose a system that alleviates the burden on doctors, by automating RA image diagnosis, that is to say an RA image diagnosis support system using ultrasound images. With this system, as it is necessary to automate RA image diagnosis, it is necessary to propose an image classification method with higher classification accuracy. Therefore, in this paper, we propose an image classification method with higher precision, by comparing methods of image classification. We also show our evaluation results. As a result, it is considered that image classification via CNN using automatic extraction images of synovial thickening surrounding areas is effective for image classification of ultrasound images for the purpose of automating RA image diagnosis.",Rheumatoid Arthritis;Ultrasound Image;Power Doppler Method;Convolutional Neural Network;Custom Vision,"599, 607",,IEEE Conferences,"2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)",IEEE
85,,Local and Global Feature Based Explainable Feature Envy Detection,X. Yin; C. Shi; S. Zhao,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9529877,10.1109/COMPSAC51774.2021.00127,"Code smell detection can help developers identify position of code smell in projects and enhance the quality of software system. Usually codes with similar semantic relationships have greater code dependencies, and most code smell detection methods ignore dependencies relationships within the source code. Thus, their detection results may be heavily influenced by inadequate code feature, which can lead to some code smell not being detected. In addition, existing methods cannot explain the correlation between detection results and code information. However, an explainable result can help developers make better judgments on code smell reconstruction. Accordingly, in this paper, we propose a local and global feature based explainable approach to detecting feature envy, one of the most common code smells. For the model to make the most of code information, we design different representation models for global code and local code respectively to extract different feature envy features, and automatically combine these features that are beneficial in terms of detection accuracy. We further design a code semantic dependency (CSD) to make the detection result easy to explain. The evaluation results of seven manual building code smell projects and three real projects show that the proposed approach improves on the state-of-the-art in detecting feature envy and boosting the explainability of results.",Feature Envy;Deep Learning;Software Refactoring,"942, 951",,IEEE Conferences,"2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)",IEEE
86,,Weighted Reward for Reinforcement Learning based Test Case Prioritization in Continuous Integration Testing,G. Li; Y. Yang; Z. Wu; T. Cao; Y. Liu; Z. Li,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9529787,10.1109/COMPSAC51774.2021.00132,"Test Case Prioritization (TCP) based on the continuous decision of Reinforcement Learning (RL) has achieved a successful application for test cases optimization in Continuous Integration (CI). The reward functions of RL describe how a test case ""ought"" to be executed in next integration, of which the design is usually based on the historical executions of the test case. The Average Percentage of Historical Failure (APHF) had been considered as one of the best reward function which has a strong correlation with the recent failure executions of a test case. However, for a test case with many historical failures but passes in recent cycles, the APHF value may be low. In this paper, two novel reward functions are proposed focusing on the impact of failure position in test case history execution sequence, which are the Average Position Exponential Weight (APEW) reward function and the Average Position Quadratic Weight (APQW) reward function, respectively. Both APEW and APQW carry out weight design of failure position but with different weights. We theoretically prove the issue of the only strong correlation with recent failure executions, and also prove that both proposed reward functions can reflect the quantity of historical failures and the distribution of these failures. Experimental verification on 10 industrial-level data sets show that the proposed reward functions can effectively improve the fault detection capability of test cases.",Continuous Integration;Reinforcement Learning;Test Case Prioritization;Reward Function,"980, 985",,IEEE Conferences,"2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)",IEEE
87,,YOLO-based Panoptic Segmentation Network,M. Diaz-Zapata; Ö. Erkent; C. Laugier,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9529610,10.1109/COMPSAC51774.2021.00170,"Autonomous vehicles need information about their surroundings to safely navigate them. For this, the task of Panoptic Segmentation is proposed as a method of fully parsing the scene by assigning each pixel a label and instance id. Given the constraints of autonomous driving, this process needs to be done in a fast manner. In this paper, we propose the first panoptic segmentation network based on the YOLOv3 real-time object detection network by adding a semantic and instance segmentation branches. YOLO-panoptic is able to do real-time inference and achieves a performance similar to the state of the art methods in some metrics.",Panoptic Segmentation;YOLOv3;Autonomous Vehicles,"1230, 1234",,IEEE Conferences,"2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)",IEEE
88,,Recognizing the Type of Mask or Respirator Worn Through a CNN Trained with a Novel Database,A. C. Marceddu; B. Montrucchio,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9529588,10.1109/COMPSAC51774.2021.00221,"Since the onset of the coronavirus pandemic, researchers from all over the world have been working on projects aimed at countering its advance. The authors of this paper want to go in this direction through the study of a system capable of recognizing the type of mask or respirator worn by a person. It can be used to implement automatic entry controls in high protection areas, where people can feel comfortable and safe. It can also be used to make sure that people who work daily in contact with particles, chemicals, or other impurities wear appropriate respiratory protection. In this paper, a proof-of-concept of this system will be presented. It has been realized by using a state-of-the-art Convolutional Neural Network (CNN), EfficientNet, which was trained on a novel database, called the Facial Masks and Respirators Database (FMR-DB). Unlike other databases released so far, it has an accurate classification of the most important types of facial masks and respirators and their degree of protection. It is also at the complete disposal of the scientific community.",Computer vision;Image databases;Machine learning;Neural networks,"1490, 1495",,IEEE Conferences,"2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)",IEEE
89,,Towards Extracting Semantics of Network Config Blocks,K. Otomo; S. Kobayashi; K. Fukuda; O. Akashi; K. Mizutani; H. Esaki,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9529369,10.1109/COMPSAC51774.2021.00214,"Configuring network devices is a main task of network operators. However, understanding and consistently updating network configuration files (config) is not an easy task especially in a large-scale and complicated networks. In this paper, we propose a semantic approach to provide better understanding of such config files, different from syntax based approaches. The key idea of the work is to extract semantics of blocks of the config files by document embedding techniques in NLP. This extraction enables us to understand context of config blocks with semantic similarity metrics instead of syntax similarity ones. Furthermore, this approach can be naturally extended to additional technical documents such as vendor’s manual documents to add more specific information on the semantics of configs. We first discuss the quality of the obtained semantics for several embedding techniques, by using clustering evaluations. We next demonstrate the effectiveness of our approach with two case studies with real network configs: (1) similar config block detection and (2) automatic labeling of config block with vendor’s documents.",Network;Configuration;Semantics;Embedding,"1443, 1448",,IEEE Conferences,"2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)",IEEE
90,,CGAN-IRB: A Novel Data Augmentation Method for Apple Leaf Diseases,X. Yuan; C. Yu; B. Liu; H. Sun; X. Zhu,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9529763,10.1109/COMPSAC51774.2021.00037,"At present, the identification of apple leaf diseases plays an important role in controlling apple leaf diseases and improving apple yield. CNNs(Convolutional Neural Networks) have been widely used in apple leaf diseases identification, but the training of the CNNs requires a large number of images. The lack of images would make the CNNs hard to generalize. Thus the CNNs are unable to recognize new disease images. Focusing on this problem, this paper proposes a new model named CGAN-IRB(Conditional Generative Adversarial Network with the Improved Residual Block) for data augmentation. Firstly, various improvements have been made based on CGAN to generate high-quality, robust, and specific-category images of apple leaf diseases. Among which the embedding of the residual block has been found to significantly improve the model performance. Then the interpolation algorithm is used instead of deconvolution to increase the image size. Finally, the TTUR(Two-Timescale Update Rule) training strategy is employed and all the convolutional layers of the network are spectrally normalized to stabilize the training of the network. The performance of CGAN-IRB was tested both on image generation and classification tasks. Experiment results show that the images generated by the network possess high quality and robust features, pro-viding a novel solution for the data augmentation of apple leaf diseases. The new GAN-based data augmentation method leads to significant improvements in the classification accuracy of CNNs. In the case of all tested CNNs, the classification accuracy improvements are 11.75% and 2.17% on average over non-augmented and traditional-augmented, respectively. Among them, the classification accuracy of GoogLeNet V2 and ShuffleNet V2 is 99.34% and 99.67%, respectively. The data augmentation approach proposed in this paper can be used more widely in the field of disease identification, solving the problem of insufficient data sets, and can be extended to related fields where data sets are difficult to obtain.",data augmentation;generative adversarial networks;apple leaf disease identification;convolutional neural networks,"192, 200",,IEEE Conferences,"2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)",IEEE
91,,A Transfer Learning Approach to Surface Detection for Accessible Routing for Wheelchair Users,V. Mokrenko; H. Yu; V. Raychoudhury; J. Edinger; R. O. Smith; M. O. Gani,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9529616,10.1109/COMPSAC51774.2021.00112,"The nature of the surface has a significant effect on how wheelchair users experience locomotion. The preferred surfaces for wheeled mobility must be even, firm and smooth while generating adequate friction. The development of accessible road maps that include ground conditions is therefore of utmost importance. Our prior work has shown how such maps can be created using surface-induced vibration data collected by motion sensors embedded in smartphones and then classifying them with machine learning algorithms. To make data collection scalable, participatory crowd-sensing can be used, where users collect and transmit sensor data while traveling on wheelchairs. The complexity here is that wheelchairs widely vary in type (manual, power-assist, power), weight, number and nature of wheels, therefore the sensor data generated by different wheelchairs varies greatly. Collecting training data on each individual wheelchair type to develop classification models is not feasible. To address this problem, in this paper we explore the possibility of transferring knowledge from known wheelchairs to unknown types. We develop a transfer learning algorithm to classify 15 surfaces with minimal training data from different wheelchairs. Our experiments with 47 subjects show that surface classification knowledge, learned from sensor data generated by manual wheelchairs, can be transferred to a power wheelchair with up to 90.02% accuracy. This allows crowd-sensing to be used effectively for data collection for generating accessible route maps. We integrate our transfer learning approach into our system for accessible routing, which we developed in previous work.",accessible routing;manual wheelchair;power wheelchair;transfer learning;surface classification,"794, 803",,IEEE Conferences,"2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)",IEEE
92,,MLNER: Exploiting Multi-source Lexicon Information Fusion for Named Entity Recognition in Chinese Medical Text,Y. Xiao; Q. Zhao; J. Li; J. Chen; Z. Cheng,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9529430,10.1109/COMPSAC51774.2021.00147,"The integration of lexicon information into character-based models is a hot topic in Chinese Named Entity Recognition(NER) research. Most methods only utilize information from a single lexicon which is usually a general lexicon. However, In the Chinese medical text scenario, due to the large amount of medical terminology, a single lexicon, especially a general lexicon, offers little performance improvement to the Chinese NER. In this paper, we propose a Multi-source Lexicon Information Fusion method for Named Entity Recognition in Chinese Medical Text(MLNER) which can utilize information from both general and medical lexicons. Considering the small medical annotated corpus, we combine the model with the pre-trained model to improve the performance of the model on small datasets by exploiting the rich representation capability of the pre-trained model. Experiments show that our method can effectively improve the performance of NER in Chinese medical text. Our model is also applicable to Chinese NER tasks in other domain specific fields, with good scalability and application value.",Lexicon-based NER;Medical Text;Information Extraction;Deep Learning;Pre-trained Model,"1079, 1084",,IEEE Conferences,"2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)",IEEE
93,,Defect Detection of Metal Nuts Applying Convolutional Neural Networks,D. Sauter; A. Schmitz; F. Dikici; H. Baumgartl; R. Buettner,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9529439,10.1109/COMPSAC51774.2021.00043,"Since the human inspection of small metal parts is complex, time-consuming and prone to human error, a convolutional neural network for the detection of defects on metal nuts was developed in order to grant fast and robust quality controls. For this approach, we built an image classification algorithm based on the Xception architecture. The evaluation of the trained model is robust and achieves reliable results after applying a hold-out 5-fold cross-validation. Implementing this algorithm on the MVTec Anomaly Detection dataset outperforms the existing benchmark on defect detection for metal nuts with a balanced accuracy of 90.00% and a value of 0.99 for the area under the curve.",CNN;quality assessment;deep learning;Xception,"248, 257",,IEEE Conferences,"2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)",IEEE
94,,A Bayesian Framework for Supporting Predictive Analytics over Big Transportation Data,M. D. Jackson; C. K. Leung; M. D. B. Mbacke; A. Cuzzocrea,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9529442,10.1109/COMPSAC51774.2021.00054,"In the current era of big data, huge volumes of valuable data can be easily generated and collected at a rapid velocity from a wide variety of rich data sources. In recent years, the initiates of open data also led to the willingness of many government, researchers, and organizations to share their data and make them publicly accessible. An example of open big data is transportation data such as public bus performance data. Analyzing these open big data can be for social good. For instance, by analyzing and mining the public bus performance data, the bus service providers could get an insight on the on-time performance or delay in bus services. By taking appropriate actions (e.g., adding more buses, rerouting some buses routes, etc.) could enhance rider experience. In this paper, we present a Bayesian framework for supporting predictive analytics over big transportation data. Specifically, our framework consists of several Bayesian networks to predict whether a bus arrive late than its scheduled time at a given bus stop. We analyze and determine the network configurations and/or parameter permutation to produce the best result for each (bus stop, bus route, arrival time)-triplet. Evaluation on an open big data for public transit bus from a North American city shows the effectiveness and practicality of our Bayesian framework in supporting predictive analytics on big open data for transportation analytics.",data science;data analytics;predictive analytics;transportation analytics;machine learning;Bayesian networks;big data;open data,"332, 337",,IEEE Conferences,"2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)",IEEE
95,,A Data Science Solution for Supporting Social and Economic Analysis,Y. Chen; C. K. Leung; H. Li; S. Shang; W. Wang; Z. Zheng,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9529779,10.1109/COMPSAC51774.2021.00252,"In the current era of big data, the advancement in data generation and management has created an avenue for decision makers to utilize these huge data collected from many data-driven application domains for different purposes. Big data science enables application developers and data scientists to utilize these big data, to learn more about the data, and then to explore and model hidden features for analysis purposes. In this paper, we present a data science solution to support social and economic analysis. Our solution makes good use of data mining techniques to cluster similar data, analyze time series, find frequent patterns, reveal interesting associations, and visualize these relationships. We evaluate our solution with two sets of real-life employment data. Our solution utilizes employment data to support social and economic analysis. It enables users to explore and discover implicit, previously unknown information and useful knowledge from the data. This, in turn, can enable the decision makers to take appropriate actions for social good and/or economic benefits. As an example, it reveals to job seekers some interesting characteristics of different data-related jobs, which helps them to find jobs that match better with their needs and profiles. As another example, it also reveals to social scientists and economists impacts of COVID-19 to the job markets, which helps them to get a better understanding of social and economic situations at the COVID-19 pandemic era and plan for the post-pandemic era.",data science;data-driven applications;social analysis;economic analysis;time series analysis;employment data analytics;visual analytics;COVID-19;data mining;frequent pattern mining,"1689, 1694",,IEEE Conferences,"2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)",IEEE
96,,Few Shot Learning of COVID-19 Classification Based on Sequential and Pretrained Models: A Thick Data Approach,D. Sawyer; J. Fiaidhi; S. Mohammed,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9529642,10.1109/COMPSAC51774.2021.00276,"Classification tasks face several issues when applied to complex data sets and sophisticated images such as CT scans. Long training times are needed to properly train traditional networks to classify images, as well as the need for large amounts of data for these networks to draw accurate conclusions. Even when supplied with large datasets, popular neural networks like VGG and ResNet fail to classify images accurately and consistently for sensitive tasks like identifying COVID-19 in a CT lung scan. To overcome these challenges, we apply Siamese neural network architecture, which has been reported to reduce training times and required training data, to a sequential network. To further empower this network, we incorporate thick data heuristics into the CT image dataset, specifically, we annotate areas of interest in the images that a radiologist would be looking for to make a diagnosis, such as ground glass opacities. Our network outperforms five leading image classification neural networks by about 3% when classifying the same CT lung scan images as positive or negative for COVID-19. By applying data thickening heuristics, we have shown that accuracy is improved, and suspect that the accuracy will continue to increase as more heuristics based on more radiologists and imaging experts are to be added on top of what we have considered in this paper.",Thick Data Analytics;Machine Learning;Classification;Siamese Neural Network;Learning from Few Shots,"1832, 1836",,IEEE Conferences,"2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)",IEEE
97,,"Motor Imagery: A Review of Existing Techniques, Challenges and Potentials",O. George; R. Smith; P. Madiraju; N. Yahyasoltani; S. I. Ahamed,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9529576,10.1109/COMPSAC51774.2021.00286,"There is the need for enhanced processing techniques that aid the development of Brain-Computer Interfaces (BCIs), considering their wide use for communication and control. Several paradigms exist for developing BCIs. One of such is motor imagery (MI). MI-based BCIs have been implemented in a variety of ways. Key factors such as the device type, task paradigm, preprocessing, feature extraction and selection and classification techniques, must be properly considered in building BCIs. Also, factors such as the task at hand, target population, processing rate and usability of the online system must be considered. Considering this need, this review presents a summary of the existing techniques for motor imagery classification, stating common trends and challenges facing MI studies, with potential improvements that might be seen. Specifically, the review focuses on electroencephalography (EEG)-based MI BCIs, with works sampled over a wide range of time.",Motor imagery;BCI;EEG;classification;communication;review,"1893, 1899",,IEEE Conferences,"2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)",IEEE
98,,SALAD: Self-Adaptive Lightweight Anomaly Detection for Real-time Recurrent Time Series,M. -C. Lee; J. -C. Lin; E. G. Gran,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9529587,10.1109/COMPSAC51774.2021.00056,"Providing a lightweight self-adaptive approach that does not need offline training in advance and meanwhile is able to detect anomalies in real time could be highly beneficial. Such an approach could be immediately applied and deployed on any commodity machine to provide timely anomaly alerts. To facilitate such an approach, this paper introduces SALAD, which is a Self-Adaptive Lightweight Anomaly Detection approach based on a special type of recurrent neural networks called Long Short-Term Memory (LSTM). Instead of using offline training, SALAD converts a target time series into a series of average absolute relative error (AARE) values on the fly and predicts an AARE value for every upcoming data point based on short-term historical AARE values. If the difference between a calculated AARE value and its corresponding forecast AARE value is higher than a self-adaptive detection threshold, the corresponding data point is considered anomalous. Otherwise, the data point is considered normal. Experiments based on a real-world time series dataset demonstrates that SALAD outperforms five other state-of-the-art anomaly detection approaches in terms of detection accuracy. In addition, the results also show that SALAD is lightweight and can be deployed on a commodity machine.",Recurrent time-series anomaly detection;lightweight LSTM;unsupervised learning;real time;self-adaptive threshold,"344, 349",,IEEE Conferences,"2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)",IEEE
99,,Screening of Viral Pneumonia and COVID-19 in Chest X-ray using Classical Machine Learning,A. U. Fonseca; G. S. Vieira; F. Soares,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9529548,10.1109/COMPSAC51774.2021.00294,"Governments, civil society, health professionals, and scientists have been facing a relentless fight against the pandemic of the COVID-19 disease; however, there are already about 150 million people infected worldwide and more than 3 million lives claimed, and numbers keep rising. One of the ways to combat this disease is the effective screening of infected patients. However, COVID-19 provides a similar pattern with diseases, such as pneumonia, and can misguide even very well-trained physicians. In this sense, a chest X-ray (CXR) is an effective alternative due to its low cost, accessibility, and quick response. Thus, inspired by research on the use of CXR for the diagnosis of COVID-19 pneumonia, we investigate classical machine learning methods to assist in this task. The main goal of this work is to present a robust, lightweight, and fast technique for the automatic detection of COVID-19 from CXR images. We extracted radiomic features from CXR images and trained classical machine learning models for two different classification schemes: i) COVID-19 pneumonia vs. Normal ii) COVID-19 vs. Normal vs. Viral pneumonia. Several evaluation metrics were used and comparison with many studies is presented. Our experimental results are equivalent to the state-of-the-art for both classification schemes. The solution’s high performance makes it a viable option as a computer-aided diagnostic tool, which can represent a significant gain in the speed and accuracy of the COVID-19 diagnosis.",Chest Radiography;Viral Pneumonia;COVID-19;Classification;Feature Extraction;Machine Learning,"1936, 1941",,IEEE Conferences,"2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)",IEEE
100,,Visual Defect Detection of Metal Screws using a Deep Convolutional Neural Network,D. Sauter; C. Atik; C. Schenk; R. Buettner; H. Baumgartl,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9529357,10.1109/COMPSAC51774.2021.00050,"In the production of screws, manual methods are often still used to detect defects. This paper aims to use a convolutional neural network-based technique to detect whether defects in screws are caused during production. Our experimental results show that a detection accuracy of 96.67% can be achieved with the proposed technique. Among the defects considered are defects on the objects' surface (e.g., scratches, dents), structural defects like distorted object parts, or defects that manifest themselves by the absence of certain object parts. Our more efficient method can be used in the future for quality control in the manufacture of screws.",Metal Screw;Deep Learning;Convolutional Neural Network;Defect Detection,"303, 311",,IEEE Conferences,"2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)",IEEE
101,,Joint Extraction of Events in Chinese Electronic Medical Records,J. Wang; J. Li; Z. Zhu; Q. Zhao; Y. Yu; L. Yang; C. Xu,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9529801,10.1109/COMPSAC51774.2021.00292,"The widely deployed of hospital information systems causes an explosive growth of the electronic medical records (EMRs). It makes the medical structured processing technologies become critical to find researchable data in the large medical dataset. However, the high quality structured processing is a challenging task, in particular due to the inherent complexity and polysemy of medical terminology. In this paper, we propose a novel approach to achieve the joint extraction of events in Chinese electronic medical records, which solves the problem of cascading error transmission in traditional models and the ambiguity of Chinese characters. We first use the Bi-directional Encoder Representation from Transformers(BERT) model to mine features from the preprocessed medical data; then based on the characteristics of Chinese, we use the Bi-directional Long Short-Term Memory(BILSTM) model to capture the semantic information of the context. The experiments were conducted on a real dataset. The F1 score of our model in the identification and classification tasks of event triggers and arguments is the highest, reaching 71.6, 68.1, 55.4 and 46.9, respectively, which proves the effectiveness of the proposed method.",Chinese Electronic Medical Records;Event Extraction;BERT;BILSTM,"1924, 1929",,IEEE Conferences,"2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)",IEEE
102,,Vision-based Hand Gesture Recognition for Human-Computer Interaction using MobileNetV2,H. Baumgartl; D. Sauter; C. Schenk; C. Atik; R. Buettner,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9529792,10.1109/COMPSAC51774.2021.00249,"In recent years, the demand for gesture recognition has increased enormously due to many applications such as computer games, human-robot interaction, assistance systems, sports, sign language interpreters, and e-commerce. The recognition of hand gestures is one of the most important gesture recognition methods. With simple hand gestures, devices in the smart home area (TV, radio, vacuum cleaner robots, etc.) should be easier to operate. Our method is based on a convolutional neural network, or more precisely on MobileNetV2. With this lean and fast network, we have been able to achieve an accuracy of 99.96 percent in recognition of hand gestures, so that in the future, we will be able to offer an application in the field of Human-Computer Interaction to interact more easily with the ever-increasing number of technologies in everyday life.",Hand gesture recognition;Convolutional neural network;Image classification;Human-Computer Interaction;Mobilenet,"1667, 1674",,IEEE Conferences,"2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)",IEEE
103,,Prediction of COVID-19 from Chest X-ray Images Using Multiresolution Texture Classification with Robust Local Features,Z. A. Oraibi; S. Albasri,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9529393,10.1109/COMPSAC51774.2021.00096,"The COVID-19 contagious disease that spread around the world, have a huge risk on people and already caused millions of deaths forcing a global pandemic in 2020. Diagnosing patients with this disease is very critical allowing fast care response and to isolate them from public. As the virus spread widely to millions of people, the fastest way to detect it is by analyzing radiology images. Early studies showed irregularity in the chest X-ray images of patients with high clinical belief of COVID-19 infection. Hence, these studies motivated us to investigate the use of machine learning techniques to help diagnosing COVID-19 patients from chest CT scans. In this paper, we propose to use a robust feature extraction descriptor and to apply a Random Forests classifier to predict COVID-19 disease in a dataset of 5000 images. First, 408 texture features are extracted using a powerful variation of Local Binary Patterns descriptor called Rotation Invariant Co-occurrence among Local Binary Patterns. Then, Random Forests classifier is applied with 250 trees to perform the classification task. Moreover, the performance of our approach was improved by using a multiresolution scheme where features are extracted from both the original input image and the subsampled image. Two metrics were used to evaluate our approach, sensitivity and specificity. We achieved 99.0% and 91.3% for both metrics, respectively. Our results are close to the state-of-the-art deep learning methods on the same dataset.",COVID-19;Random Forests;Texture Features;X-ray Imaging,"663, 668",,IEEE Conferences,"2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)",IEEE
104,,A CNN sign language recognition system with single & double-handed gestures,N. Buckley; L. Sherrett; E. Lindo Secco,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9529449,10.1109/COMPSAC51774.2021.00173,"This work aims at presenting a novel Computer Vision approach in the development of a real-time, web-camera based, British Sign Language recognition system. A literature review focused on current (1) state of sign language recognition systems and (2) techniques used is conducted. This review process is used as a foundation on which a Convolutional Neural Network (CNN) based system is designed and then implemented. A bespoke British Sign Language dataset -containing 11,875 images - is then performed to train and test the CNN which is used for the classification of human hand performed gestures. Finally, the CNN architecture recognized 19 static British Sign Language gestures, incorporating both single and double-handed gestures. During testing, the system achieved an average recognition accuracy of 89%.",sign language recognition;AI;CNN;human-machine interaction,"1250, 1253",,IEEE Conferences,"2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)",IEEE
105,,Research on Chinese Event Extraction Method Based on RoBERTa-WWM-CRF,Z. Li; N. Cheng; W. Song,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9522150,10.1109/ICSESS52187.2021.9522150,"To extract the event information contained in the Chinese text effectively, this paper takes Chinese event extraction as a sequential labeling task, and proposes a method to extract events based on the combination of RoBERTa-WWM (A Robustly Optimized BERT Pre-training Approach-Whole Word Masking) and Conditional Random Fields (CRF). This method uses RoBERTa-WWM to generate semantic representation with prior knowledge, and then inputs them into the Conditional Random Fields (CRF) model. The argument is predicted by the output label sequence. The experimental results show that this method can effectively improve accuracy, recall, and F1-score on the Chinese event extraction dataset DUEE1.0, which Baidu recently released, and improve the performance of event extraction in Chinese text.",-component;Chinese Event Extraction;Pretraining Model;RoBERTa-WWM-CRF,"100, 104",,IEEE Conferences,2021 IEEE 12th International Conference on Software Engineering and Service Science (ICSESS),IEEE
106,,A Few-shot Learning Method Based on Bidirectional Encoder Representation from Transformers for Relation Extraction,Y. Gao; R. Qi; S. Li,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9522191,10.1109/ICSESS52187.2021.9522191,"Relation extraction is one of the fundamental subtasks of the information extraction. The purpose is to determine the implicit relation between two entities in a sentence. Therefore, Convolutional Neural Networks and Feature Attention-based Prototypical Networks (CNN-Proto-FATT), a typical few-shot learning method, is proposed and achieve competitive performance. However, convolutional neural networks suffer from the insufficient instances of relation in real scenes, leading to undesirable results. To extract long-distance features more comprehensively, the pre-trained model Bidirectional Encoder Representation from Transformers (BERT) is incorporated into CNN-Proto-FATT. In this model, named Bidirectional Encoder Representation from Transformers and Feature Attention-based Prototypical Networks (BERT-Proto-FATT), the multi-head attention helps the network extract semantic features cross long- and short-distance to enhance the encoded representations. Experimental results indicate that BERT-Proto-FATT demonstrates significant improvements on the FewRel dataset.",relation extraction;information extraction;pre-trained model;few-shot learning,"158, 161",,IEEE Conferences,2021 IEEE 12th International Conference on Software Engineering and Service Science (ICSESS),IEEE
107,,End-to-End Chained Pedestrian Multi-Object Tracking Based on Multi-Feature Fusion,H. Zhou; X. Xiang; X. Wang; W. Ren,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9522289,10.1109/ICSESS52187.2021.9522289,"An end-to-end chained network with multi-feature fusion is proposed for the trade-off of tracking speed and accuracy, which integrates target detection, feature extraction and data association into a framework. It chains paired bounding boxes estimated from overlapping nodes by IOU (Intersection Over Union) matching, whose each node covers two adjacent frames. Besides, the bidirectional feature pyramid that includes two aggregation paths is presented for multi-feature fusion, in which deformable convolution V2 is applied. Decreasing sample imbalance and gradient contribution difference, focal loss and BalancedL1 Loss form multi-task learning loss. The results on MOT17 dataset indicate that the model achieve superior tracking speed (21.6FPS) and accuracy (69.6MOTA, 81.0MOTP).",Multi-Object Tracking;Chained Tracker;Multi-Task Loss;Multi-Feature Fusion;Bidirectional Pyramid,"150, 153",,IEEE Conferences,2021 IEEE 12th International Conference on Software Engineering and Service Science (ICSESS),IEEE
108,,Automatic Deployment Method of USV Software Based on Functional Domain,C. Qing; Z. Lei; L. Feng,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9522160,10.1109/ICSESS52187.2021.9522160,"In order to meet the unmanned requirements of Unmanned Surface Vessels (USV), aiming at the automatic deployment of USV software system, based on functional semantic analysis and rough set reduction, the functional domain division of USV software is realized to support automatic software deployment. The Bidirectional Encoder Representation from Transformer (BERT) model is pre-trained by the relevant standards and design documents of USV, and the semantic correlation between the function descriptions of USV software services is analyzed by using the trained model. The rough set information system for USV software system is constructed. The appropriate functional domain partition is extracted by rough set attribute reduction method, and the functional domain attribution of software service is obtained according to the equivalent partition of rough set. So USV software system can use functional domain to deploy automatically. The results show that the proposed method can meet the requirements of automatic deployment of USV software services developed from different sources and in different periods, and can realize automatic re deployment according to task changes.",component;functional domain;rough set;BERT;automatic deployment,"89, 95",,IEEE Conferences,2021 IEEE 12th International Conference on Software Engineering and Service Science (ICSESS),IEEE
109,,Research on Entity Recognition Based on Multi-criteria Fusion Model,Y. Qiyu,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9522307,10.1109/ICSESS52187.2021.9522307,"In order to fully and comprehensively utilize the Chinese named entity recognition corpus marked according to different labeling criteria, this paper, by applying the word-based BERT bidirectional language model, introducing multi-criteria shared connection layer and conditional random field (CRF) system, and using Microsoft Research Asia MSRA-NER corpus and Peking University’s People’s Daily part-of-speech tagging corpus (RMRB-98-1), firstly formed each Chinese named entity recognition model separately, and then mixed a multi-criteria fusion model with two corpora. Experiments show that the recognition effect of multi-criteria fusion model is better than that of each corpus independent model, reaching 94.46% F1 value and 94.32% F1 value respectively on MSRA-NER and RMRB-98-1 corpus. However, the experiment still has its limit in the scale of the corpus involved in the fusion. Later, integration of more corpora, and combination with entity recognition tasks in specific fields such as biology and military will be further explored to enhance the recognition effect.",entity recognition;attention mechanism;multicriteria learning;transfer learning,"85, 88",,IEEE Conferences,2021 IEEE 12th International Conference on Software Engineering and Service Science (ICSESS),IEEE
110,,Named Entity Recognition Method for Educational Emergency Field Based on BERT,K. Wei; B. Wen,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9522262,10.1109/ICSESS52187.2021.9522262,"Educational emergencies provide a lot of information. Aiming at the problem that traditional named entity recognition methods in this field cannot represent the ambiguity of a word, this article proposes a named entity recognition method BERT+BiLSTM-CRF in the field of educational emergencies based on the BERT pre-training language model. Firstly, BERT is trained on the corpus of educational emergencies to obtain the vectorized representation of the words, and then the context encoding of the serialized text is obtained using BiLSTM, and then the sequence is decoded and annotated by CRF to obtain the corresponding entities in the educational emergencies. Experiments show that the BERT+BiLSTM-CRF fusion model has achieved an accuracy of 91.62% on the educational emergency data set, which is a significant improvement compared to the traditional named entity recognition model.",Education emergencies;Named entity recognition;BERT;Vectorization,"145, 149",,IEEE Conferences,2021 IEEE 12th International Conference on Software Engineering and Service Science (ICSESS),IEEE
111,,Visual Loop Closure Detection Based on Lightweight Convolutional Neural Network and Product Quantization,L. Huang; M. Zhu; M. Zhang,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9522158,10.1109/ICSESS52187.2021.9522158,"Mobile robots rely heavily on the creation of the scene map and the positioning in the map in an unknown environment, but no matter what type of map is created, it is inevitably affected by cumulative errors. This presents a huge challenge for loop closure detection technology. Using traditional loop closure detection methods to perform scene recognition is difficult to extract the appearance changes caused by time, weather, or seasonal conditions in the image and deep semantic information, and the speed of extracting image features is slow, which is difficult to meet the real-time performance of robots. Because of the success of deep convolutional neural networks(CNN), it is possible to enrich the information of image features. First of all, this paper uses the pre-trained CNN model SSE-Net to extract the deep visual appearance and semantic features of the image, and obtain the feature description vector. Then, after product quantization(PQ) and encoding, the final pair of candidate frames is quickly searched and matched to obtain the most similar pair of candidate frames and judged as a loop . After the verification of the New collage dataset and the City Center dataset, this algorithm has achieved a good Precision-Recall rate and a faster speed compared with the recently proposed large-scale convolution network VGG16 method and traditional feature extraction methods.",mobile robots;loop closure detection;convolutional neural networks;product quantization,"122, 126",,IEEE Conferences,2021 IEEE 12th International Conference on Software Engineering and Service Science (ICSESS),IEEE
112,,Improving Long-tail Relation Extraction with Knowledge-aware Hierarchical Attention,X. Zhao; R. Qi,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9522255,10.1109/ICSESS52187.2021.9522255,"Relation Extraction (RE) is a crucial step to complete Knowledge Graph (KG) by recognizing relations between entity pairs. However, it usually suffers from the long-tail issue, especially when using distantly supervision algorithm. In this paper, inspired by the rich semantic correlations between head relations and tail relations, we proposed a knowledge-aware hierarchical attention (KA-HATT) relation extraction model. According to relational hierarchy, the multiple layers of attention were established, which take advantage of the knowledge from data-rich classes to boost the performance of data-poor classes at the tail. We have conducted extensive experiments on available dataset New York Times (NYT). Experimental results show that, compared with baseline models, our model achieves significant improvements on relation extraction, especially for long-tail relations.",relation extraction;attention mechanism;distant supervision,"166, 169",,IEEE Conferences,2021 IEEE 12th International Conference on Software Engineering and Service Science (ICSESS),IEEE
113,,Research on Chinese Minority Clothing Based on Deep Convolution Neural Network,Y. Zhang; W. Zhong; X. Li,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9522354,10.1109/ICSESS52187.2021.9522354,"There are 55 minority nationalities in China. The traditional clothing of minority nationality is a part of Chinese traditional culture. How to use computer technology to identify these clothing has great significance for the protection and inheritance of Chinese traditional culture. Some scholars have studied the methods of minority nationality clothing recognition, but these methods require people to manually mark the semantic attributes of clothing on the training set pictures. In this paper, an end-to-end method of deep convolution neural network is adopted to implement the Chinese minority nationality clothing classifier. On the basis of CNN pre-training model mobilenet-v2, Fine-tune training can classify minority nationality clothing without manual marking on the training set pictures, and the accuracy rate is 94.5%.",Chinese minority clothing;Image Recognition;Deep convolution neural network;CNN,"235, 238",,IEEE Conferences,2021 IEEE 12th International Conference on Software Engineering and Service Science (ICSESS),IEEE
114,,Security and Machine Learning Adoption in IoT: A Preliminary Study of IoT Developer Discussions,G. Uddin,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9516812,10.1109/SERP4IoT52556.2021.00013,"Internet of Things (IoT) is defined as the connection between places and physical objects (i.e., things) over the internet/network via smart computing devices. IoT is a rapidly emerging paradigm that now encompasses almost every aspect of our modern life. As such, it is crucial to ensure IoT devices follow strict security requirements. At the same time, the prevalence of IoT devices offers developers a chance to design and develop Machine Learning (ML)-based intelligent software systems using their IoT devices. However, given the diversity of IoT devices, IoT developers may find it challenging to introduce appropriate security and ML techniques into their devices. Traditionally, we learn about the IoT ecosystem/problems by conducting surveys of IoT developers/practitioners. Another way to learn is by analyzing IoT developer discussions in popular online developer forums like Stack Overflow (SO). However, we are aware of no such studies that focused on IoT developers’ security and ML-related discussions in SO. This paper offers the results of preliminary study of IoT developer discussions in SO. First, we collect around 53K IoT posts (questions + accepted answers) from SO. Second, we tokenize each post into sentences. Third, we automatically identify sentences containing security and ML-related discussions. We find around 12% of sentences contain security discussions, while around 0.12% sentences contain ML-related discussions. There is no overlap between security and ML-related discussions, i.e., IoT developers discussing security requirements did not discuss ML requirements and vice versa. We find that IoT developers discussing security issues frequently inquired about how the shared data can be stored, shared, and transferred securely across IoT devices and users. We also find that IoT developers are interested to adopt deep neural network-based ML models into their IoT devices, but they find it challenging to accommodate those into their resource-constrained IoT devices. Our findings offer implications for IoT vendors and researchers to develop and design novel techniques for improved security and ML adoption into IoT devices.",IoT;Security;Machine Learning;Developer Discussions,"36, 43",,IEEE Conferences,2021 IEEE/ACM 3rd International Workshop on Software Engineering Research and Practices for the IoT (SERP4IoT),IEEE
115,,Naturally!: How Breakthroughs in Natural Language Processing Can Dramatically Help Developers,A. A. Sawant; P. Devanbu,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9520225,10.1109/MS.2021.3086338,"Taking advantage of the naturalness hypothesis for code, recent development, and research has focused on applying machine learning (ML) techniques originally developed for natural language processing (NLP) to drive a new wave of tools and applications aimed specifically for software engineering (SE) tasks. This drive to apply ML and deep learning (DL) has been animated by the large-scale availability of software development data (e.g., source code, code comments, code review comments, commit data, and so on) available from open source platforms such as GitHub and Bitbucket.",,"118, 123",,IEEE Magazines,IEEE Software,IEEE
116,,A Light-weight Ship Detection and Recognition Method Based on YOLOv4,T. Yue; Y. Yang; J. -M. Niu,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9513040,10.1109/AEMCSE51986.2021.00137,"Ship detection and recognition based on deep learning often needs high standard hardware support while achieving high precision, which is difficult to adapt to offshore resource-limited platforms. Trying to solve this problem, this paper adopts the one-step target detection model YOLOv4 as the framework and applies a comprehensive network simplifying method. Firstly, this method applies different lightweight backbone networks in the framework to obtain the ideal Mobilenetv2-YOLOv4 network, and then conducts sparse training based on the scale factor of the batch normalization layer. Finally, it selects an appropriate threshold to prune unessential channel, which obtains a light-weight ship detection neural network for ship detection and recognition. The average accuracy of the network for detecting and identifying targets of 8 types of ships reaches 92.8% on average, the real-time detection speed is 37 frames per second, and the detection efficiency is 70% higher than that of the original network, which is capable of real-time detection under the condition of limited resources. The results also show that under simple tasks, appropriate methods can effectively compress the network parameters and computations while maintaining accuracy.",ship detection and recognition;neural network;channel pruning;YOLOv4;Mobilenetv2,"661, 670",,IEEE Conferences,"2021 4th International Conference on Advanced Electronic Materials, Computers and Software Engineering (AEMCSE)",IEEE
117,,Weak Supervised Behavior Recognition Algorithm Based on Domain Knowledge Graph,J. Xie; X. Li; D. H. Xu; H. L. Zhou; M. Liang; J. P. Guo,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9513187,10.1109/AEMCSE51986.2021.00183,"The use of artificial intelligence technology for technical and tactical analysis has become an important means and method in karate competitions and training. The integration of artificial intelligence technology and sports technical and tactical analysis is an important way to innovate and improve the technical and tactical level. In this paper, a knowledge graph in the karate domain is designed. In order to effectively use the domain knowledge graph, a new two-flow graph convolutional network is adopted, which includes classifier branch and attribute feature branch. The attribute feature branch maps the attribute embedding and score of each video instance to an attribute feature space. The semantic word embedding vectors of all concepts are used as the input of the classifier branch, and finally a classifier for behavior classification is generated. Finally, the generated classifier is evaluated according to the attribute characteristics of each video, and the whole network is optimized by the classification loss pair. In addition, the time information of the video is modeled by the attention mechanism. This technology effectively solves the problems of high labor cost, serious data loss, long delay, and low accuracy in traditional methods. The results show that the new model has a significant effect on improving the accuracy of behavior recognition, and it also lays a foundation for intelligent technical and tactical analysis. basis.",behavior identification;knowledge Graph;karate;intelligent technique;tactics analysis,"899, 904",,IEEE Conferences,"2021 4th International Conference on Advanced Electronic Materials, Computers and Software Engineering (AEMCSE)",IEEE
118,,Distillation for text classification task based on BERT,C. Sun; X. Li; S. Ge; Z. An; C. Zhang,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9513206,10.1109/AEMCSE51986.2021.00103,"In recent years, with the rapid development of the Internet and the surge in the number of web texts, the demand for text classification technology has become increasingly significant. However, there are also the following problems: 1. The maximum input length of the model is 512, and some information will be lost if the longer text is directly truncated; 2. The model is large and the reasoning time is long, which is not convenient for mobile terminal deployment requirements. Aiming at problem 1: Firstly, the text with a length of more than 512 is intercepted. Considering that the end of the text usually contains more emotional information, the intercepting strategy is 170th and 340th. At the same time, another kind of text feature surface of the model is selected: The mean and maximum values are calculated respectively along the dimension of sequence length, which are spliced into column vectors as the input features of the model. Aiming at problem 2, the knowledge distillation of the model for classification tasks is carried out to achieve the reduction parameters to improve the inference efficiency to facilitate the actual deployment and application. Three groups of control experiments show that the overall classification accuracy of the improved BERT model is 97%, and the overall performance is more balanced, and the overall performance is more robust, which is slightly better than the BERT text classification model. The effect of the distilled BERT model is only 1.5% lower than that of the BERT model, but the number of parameters of the model is 92.6% less than that of the original model, and the reasoning time is nearly 4 times faster than the original model, which also shows the effectiveness of improving input features and model compression.",Text classification;BERT;Input characteristics;Knowledge distillation;Actual deployment,"472, 478",,IEEE Conferences,"2021 4th International Conference on Advanced Electronic Materials, Computers and Software Engineering (AEMCSE)",IEEE
119,,An indoor positioning system using Channel State Information based on TrAdaBoost Tranfer Learning,Z. Yong; W. Chengbin,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9512983,10.1109/AEMCSE51986.2021.00263,"With the rapidly growing demand for Location-Based Services in indoor environments, fingerprint-based indoor positioning has caused great interest due to its high positioning accuracy and low equipment cost. However, the standard signal radio map cannot provide consistent high positioning accuracy under environmental changes and new scenarios. To address this problem, we present a novel indoor positioning Transfer Learning(TL) system based on improved TrAdaBoost. We perform phase correction on the raw CSI phase, then use One-vs-Rest algorithm and One-Hot coding, which can realize the multi-classification ability of the TrAdaBoost algorithm. Meanwhile, we use a correction factor to slow down the weight of the source domain and make the fingerprint of the source domain better transfer to the target domain by the TrAdaBoost in order to form a new fingerprint database. Experimental results show that the positioning accuracy can be improved by 35% in dynamic environment conditions. The proposed method can improve the positioning accuracy by an average of 30% in new scenes and the Site Survey Overhead(SSO) is reduced by 40%. Experiments show that our proposed method has robustness in time and space, and has lower SSO under the same positioning accuracy.",Component;CSI;WiFi;Transfer Learning;Indoor Location,"1286, 1293",,IEEE Conferences,"2021 4th International Conference on Advanced Electronic Materials, Computers and Software Engineering (AEMCSE)",IEEE
120,,Learning to Slim Deep Networks with Bandit Channel Pruning,Q. Yang; H. Hu; S. Zhao; H. Zhong,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9513026,10.1109/AEMCSE51986.2021.00217,"Recent years, deep neural network has achieved great success in machine vision, natural language processing, and reinforcement learning. While deploying these models on embedded devices and large clusters faces challenge in high energy consumption and low efficiency. In this paper, we propose an effective approach named Bandit Channel Pruning (BCP) to accelerate neural network by channel-level pruning.Inspired by autoML, we use Multi-Armed Bandit (MAB) method to explore and exploit the impact of each channel on model performance. Specifically, we use the loss value of model’s output as penalty term to find the set of redundant channels. In addition, we prove that the change of this loss value can be used as criterion of channel redundant. We analyze the complexity of BCP and give the upper bound of search times.Our approach is validated with several deep neural networks, including VGGNet, ResNet56, ResNet110, on different image classification datasets. Extensive experiments on these models and datasets demonstrate the performance of this method is better than state-of-the-art channel pruning methods.",model compression;network acceleration;multi-armed bandit;deep neural network,"1059, 1066",,IEEE Conferences,"2021 4th International Conference on Advanced Electronic Materials, Computers and Software Engineering (AEMCSE)",IEEE
121,,Hybrid Pruning for Convolutional Neural Network Convolution Kernel,C. Y. Guo; P. Li,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9513213,10.1109/AEMCSE51986.2021.00096,"In the model compression, to achieve the optimal pruning effect, weighted pruning or filter pruning was used to prune the network in traditional methods, in fact, the network can continue to be compressed. Therefore, a hybrid pruning method of convolution kernel weight pruning and filter pruning are proposed in this paper. First of all, the weight pruning is carried out for the convolutional neural network model, this paper adopts the method of dynamic convolution kernel for weight pruning, and the fluctuation value of each time normalized value in network update process is taken as the pruning criterion for the weight of the convolutional kernel. Because the weight pruning of the convolution kernel is dynamic, the floating-point operation (FLOP) is significantly reduced, and the parameter scale does not decrease significantly. Then, the model was pruning by convolution kernel ℓ<inf>1</inf>-norm [1] method, which is not only effectively reduce the parameter scale, but also no extra calculations are introduced. In the case of a small loss of network accuracy, the hybrid pruning of the convolution kernel can maximize the compression of the model to meet the conditions for deployment in mobile edge devices, and has significant improvements in floating-point operations and parameter compression. Therefore, mixed pruning in this paper can effectively make up for the shortage of single pruning method. Through VGG-16, ResNet and other networks, it has been proved in practice that the convolution kernel hybrid pruning method proposed in this paper can effectively reduce the model size and maintain a high level of accuracy.",weight pruning;Filter pruning;hybrid pruning;mobile edge devices,"432, 438",,IEEE Conferences,"2021 4th International Conference on Advanced Electronic Materials, Computers and Software Engineering (AEMCSE)",IEEE
122,,Design of the Fire Evacuation Training System for Underground Buildings Based on VR,C. Lu; Y. Zhang; C. Liu; Y. Li,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9513144,10.1109/AEMCSE51986.2021.00187,"In order to improve the self-rescue awareness and self-evacuation ability of the trapped people in the burning underground building and reduce casualties, we use Unity3D to design an underground building fire evacuation training system based on virtual reality (VR), which includes four modules: pre-training module, fire scene perception module, evacuation self-rescue experience module, and post-training feedback module. Practices have shown that the system allows trainees to conduct simulated evacuation training in an environment with a good sense of interaction and strong sense of immersion, and have a good training effect.",Unity3D;VR;underground building fire;personnel evacuation;immersive training,"921, 924",,IEEE Conferences,"2021 4th International Conference on Advanced Electronic Materials, Computers and Software Engineering (AEMCSE)",IEEE
123,,Online detection system for electric bike in elevator or corridors based on multi-scale fusion,Y. Lin; X. Chen; W. Zhong; Z. Pan,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9512956,10.1109/AEMCSE51986.2021.00014,"The traditional elevator building room electric vehicle monitoring system adopts manual monitoring, which has disadvantages such as easy fatigue of personnel and difficulty in monitoring each video channel; to address this situation, we propose an online detection system for elevator building room electric vehicles based on the improved YOLOv4-tiny algorithm, which first crawls pictures of electric vehicles as positive samples, and crawls pictures with relevant backgrounds or similarity in order to reduce the error detection rate and leakage detection rate of the system. The experimental results show that the map of the online detection system based on the improved YOLOv4-tiny algorithm is 94.07%, which is about 2 percentage points higher than the original algorithm.",YOLOv4 algorithm;data augmentation;K-means;migration learning;multi-scale fusion,"31, 34",,IEEE Conferences,"2021 4th International Conference on Advanced Electronic Materials, Computers and Software Engineering (AEMCSE)",IEEE
124,,Quantize YOLOv3-tiny For 5-bit Hardware,Y. Hua; L. Yu; X. Meng; Z. Qin,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9513090,10.1109/AEMCSE51986.2021.00214,"As deep neural networks have been performing better and better on various tasks, their number of parameters has been increasing, and the demand for computing power and storage has been increasing. In 2016, Joseph Redmon et al. proposed a one-stage target detection method: You Only Look Once [1], which has been widely used worldwide in the past 5 years. However, such a scale network is not possible to be directly applied on mobile devices nowadays [2]. In order to reduce the network model, reduce energy consumption and improve the computational speed, we consider both network pruning and weight quantization for five types of large target detection datasets, prune the YOLOv3-tiny network to 16 layers and quantize it to 5 bits and successfully run it on FPGA, and the mAP drops about 5%.",YOLO;quantization,"1047, 1050",,IEEE Conferences,"2021 4th International Conference on Advanced Electronic Materials, Computers and Software Engineering (AEMCSE)",IEEE
125,,Image super-resolution method based on generative adversarial network,Y. Lijun; Z. Xiaoming; L. Fan; S. Gang; C. Zhou; Y. Jing; Z. Min; C. Yongchang; W. Lingling; C. Zelong; P. Lan; B. Fengqing; Y. Zifang; X. Hongqiu; L. Pengjian; L. Zhisheng; T. Qiang,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9513030,10.1109/AEMCSE51986.2021.00185,"Although deep convolutional neural networks have made breakthroughs in the accuracy and speed of single-image super-resolution, there are still many unsolved problems: firstly, How to refine the texture problem when performing super-resolution processing at a larger magnification ratio. Secondly, the existing convolutional neural network image super-resolution algorithms are prone to overfitting and insufficient convergence of the loss function. Aiming at two problems, an image super-resolution method based on generative adversarial network is proposed. The feature map is spatially transformed on the network to solve the problem of refined texture, combined with CycleGAN and SRGAN, the network structure is improved and the loss function is optimized, and the SRCICGAN algorithm is proposed to restore the four times down-sampled image to solve the loss function problem. The experiment is compared with the latest six methods on three data sets. The PSNR and SSIM indicators are 1.92% and 5.49% higher in the Flickr2K data set, respectively, and better visual effects can be obtained in terms of detailed texture.",SRCICGAN;ESFT method;ASSP semantic segmentation probability map;generating adversarial networks,"909, 915",,IEEE Conferences,"2021 4th International Conference on Advanced Electronic Materials, Computers and Software Engineering (AEMCSE)",IEEE
126,,Segmentation of Hippocampus based on 3DUnet-CBAM Model,S. Li; L. Zhao; J. Li; Q. Chen,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9513188,10.1109/AEMCSE51986.2021.00125,"The assessment of the hippocampus is indispensable for the supplementary diagnosis of Alzheimer's disease. In response to the small size of the hippocampus and the difficulty of accurate segmentation, this paper proposes a 3DUnet-CBAM based hippocampal segmentation model. A 3DUnet convolutional network is used to avoid information loss between two-dimensional slices, and a CBAM attention mechanism is incorporated in the last layer of downsampling. The deep and superficial features of the hippocampus are effectively combined. The segmentation accuracy of the hippocampus is finally improved. Experimental results show that the model achieves 89.01%, 89.04% and 88.97% for dice, precision and recall under the validation set, respectively, which can better assist doctors in diagnosing Alzheimer's disease.",Component;Alzheimer's disease;hippocampus;3DUnet;CBAM,"595, 599",,IEEE Conferences,"2021 4th International Conference on Advanced Electronic Materials, Computers and Software Engineering (AEMCSE)",IEEE
127,,Research on Steganalysis of Digital Image Based on Deep Learning,P. Li; Y. Li; H. Wang; C. Liu,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9513018,10.1109/AEMCSE51986.2021.00114,"Steganography is a technique and science of information hiding. Corresponding to the development of image steganography, steganalysis has made substantial progress. In recent years, deep learning is widely used in the fields of computer vision and natural language processing. With people's in-depth understanding of deep learning, convolutional neural network technology has been introduced into the field of steganography and steganalysis, and has made a series of breakthroughs and development in methods and performance. Under this background, this paper summarizes and discusses the main methods and the most representative work of how to apply convolutional neural network model of deep learning to steganalysis of digital images. Finally, it discusses the problems and future development of steganalysis and steganalysis of digital image based on deep learning.",information hiding;Steganalysis;Convolutional neural network;Deep learning,"528, 534",,IEEE Conferences,"2021 4th International Conference on Advanced Electronic Materials, Computers and Software Engineering (AEMCSE)",IEEE
128,,Power system distributed state estimation method based on transfer learning under sparse data,Q. Mu; J. Qian,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9513163,10.1109/AEMCSE51986.2021.00053,"With the development of smart grids and the continuous penetration of new energy power generation, the scale of the power grid has increased, and the structure and operation methods have become increasingly complex. In order to more accurately grasp the operating status of the power grid, it is necessary to estimate the operating status of the power system. In recent years, with the development of artificial intelligence and data mining technology, neural networks have also been widely used in the field of state estimation. However, there are few researches on artificial intelligence algorithms in active distribution network state estimation, and the small amount of data in the bad data detection link that needs to be carried out before state estimation also limits the application of many artificial intelligence algorithms that require a large number of training samples. A small sample learning method applied to the detection of bad data for power system state estimation is proposed, which successfully solves the problem of the lack of available training data in building a neural network model for bad data detection, and solves the traditional method calculation speed brought by the massive increase of grid data. The problem of serious decline, and the accuracy of the traditional state estimation method is improved by more than 47%.",transfer learning;convolutional neural network;power system;bad data detection;state estimation,"221, 224",,IEEE Conferences,"2021 4th International Conference on Advanced Electronic Materials, Computers and Software Engineering (AEMCSE)",IEEE
129,,Summary of Research on News Text Classification,L. Deping; W. Hongjuan,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9513135,10.1109/AEMCSE51986.2021.00224,"With the rapid development of modern technology and the rapid spread of network information, the way people obtain news information has gradually shifted from traditional paper documents to electronic text. How to efficiently classify these electronic news texts has become a current research hotspot. In this paper, we introduce the concept of text classification and its main process, then summarize the existing classification methods and challenges now, as well as forecast the future trends.",news text classification;machine learning;graph neural network,"1097, 1100",,IEEE Conferences,"2021 4th International Conference on Advanced Electronic Materials, Computers and Software Engineering (AEMCSE)",IEEE
130,,An Interleaved Model for Chinese Socio-Economic Indicator Extraction,L. Guo; Z. Zhang; X. Zhang; Y. Tian; B. Yang,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9513116,10.1109/AEMCSE51986.2021.00115,"Automatically extracting socio-economic indicators from text allows us to know the real status of a country or a company. Most conventional methods rely on hand-crafted features or use pipeline or joint deep learning model. They are either effort intensive or do not utilize the dependency between indicator detection and element extraction. In this paper, we use an interleaved model to cope with the challenges in this task. After feature extraction, the input sequence is first fed into the preliminary detection module to test whether it mentions indicators. If the sequence contains indicator mentions, it will then be input into element extraction module to extract indicator elements. If there are no elements, the sequence will be judged containing no indicators. We conduct extensive experiments on the dataset we build, which shows that the proposed model performs better both on f1-score and efficiency.",indicator extraction;interleaved model;sequence labeling;CRF,"535, 540",,IEEE Conferences,"2021 4th International Conference on Advanced Electronic Materials, Computers and Software Engineering (AEMCSE)",IEEE
131,,Some Improved Strategies of YOLOv3 Algorithm,X. Chen; S. Tang,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9512881,10.1109/AEMCSE51986.2021.00104,"To enhance the representational ability of object detection network and make it learn features more comprehensively, thereby improving the accuracy of algorithm, some improved strategies are proposed. Based on widely used YOLOv3, the spatial pyramid pooling is embedded to strengthen the ability of feature extraction in the local area, and the integrated dilated convolution and dual attention are used to improve the quality of feature expression. Additionally, the regression loss is calculated by CIoU to improve the localization effect of bounding boxes. Experimental results on the MS COCO (test-dev 2017) dataset show that the mAP of improved model increases by 8.8%, while the detection speed of algorithm is maintained at the original level.",object detection;convolution;upsampling;feature fusion,"479, 482",,IEEE Conferences,"2021 4th International Conference on Advanced Electronic Materials, Computers and Software Engineering (AEMCSE)",IEEE
132,,Albert-based sentiment analysis of movie review,Z. Ding; Y. Qi; D. Lin,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9513054,10.1109/AEMCSE51986.2021.00254,"Movie reviews include the real evaluation of the movie by the public. Through these reviews, the audience can better judge whether the movie is worth watching. However, as the amount of data on movie reviews continues to grow, it takes a lot of manpower and material resources to manually analyze the emotional tendency of each movie review. As an important research field of machine learning, sentiment analysis focuses on extracting topic information from text reviews. The field of sentiment analysis is closely related to natural language processing and text mining. It can be successfully used to determine the reviewer's attitude towards various topics or the overall polarity of the review. As far as movie reviews are concerned, in addition to scoring movies digitally, they can also quantitatively enlighten us on the advantages and disadvantages of watching movies. This article uses the Albert model to build a classifier, and uses the ""movie review dataset"" issued by Stanford University for network training. Experiments show that the trained Albert model can reach an accuracy of 89.05% when performing sentiment analysis of movie reviews. Compared with the traditional LSTM and GRU, the accuracy of the Albert model is improved by 3%.",component;machine learning;natural language processing;sentiment analysis,"1243, 1246",,IEEE Conferences,"2021 4th International Conference on Advanced Electronic Materials, Computers and Software Engineering (AEMCSE)",IEEE
133,,An efficient method for cross-subject EEG-based mental fatigue recognition,K. Zhang; W. Tang,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9513028,10.1109/AEMCSE51986.2021.00044,"Mental fatigue has an unignorable impact on people's daily life and work. Mental fatigue recognition methods based on EEG are commonly thought as objective standard. However, the procedure of initiating fatigue needs a long time and EEG signals vary greatly, so mental fatigue recognition based on EEG is challenging when subject-specific data are limited and imbalanced. In this paper, we explored the performance of cross-subject fatigue recognition on a general multitask learning framework with two data sampling methods for imbalanced classification. One was to synthesize some minority samples (MTLSMS) until the two classes were balanced and another was to under-sample the majority samples (MTLUMS). EEG data of 11 subjects from a public EEG fatigue dataset were selected to validate our fatigue recognition methods. The MTLSMS method improved the cross-subject mental fatigue recognition accuracies to 81.07%, and much better than other transfer learning domain adaptation methods Transfer Component Analysis (TCA) and Maximum Independence Domain Adaptation (MIDA). The experiment results showed that the MTLSMS method can effectively recognize the cross-subject mental fatigue.",EEG;transfer learning;cross-subject mental fatigue recognition;multitask learning;class imbalances,"176, 181",,IEEE Conferences,"2021 4th International Conference on Advanced Electronic Materials, Computers and Software Engineering (AEMCSE)",IEEE
134,,Generating Functional Requirements Based on Classification of Mobile Application User Reviews,T. Panthum; T. Senivongse,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9509277,10.1109/SERA51205.2021.9509277,"User reviews are important resources for mobile developers for maintaining and evolving mobile applications that have been released. Since there can be a lot of user reviews, it is cumbersome for the mobile development team to go through all user reviews to identify which ones contain useful information for further maintenance and evolution. This paper proposes an initial attempt to facilitate the maintenance and evolution process of a mobile development team by automating the generation of functional requirements from mobile application user reviews on the App Store and Play Store. Specifically, these functional requirements serve as change requirements for maintaining existing functions of the application or enhancing the application with new functions. The proposed approach is based on machine learning and natural language processing and consists of three steps. Firstly, user reviews that identify functional requirements are classified by text classification algorithms. Secondly, since some user reviews may address the same issues, distinct user reviews are identified by clustering techniques and text similarity. Finally, relevant information is extracted from the user reviews to generate functional requirements based on user review patterns and requirement boilerplates. In an evaluation, the generated functional requirements obtained moderate to high scores in terms of readability, unambiguity, completeness, and validity. The proposed approach can help the development team identify functional requirements from direct feedback of the users which should be considered and further refined in the maintenance and evolution of the mobile application.",software requirement generation;functional requirements;user reviews;text classification;clustering;text similarity;requirement boilerplates;machine learning;natural language processing,"15, 20",,IEEE Conferences,"2021 IEEE/ACIS 19th International Conference on Software Engineering Research, Management and Applications (SERA)",IEEE
135,,Topic Modeling Enhancement using Word Embeddings,S. Limwattana; S. Prom-on,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9493816,10.1109/JCSSE53117.2021.9493816,"Latent Dirichlet Allocation(LDA) is one of the powerful techniques in extracting topics from a document. The original LDA takes the Bag-of-Word representation as the input and produces topic distributions in documents as output. The drawback of Bag-of-Word is that it represents each word with a plain one-hot encoding which does not encode the word level information. Later research in Natural Language Processing(NLP) demonstrate that word embeddings technique such as Skipgram model can provide a good representation in capturing the relationship and semantic information between words. In recent studies, many NLP tasks could gain better performance by applying the word embedding as the representation of words. In this paper, we propose Deep Word-Topic Latent Dirichlet Allocation(DWT-LDA), a new process for training LDA with word embedding. A neural network with word embedding is applied to the Collapsed Gibbs Sampling process as another choice for word topic assignment. To quantitatively evaluate our model, the topic coherence framework and topic diversity are the metrics used to compare between our approach and the original LDA. The experimental result shows that our method generates more coherent and diverse topics.",Topic Modeling;Latent Dirichlet Allocation;Word Embedding,"1, 5",,IEEE Conferences,2021 18th International Joint Conference on Computer Science and Software Engineering (JCSSE),IEEE
136,,Detecting Facial Images In Public With And Without Masks Using VGG And FR-TSVM Models,H. Wang; C. Lursinsap,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9493848,10.1109/JCSSE53117.2021.9493848,"Since 2019, Covid-19 has become a common problem affecting all mankind. The disease has successfully spread all over the world. Wearing a mask can practically protect the infection. Thus, detecting people wearing and not wearing masks in public is essential. However, there is still some room to improve detection accuracy of the present methods. In this paper, the transfer learning model and FR-TSVM model are used to study the latest data of pneumonia epidemic situation in Covid-19. First, a data set of 12,000 facial images wearing masks and not wearing masks in public was collected for training, testing, and validation. The pictures will be put into the improved VGG model. Then the structure of VGG model was used to extract the features of images. These features were trained by FR-TSVM with fuzzy concept included. This approach can achieve 95.5% accuracy, and it is also higher than the detection results of other methods.",VGG-16;COVID-19;TSVM;FR-TSVM;Wear Mask;Public Places,"1, 5",,IEEE Conferences,2021 18th International Joint Conference on Computer Science and Software Engineering (JCSSE),IEEE
137,,Combination Ultrasound and Mammography for Breast Cancer Classification using Deep Learning,O. Chunhapran; T. Yampaka,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9493840,10.1109/JCSSE53117.2021.9493840,"The most widely used methods for early detection of breast cancer are Ultrasound and Mammography. However, single ultrasound or single mammography shows false classification that causes unnecessary biopsy. Therefore, the combination approach is proposed to improve breast cancer classification using the deep learning technique. The proposed method has been divided into two steps. First, images are randomly combined using the k-combination method. Second, deep learning based on MobileNet is used to classify breast tumors. The result demonstrated that the combination approach produces a variety of patterns and a large image dataset and improves the accuracy. In addition, the false positive tend to reduce by 13% and the false negative tend to reduce by 14%. It is useful to avoid unnecessary surgery and to plan aggressive treatment.",breast cancer classification;combination imae;breast ultrasound;breast mammography,"1, 6",,IEEE Conferences,2021 18th International Joint Conference on Computer Science and Software Engineering (JCSSE),IEEE
138,,Smart Inventory Access Monitoring System (SIAMS) using Embedded System with Face Recognition,K. Eiamsaard; P. Bamrungthai; S. Jitpakdeebodin,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9493815,10.1109/JCSSE53117.2021.9493815,"In this paper, we present a system called Smart Inventory Access Monitoring System (SIAMS) that integrates an embedded system with face recognition into an inventory system. It is developed to prevent theft in warehouses from authorized staff. The embedded system is attached with an RGB camera and deployed three software modules: image capturing, face detection, and face recognition. The face detection module sends detected face images to the face recognition module to identify a person as the person's name or unknown class using a deep learning approach. The system achieved competitive accuracy by performing standard evaluation metrics for face detection and recognition. The inventory system that was developed will receive data via TCP/IP socket communication to log access history. The retrieved information can be used to investigate an unusual situation. The system can be improved with object detection and person tracking system to detect theft in real-time.",Face recognition;Face detection;Embedded system;Inventory management system,"1, 4",,IEEE Conferences,2021 18th International Joint Conference on Computer Science and Software Engineering (JCSSE),IEEE
139,,COVID-19 Classification using DCNNs and Exploration Correlation using Canonical Correlation Analysis,R. Jullapak; T. Yampaka,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9493846,10.1109/JCSSE53117.2021.9493846,"Coronavirus disease (COVID-19) has rapidly spread among people living in many countries. Chest radiography (CXR) image is an alternative diagnosis option to observe COVID-19. However, CXR usually requires an expert radiologist to distinguish the lesion from viral pneumonia and COVID-19 because the symptoms of COVID-19 pneumonia may be similar to other types of viral pneumonia. In this study, three different convolutional neural network based models (VGG19, ResNet50, and InceptionV3) have been proposed for the detection of coronavirus pneumonia infected patient using chest X-ray. In addition, this studies can potentially find the correlation between COVID-19 pneumonia and viral pneumonia using canonical correlation analysis. Considering the performance results obtained the best performance as an accuracy of 0.97, sensitivity of 0.97, specificity of 0.93, and F1-score value of 0.97 for VGG19 pre-trained model. The experiment results also show that the viral lesion of Viral pneumonia and COVID-19 is less similarity.",COVID-19 classification;deep convolution neuron networks;canonical correlation analysis,"1, 6",,IEEE Conferences,2021 18th International Joint Conference on Computer Science and Software Engineering (JCSSE),IEEE
140,,Towards robust Machine Learning models for grape ripeness assessment,V. Gomes; P. Melo-Pinto,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9493822,10.1109/JCSSE53117.2021.9493822,"Artificial intelligence methods need to be more transparent for wider acceptance by the industry. In particular deep neural networks (DNN) are not explainable, due to the complex processes the input undergo. The present work addresses model explainability for wine grapes quality assessment through 1D-CNN, using regression activation maps (RAM) to show the contribution score of each wavelength for the prediction of sugar content. This way we identify the relevant regions related to this enological parameter. The results obtained indicate that the proposed approach can successfully highlight important spectral regions related to sugars absorption, improving the current state of the art, and opening way to dimensionality reduction methods and further model interpretation.",One-dimensional convolutional neural networks;model explainability;hyperspectral imaging;grape berries;sugar content;regression activation maps,"1, 5",,IEEE Conferences,2021 18th International Joint Conference on Computer Science and Software Engineering (JCSSE),IEEE
141,,Classification of Abusive Thai Language Content in Social Media Using Deep Learning,R. Wanasukapunt; S. Phimoltares,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9493829,10.1109/JCSSE53117.2021.9493829,"This paper presents binomial and multinomial models for Thai language abusive speech classification in social media. While previous similar research focused on using traditional machine learning models for binomial classification, we showed that deep learning models have better performance. Our binomial and multinomial models achieved F1 scores of 0.8510 and 0.9067, respectively. These scores were significantly better than the machine learning models' respective best F1 scores of 0.7452 and 0.8090. While the bidirectional LSTM performed well, the DistilBERT had higher accuracy and recall. Moreover, the recall was especially higher for the “figurative” class where certain words were more likely to have different meanings depending on context.",Abusive language detection;Thai natural language processing;Large scale social networks),"1, 6",,IEEE Conferences,2021 18th International Joint Conference on Computer Science and Software Engineering (JCSSE),IEEE
142,,Efficient User-Generated Video Quality Prediction,Z. Tu; C. -J. Chen; Y. Wang; N. Birkbeck; B. Adsumilli; A. C. Bovik,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9477483,10.1109/PCS50896.2021.9477483,"Blind video quality assessment of user-generated content (UGC) has become a trending, challenging, unsolved problem. Accurate and efficient video quality predictors suitable for this content are thus in great demand to achieve intelligent analysis and processing of UGC videos. However, previous video quality models are either incapable or inefficient for predicting the quality of complex, diverse UGC videos in practical applications. Here we introduce an effective and efficient video quality model for UGC content, which we dub the Rapid and Accurate Video Quality Evaluator (RAPIQUE), which we show performs comparably to state-of-the-art models but with orders-of-magnitude faster runtime. Our experimental results on recent large-scale UGC video quality databases show that RAPIQUE delivers top performances on all datasets at a considerably lower computational expense. An implementation of RAPIQUE is online: https://github.com/vztu/RAPIQUE.",Video quality assessment;scene statistics;temporal;perceptual quality;user-generated content,"1, 5",,IEEE Conferences,2021 Picture Coding Symposium (PCS),IEEE
143,,Multitask Learning for VVC Quality Enhancement and Super-Resolution,C. Bonnineau; W. Hamidouche; J. -F. Travers; N. Sidaty; O. Deforges,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9477492,10.1109/PCS50896.2021.9477492,"The latest video coding standard, called versatile video coding (VVC), includes several novel and refined coding tools at different levels of the coding chain. These tools bring significant coding gains with respect to the previous standard, high efficiency video coding (HEVC). However, the encoder may still introduce visible coding artifacts, mainly caused by coding decisions applied to adjust the bitrate to the available bandwidth. Hence, pre and post-processing techniques are generally added to the coding pipeline to improve the quality of the decoded video. These methods have recently shown outstanding results compared to traditional approaches, thanks to the recent advances in deep learning. Generally, multiple neural networks are trained independently to perform different tasks, thus omitting to benefit from the redundancy that exists between the models. In this paper, we investigate a learning-based solution as a post-processing step to enhance the decoded VVC video quality. Our method relies on multitask learning to perform both quality enhancement and super-resolution using a single shared network optimized for multiple degradation levels. The proposed solution enables a good performance in both mitigating coding artifacts and super-resolution with fewer network parameters compared to traditional specialized architectures.",VVC;Neural Networks;Multitask Learning;Super-Resolution;Quality Enhancement,"1, 5",,IEEE Conferences,2021 Picture Coding Symposium (PCS),IEEE
144,,Model Selection CNN-based VVC Quality Enhancement,F. Nasiri; W. Hamidouche; L. Morin; N. Dhollande; G. Cocherel,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9477473,10.1109/PCS50896.2021.9477473,"Artifact removal and filtering methods are inevitable parts of video coding. On one hand, new codecs and compression standards come with advanced in-loop filters and on the other hand, displays are equipped with high capacity processing units for post-treatment of decoded videos. This paper proposes a Convolutional Neural Network (CNN)-based post-processing algorithm for intra and inter frames of Versatile Video Coding (VVC) coded streams. Depending on the frame type, this method benefits from normative prediction signal by feeding it as an additional input along with reconstructed signal and a Quantization Parameter (QP)-map to the CNN. Moreover, an optional Model Selection (MS) strategy is adopted to pick the best trained model among available ones at the encoder side, and signal it to the decoder side. This MS strategy is applicable at both frame level and block level. The experiments under the Random Access (RA) configuration of the VVC Test Model (VTM-10.0) show that the proposed prediction-aware algorithm can bring an additional BD-BR gain of -1.3% compared to the method without the prediction information. Furthermore, the proposed MS scheme brings -0.5% more BD-BR gain on top of the prediction-aware method.",Quality Enhancement;VVC;Model selection,"1, 5",,IEEE Conferences,2021 Picture Coding Symposium (PCS),IEEE
145,,Sparse Coding-based Intra Prediction in VVC,J. Schneider; D. Mehlem; M. Meyer; C. Rohlfing,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9477474,10.1109/PCS50896.2021.9477474,"Intra prediction is crucial to video coding as it is the only option for prediction when motion compensation either fails or no reference frames are available. Hence, current state-of-the-art video coding standards utilize numerous intra prediction modes in order to predict different angled structures as well as smooth areas. This contribution introduces an additional mode for intra prediction which is based on the concepts of Dictionary Learning (DL), Sparse Coding (SC) [1] and adjusted Anchored Neighborhood Regression (ANR) [2] to be able to adapt to more arbitrary structures. The general idea is built on trained dictionaries, which sparsely represent the reference area of a block to be predicted. Alongside learning the dictionaries, linear projection matrices, projecting the reference areas to the corresponding blocks, are trained with ANR. For the actual intra prediction step, each given reference area is then projected onto the to-be-predicted block by multiple linear projections, which are blended according to the sparse codes representing the reference area. Experimentally, offering the proposed mode to the state-of-the-art video coding standard Versatile Video Coding (VVC) outperforms the traditional VVC modes: In particular, -0.26% BD-rate gains in comparison to the VVC reference software VTM-9.3 and a usage percentage of 12.83% can be achieved on average for the All Intra (AI) coding configuration. Furthermore, a peak coding gain of -0.6% and a usage percentage of 26.66% is observed for the same setup.",video compression;intra prediction;dictionary learning;versatile video coding,"1, 5",,IEEE Conferences,2021 Picture Coding Symposium (PCS),IEEE
146,,Texture-aware Video Frame Interpolation,D. Danier; D. Bull,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9477504,10.1109/PCS50896.2021.9477504,"Temporal interpolation has the potential to be a powerful tool for video compression. Existing methods for frame interpolation do not discriminate between video textures and generally invoke a single general model capable of interpolating a wide range of video content. However, past work on video texture analysis and synthesis has shown that different textures exhibit vastly different motion characteristics and they can be divided into three classes (static, dynamic continuous and dynamic discrete). In this work, we study the impact of video textures on video frame interpolation, and propose a novel framework where, given an interpolation algorithm, separate models are trained on different textures. Our study shows that video texture has significant impact on the performance of frame interpolation models and it is beneficial to have separate models specifically adapted to these texture classes, instead of training a single model that tries to learn generic motion. Our results demonstrate that models fine-tuned using our framework achieve, on average, a 0.3dB gain in PSNR on the test set used.",,"1, 5",,IEEE Conferences,2021 Picture Coding Symposium (PCS),IEEE
147,,A Practical Approach for Rate-Distortion-Perception Analysis in Learned Image Compression,O. Kirmemis; A. M. Tekalp,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9477479,10.1109/PCS50896.2021.9477479,"Rate-distortion optimization (RDO) of codecs, where distortion is quantified by the mean-square error, has been a standard practice in image/video compression over the years. RDO serves well for optimization of codec performance for evaluation of the results in terms of PSNR. However, it is well known that the PSNR does not correlate well with perceptual evaluation of images; hence, RDO is not well suited for perceptual optimization of codecs. Recently, rate-distortion-perception trade-off has been formalized by taking the Kullback-Leibler (KL) divergence between the distributions of the original and reconstructed images as a perception measure. Learned image compression methods that simultaneously optimize rate, mean-square loss, VGG loss, and an adversarial loss were proposed. Yet, there exists no easy approach to fix the rate, distortion or perception at a desired level in a practical learned image compression solution to perform an analysis of the trade-off between rate, distortion and perception measures. In this paper, we propose a practical approach to fix the rate to carry out perception-distortion analysis at a fixed rate in order to perform perceptual evaluation of image compression results in a principled manner. Experimental results provide several insights for practical rate-distortion-perception analysis in learned image compression.",learned image compression;PSNR;learned entropy models;perceptual quality evaluation;rate-distortion-perception optimization,"1, 5",,IEEE Conferences,2021 Picture Coding Symposium (PCS),IEEE
148,,Exploring Plausible Patches Using Source Code Embeddings in JavaScript,V. Csuvik; D. Horváth; M. Lajkó; L. Vidács,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9474540,10.1109/APR52552.2021.00010,"Despite the immense popularity of the Automated Program Repair (APR) field, the question of patch validation is still open. Most of the present-day approaches follow the so-called Generate-and-Validate approach, where first a candidate solution is being generated and after validated against an oracle. The latter, however, might not give a reliable result, because of the imperfections in such oracles; one of which is usually the test suite. Although (re-) running the test suite is right under one's nose, in real life applications the problem of over- and underfitting often occurs, resulting in inadequate patches. Efforts that have been made to tackle with this problem include patch filtering, test suite expansion, careful patch producing and many more. Most approaches to date use post-filtering relying either on test execution traces or make use of some similarity concept measured on the generated patches. Our goal is to investigate the nature of these similarity-based approaches. To do so, we trained a Doc2Vec model on an open-source JavaScript project and generated 465 patches for 10 bugs in it. These plausible patches alongside with the developer fix are then ranked based on their similarity to the original program. We analyzed these similarity lists and found that plain document embeddings may lead to misclassification - it fails to capture nuanced code semantics. Nevertheless, in some cases it also provided useful information, thus helping to better understand the area of Automated Program Repair.",Automatic Program Repair;Patch Correctness;Code Embeddings;Doc2vec;Machine learning,"11, 18",,IEEE Conferences,2021 IEEE/ACM International Workshop on Automated Program Repair (APR),IEEE
149,,Inferred Interactive Controls Through Provenance Tracking of ROS Message Data,T. Witte; M. Tichy,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9474532,10.1109/RoSE52553.2021.00018,"Interactive controls that enrich visualizations need domain knowledge to create a sensible visual representation, as well as access to parameters and data to manipulate. However, source data and the means to visualize them are often scattered across multiple components, making it hard to link a value change in the interface to the appropriate source data. Provenance, the documentation of the origin and history of message data, can be used to reverse the evaluation of a value and change it at its source. We present a communication pattern as well as a C++ support library for ROS to track the provenance of message data across multiple nodes and apply source changes, reversing any transformation on the tracked data. We demonstrate that it is possible to automatically infer interactive 3D user interfaces from standard, non-interactive ROS visualizations by leveraging this additional tracking information. Preliminary results from a prototypical implementation of multiple origin tracking enabled ROS nodes indicate, that this tracking introduces a significant but still practicable message size and serialization performance overhead. To apply this tracking to existing C++ codebases only small, syntactic changes are necessary: a wrapper type around tracked values hides all necessary bookkeeping.",data provenance;ROS;source location tracking;bidirectional evaluation;interactive markers,"67, 74",,IEEE Conferences,2021 IEEE/ACM 3rd International Workshop on Robotics Software Engineering (RoSE),IEEE
150,,Emotions in Computer Vision Service Q&A,A. Cummaudo; U. M. Graetsch; M. K. Curumsing; R. Vasa; S. Barnett; J. Grundy,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9474725,10.1109/SEmotion52567.2021.00011,"Software developers are increasingly using cloud-based services that provide machine learning capabilities to implement `intelligent' features. Studies show that incorporating machine learning into an application increases technical debt, creates data dependencies, and introduces uncertainty due to their non-deterministic behaviour. We know very little about the emotional state of software developers who have to deal with such issues; and the impacts on productivity. This paper presents a preliminary effort to better understand the emotions of developers when experiencing issues with these services with the wider goal of discovering potential service improvements. We conducted a landscape analysis of emotions found in 1,425 Stack Overflow questions about a specific and mature subset of these cloud-based services, namely those that provide computer vision techniques. To speed up the emotion identification process, we trialled an automatic approach using a pre-trained emotion classifier that was specifically trained on Stack Overflow content, EmoTxt, and manually verified its classification results. We found that the identified emotions vary for different types of questions, and a discrepancy exists between automatic and manual emotion analysis due to subjectivity.",emotion mining;stack overflow;DevX;computer vision services;empirical study,"13, 18",,IEEE Conferences,2021 IEEE/ACM Sixth International Workshop on Emotion Awareness in Software Engineering (SEmotion),IEEE
151,,Engineering an Intelligent Essay Scoring and Feedback System: An Experience Report,A. Chadda; K. Song; R. Chandrasekar; I. Gorton,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9474379,10.1109/WAIN52551.2021.00029,"Artificial Intelligence (AI)/Machine Learning (ML)-based systems are widely sought-after commercial solutions that can automate and augment core business services. Intelligent systems can improve the quality of services offered and support scalability through automation. In this paper we describe our experience in engineering an exploratory system for assessing the quality of essays supplied by customers of a specialized recruitment support service. The problem domain is challenging because the open-ended customer-supplied source text has considerable scope for ambiguity and error, making models for analysis hard to build. There is also a need to incorporate specialized business domain knowledge into the intelligent processing systems. To address these challenges, we experimented with and exploited a number of cloud-based machine learning models and composed them into an application-specific processing pipeline. This design allows for modification of the underlying algorithms as more data and improved techniques become available. We describe our design, and the main challenges we faced, namely keeping a check on the quality control of the models, testing the software and deploying the computationally expensive ML models on the cloud.",software engineering;machine learning;software architecture;cloud-based systems;essay grading,"141, 144",,IEEE Conferences,2021 IEEE/ACM 1st Workshop on AI Engineering - Software Engineering for AI (WAIN),IEEE
152,,Concepts in Testing of Autonomous Systems: Academic Literature and Industry Practice,Q. Song; E. Engström; P. Runeson,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9474374,10.1109/WAIN52551.2021.00018,"Testing of autonomous systems is extremely important as many of them are both safety-critical and security-critical. The architecture and mechanism of such systems are fundamentally different from traditional control software, which appears to operate in more structured environments and are explicitly instructed according to the system design and implementation. To gain a better understanding of autonomous systems practice and facilitate research on testing of such systems, we conducted an exploratory study by synthesizing academic literature with a focus group discussion and interviews with industry practitioners. Based on thematic analysis of the data, we provide a conceptualization of autonomous systems, classifications of challenges and current practices as well as of available techniques and approaches for testing of autonomous systems. Our findings also indicate that more research efforts are required for testing of autonomous systems to improve both the quality and safety aspects of such systems.",Software testing;autonomous system,"74, 81",,IEEE Conferences,2021 IEEE/ACM 1st Workshop on AI Engineering - Software Engineering for AI (WAIN),IEEE
153,,Integration of Convolutional Neural Networks in Mobile Applications,R. C. Castanyer; S. Martínez-Fernández; X. Franch,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9474372,10.1109/WAIN52551.2021.00010,"When building Deep Learning (DL) models, data scientists and software engineers manage the trade-off between their accuracy, or any other suitable success criteria, and their complexity. In an environment with high computational power, a common practice is making the models go deeper by designing more sophisticated architectures. However, in the context of mobile devices, which possess less computational power, keeping complexity under control is a must. In this paper, we study the performance of a system that integrates a DL model as a trade-off between the accuracy and the complexity. At the same time, we relate the complexity to the efficiency of the system. With this, we present a practical study that aims to explore the challenges met when optimizing the performance of DL models becomes a requirement. Concretely, we aim to identify: (i) the most concerning challenges when deploying DL-based software in mobile applications; and (ii) the path for optimizing the performance trade-off. We obtain results that verify many of the identified challenges in the related work such as the availability of frameworks and the software-data dependency. We provide a documentation of our experience when facing the identified challenges together with the discussion of possible solutions to them. Additionally, we implement a solution to the sustainability of the DL models when deployed in order to reduce the severity of other identified challenges. Moreover, we relate the performance trade-off to a new defined challenge featuring the impact of the complexity in the obtained accuracy. Finally, we discuss and motivate future work that aims to provide solutions to the more open challenges found.",DL-based software;Convolutional Neural Networks;Software Engineering;AI Model Development Process;DevOps,"27, 34",,IEEE Conferences,2021 IEEE/ACM 1st Workshop on AI Engineering - Software Engineering for AI (WAIN),IEEE
154,,Investigation of Coronavirus impact on Blockchain and cryptocurrencies markets,S. Vacca; C. L. Costerbosa; A. Spada; G. Riotta; N. Uras,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9474802,10.1109/WETSEB52558.2021.00015,"Coronavirus had a tremendous global economic impact due to mistrust and reduced consumption. Especially in China, the country from which the infection started and which presents more than 90,000 infections on March 4th, the global economy has been experiencing the first sharp economic contraction since 1970. On February 24 th 2020, the Dow Jones Industrial Average lost 1800 points. Blockchain development communities designed and launch a new cryptocurrency called CoronaCoin. This research aims to study and identify whether users' mood and sentiment may affect CoronaCoin price. For this reason, two official CoronaCoin communities of two popular social media platforms, i.e. Telegram and Reddit, were analysed. Through the novel usage of the Hawkes models, we found that users of these social media platforms may affect Coronacoin's behaviour based on their sentiment, with significant price changes.",Cryptocurrency;Blockchain;Software Engineering;Human factor;Hawkes Models;COVID-19;Finance;Sentiment Analysis,"56, 60",,IEEE Conferences,2021 IEEE/ACM 4th International Workshop on Emerging Trends in Software Engineering for Blockchain (WETSEB),IEEE
155,,Towards a question answering assistant for software development using a transformer-based language model,L. do Nascimento Vale; M. de Almeida Maia,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9474381,10.1109/BotSE52550.2021.00016,"Question answering platforms, such as Stack Overflow, have impacted substantially how developers search for solutions for their programming problems. The crowd knowledge content available from such platforms has also been used to leverage software development tools. The recent advances on Natural Language Processing, specifically on more powerful language models, have demonstrated ability to enhance text understanding and generation. In this context, we aim at investigating the factors that can influence on the application of such models for understanding source code related data and produce more interactive and intelligent assistants for software development. In this preliminary study, we particularly investigate if a how-to question filter and the level of context in the question may impact the results of a question answering transformer-based model. We suggest that fine-tuning models with corpus based on how-to questions can impact positively in the model and more contextualized questions also induce more objective answers.",GPT 2 language model;Question and answer (Q&A);Stack Overflow posts,"39, 42",,IEEE Conferences,2021 IEEE/ACM Third International Workshop on Bots in Software Engineering (BotSE),IEEE
156,,Automatic Classification and Entity Relation Detection in Hungarian Spinal MRI Reports,A. Kicsi; K. S. Ledenyi; P. Pusztai; L. Vidács,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9470892,10.1109/SEH52539.2021.00010,"A great number of radiologic reports are created each year which incorporate the expertise of radiologists. This knowledge could be exploited via machine understanding. This could provide valuable statistics and visualization of the reports, and as training data, and it could also contribute to later automatic reporting applications. In our current work, we present our first steps toward the machine understanding of clinical reports of the spinal region, written in the Hungarian language. Our system provides an automatic classification and connection detection for various entities in the text. Our classification is achieved via bi-directional long short-term memory and conditional random fields producing 0.87-0.95 F1-score values, while the extraction of connection relies on linguistic analysis and predefined rules. The extracted information is displayed in an easily comprehensible, well-formed tree-structure.",nlp;radiology;machine understanding;visualization;bi-lstm-crf;clinical reports,"13, 19",,IEEE Conferences,2021 IEEE/ACM 3rd International Workshop on Software Engineering for Healthcare (SEH),IEEE
157,,BinDiff<sub>NN</sub>: Learning Distributed Representation of Assembly for Robust Binary Diffing against Semantic Differences,S. Ullah; H. Oh,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9470904,10.1109/TSE.2021.3093926,"Binary diffing is a process to discover the patched content between two binary programs. Previous research on binary diffing approach it as a function matching problem to formulate an initial 1:1 mapping between functions, and later a sequence matching ratio is computed to classify two functions being an exact match, a partial match or no-match. The accuracy of existing techniques is best only when detecting exact matches and they are not efficient in detecting partially changed functions; especially those with minor patch. These drawbacks are due to two major challenges (i) In 1:1 mapping phase, using a strict policy to match function features (ii) In classification phase, considering an assembly snippet as a normal text and using sequence matching for similarity comparison. An instruction has a unique structure i.e. mnemonics and registers have specific position in an instruction and also have semantic relationship, which makes assembly code different from general text. Sequence matching performs best for general text but it fail to detect structural and semantic changes at an instruction level thus, its use for classification produces many false results. In this research, we have addressed the aforementioned underlying challenges by proposing a two-fold solution. For 1:1 mapping phase, we have proposed computationally inexpensive features, which are compared with a distance based selection criteria to map the similar functions and filter unmatched functions. For classification phase, we have proposed a Siamese classification embedding neural network where each branch is an attention based distributed learning neural networkthat learn the semantic similarity among assembly instructions, learn to highlight the changes at an instruction level and a final stage fully connected layer learn to accurately classify two 1:1 mapped function either an exact or a partial match. We have used x86 kernel binaries for training and achieved 99% classification accuracy; which is higher than existing binary diffing techniques and tools.",Asm2Vec;Attention Network;Binary Diffing;Exact Match;Inst2vec;Partial Match;Siamese Neural Network,"1, 1",,IEEE Early Access Articles,IEEE Transactions on Software Engineering,IEEE
158,,Adaptation to Unknown Situations as the Holy Grail of Learning-Based Self-Adaptive Systems: Research Directions,N. Cardozo; I. Dusparic,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9462036,10.1109/SEAMS51251.2021.00041,"Self-adaptive systems continuously adapt to changes in their execution environment. Capturing all possible changes to define suitable behaviour beforehand is unfeasible, or even impossible in the case of unknown changes, hence human intervention may be required. We argue that adapting to unknown situations is the ultimate challenge for self-adaptive systems. Learning-based approaches are used to learn the suitable behaviour to exhibit in the case of unknown situations, to minimize or fully remove human intervention. While such approaches can, to a certain extent, generalize existing adaptations to new situations, there is a number of breakthroughs that need to be achieved before systems can adapt to general unknown and unforeseen situations. We posit the research directions that need to be explored to achieve unanticipated adaptation from the perspective of learning-based self-adaptive systems. At minimum, systems need to define internal representations of previously unseen situations on-the-fly, extrapolate the relationship to the previously encountered situations to evolve existing adaptations, and reason about the feasibility of achieving their intrinsic goals in the new set of conditions. We close discussing whether, even when we can, we should indeed build systems that define their own behaviour and adapt their goals, without involving a human supervisor.",Self-adaptive systems;Reinforcement Learning,"252, 253",,IEEE Conferences,2021 International Symposium on Software Engineering for Adaptive and Self-Managing Systems (SEAMS),IEEE
159,,Automatically Selecting Follow-up Questions for Deficient Bug Reports,M. M. Imran; A. Ciborowska; K. Damevski,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9463135,10.1109/MSR52588.2021.00029,"The availability of quality information in bug reports that are created daily by software users is key to rapidly fixing software faults. Improving incomplete or deficient bug reports, which are numerous in many popular and actively developed open source software projects, can make software maintenance more effective and improve software quality. In this paper, we propose a system that addresses the problem of bug report incompleteness by automatically posing follow-up questions, intended to elicit answers that add value and provide missing information to a bug report. Our system is based on selecting follow-up questions from a large corpus of already posted follow-up questions on GitHub. To estimate the best follow-up question for a specific deficient bug report we combine two metrics based on: 1) the compatibility of a follow-up question to a specific bug report; and 2) the utility the expected answer to the follow-up question would provide to the deficient bug report. Evaluation of our system, based on a manually annotated held-out data set, indicates improved performance over a set of simple and ablation baselines. A survey of software developers confirms the held-out set evaluation result that about half of the selected follow-up questions are considered valid. The survey also indicates that the valid follow-up questions are useful and can provide new information to a bug report most of the time, and are specific to a bug report some of the time.",follow-up questions;bug reporting;bug triage,"167, 178",,IEEE Conferences,2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR),IEEE
160,,Practitioners’ Perceptions of the Goals and Visual Explanations of Defect Prediction Models,J. Jiarpakdee; C. K. Tantithamthavorn; J. Grundy,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9463073,10.1109/MSR52588.2021.00055,"Software defect prediction models are classifiers that are constructed from historical software data. Such software defect prediction models have been proposed to help developers optimize the limited Software Quality Assurance (SQA) resources and help managers develop SQA plans. Prior studies have different goals for their defect prediction models and use different techniques for generating visual explanations of their models. Yet, it is unclear what are the practitioners' perceptions of (1) these defect prediction model goals, and (2) the model-agnostic techniques used to visualize these models. We conducted a qualitative survey to investigate practitioners' perceptions of the goals of defect prediction models and the model-agnostic techniques used to generate visual explanations of defect prediction models. We found that (1) 82%-84% of the respondents perceived that the three goals of defect prediction models are useful; (2) LIME is the most preferred technique for understanding the most important characteristics that contributed to a prediction of a file, while ANOVA/VarImp is the second most preferred technique for understanding the characteristics that are associated with software defects in the past. Our findings highlight the significance of investigating how to improve the understanding of defect prediction models and their predictions. Hence, model-agnostic techniques from explainable AI domain may help practitioners to understand defect prediction models and their predictions.",Software Quality Assurance;Defect Prediction;Explainable AI;Software Analytics,"432, 443",,IEEE Conferences,2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR),IEEE
161,,Applying CodeBERT for Automated Program Repair of Java Simple Bugs,E. Mashhadi; H. Hemmati,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9463106,10.1109/MSR52588.2021.00063,"Software debugging, and program repair are among the most time-consuming and labor-intensive tasks in software engineering that would benefit a lot from automation. In this paper, we propose a novel automated program repair approach based on CodeBERT, which is a transformer-based neural architecture pre-trained on large corpus of source code. We fine-tune our model on the ManySStuBs4J small and large datasets to automatically generate the fix codes. The results show that our technique accurately predicts the fixed codes implemented by the developers in 19-72% of the cases, depending on the type of datasets, in less than a second per bug. We also observe that our method can generate varied-length fixes (short and long) and can fix different types of bugs, even if only a few instances of those types of bugs exists in the training dataset.",Program repair;CodeBERT;Sequence to sequence learning;Transformers;Deep learning,"505, 509",,IEEE Conferences,2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR),IEEE
162,,An Empirical Study on the Usage of BERT Models for Code Completion,M. Ciniselli; N. Cooper; L. Pascarella; D. Poshyvanyk; M. Di Penta; G. Bavota,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9463129,10.1109/MSR52588.2021.00024,"Code completion is one of the main features of modern Integrated Development Environments (IDEs). Its objective is to speed up code writing by predicting the next code token(s) the developer is likely to write. Research in this area has substantially bolstered the predictive performance of these techniques. However, the support to developers is still limited to the prediction of the next few tokens to type. In this work, we take a step further in this direction by presenting a large-scale empirical study aimed at exploring the capabilities of state-of-the-art deep learning (DL) models in supporting code completion at different granularity levels, including single tokens, one or multiple entire statements, up to entire code blocks (e.g., the iterated block of a for loop). To this aim, we train and test several adapted variants of the recently proposed RoBERTa model, and evaluate its predictions from several perspectives, including: (i) metrics usually adopted when assessing DL generative models (i.e., BLEU score and Levenshtein distance); (ii) the percentage of perfect predictions (i.e., the predicted code snippets that match those written by developers); and (iii) the ""semantic"" equivalence of the generated code as compared to the one written by developers. The achieved results show that BERT models represent a viable solution for code completion, with perfect predictions ranging from ~7%, obtained when asking the model to guess entire blocks, up to ~58%, reached in the simpler scenario of few tokens masked from the same code statement.",Code Completion;BERT,"108, 119",,IEEE Conferences,2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR),IEEE
163,,Automatic Part-of-Speech Tagging for Security Vulnerability Descriptions,S. Yitagesu; X. Zhang; Z. Feng; X. Li; Z. Xing,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9463114,10.1109/MSR52588.2021.00016,"In this paper, we study the problem of part-of-speech (POS) tagging for security vulnerability descriptions (SVD). In contrast to newswire articles, SVD often contains a high-level natural language description of the text composed of mixed language studded with codes, domain-specific jargon, vague language, and abbreviations. Moreover, training data dedicated to security vulnerability research is not widely available. Existing neural network-based POS tagging has often relied on manually annotated training data or applying natural language processing (NLP) techniques, suffering from two significant drawbacks. The former is extremely time-consuming and requires labor-intensive feature engineering and expertise. The latter is inadequate to identify linguistically-informed words specific to the SVD domain. In this paper, we propose an automatic approach to assign POS tags to tokens in SVD. Our approach uses the character-level representation to automatically extract orthographic features and unsupervised word embeddings to capture meaningful syntactic and semantic regularities from SVD. The character level representations are then concatenated with the word embedding as a combined feature, which is then learned and used to predict the POS tagging. To deal with the issue of the poor availability of annotated security vulnerability data, we implement a finetuning approach. Our approach provides public access to a POS annotated corpus of ~8M tokens, which serves as a training dataset in this domain. Our evaluation results show a significant improvement in accuracy (17.72%-28.22%) of POS tagging in SVD over the current approaches.",Fine-Tuning;Part-of-Speech tagging;Unsupervised word embedding;Security vulnerability descriptions,"29, 40",,IEEE Conferences,2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR),IEEE
164,,On Improving Deep Learning Trace Analysis with System Call Arguments,Q. Fournier; D. Aloise; S. V. Azhari; F. Tetreault,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9463093,10.1109/MSR52588.2021.00025,"Kernel traces are sequences of low-level events comprising a name and multiple arguments, including a timestamp, a process id, and a return value, depending on the event. Their analysis helps uncover intrusions, identify bugs, and find latency causes. However, their effectiveness is hindered by omitting the event arguments. To remedy this limitation, we introduce a general approach to learning a representation of the event names along with their arguments using both embedding and encoding. The proposed method is readily applicable to most neural networks and is task-agnostic. The benefit is quantified by conducting an ablation study on three groups of arguments: call-related, process-related, and time-related. Experiments were conducted on a novel web request dataset and validated on a second dataset collected on pre-production servers by Ciena, our partnering company. By leveraging additional information, we were able to increase the performance of two widely-used neural networks, an LSTM and a Transformer, by up to 11.3% on two unsupervised language modelling tasks. Such tasks may be used to detect anomalies, pre-train neural networks to improve their performance, and extract a contextual representation of the events.",Tracing;Machine Learning;Deep Learning,"120, 130",,IEEE Conferences,2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR),IEEE
165,,Data Balancing Improves Self-Admitted Technical Debt Detection,M. Sridharan; M. Mantyla; L. Rantala; M. Claes,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9463080,10.1109/MSR52588.2021.00048,"A high imbalance exists between technical debt and non-technical debt source code comments. Such imbalance affects Self-Admitted Technical Debt (SATD) detection performance, and existing literature lacks empirical evidence on the choice of balancing technique. In this work, we evaluate the impact of multiple balancing techniques, including Data level, Classifier level, and Hybrid, for SATD detection in Within-Project and Cross-Project setup. Our results show that the Data level balancing technique SMOTE or Classifier level Ensemble approaches Random Forest or XGBoost are reasonable choices depending on whether the goal is to maximize Precision, Recall, F1, or AUC-ROC. We compared our best-performing model with the previous SATD detection benchmark (cost-sensitive Convolution Neural Network). Interestingly the top-performing XGBoost with SMOTE sampling improved the Within-project F1 score by 10% but fell short in Cross-Project set up by 9%. This supports the higher generalization capability of deep learning in Cross-Project SATD detection, yet while working within individual projects, classical machine learning algorithms can deliver better performance. We also evaluate and quantify the impact of duplicate source code comments in SATD detection performance. Finally, we employ SHAP and discuss the interpreted SATD features. We have included the replication package<sup>1</sup> and shared a web-based SATD prediction tool<sup>2</sup> with the balancing techniques in this study.",Self-Admitted Technical Debt;data imbalance;classification;data sampling techniques;cost-sensitive technique;ensemble techniques,"358, 368",,IEEE Conferences,2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR),IEEE
166,,Search4Code: Code Search Intent Classification Using Weak Supervision,N. Rao; C. Bansal; J. Guan,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9463144,10.1109/MSR52588.2021.00077,"Developers use search for various tasks such as finding code, documentation, debugging information, etc. In particular, web search is heavily used by developers for finding code examples and snippets during the coding process. Recently, natural language based code search has been an active area of research. However, the lack of real-world large-scale datasets is a significant bottleneck. In this work, we propose a weak supervision based approach for detecting code search intent in search queries for C# and Java programming languages. We evaluate the approach against several baselines on a real-world dataset comprised of over 1 million queries mined from Bing web search engine and show that the CNN based model can achieve an accuracy of 77% and 76% for C# and Java respectively. Furthermore, we are also releasing Search4Code, the first large-scale real-world dataset of code search queries mined from Bing web search engine. We hope that the dataset will aid future research on code search.",code search;weak supervision,"575, 579",,IEEE Conferences,2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR),IEEE
167,,Attention-based model for predicting question relatedness on Stack Overflow,J. Pei; Y. Wu; Z. Qin; Y. Cong; J. Guan,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9463123,10.1109/MSR52588.2021.00023,"Stack Overflow is one of the most popular Programming Community-based Question Answering (PCQA) websites that has attracted more and more users in recent years. When users raise or inquire questions in Stack Overflow, providing related questions can help them solve problems. Although there are many approaches based on deep learning that can automatically predict the relatedness between questions, those approaches are limited since interaction information between two questions may be lost. In this paper, we adopt the deep learning technique, propose an Attention-based Sentence pair Interaction Model (ASIM) to predict the relatedness between questions on Stack Overflow automatically. We adopt the attention mechanism to capture the semantic interaction information between the questions. Besides, we have pre-trained and released word embeddings specific to the software engineering domain for this task, which may also help other related tasks. The experiment results demonstrate that ASIM has made significant improvement over the baseline approaches in Precision, Recall, and Micro-F1 evaluation metrics, achieving state-of-the-art performance in this task. Our model also performs well in the duplicate question detection task of AskUbuntu, which is a similar but different task, proving its generalization and robustness.",Stack Overflow;Question Relatedness;Deep Learning;Attention Mechanism;Word Embeddings,"97, 107",,IEEE Conferences,2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR),IEEE
168,,An Exploratory Study of Log Placement Recommendation in an Enterprise System,J. Cândido; J. Haesen; M. Aniche; A. van Deursen,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9463118,10.1109/MSR52588.2021.00027,"Logging is a development practice that plays an important role in the operations and monitoring of complex systems. Developers place log statements in the source code and use log data to understand how the system behaves in production. Unfortunately, anticipating where to log during development is challenging. Previous studies show the feasibility of leveraging machine learning to recommend log placement despite the data imbalance since logging is a fraction of the overall code base. However, it remains unknown how those techniques apply to an industry setting, and little is known about the effect of imbalanced data and sampling techniques. In this paper, we study the log placement problem in the code base of Adyen, a large-scale payment company. We analyze 34,526 Java files and 309,527 methods that sum up +2M SLOC. We systematically measure the effectiveness of five models based on code metrics, explore the effect of sampling techniques, understand which features models consider to be relevant for the prediction, and evaluate whether we can exploit 388,086 methods from 29 Apache projects to learn where to log in an industry setting. Our best performing model achieves 79% of balanced accuracy, 81% of precision, 60% of recall. While sampling techniques improve recall, they penalize precision at a prohibitive cost. Experiments with open-source data yield under-performing models over Adyen's test set; nevertheless, they are useful due to their low rate of false positives. Our supporting scripts and tools are available to the community.",Log Placement;Log Recommendation;Logging Practices;Supervised Learning,"143, 154",,IEEE Conferences,2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR),IEEE
169,,Can I Solve It? Identifying APIs Required to Complete OSS Tasks,F. Santos; I. Wiese; B. Trinkenreich; I. Steinmacher; A. Sarma; M. A. Gerosa,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9463078,10.1109/MSR52588.2021.00047,"Open Source Software projects add labels to open issues to help contributors choose tasks. However, manually labeling issues is time-consuming and error-prone. Current automatic approaches for creating labels are mostly limited to classifying issues as a bug/non-bug. In this paper, we investigate the feasibility and relevance of labeling issues with the domain of the APIs required to complete the tasks. We leverage the issues' description and the project history to build prediction models, which resulted in precision up to 82% and recall up to 97.8%. We also ran a user study (n=74) to assess these labels' relevancy to potential contributors. The results show that the labels were useful to participants in choosing tasks, and the API-domain labels were selected more often than the existing architecture-based labels. Our results can inspire the creation of tools to automatically label issues, helping developers to find tasks that better match their skills.",API identification;Labelling;Tagging;Skills;Multi-Label Classification;Mining Software Repositories;Case Study,"346, 257",,IEEE Conferences,2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR),IEEE
170,,Learning Off-By-One Mistakes: An Empirical Study,H. Sellik; O. van Paridon; G. Gousios; M. Aniche,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9463090,10.1109/MSR52588.2021.00019,"Mistakes in binary conditions are a source of error in many software systems. They happen when developers use, e.g., `<;' or `>' instead of `<;=' or `>='. These boundary mistakes are hard to find and impose manual, labor-intensive work for software developers. While previous research has been proposing solutions to identify errors in boundary conditions, the problem remains open. In this paper, we explore the effectiveness of deep learning models in learning and predicting mistakes in boundary conditions. We train different models on approximately 1.6M examples with faults in different boundary conditions. We achieve a precision of 85% and a recall of 84% on a balanced dataset, but lower numbers in an imbalanced dataset. We also perform tests on 41 real-world boundary condition bugs found from GitHub, where the model shows only a modest performance. Finally, we test the model on a large-scale Java code base from Adyen, our industrial partner. The model reported 36 buggy methods, but none of them were confirmed by developers.",machine learning for software engineering;deep learning for software engineering;software testing;boundary testing,"58, 67",,IEEE Conferences,2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR),IEEE
171,,Project-Level Encoding for Neural Source Code Summarization of Subroutines,A. Bansal; S. Haque; C. McMillan,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9463018,10.1109/ICPC52881.2021.00032,"Source code summarization of a subroutine is the task of writing a short, natural language description of that subroutine. The description usually serves in documentation aimed at programmers, where even brief phrase (e.g. ""compresses data to a zip file"") can help readers rapidly comprehend what a subroutine does without resorting to reading the code itself. Techniques based on neural networks (and encoder-decoder model designs in particular) have established themselves as the state-of-the-art. Yet a problem widely recognized with these models is that they assume the information needed to create a summary is present within the code being summarized itself - an assumption which is at odds with program comprehension literature. Thus a current research frontier lies in the question of encoding source code context into neural models of summarization. In this paper, we present a project-level encoder to improve models of code summarization. By project-level, we mean that we create a vectorized representation of selected code files in a software project, and use that representation to augment the encoder of state-of-the-art neural code summarization techniques. We demonstrate how our encoder improves several existing models, and provide guidelines for maximizing improvement while controlling time and resource costs in model size.",source code summarization;automatic documentation generation;neural networks,"253, 264",,IEEE Conferences,2021 IEEE/ACM 29th International Conference on Program Comprehension (ICPC),IEEE
172,,API2Com: On the Improvement of Automatically Generated Code Comments Using API Documentations,R. Shahbazi; R. Sharma; F. H. Fard,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9463033,10.1109/ICPC52881.2021.00049,"Code comments can help in program comprehension and are considered as important artifacts to help developers in software maintenance. However, the comments are mostly missing or are outdated, specially in complex software projects. As a result, several automatic comment generation models are developed as a solution. The recent models explore the integration of external knowledge resources such as Unified Modeling Language class diagrams to improve the generated comments. In this paper, we propose API2Com, a model that leverages the Application Programming Interface Documentations (API Docs) as a knowledge resource for comment generation. The API Docs include the description of the methods in more details and therefore, can provide better context in the generated comments. The API Docs are used along with the code snippets and Abstract Syntax Trees in our model.We apply the model on a large Java dataset of over 130,000 methods and evaluate it using both Transformer and RNN- base architectures. Interestingly, when API Docs are used, the performance increase is negligible. We therefore run different experiments to reason about the results. For methods that only contain one API, adding API Docs improves the results by 4% BLEU score on average (BLEU score is an automatic evaluation metric used in machine translation). However, as the number of APIs that are used in a method increases, the performance of the model in generating comments decreases due to long documentations used in the input. Our results confirm that the API Docs can be useful in generating better comments, but, new techniques are required to identify the most informative ones in a method rather than using all documentations simultaneously.",Code comment generation;API documentation;External knowledge source,"411, 421",,IEEE Conferences,2021 IEEE/ACM 29th International Conference on Program Comprehension (ICPC),IEEE
173,,Where to Handle an Exception? Recommending Exception Handling Locations from a Global Perspective,X. Jia; S. Chen; X. Zhou; X. Li; R. Yu; X. Chen; J. Xuan,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9462986,10.1109/ICPC52881.2021.00042,"Exception handling is an effective mechanism to guarantee software reliability in modern programming languages. An exception interrupts the program execution and propagates backwards along the call chain until the exception is caught by an exception handler. In software development practices, developers may be confused in determining where to place the exception handler in the call chain. The reason is that exception handling requires a developer to take a comprehensive consideration from a global perspective of the software project. In this paper, we propose an automatic approach EHAdvisor, which recommends exception handling locations from the global perspective of the project. EHAdvisor first trains a binary classification model based on four types of features, including architectural features, project features, functional features, and exception features. Then, for a new code snippet with exceptions, EHAdvisor predicts the exception catching probability for each method in the call chain based on the classification model and recommends Top-K exception handling locations based on the probability ranking. We conducted experiments on a dataset from 29 high-quality open source projects. Experimental results show that EHAdvisor achieves an average Top-1 recommendation success rate of 70.83% for across-project location recommendation and an average Top-1 accuracy of 86.21% for intra-project recommendation. Experiments on the importance scores show that global features, such as project features and architectural features, are evidently important to the recommendation of exception handling locations.",exception handling;location recommendation;machine learning;global features;classification algorithm,"369, 380",,IEEE Conferences,2021 IEEE/ACM 29th International Conference on Program Comprehension (ICPC),IEEE
174,,Bug or not bug? That is the question,Q. Perez; P. -A. Jean; C. Urtado; S. Vauttier,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9462993,10.1109/ICPC52881.2021.00014,"Nowadays, development teams often rely on tools such as Jira or Bugzilla to manage backlogs of issues to be solved to develop or maintain software. Although they relate to many different concerns (e.g., bug fixing, new feature development, architecture refactoring), few means are proposed to identify and classify these different kinds of issues, except for non mandatory labels that can be manually associated to them. This may lead to a lack of issue classification or to issue misclassification that may impact automatic issue management (planning, assignment) or issue-derived metrics. Automatic issue classification thus is a relevant topic for assisting backlog management. This paper proposes a binary classification solution for discriminating bug from non bug issues. This solution combines natural language processing (TF-IDF) and classification (multi-layer perceptron) techniques, selected after comparing commonly used solutions to classify issues. Moreover, hyper-parameters of the neural network are optimized using a genetic algorithm. The obtained results, as compared to existing works on a commonly used benchmark, show significant improvements on the F1 measure for all datasets.",Bug Classification;Bug Tickets;Empirical Software Engineering;Natural Language Processing;Neural Network Optimization;Genetic Algorithm,"47, 58",,IEEE Conferences,2021 IEEE/ACM 29th International Conference on Program Comprehension (ICPC),IEEE
175,,Shallow or Deep? An Empirical Study on Detecting Vulnerabilities using Deep Learning,A. Mazuera-Rozo; A. Mojica-Hanke; M. Linares-Vásquez; G. Bavota,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9462962,10.1109/ICPC52881.2021.00034,"Deep learning (DL) techniques are on the rise in the software engineering research community. More and more approaches have been developed on top of DL models, also due to the unprecedented amount of software-related data that can be used to train these models. One of the recent applications of DL in the software engineering domain concerns the automatic detection of software vulnerabilities. While several DL models have been developed to approach this problem, there is still limited empirical evidence concerning their actual effectiveness especially when compared with shallow machine learning techniques. In this paper, we partially fill this gap by presenting a large-scale empirical study using three vulnerability datasets and five different source code representations (i.e., the format in which the code is provided to the classifiers to assess whether it is vulnerable or not) to compare the effectiveness of two widely used DL-based models and of one shallow machine learning model in (i) classifying code functions as vulnerable or non-vulnerable (i.e., binary classification), and (ii) classifying code functions based on the specific type of vulnerability they contain (or ""clean"", if no vulnerability is there). As a baseline we include in our study the AutoML utility provided by the Google Cloud Platform. Our results show that the experimented models are still far from ensuring reliable vulnerability detection, and that a shallow learning classifier represents a competitive baseline for the newest DL-based models.",Vulnerability detection;empirical study,"276, 287",,IEEE Conferences,2021 IEEE/ACM 29th International Conference on Program Comprehension (ICPC),IEEE
176,,Exploiting Method Names to Improve Code Summarization: A Deliberation Multi-Task Learning Approach,R. Xie; W. Ye; J. Sun; S. Zhang,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9462964,10.1109/ICPC52881.2021.00022,"Code summaries are brief natural language descriptions of source code pieces. The main purpose of code summarization is to assist developers in understanding code and to reduce documentation workload. In this paper, we design a novel multi-task learning (MTL) approach for code summarization through mining the relationship between method code summaries and method names. More specifically, since a method's name can be considered as a shorter version of its code summary, we first introduce the tasks of generation and informativeness prediction of method names as two auxiliary training objectives for code summarization. A novel two-pass deliberation mechanism is then incorporated into our MTL architecture to generate more consistent intermediate states fed into a summary decoder, especially when informative method names do not exist. To evaluate our deliberation MTL approach, we carried out a large-scale experiment on two existing datasets for Java and Python. The experiment results show that our technique can be easily applied to many state-of-the-art neural models for code summarization and improve their performance. Meanwhile, our approach shows significant superiority when generating summaries for methods with non-informative names.",code summarization;method name prediction;multi-task learning;deliberation network,"138, 148",,IEEE Conferences,2021 IEEE/ACM 29th International Conference on Program Comprehension (ICPC),IEEE
177,,Improving Code Summarization with Block-wise Abstract Syntax Tree Splitting,C. Lin; Z. Ouyang; J. Zhuang; J. Chen; H. Li; R. Wu,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9463007,10.1109/ICPC52881.2021.00026,"Automatic code summarization frees software developers from the heavy burden of manual commenting and benefits software development and maintenance. Abstract Syntax Tree (AST), which depicts the source code's syntactic structure, has been incorporated to guide the generation of code summaries. However, existing AST based methods suffer from the difficulty of training and generate inadequate code summaries. In this paper, we present the Block-wise Abstract Syntax Tree Splitting method (BASTS for short), which fully utilizes the rich tree-form syntax structure in ASTs, for improving code summarization. BASTS splits the code of a method based on the blocks in the dominator tree of the Control Flow Graph, and generates a split AST for each code split. Each split AST is then modeled by a Tree-LSTM using a pre-training strategy to capture local non-linear syntax encoding. The learned syntax encoding is combined with code encoding, and fed into Transformer to generate high-quality code summaries. Comprehensive experiments on benchmarks have demonstrated that BASTS significantly outperforms state-of-the-art approaches in terms of various evaluation metrics. To facilitate reproducibility, our implementation is available at https://github.com/XMUDM/BASTS.",Code Summarization;Code Splitting;Abstract Syntax Tree,"184, 195",,IEEE Conferences,2021 IEEE/ACM 29th International Conference on Program Comprehension (ICPC),IEEE
178,,Locating Faulty Methods with a Mixed RNN and Attention Model,S. Yang; J. Cao; H. Zeng; B. Shen; H. Zhong,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9462960,10.1109/ICPC52881.2021.00028,"IR-based fault localization approaches achieves promising results when locating faulty files by comparing a bug report with source code. Unfortunately, they become less effective to locate faulty methods. We conduct a preliminary study to explore its challenges, and identify three problems: the semantic gap problem, the representation sparseness problem, and the single revision problem.To tackle these problems, we propose MRAM, a mixed RNN and attention model, which combines bug-fixing features and method structured features to explore both implicit and explicit relevance between methods and bug reports for method level fault localization task. The core ideas of our model are: (1) constructing code revision graphs from code, commits and past bug reports, which reveal the latent relations among methods to augment short methods and as well provide all revisions of code and past fixes to train more accurate models; (2) embedding three method structured features (token sequences, API invocation sequences, and comments) jointly with RNN and soft attention to represent source methods and obtain their implicit relevance with bug reports; and (3) integrating multi-revision bug-fixing features, which provide the explicit relevance between bug reports and methods, to improve the performance.We have implemented MRAM and conducted a controlled experiment on five open-source projects. Comparing with state-of-the-art approaches, our MRAM improves MRR values by 3.8-5.1% (3.7-5.4%) when the dataset contains (does not contain) localized bug reports. Our statistics test shows that our improvements are significant.",fault localization;code revision graph;recurrent neural network;soft attention,"207, 218",,IEEE Conferences,2021 IEEE/ACM 29th International Conference on Program Comprehension (ICPC),IEEE
179,,A framework for the automation of testing computer vision systems,F. Wotawa; L. Klampfl; L. Jahaj,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9462996,10.1109/AST52587.2021.00023,"Vision systems, i.e., systems that enable the detection and tracking of objects in images, have gained substantial importance over the past decades. They are used in quality assurance applications, e.g., for finding surface defects in products during manufacturing, surveillance, but also automated driving, requiring reliable behavior. Interestingly, there is only little work on quality assurance and especially testing of vision systems in general. In this paper, we contribute to the area of testing vision software, and present a framework for the automated generation of tests for systems based on vision and image recognition with the focus on easy usage, uniform usability and expandability. The framework makes use of existing libraries for modifying the original images and to obtain similarities between the original and modified images. We show how such a framework can be used for testing a particular industrial application on identifying defects on riblet surfaces and present preliminary results from the image classification domain.",test case generation;testing vision software;testing image classifiers,"121, 124",,IEEE Conferences,2021 IEEE/ACM International Conference on Automation of Software Test (AST),IEEE
180,,Multimodal Surprise Adequacy Analysis of Inputs for Natural Language Processing DNN Models,S. Kim; S. Yoo,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9462987,10.1109/AST52587.2021.00017,"As Deep Neural Networks (DNNs) are rapidly adopted in various domains, many test adequacy metrics for DNN inputs have been introduced to help evaluating, and validating, trained DNN models. Surprise Adequacy (SA) is one such metric that aims to quantitatively measure how surprising a new input is with respect to the data used to train the given model. While SA has been shown to be effective for computer vision tasks such as image classification or object segmentation, its efficacy for DNN based Natural Language Processing has not been thoroughly studied. This paper evaluates whether it is feasible to apply SA analysis to DNN models trained for NLP tasks. We also show that the input distribution captured in the latent embedding space can be multimodal<sup>1</sup> for some NLP tasks, unlike those observed in computer vision tasks, and investigate if catering for the multimodal property of NLP models can improve SA analysis. An empirical evaluation of extended SA metrics with three NLP tasks and nine DNN models shows that, while unimodal SAs perform sufficiently well for text classification, multimodal SA can outperform unimodal metrics.",Deep Learning;Natural Language Processing;Software Testing,"80, 89",,IEEE Conferences,2021 IEEE/ACM International Conference on Automation of Software Test (AST),IEEE
181,,The Impacts of Sentiments and Tones in Community-Generated Issue Discussions,A. Sanei; J. Cheng; B. Adams,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9463189,10.1109/CHASE52884.2021.00009,"The diverse community members who contribute to the discussions on issue tracking systems of open-source software projects often exhibit complex affective states such as sentiments and tones. These affective states can significantly influence the effectiveness of the issue discussions in elaborating the initial ideas into actionable tasks that the development teams need to address. In this paper, we present an extended empirical study to investigate the impacts of sentiments and tones in community-generated issue discussions. We created and validated a large dataset of sentiments and tones in the issues posts and comments created by diverse community members in three popular open source projects. Our analysis results drew a complex picture of the relationships between, on the one hand, the sentiments and tones in the issue discussions, and on the other hand, various discussion and development-related measures such as the discussion length and the issue resolution time. We also found that when factors such as the issue poster roles and the issue types were controlled, sentiments and tones had varied associations with the measures. Insights gained from these findings can support open source community members in making and moderating effective issue discussions and guide the design of tools to better support community engagement.",Open source software;issue tracking systems;tone analysis;sentiment analysis,"1, 10",,IEEE Conferences,2021 IEEE/ACM 13th International Workshop on Cooperative and Human Aspects of Software Engineering (CHASE),IEEE
182,,Research on Electronic Nose Drift Suppression Algorithm based on Classifier Integration and Active Learning,Q. Li; P. Wu; Z. Liang; Y. Tao,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9463654,10.1109/ICCSN52437.2021.9463654,"In the field of electronic nose(E-nose) research, the underlying gas sensor is affected by environmental changes, aging of its own devices, sensor poisoning and other factors, which will cause the detection value to drift. Besides, the need for a large number of labeled samples in the pattern recognition algorithm will lead to excessively high model training costs. In order to solve the problems mentioned above, a method that combines classifier integration and active learning to reduce the model training cost by reducing the number of labeled samples is proposed in this paper. Using this method, the trend of sensor drift is captured by classifier integration, the number of single-labeled samples is dynamically adjusted, and finally the drift of the gas sensor array is suppressed. From the experiment results, it can be found that the sensor drift can be satisfactorily solved by the proposed method.",sensor drift;classifier integration;active learning;committee query;dynamic update,"277, 282",,IEEE Conferences,2021 13th International Conference on Communication Software and Networks (ICCSN),IEEE
183,,A Preprocessing Method of Facial Expression Image under Different Illumination,Y. Hu; X. Zeng; Z. Huang; X. Dong,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9463605,10.1109/ICCSN52437.2021.9463605,"In this work, we propose an image processing method which combines the limited contrast adaptive histogram equalization (CLAHE) with Gamma transform to solve the illumination problem in facial expression recognition. We apply this algorithm to professional illumination datasets (Extended Yale B) and get better visual results, compared with using CLAHE and Gamma correction separately. Moreover, we use a convolution neural network (CNN) that pre-trained on FER2013 datasets to evaluate the effect of this method in facial expression recognition. We use this preprocessing algorithm to enhance the CK+ and Oulu expression datasets, and get accuracy of 89.24% and 70.24% respectively. Compared with the datasets that have not been pre-processed, it has provided an increase in classification accuracy of 7% on the Oulu datasets.",CLAHE;illumination;facial expression recognition,"318, 322",,IEEE Conferences,2021 13th International Conference on Communication Software and Networks (ICCSN),IEEE
184,,Pedestrian Traffic Lights Classification Using Transfer Learning in Smart City Application,S. Khan; Y. Teng; J. Cui,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9463615,10.1109/ICCSN52437.2021.9463615,"Traffic accidents have become a serious issue in cities. Millions of people die in traffic accidents annually and among them the major cause is the pedestrian jaywalking. To solve this traffic issue and ensure efficient traffic monitoring, we introduced the surveillance system using AI powered UAVs in Internet of flying things based smart city scenario. To accurately classify the pedestrian traffic lights, we use the computer vision technology. We have created our own local dataset containing 809 images where 441 images belong to red signal class while 368 images belong to green signal class. We explore the power of transfer learning based on DNNs to overcome the limitation of dataset for pedestrian traffic lights classification. In this approach, we use the pre-trained MobileNetV2 model and freeze the weights. By leveraging the pre-trained convolutional base, we add our own fully connected layers on top of the model for classification. To handle the problem of limited data, we also perform the data augmentation. The task is formulated as binary classification problem. By using the MobileNetV2 on challenging and very diverse dataset, we achieve the accuracy of 94.92%, 91.84% specificity and 97.10% sensitivity.",IoFT;computer vision;transfer learning;pedestrian traffic lights classification,"352, 356",,IEEE Conferences,2021 13th International Conference on Communication Software and Networks (ICCSN),IEEE
185,,A Novel Sparse Subspace Correlation Analysis-Based Domain Adaptation Method for Sensor Drift Suppression in E-nose,Z. Liang; L. Yang; T. Guo; J. Li,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9463598,10.1109/ICCSN52437.2021.9463598,"Sensor drift caused by the sensor aging and environmental factors is an urgent problem that seriously affects the detection performance and service life of electronic nose (E-nose). It is necessary to research the sensor drift suppression methods to realize the long-term and stable detection of E-nose. In this paper, a highly efficient sparse subspace correlation analysis-based domain adaptation(SSCA-DA) method is proposed to suppress the sensor drift. This method is to find the optimal subspace for each dataset, and the transformed data after transforming to the optimal subspace is sparsely reconstructed, which can realize the knowledge transfer in the data domains with and without drift information. From the experiment results, it can be found that the sensor drift can be satisfactorily solved by the proposed method.",electronic nose;sensor drift;sparse reconstruction;domain adaptation,"242, 246",,IEEE Conferences,2021 13th International Conference on Communication Software and Networks (ICCSN),IEEE
186,,MeetDurian: A Gameful Mobile App to Prevent COVID-19 Infection,D. Chen; A. Bucchiarone; Z. Lv,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460945,10.1109/MobileSoft52590.2021.00016,"The COVID-19 problem has not gone away with the passing of the seasons. Even though most countries have achieved remarkable results in fighting against epidemic diseases and preventing and controlling viruses, the general public is still far from understanding the new crown virus and lacks imagination on its transmission law. In this paper, we propose MeetDurian: a cross-platform mobile application that exploits a location-based game to improve users' hygiene habits and reduce virus dispersal. We present its main features, its architecture, and its core technologies. Finally, we report a set of experiments that prove the acceptability and usability of MeetDurian. An illustrative demo of the mobile app features is shown in the following video: https://youtu.be/Vqg7nFDQuOU.",Mobile App;Location-Based Game;Exergaming;Education,"69, 72",,IEEE Conferences,2021 IEEE/ACM 8th International Conference on Mobile Software Engineering and Systems (MobileSoft),IEEE
187,,TaskAllocator: A Recommendation Approach for Role-based Tasks Allocation in Agile Software Development,S. Shafiq; A. Mashkoor; C. Mayr-Dorn; A. Egyed,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9461028,10.1109/ICSSP-ICGSE52873.2021.00014,"In this paper, we propose a recommendation approach - TaskAllocator - in order to predict the assignment of incoming tasks to potential befitting roles. The proposed approach, identifying team roles rather than individual persons, allows project managers to perform better tasks allocation in case the individual developers are over-utilized or moved on to different roles/projects. We evaluated our approach on ten agile case study projects obtained from the Taiga. io repository. In order to determine the TaskAllocator's performance, we have conducted a benchmark study by comparing it with contemporary machine learning models. The applicability of the TaskAllocator was assessed through a plugin that can be integrated with JIRA and provides recommendations about suitable roles whenever a new task is added to the project. Lastly, the source code of the plugin and the dataset employed have been made public.",Distributed agile software development;task allocation;natural language processing,"39, 49",,IEEE Conferences,2021 IEEE/ACM Joint 15th International Conference on Software and System Processes (ICSSP) and 16th ACM/IEEE International Conference on Global Software Engineering (ICGSE),IEEE
188,,Deep Learning based Vulnerability Detection: Are We There Yet,S. Chakraborty; R. Krishna; Y. Ding; B. Ray,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9448435,10.1109/TSE.2021.3087402,"Automated detection of software vulnerabilities is a fundamental problem in software security. Existing program analysis techniques either suffer from high false positives or false negatives. Recent progress in Deep Learning (DL) has resulted in a surge of interest in applying DL for automated vulnerability detection. Several recent studies have demonstrated promising results achieving an accuracy of up to 95% at detecting vulnerabilities. In this paper, we ask, ""how well do the state-of-the-art DL-based techniques perform in a real-world vulnerability prediction scenario"". To our surprise, we find that their performance drops by more than 50%. A systematic investigation of what causes such precipitous performance drop reveals that existing DL-based vulnerability prediction approaches suffer from challenges with the training data (e.g., data duplication, unrealistic distribution of vulnerable classes, etc.) and with the model choices (e.g., simple token-based models). As a result, these approaches often do not learn features related to the actual cause of the vulnerabilities. Instead, they learn unrelated artifacts from the dataset (e.g., specific variable/function names, etc.). Leveraging these empirical findings, we demonstrate how a more principled approach to data collection and model design, based on realistic settings of vulnerability prediction, can lead to better solutions. The resulting tools perform significantly better than the studied baseline up to 33.57% boost in precision and 128.38% boost in recall compared to the best performing model in the literature. Overall, this paper elucidates existing DL-based vulnerability prediction systems' potential issues and draws a roadmap for future DL-based vulnerability prediction research. In that spirit, we make available all the artifacts supporting our results: https://git.io/Jf6IA.",datasets;neural networks;gaze detection;text tagging,"1, 1",,IEEE Early Access Articles,IEEE Transactions on Software Engineering,IEEE
189,,Combinatorial Testing Metrics for Machine Learning,E. Lanus; L. J. Freeman; D. Richard Kuhn; R. N. Kacker,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9440189,10.1109/ICSTW52544.2021.00025,"This paper defines a set difference metric for comparing machine learning (ML) datasets and proposes the difference between datasets be a function of combinatorial coverage. We illustrate its utility for evaluating and predicting performance of ML models. Identifying and measuring differences between datasets is of significant value for ML problems, where the accuracy of the model is heavily dependent on the degree to which training data are sufficiently representative of data encountered in application. The method is illustrated for transfer learning without retraining, the problem of predicting performance of a model trained on one dataset and applied to another.",combinatorial testing;machine learning;operating envelopes;transfer learning;test set selection,"81, 84",,IEEE Conferences,"2021 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)",IEEE
190,,A Combinatorial Approach to Testing Deep Neural Network-based Autonomous Driving Systems,J. Chandrasekaran; Y. Lei; R. Kacker; D. Richard Kuhn,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9440193,10.1109/ICSTW52544.2021.00022,"Recent advancements in the field of deep learning have enabled its application in Autonomous Driving Systems (ADS). A Deep Neural Network (DNN) model is often used to perform tasks such as pedestrian detection, object detection, and steering control in ADS. Unfortunately, DNN models could exhibit incorrect or unexpected behavior in real-world scenarios. There is a need to rigorously test these models with real-world driving scenarios so that safety-critical bugs can be detected before their deployment in the real world.In this paper, we propose a combinatorial approach to testing DNN models. Our approach generates test images by applying a set of combinations of some basic image transformation operations to a seed image. First, we identify a set of valid transformation operations or simply transformations. Next, we design an input parameter model based on the valid transformations and generate a t-way (t=2) combinatorial test set. Each test represents a combination of transformations, and can be used to produce a test image. We execute the test images on a DNN model and distinguish between consistent and inconsistent behavior using a relation. We conducted an experimental evaluation of our approach on three DNN models that are used in the Udacity challenge. Our results suggest that test images generated by our approach can effectively identify inconsistent behaviors and can significantly increase neuron coverage. To the best of our knowledge, our work is the first effort to use a combinatorial testing approach to generating test images based on image transformations for testing DNNs used in ADS.",Testing DNN models;Combinatorial Testing;Deep Learning Testing;Neural Network Testing;Testing Selfdriving cars;Testing autonomous vehicles,"57, 66",,IEEE Conferences,"2021 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)",IEEE
191,,AutoKG - An Automotive Domain Knowledge Graph for Software Testing: A position paper,V. Kesri; A. Nayak; K. Ponnalagu,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9440180,10.1109/ICSTW52544.2021.00047,"Industries have a significant amount of data in semi-structured and unstructured formats which are typically captured in text documents, spreadsheets, images, etc. This is especially the case with the software description documents used by domain experts in the automotive domain to perform tasks at various phases of the Software Development Life Cycle (SDLC). In this paper, we propose an end-to-end pipeline to extract an Automotive Knowledge Graph (AutoKG) from textual data using Natural Language Processing (NLP) techniques with the application of automatic test case generation. The proposed pipeline primarily consists of the following components: 1) AutoOntology, an ontology that has been derived by analyzing several industry scale automotive domain software systems, 2) AutoRE, a Relation Extraction (RE) model to extract triplets from various sentence types typically found in the automotive domain, and 3) AutoVec, a neural embedding based algorithm for triplet matching and context-based search. We demonstrate the pipeline with an application of automatic test case generation from requirements using AutoKG.",Automotive Domain Knowledge Graph;Software Testing;Natural Language Processing,"234, 238",,IEEE Conferences,"2021 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)",IEEE
192,,A Combinatorial Approach to Explaining Image Classifiers,J. Chandrasekaran; Y. Lei; R. Kacker; D. Richard Kuhn,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9440187,10.1109/ICSTW52544.2021.00019,"Machine Learning (ML) models, a core component to artificial intelligence systems, often come as a black box to the user, leading to the problem of interpretability. Explainable Artificial Intelligence (XAI) is key to providing confidence and trustworthiness for machine learning-based software systems. We observe a fundamental connection between XAI and software fault localization. In this paper, we present an approach that uses BEN, a combinatorial testing-based software fault localization approach, to produce explanations for decisions made by ML models.",explainability;deep learning;software testing;debugging DNN models;explainable AI;combinatorial testing;Image classifiers;model-agnostic;counterfactual explanation;instance-level explanations,"35, 43",,IEEE Conferences,"2021 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)",IEEE
193,,Test Automation with Grad-CAM Heatmaps - A Future Pipe Segment in MLOps for Vision AI?,M. Borg; R. Jabangwe; S. Åberg; A. Ekblom; L. Hedlund; A. Lidfeldt,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9440142,10.1109/ICSTW52544.2021.00039,"Machine Learning (ML) is a fundamental part of modern perception systems. In the last decade, the performance of computer vision using trained deep neural networks has outperformed previous approaches based on careful feature engineering. However, the opaqueness of large ML models is a substantial impediment for critical applications such as in the automotive context. As a remedy, Gradient-weighted Class Activation Mapping (Grad-CAM) has been proposed to provide visual explanations of model internals. In this paper, we demonstrate how Grad-CAM heatmaps can be used to increase the explainability of an image recognition model trained for a pedestrian underpass. We argue how the heatmaps support compliance to the EU's seven key requirements for Trustworthy AI. Finally, we propose adding automated heatmap analysis as a pipe segment in an MLOps pipeline. We believe that such a building block can be used to automatically detect if a trained ML-model is activated based on invalid pixels in test images, suggesting biased models.",machine learning testing;neural networks;image recognition;Grad-CAM;test automation,"175, 181",,IEEE Conferences,"2021 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)",IEEE
194,,Summary of: Adaptive Metamorphic Testing with Contextual Bandits,H. Spieker; A. Gotlieb,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9438595,10.1109/ICST49551.2021.00037,"Metamorphic Testing (MT) is a software testing paradigm that aims at using user-specified properties of a program under test to either check its expected outputs or to generate new test cases [1] , [2] . More precisely, MT tackles the so-called oracle problem which occurs whenever predicting the expected outputs of a system is just too difficult or even impossible. A typical example where MT has been successfully deployed is for testing machine learning models. For instance, in supervised machine learning, we train models for classification problems, but testing these models is hard as only stochastic behaviors of these models can be specified [3] . Indeed, we initially train these models with existing labelled datasets and then we exploit them to classify new data samples. Testing these models means only to reserve some portion of the labelled datasets to control that the correct classification is given for these reserved datasets. However, nothing is really available to test these models on unlabelled data samples.",,"275, 277",,IEEE Conferences,"2021 14th IEEE Conference on Software Testing, Verification and Validation (ICST)",IEEE
195,,Uncertainty-Wizard: Fast and User-Friendly Neural Network Uncertainty Quantification,M. Weiss; P. Tonella,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9438604,10.1109/ICST49551.2021.00056,"Uncertainty and confidence have been shown to be useful metrics in a wide variety of techniques proposed for deep learning testing, including test data selection and system supervision. We present Uncertainty-Wizard, a tool that allows to quantify such uncertainty and confidence in artificial neural networks. It is built on top of the industry-leading TF.KERAS deep learning API and it provides a near-transparent and easy to understand interface. At the same time, it includes major performance optimizations that we benchmarked on two different machines and different configurations.",fault tolerance;software reliability;software testing;art neural networks;software tools,"436, 441",,IEEE Conferences,"2021 14th IEEE Conference on Software Testing, Verification and Validation (ICST)",IEEE
196,,Digital Twin-based Anomaly Detection in Cyber-physical Systems,Q. Xu; S. Ali; T. Yue,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9438560,10.1109/ICST49551.2021.00031,"Cyber-Physical Systems (CPS) are susceptible to various anomalies during their operations. Thus, it is important to detect such anomalies. Detecting such anomalies is challenging since it is uncertain when and where anomalies can happen. To this end, we present a novel approach called Anomaly deTection with digiTAl twIN (ATTAIN), which continuously and automatically builds a digital twin with live data obtained from a CPS for anomaly detection. ATTAIN builds a Timed Automaton Machine (TAM) as the digital representation of the CPS, and implements a Generative Adversarial Network (GAN) to detect anomalies. GAN uses a GCN-LSTM-based module as a generator, which can capture temporal and spatial characteristics of the input data and learn to produce realistic unlabeled adversarial samples. TAM labels these adversarial samples, which are then fed into a discriminator along with real labeled samples. After training, the discriminator is capable of distinguishing anomalous data from normal data with a high F1 score. To evaluate our approach, we used three publicly available datasets collected from three CPS testbeds. Evaluation results show that ATTAIN improved the performance of two state-of-art anomaly detection methods by 2.413%, 8.487%, and 5.438% on average on the three datasets, respectively. Moreover, ATTAIN achieved on average 8.39% increase in the anomaly detection capability with digital twins as compared with an approach of not using digital twins.",cyber-physical system;digital twin;machine learning;anomaly detection,"205, 216",,IEEE Conferences,"2021 14th IEEE Conference on Software Testing, Verification and Validation (ICST)",IEEE
197,,Boosting Exploratory Testing of Industrial Automation Systems with AI,R. Eidenbenz; C. Franke; T. Sivanthi; S. Schoenborn,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9438589,10.1109/ICST49551.2021.00048,Testing of large and complex industrial control systems is challenging as the space of possible input and environmental parameters is large. Searching the entire space for potential failures is practically infeasible. This paper introduces an industrial control system robustness testing problem and evaluates artificial intelligence (AI) based strategies to efficiently explore the space and to identify parameter sets that can cause the system to fail. The proposed solution approach uses regression techniques to speed up the search and clustering methods to identify parameter sets that represent distinct system failures.,testing;artificial intelligence;machine learning;automation systems;exploratory testing,"362, 371",,IEEE Conferences,"2021 14th IEEE Conference on Software Testing, Verification and Validation (ICST)",IEEE
198,,A Search-Based Testing Framework for Deep Neural Networks of Source Code Embedding,M. V. Pour; Z. Li; L. Ma; H. Hemmati,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9438605,10.1109/ICST49551.2021.00016,"Over the past few years, deep neural networks (DNNs) have been continuously expanding their real-world applications for source code processing tasks across the software engineering domain, e.g., clone detection, code search, comment generation. Although quite a few recent works have been performed on testing of DNNs in the context of image and speech processing, limited progress has been achieved so far on DNN testing in the context of source code processing, that exhibits rather unique characteristics and challenges.In this paper, we propose a search-based testing framework for DNNs of source code embedding and its downstream processing tasks like Code Search. To generate new test inputs, we adopt popular source code refactoring tools to generate the semantically equivalent variants. For more effective testing, we leverage the DNN mutation testing to guide the testing direction. To demonstrate the usefulness of our technique, we perform a large-scale evaluation on popular DNNs of source code processing based on multiple state-of-the-art code embedding methods (i.e., Code2vec, Code2seq and CodeBERT). The testing results show that our generated adversarial samples can on average reduce the performance of these DNNs from 5.41% to 9.58%. Through retraining the DNNs with our generated adversarial samples, the robustness of DNN can improve by 23.05% on average. The evaluation results also show that our adversarial test generation strategy has the least negative impact (median of 3.56%), on the performance of the DNNs for regular test data, compared to the other methods.",Source Code Processing;Deep Neural Network;Testing,"36, 46",,IEEE Conferences,"2021 14th IEEE Conference on Software Testing, Verification and Validation (ICST)",IEEE
199,,Fail-Safe Execution of Deep Learning based Systems through Uncertainty Monitoring,M. Weiss; P. Tonella,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9438598,10.1109/ICST49551.2021.00015,"Modern software systems rely on Deep Neural Networks (DNN) when processing complex, unstructured inputs, such as images, videos, natural language texts or audio signals. Provided the intractably large size of such input spaces, the intrinsic limitations of learning algorithms and the ambiguity about the expected predictions for some of the inputs, not only there is no guarantee that DNN's predictions are always correct, but rather developers must safely assume a low, though not negligible, error probability. A fail-safe Deep Learning based System (DLS) is one equipped to handle DNN faults by means of a supervisor, capable of recognizing predictions that should not be trusted and that should activate a healing procedure bringing the DLS to a safe state.In this paper, we propose an approach to use DNN uncertainty estimators to implement such supervisor. We first discuss advantages and disadvantages of existing approaches to measure uncertainty for DNNs and propose novel metrics for the empirical assessment of the supervisor that rely on such approaches. We then describe our publicly available tool UNCERTAINTY-WIZARD, which allows transparent estimation of uncertainty for regular tf.keras DNNs. Lastly, we discuss a large-scale study conducted on four different subjects to empirically validate the approach, reporting the lessons-learned as guidance for software engineers who intend to monitor uncertainty for fail-safe execution of DLS.",fault tolerance;software reliability;software testing;art neural networks,"24, 35",,IEEE Conferences,"2021 14th IEEE Conference on Software Testing, Verification and Validation (ICST)",IEEE
200,,MulCode: A Multi-task Learning Approach for Source Code Understanding,D. Wang; Y. Yu; S. Li; W. Dong; J. Wang; L. Qing,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9426045,10.1109/SANER50967.2021.00014,"Recent years have witnessed the significant rise of Deep Learning (DL) techniques applied to source code. Researchers exploit DL for a multitude of tasks and achieve impressive results. However, most tasks are explored separately, resulting in a lack of generalization of the solutions. In this work, we propose MulCode, a multi-task learning approach for source code understanding that learns unified representation space for tasks, with the pre-trained BERT model for the token sequence and the Tree-LSTM model for abstract syntax trees. Furthermore, we integrate two source code views into a hybrid representation via the attention mechanism and set learnable uncertainty parameters to adjust the tasks' relationship.We train and evaluate MulCode in three downstream tasks: comment classification, author attribution, and duplicate function detection. In all tasks, MulCode outperforms the state-of-the-art techniques. Moreover, experiments on three unseen tasks demonstrate the generalization ability of MulCode compared with state-of-the-art embedding methods.",representation learning;deep learning;multi-task learning;attention mechanism,"48, 59",,IEEE Conferences,"2021 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)",IEEE
201,,Graph Neural Network Based Collaborative Filtering for API Usage Recommendation,C. Ling; Y. Zou; B. Xie,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9425944,10.1109/SANER50967.2021.00013,"Developers often face the need to find out how to use different APIs suitable for their purposes. API usage recommendation has been shown very useful to facilitate the process of software reuse and daily development. Previous approaches mainly use statistical models and collaborative filtering(CF) techniques to improve the accuracy of recommendation. However, they fail to exploit the high-order connectivity of the interaction of API calls and the structural information of software projects. In this paper, we formulate this problem in terms of the graph-based collaborative filtering recommendation. We propose a novel approach for API usage recommendation, named GAPI, which uses graph neural networks (GNNs) to capture the high-order collaborative signals from API calls. Besides, GAPI integrates project structures into the graph and incorporates text attributes in the network, which are helpful to represent the program semantics. We evaluate our approach on large-scale open-source repositories collected from Github and Maven Central. The experimental results demonstrate that our approach is effective and outperforms the state-of-the-art approaches in terms of success rate and accuracy.",API usage recommendation;graph neural networks;collaborative filtering,"36, 47",,IEEE Conferences,"2021 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)",IEEE
202,,Plot2API: Recommending Graphic API from Plot via Semantic Parsing Guided Neural Network,Z. Wang; S. Huang; Z. Liu; M. Yan; X. Xia; B. Wang; D. Yang,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9425892,10.1109/SANER50967.2021.00049,"Plot-based Graphic API recommendation (Plot2API) is an unstudied but meaningful issue, which has several important applications in the context of software engineering and data visualization, such as the plotting guidance of the beginner, graphic API correlation analysis, and code conversion for plotting. Plot2API is a very challenging task, since each plot is often associated with multiple APIs and the appearances of the graphics drawn by the same API can be extremely varied due to the different settings of the parameters. Additionally, the samples of different APIs also suffer from extremely imbalanced.Considering the lack of technologies in Plot2API, we present a novel deep multi-task learning approach named Semantic Parsing Guided Neural Network (SPGNN) which translates the Plot2API issue as a multi-label image classification and an image semantic parsing tasks for the solution. In SPGNN, the recently advanced Convolutional Neural Network (CNN) named EfficientNet is employed as the backbone network for API recommendation. Meanwhile, a semantic parsing module is complemented to exploit the semantic relevant visual information in feature learning and eliminate the appearance-relevant visual information which may confuse the visual-information-based API recommendation. Moreover, the recent data augmentation technique named random erasing is also applied for alleviating the imbalance of API categories.We collect plots with the graphic APIs used to drawn them from Stack Overflow, and release three new Plot2API datasets corresponding to the graphic APIs of R and Python programming languages for evaluating the effectiveness of Plot2API techniques. Extensive experimental results not only demonstrate the superiority of our method over the recent deep learning baselines but also show the practicability of our method in the recommendation of graphic APIs.",API Recommendation;Data Visualization;Image Recognition,"458, 469",,IEEE Conferences,"2021 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)",IEEE
203,,Bug Question Answering with Pretrained Encoders,L. Bo; J. Lu,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9425955,10.1109/SANER50967.2021.00083,"Bug question answering is an effective way to acquire the required bug information and to help bug comprehension. Many existing approaches use keyword matching techniques to obtain more bug information directly without understanding the semantic information of bug data, which make the returned results irrelevant to the input queries. To alleviate this problem, we present a novel bug question answering approach named BERT-BugQA that takes advantage of the Bidirectional Encoder Representations from Transformers (BERT) which can fully consider the bidirectional context of bug information. In special, we design a common paradigm to construct the bug reading comprehension dataset for this approach. Empirical study demonstrates that BERT-BugQA is effective to automatically obtain the answers, and the F1-score values of Mozilla and Eclipse project are 0.84 and 0.83, respectively, which are better than the state-of-the-art Q&A approaches. Index Terms-Bug question answering, BERT, Bug natural language reading comprehension.",Bug question answering;BERT;Bug natural language reading comprehension,"654, 660",,IEEE Conferences,"2021 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)",IEEE
204,,DeepCon: Contribution Coverage Testing for Deep Learning Systems,Z. Zhou; W. Dou; J. Liu; C. Zhang; J. Wei; D. Ye,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9425998,10.1109/SANER50967.2021.00026,"Deep learning (DL) has been widely adopted in many safety-critical scenarios. Deep neural networks (DNNs) usually play the core part in these DL systems. Existing studies have shown that DNNs can suffer from various vulnerabilities, and cause severe consequences. To improve the testing adequacy of DNNs, researchers have proposed several coverage criteria, e.g., neuron coverage in DeepXplore. The prediction result of a DNN is jointly determined by the outputs of neurons and the connection weights that they connect into next-level neurons. However, existing coverage criteria use only the output of a neuron to determine the activation state of the neuron and ignore the connection weights it emits.In this paper, we propose DeepCon, a novel contribution coverage. In DeepCon, we define a term contribution as the combination of the output of a neuron and the connection weight it emits, and use the contribution coverage to gauge the testing adequacy of DNNs. DeepCon can thoroughly cover both neurons and the connection weights they emit and can scale well to large DNNs. We further propose a contribution coverage guided test generation approach, DeepCon-Gen, which can automatically generate tests and activate inactivated contributions of DNNs. We evaluate DeepCon and DeepCon-Gen on five different DNNs over two popular datasets. The experimental results show that DeepCon can well present the testing adequacy of these DNNs. DeepCon-Gen can effectively activate the inactivated contributions, and 62.6% of the generated tests can lead to mispredictions.",Deep learning;deep neural network;coverage criteria;contribution coverage;adversarial example detection,"189, 200",,IEEE Conferences,"2021 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)",IEEE
205,,Using Structural and Semantic Information to Identify Software Components,C. Sas; A. Capiluppi,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9425947,10.1109/SANER50967.2021.00063,"Component Based Software Engineering (CBSE) seeks to promote the reuse of software by using existing software modules into the development process. However, the availability of such a reusable component is not immediate and is costly and time consuming. As an alternative, the extraction from preexisting OO software can be considered.In this work, we evaluate two community detection algorithms for the task of software components identification. Considering `components' as `communities', the aim is to evaluate how independent, yet cohesive, the components are when extracted by structurally informed algorithms.We analyze 412 Java systems and evaluate the cohesion of the extracted communities using four document representation techniques. The evaluation aims to find which algorithm extracts the most semantically cohesive, yet separated communities.The results show a good performance in both algorithms, however, each has its own strengths. Leiden extracts less cohesive, but better separated, and better clustered components that depend more on similar ones. Infomap, on the other side, creates more cohesive, slightly overlapping clusters that are less likely to depend on other semantically similar components.",Components Identification;Community Detection;Components Semantic Analysis,"546, 550",,IEEE Conferences,"2021 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)",IEEE
206,,Leveraging Stack Overflow to Detect Relevant Tutorial Fragments of APIs,D. Wu; X. -Y. Jing; H. Zhang; Y. Zhou; B. Xu,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9426050,10.1109/SANER50967.2021.00020,"Developers often use learning resources such as API tutorials and Stack Overflow (SO) to learn how to use an unfamiliar API. An API tutorial can be divided into a number of consecutive units that describe the same topic, denoted as tutorial fragments. We consider a tutorial fragment explaining the API usage knowledge as a relevant fragment of the API. Discovering relevant tutorial fragments of APIs can facilitate API understanding and learning. However, existing approaches, based on supervised or unsupervised approaches, often suffer from either high manual efforts or lack of consideration of the relevance information. In this paper, we propose a novel approach, called SO2RT, to detect relevant tutorial fragments of APIs based on SO posts. SO2RT first automatically extracts relevant and irrelevant 〈API,QA〉 pairs based on heuristic rules of SO, and constructs 〈API, FRA〉 pairs (FRA stands out fragment) by using tutorial fragments and APIs. SO2RT then trains a semi-supervised transfer learning based detection model, which can transfer the API usage knowledge in SO Q&A pairs to tutorial fragments by utilizing the easy-to-extract relevance of 〈API, QA〉 pairs. Finally, relevant fragments of APIs can be discovered by consulting the trained model. In this way, the effort for labeling the relevance between tutorial fragments and APIs can be reduced. We evaluate SO2RT on Java and Android datasets containing 21,008 〈API, QA〉 pairs. Experimental results show that SO2RT improves the state-of-the-art approaches in terms of F-Measure on both datasets. Our user study further confirms the effectiveness of SO2RT in practice.",,"119, 130",,IEEE Conferences,"2021 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)",IEEE
207,,Representation vs. Model: What Matters Most for Source Code Vulnerability Detection,W. Zheng; A. O. Abdallah Semasaba; X. Wu; S. A. Agyemang; T. Liu; Y. Ge,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9426055,10.1109/SANER50967.2021.00082,"Vulnerabilities in the source code of software are critical issues in the realm of software engineering. Coping with vulnerabilities in software source code is becoming more challenging due to several aspects of complexity and volume. Deep learning has gained popularity throughout the years as a means of addressing such issues. In this paper, we propose an evaluation of vulnerability detection performance on source code representations and evaluate how Machine Learning (ML) strategies can improve them. The structure of our experiment consists of 3 Deep Neural Networks (DNNs) in conjunction with five different source code representations; Abstract Syntax Trees (ASTs), Code Gadgets (CGs), Semantics-based Vulnerability Candidates (SeVCs), Lexed Code Representations (LCRs), and Composite Code Representations (CCRs). Experimental results show that employing different ML strategies in conjunction with the base model structure influences the performance results to a varying degree. However, ML-based techniques suffer from poor performance on class imbalance handling when used in conjunction with source code representations for software vulnerability detection.",security;software vulnerability detection;deep learning,"647, 653",,IEEE Conferences,"2021 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)",IEEE
208,,Automatic Detection of Five API Documentation Smells: Practitioners’ Perspectives,J. Y. Khan; M. Tawkat Islam Khondaker; G. Uddin; A. Iqbal,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9425926,10.1109/SANER50967.2021.00037,"The learning and usage of an API is supported by official documentation. Like source code, API documentation is itself a software product. Several research results show that bad design in API documentation can make the reuse of API features difficult. Indeed, similar to code smells or code anti-patterns, poorly designed API documentation can also exhibit `smells'. Such documentation smells can be described as bad documentation styles that do not necessarily produce an incorrect documentation but nevertheless make the documentation difficult to properly understand and to use. Recent research on API documentation has focused on finding content inaccuracies in API documentation and to complement API documentation with external resources (e.g., crowd-shared code examples). We are aware of no research that focused on the automatic detection of API documentation smells. This paper makes two contributions. First, we produce a catalog of five API documentation smells by consulting literature on API documentation presentation problems. We create a benchmark dataset of 1,000 API documentation units by exhaustively and manually validating the presence of the five smells in Java official API reference and instruction documentation. Second, we conduct a survey of 21 professional software developers to validate the catalog. The developers agreed that they frequently encounter all five smells in API official documentation and 95.2% of them reported that the presence of the documentation smells negatively affects their productivity. The participants wished for tool support to automatically detect and fix the smells in API official documentation. We develop a suite of rule-based, deep and shallow machine learning classifiers to automatically detect the smells. The best performing classifier BERT, a deep learning model, achieves F1-scores of 0.75 - 0.97.",API Documentation;Smell;Benchmark;Survey;Shallow Learning;Deep Learning,"318, 329",,IEEE Conferences,"2021 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)",IEEE
209,,Hybrid Quantum Network for classification of finance and MNIST data,G. Hellstem,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9425825,10.1109/ICSA-C52384.2021.00027,"In the ongoing era of noisy intermediate scaled quantum computers, one of the possible applications to search for an advantage of quantum computing is machine learning. Here we report about an approach, where a hybrid quantumclassical network is applied to classify non-trivial datasets (finance and MNIST data). In comparison to a pure classical network, we find an advantage when looking at several performance measures. However, as in classical machine learning problems around overfitting the dataset arise. Therefore, we outline the path to future research, which has to be done in the field of (hybrid) quantum networks.",Quantum Computing;Machine Learning;Tensorflow;Regularization;Finance;MNIST,"1, 4",,IEEE Conferences,2021 IEEE 18th International Conference on Software Architecture Companion (ICSA-C),IEEE
210,,Image-Based Social Sensing: Combining AI and the Crowd to Mine Policy-Adherence Indicators from Twitter,V. Negri; D. Scuratti; S. Agresti; D. Rooein; G. Scalia; A. Ravi Shankar; J. L. Fernandez Marquez; M. J. Carman; B. Pernici,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402168,10.1109/ICSE-SEIS52602.2021.00019,"Social Media provides a trove of information that, if aggregated and analysed appropriately can provide important statistical indicators to policy makers. In some situations these indicators are not available through other mechanisms. For example, given the ongoing COVID-19 outbreak, it is essential for governments to have access to reliable data on policy-adherence with regards to mask wearing, social distancing, and other hard-to-measure quantities. In this paper we investigate whether it is possible to obtain such data by aggregating information from images posted to social media. The paper presents VisualCit, a pipeline for image-based social sensing combining recent advances in image recognition technology with geocoding and crowdsourcing techniques. Our aim is to discover in which countries, and to what extent, people are following COVID-19 related policy directives. We compared the results with the indicators produced within the CovidDataHub behavior tracker initiative. Preliminary results shows that social media images can produce reliable indicators for policy makers.",social media;social sensing;citizen science;crowdsourcing;machine learning;image classification,"92, 101",,IEEE Conferences,2021 IEEE/ACM 43rd International Conference on Software Engineering: Software Engineering in Society (ICSE-SEIS),IEEE
211,,IdBench: Evaluating Semantic Representations of Identifier Names in Source Code,Y. Wainakh; M. Rauf; M. Pradel,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9401986,10.1109/ICSE43902.2021.00059,"Identifier names convey useful information about the intended semantics of code. Name-based program analyses use this information, e.g., to detect bugs, to predict types, and to improve the readability of code. At the core of name-based analyses are semantic representations of identifiers, e.g., in the form of learned embeddings. The high-level goal of such a representation is to encode whether two identifiers, e.g., len and size, are semantically similar. Unfortunately, it is currently unclear to what extent semantic representations match the semantic relatedness and similarity perceived by developers. This paper presents IdBench, the first benchmark for evaluating semantic representations against a ground truth created from thousands of ratings by 500 software developers. We use IdBench to study state-of-the-art embedding techniques proposed for natural language, an embedding technique specifically designed for source code, and lexical string distance functions. Our results show that the effectiveness of semantic representations varies significantly and that the best available embeddings successfully represent semantic relatedness. On the downside, no existing technique provides a satisfactory representation of semantic similarities, among other reasons because identifiers with opposing meanings are incorrectly considered to be similar, which may lead to fatal mistakes, e.g., in a refactoring tool. Studying the strengths and weaknesses of the different techniques shows that they complement each other. As a first step toward exploiting this complementarity, we present an ensemble model that combines existing techniques and that clearly outperforms the best available semantic representation.","source code, Neural Networks, embeddings, identifiers, benchmark","562, 573",,IEEE Conferences,2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE),IEEE
212,,An Evolutionary Study of Configuration Design and Implementation in Cloud Systems,Y. Zhang; H. He; O. Legunsen; S. Li; W. Dong; T. Xu,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9401962,10.1109/ICSE43902.2021.00029,"Many techniques were proposed for detecting software misconfigurations in cloud systems and for diagnosing unintended behavior caused by such misconfigurations. Detection and diagnosis are steps in the right direction: misconfigurations cause many costly failures and severe performance issues. But, we argue that continued focus on detection and diagnosis is symptomatic of a more serious problem: configuration design and implementation are not yet first-class software engineering endeavors in cloud systems. Little is known about how and why developers evolve configuration design and implementation, and the challenges that they face in doing so. This paper presents a source-code level study of the evolution of configuration design and implementation in cloud systems. Our goal is to understand the rationale and developer practices for revising initial configuration design/implementation decisions, especially in response to consequences of misconfigurations. To this end, we studied 1178 configuration-related commits from a 2.5 year version-control history of four large-scale, actively-maintained open-source cloud systems (HDFS, HBase, Spark, and Cassandra). We derive new insights into the software configuration engineering process. Our results motivate new techniques for proactively reducing misconfigurations by improving the configuration design and implementation process in cloud systems. We highlight a number of future research directions.",,"188, 200",,IEEE Conferences,2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE),IEEE
213,,Operation is the Hardest Teacher: Estimating DNN Accuracy Looking for Mispredictions,A. Guerriero; R. Pietrantuono; S. Russo,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402144,10.1109/ICSE43902.2021.00042,"Deep Neural Networks (DNN) are typically tested for accuracy relying on a set of unlabelled real world data (operational dataset), from which a subset is selected, manually labelled and used as test suite. This subset is required to be small (due to manual labelling cost) yet to faithfully represent the operational context, with the resulting test suite containing roughly the same proportion of examples causing misprediction (i.e., failing test cases) as the operational dataset. However, while testing to estimate accuracy, it is desirable to also learn as much as possible from the failing tests in the operational dataset, since they inform about possible bugs of the DNN. A smart sampling strategy may allow to intentionally include in the test suite many examples causing misprediction, thus providing this way more valuable inputs for DNN improvement while preserving the ability to get trustworthy unbiased estimates. This paper presents a test selection technique (DeepEST) that actively looks for failing test cases in the operational dataset of a DNN, with the goal of assessing the DNN expected accuracy by a small and ""informative"" test suite (namely with a high number of mispredictions) for subsequent DNN improvement. Experiments with five subjects, combining four DNN models and three datasets, are described. The results show that DeepEST provides DNN accuracy estimates with precision close to (and often better than) those of existing sampling-based DNN testing techniques, while detecting from 5 to 30 times more mispredictions, with the same test suite size.","Software testing, Artificial Neural Networks","348, 358",,IEEE Conferences,2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE),IEEE
214,,Semi-Supervised Log-Based Anomaly Detection via Probabilistic Label Estimation,L. Yang; J. Chen; Z. Wang; W. Wang; J. Jiang; X. Dong; W. Zhang,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9401970,10.1109/ICSE43902.2021.00130,"With the growth of software systems, logs have become an important data to aid system maintenance. Log-based anomaly detection is one of the most important methods for such purpose, which aims to automatically detect system anomalies via log analysis. However, existing log-based anomaly detection approaches still suffer from practical issues due to either depending on a large amount of manually labeled training data (supervised approaches) or unsatisfactory performance without learning the knowledge on historical anomalies (unsupervised and semi-supervised approaches). In this paper, we propose a novel practical log-based anomaly detection approach, PLELog, which is semi-supervised to get rid of time-consuming manual labeling and incorporates the knowledge on historical anomalies via probabilistic label estimation to bring supervised approaches' superiority into play. In addition, PLELog is able to stay immune to unstable log data via semantic embedding and detect anomalies efficiently and effectively by designing an attention-based GRU neural network. We evaluated PLELog on two most widely-used public datasets, and the results demonstrate the effectiveness of PLELog, significantly outperforming the compared approaches with an average of 181.6% improvement in terms of F1-score. In particular, PLELog has been applied to two real-world systems from our university and a large corporation, further demonstrating its practicability.",Log Analysis;Anomaly Detection;Deep Learning;Probabilistic Estimation;Label,"1448, 1460",,IEEE Conferences,2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE),IEEE
215,,PyART: Python API Recommendation in Real-Time,X. He; L. Xu; X. Zhang; R. Hao; Y. Feng; B. Xu,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402054,10.1109/ICSE43902.2021.00145,"API recommendation in real-time is challenging for dynamic languages like Python. Many existing API recommendation techniques are highly effective, but they mainly support static languages. A few Python IDEs provide API recommendation functionalities based on type inference and training on a large corpus of Python libraries and third-party libraries. As such, they may fail to recommend or make poor recommendations when type information is missing or target APIs are project-specific. In this paper, we propose a novel approach, PyART, to recommend APIs for Python programs in real-time. It features a light-weight analysis to derives so-called optimistic data-flow, which is neither sound nor complete, but simulates the local data-flow information humans can derive. It extracts three kinds of features: data-flow, token similarity, and token co-occurrence, in the context of the program point where a recommendation is solicited. A predictive model is trained on these features using the Random Forest algorithm. Evaluation on 8 popular Python projects demonstrates that PyART can provide effective API recommendations. When historic commits can be leveraged, which is the target scenario of a state-of-the-art tool ARIREC, our average top-1 accuracy is over 50% and average top-10 accuracy over 70%, outperforming APIREC and Intellicode (i.e., the recommendation component in Visual Studio) by 28.48%-39.05% for top-1 accuracy and 24.41%-30.49% for top-10 accuracy. In other applications such as when historic comments are not available and cross-project recommendation, PyART also shows better overall performance. The time to make a recommendation is less than a second on average, satisfying the real-time requirement.","API recommendation, context analysis, data flow analysis, real time recommendation, Python","1634, 1645",,IEEE Conferences,2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE),IEEE
216,,Automatic Extraction of Opinion-Based Q&A from Online Developer Chats,P. Chatterjee; K. Damevski; L. Pollock,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402078,10.1109/ICSE43902.2021.00115,"Virtual conversational assistants designed specifically for software engineers could have a huge impact on the time it takes for software engineers to get help. Research efforts are focusing on virtual assistants that support specific software development tasks such as bug repair and pair programming. In this paper, we study the use of online chat platforms as a resource towards collecting developer opinions that could potentially help in building opinion Q&A systems, as a specialized instance of virtual assistants and chatbots for software engineers. Opinion Q&A has a stronger presence in chats than in other developer communications, thus mining them can provide a valuable resource for developers in quickly getting insight about a specific development topic (e.g., What is the best Java library for parsing JSON?). We address the problem of opinion Q&A extraction by developing automatic identification of opinion-asking questions and extraction of participants' answers from public online developer chats. We evaluate our automatic approaches on chats spanning six programming communities and two platforms. Our results show that a heuristic approach to opinion-asking questions works well (.87 precision), and a deep learning approach customized to the software domain outperforms heuristics-based, machine-learning-based and deep learning for answer extraction in community question answering.",opinion question-answering system;public chats;opinion-asking question;answer extraction,"1260, 1272",,IEEE Conferences,2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE),IEEE
217,,TransRegex: Multi-modal Regular Expression Synthesis by Generate-and-Repair,Y. Li; S. Li; Z. Xu; J. Cao; Z. Chen; Y. Hu; H. Chen; S. -C. Cheung,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9401951,10.1109/ICSE43902.2021.00111,"Since regular expressions (abbrev. regexes) are difficult to understand and compose, automatically generating regexes has been an important research problem. This paper introduces TransRegex, for automatically constructing regexes from both natural language descriptions and examples. To the best of our knowledge, TransRegex is the first to treat the NLP-and-example-based regex synthesis problem as the problem of NLP-based synthesis with regex repair. For this purpose, we present novel algorithms for both NLP-based synthesis and regex repair. We evaluate TransRegex with ten relevant state-of-the-art tools on three publicly available datasets. The evaluation results demonstrate that the accuracy of our TransRegex is 17.4%, 35.8% and 38.9% higher than that of NLP-based approaches on the three datasets, respectively. Furthermore, TransRegex can achieve higher accuracy than the state-of-the-art multi-modal techniques with 10% to 30% higher accuracy on all three datasets. The evaluation results also indicate TransRegex utilizing natural language and examples in a more effective way.",regex synthesis;regex repair;programming by natural languages;programming by example,"1210, 1222",,IEEE Conferences,2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE),IEEE
218,,Scalable Quantitative Verification for Deep Neural Networks,T. Baluta; Z. L. Chua; K. S. Meel; P. Saxena,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402111,10.1109/ICSE43902.2021.00039,"Despite the functional success of deep Neural Networks, their trustworthiness remains a crucial open challenge. To address this challenge, both testing and verification techniques have been proposed. But these existing techniques provide either scalability to large networks or formal guarantees, not both. In this paper, we propose a scalable quantitative verification framework for deep Neural Networks, i.e., a test-driven approach that comes with formal guarantees that a desired probabilistic property is satisfied. Our technique performs enough tests until soundness of a formal probabilistic property can be proven. It can be used to certify properties of both deterministic and randomized DNNs. We implement our approach in a tool called PROVERO and apply it in the context of certifying adversarial robustness of DNNs. In this context, we first show a new attack-agnostic measure of robustness which offers an alternative to purely attack-based methodology of evaluating robustness being reported today. Second, PROVERO provides certificates of robustness for large DNNs, where existing state-of-the-art verification tools fail to produce conclusive results. Our work paves the way forward for verifying properties of distributions captured by real-world deep Neural Networks, with provable guarantees, even where testers only have black-box access to the neural network.",deep Neural Networks;quantitative verification;probabilistic;robustness,"312, 323",,IEEE Conferences,2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE),IEEE
219,,DeepLocalize: Fault Localization for Deep Neural Networks,M. Wardat; W. Le; H. Rajan,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402065,10.1109/ICSE43902.2021.00034,"Deep Neural Networks (DNNs) are becoming an integral part of most software systems. Previous work has shown that DNNs have bugs. Unfortunately, existing debugging techniques don't support localizing DNN bugs because of the lack of understanding of model behaviors. The entire DNN model appears as a black box. To address these problems, we propose an approach and a tool that automatically determines whether the model is buggy or not, and identifies the root causes for DNN errors. Our key insight is that historic trends in values propagated between layers can be analyzed to identify faults, and also localize faults. To that end, we first enable dynamic analysis of deep learning applications: by converting it into an imperative representation and alternatively using a callback mechanism. Both mechanisms allows us to insert probes that enable dynamic analysis over the traces produced by the DNN while it is being trained on the training data. We then conduct dynamic analysis over the traces to identify the faulty layer or hyperparameter that causes the error. We propose an algorithm for identifying root causes by capturing any numerical error and monitoring the model during training and finding the relevance of every layer/parameter on the DNN outcome. We have collected a benchmark containing 40 buggy models and patches that contain real errors in deep learning applications from Stack Overflow and GitHub. Our benchmark can be used to evaluate automated debugging tools and repair techniques. We have evaluated our approach using this DNN bug-and-patch benchmark, and the results showed that our approach is much more effective than the existing debugging approach used in the state-of-the-practice Keras library. For 34/40 cases, our approach was able to detect faults whereas the best debugging approach provided by Keras detected 32/40 faults. Our approach was able to localize 21/40 bugs whereas Keras did not localize any faults.",Deep Neural Networks;Fault Location;Debugging;Program Analysis;Deep learning bugs,"251, 262",,IEEE Conferences,2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE),IEEE
220,,Traceability Transformed: Generating More Accurate Links with Pre-Trained BERT Models,J. Lin; Y. Liu; Q. Zeng; M. Jiang; J. Cleland-Huang,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402118,10.1109/ICSE43902.2021.00040,"Software traceability establishes and leverages associations between diverse development artifacts. Researchers have proposed the use of deep learning trace models to link natural language artifacts, such as requirements and issue descriptions, to source code; however, their effectiveness has been restricted by availability of labeled data and efficiency at runtime. In this study, we propose a novel framework called Trace BERT (T-BERT) to generate trace links between source code and natural language artifacts. To address data sparsity, we leverage a three-step training strategy to enable trace models to transfer knowledge from a closely related Software Engineering challenge, which has a rich dataset, to produce trace links with much higher accuracy than has previously been achieved. We then apply the T-BERT framework to recover links between issues and commits in Open Source Projects. We comparatively evaluated accuracy and efficiency of three BERT architectures. Results show that a Single-BERT architecture generated the most accurate links, while a Siamese-BERT architecture produced comparable results with significantly less execution time. Furthermore, by learning and transferring knowledge, all three models in the framework outperform classical IR trace models. On the three evaluated real-word OSS projects, the best T-BERT stably outperformed the VSM model with average improvements of 60.31% measured using Mean Average Precision (MAP). RNN severely underperformed on these projects due to insufficient training data, while T-BERT overcame this problem by using pretrained language models and transfer learning.",software traceability;deep learning;langauge model,"324, 335",,IEEE Conferences,2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE),IEEE
221,,CURE: Code-Aware Neural Machine Translation for Automatic Program Repair,N. Jiang; T. Lutellier; L. Tan,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9401997,10.1109/ICSE43902.2021.00107,"Automatic program repair (APR) is crucial to improve software reliability. Recently, neural machine translation (NMT) techniques have been used to automatically fix software bugs. While promising, these approaches have two major limitations. Their search space often does not contain the correct fix, and their search strategy ignores software knowledge such as strict code syntax. Due to these limitations, existing NMT-based techniques underperform the best template-based approaches. We propose CURE, a new NMT-based APR technique with three major novelties. First, CURE pre-trains a programming language (PL) model on a large software codebase to learn developer-like source code before the APR task. Second, CURE designs a new code-aware search strategy that finds more correct fixes by focusing on searching for compilable patches and patches that are close in length to the buggy code. Finally, CURE uses a subword tokenization technique to generate a smaller search space that contains more correct fixes. Our evaluation on two widely-used benchmarks shows that CURE correctly fixes 57 Defects4J bugs and 26 QuixBugs bugs, outperforming all existing APR techniques on both benchmarks.","automatic program repair, software reliability","1161, 1173",,IEEE Conferences,2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE),IEEE
222,,InferCode: Self-Supervised Learning of Code Representations by Predicting Subtrees,N. D. Q. Bui; Y. Yu; L. Jiang,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402028,10.1109/ICSE43902.2021.00109,"Learning code representations has found many uses in software engineering, such as code classification, code search, comment generation, and bug prediction, etc. Although representations of code in tokens, syntax trees, dependency graphs, paths in trees, or the combinations of their variants have been proposed, existing learning techniques have a major limitation that these models are often trained on datasets labeled for specific downstream tasks, and as such the code representations may not be suitable for other tasks. Even though some techniques generate representations from unlabeled code, they are far from being satisfactory when applied to the downstream tasks. To overcome the limitation, this paper proposes InferCode, which adapts the self-supervised learning idea from natural language processing to the abstract syntax trees (ASTs) of code. The novelty lies in the training of code representations by predicting subtrees automatically identified from the contexts of ASTs. With InferCode, subtrees in ASTs are treated as the labels for training the code representations without any human labelling effort or the overhead of expensive graph construction, and the trained representations are no longer tied to any specific downstream tasks or code units. We have trained an instance of InferCode model using Tree-Based Convolutional Neural Network (TBCNN) as the encoder of a large set of Java code. This pre-trained model can then be applied to downstream unsupervised tasks such as code clustering, code clone detection, cross-language code search, or be reused under a transfer learning scheme to continue training the model weights for supervised tasks such as code classification and method name prediction. Compared to prior techniques applied to the same downstream tasks, such as code2vec, code2seq, ASTNN, using our pre-trained InferCode model higher performance is achieved with a significant margin for most of the tasks, including those involving different programming languages. The implementation of InferCode and the trained embeddings are available at the link: https://github.com/bdqnghi/infercode.",code search;self supervised;code clone detection;cross language;fine tuning;code retrieval;unlabel data;unlabelled data,"1186, 1197",,IEEE Conferences,2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE),IEEE
223,,DeepPayload: Black-box Backdoor Attack on Deep Learning Models through Neural Payload Injection,Y. Li; J. Hua; H. Wang; C. Chen; Y. Liu,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402020,10.1109/ICSE43902.2021.00035,"Deep learning models are increasingly used in mobile applications as critical components. Unlike the program bytecode whose vulnerabilities and threats have been widely-discussed, whether and how the deep learning models deployed in the applications can be compromised are not well-understood since Neural Networks are usually viewed as a black box. In this paper, we introduce a highly practical backdoor attack achieved with a set of reverse-engineering techniques over compiled deep learning models. The core of the attack is a neural conditional branch constructed with a trigger detector and several operators and injected into the victim model as a malicious payload. The attack is effective as the conditional logic can be flexibly customized by the attacker, and scalable as it does not require any prior knowledge from the original model. We evaluated the attack effectiveness using 5 state-of-the-art deep learning models and real-world samples collected from 30 users. The results demonstrated that the injected backdoor can be triggered with a success rate of 93.5%, while only brought less than 2ms latency overhead and no more than 1.4% accuracy decrease. We further conducted an empirical study on real-world mobile deep learning apps collected from Google Play. We found 54 apps that were vulnerable to our attack, including popular and security-critical ones. The results call for the awareness of deep learning application developers and auditors to enhance the protection of deployed models.",deep learning;backdoor attack;mobile applications;reverse engineering;malicious payload,"263, 274",,IEEE Conferences,2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE),IEEE
224,,Studying the Usage of Text-To-Text Transfer Transformer to Support Code-Related Tasks,A. Mastropaolo; S. Scalabrino; N. Cooper; D. Nader Palacio; D. Poshyvanyk; R. Oliveto; G. Bavota,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9401982,10.1109/ICSE43902.2021.00041,"Deep learning (DL) techniques are gaining more and more attention in the software engineering community. They have been used to support several code-related tasks, such as automatic bug fixing and code comments generation. Recent studies in the Natural Language Processing (NLP) field have shown that the Text-To-Text Transfer Transformer (T5) architecture can achieve state-of-the-art performance for a variety of NLP tasks. The basic idea behind T5 is to first pre-train a model on a large and generic dataset using a self-supervised task (e.g., filling masked words in sentences). Once the model is pre-trained, it is fine-tuned on smaller and specialized datasets, each one related to a specific task (e.g., language translation, sentence classification). In this paper, we empirically investigate how the T5 model performs when pre-trained and fine-tuned to support code-related tasks. We pre-train a T5 model on a dataset composed of natural language English text and source code. Then, we fine-tune such a model by reusing datasets used in four previous works that used DL techniques to: (i) fix bugs, (ii) inject code mutants, (iii) generate assert statements, and (iv) generate code comments. We compared the performance of this single model with the results reported in the four original papers proposing DL-based solutions for those four tasks. We show that our T5 model, exploiting additional data for the self-supervised pre-training phase, can achieve performance improvements over the four baselines.","Empirical software engineering, Deep Learning","336, 347",,IEEE Conferences,2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE),IEEE
225,,Are Machine Learning Cloud APIs Used Correctly?,C. Wan; S. Liu; H. Hoffmann; M. Maire; S. Lu,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402073,10.1109/ICSE43902.2021.00024,"Machine learning (ML) cloud APIs enable developers to easily incorporate learning solutions into software systems. Unfortunately, ML APIs are challenging to use correctly and efficiently, given their unique semantics, data requirements, and accuracy-performance tradeoffs. Much prior work has studied how to develop ML APIs or ML cloud services, but not how open-source applications are using ML APIs. In this paper, we manually studied 360 representative open-source applications that use Google or AWS cloud-based ML APIs, and found 70% of these applications contain API misuses in their latest versions that degrade functional, performance, or economical quality of the software. We have generalized 8 anti-patterns based on our manual study and developed automated checkers that identify hundreds of more applications that contain ML API misuses.",machine learning;software engineering;cloud API,"125, 137",,IEEE Conferences,2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE),IEEE
226,,Automatic Solution Summarization for Crash Bugs,H. Wang; X. Xia; D. Lo; J. Grundy; X. Wang,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9401996,10.1109/ICSE43902.2021.00117,"The causes of software crashes can be hidden anywhere in the source code and development environment. When encountering software crashes, recurring bugs that are discussed on Q&A sites could provide developers with solutions to their crashing problems. However, it is difficult for developers to accurately search for relevant content on search engines, and developers have to spend a lot of manual effort to find the right solution from the returned results. In this paper, we present CRASOLVER, an approach that takes into account both the structural information of crash traces and the knowledge of crash-causing bugs to automatically summarize solutions from crash traces. Given a crash trace, CRASOLVER retrieves relevant questions from Q&A sites by combining a proposed position dependent similarity - based on the structural information of the crash trace - with an extra knowledge similarity, based on the knowledge from official documentation sites. After obtaining the answers to these questions from the Q&A site, CRASOLVER summarizes the final solution based on a multi-factor scoring mechanism. To evaluate our approach, we built two repositories of Java and Android exception-related questions from Stack Overflow with size of 69,478 and 33,566 questions respectively. Our user study results using 50 selected Java crash traces and 50 selected Android crash traces show that our approach significantly outperforms four baselines in terms of relevance, usefulness, and diversity. The evaluation also confirms the effectiveness of the relevant question retrieval component in our approach for crash traces.",,"1286, 1297",,IEEE Conferences,2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE),IEEE
227,,GUIGAN: Learning to Generate GUI Designs Using Generative Adversarial Networks,T. Zhao; C. Chen; Y. Liu; X. Zhu,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402029,10.1109/ICSE43902.2021.00074,"Graphical User Interface (GUI) is ubiquitous in almost all modern desktop software, mobile applications and online websites. A good GUI design is crucial to the success of the software in the market, but designing a good GUI which requires much innovation and creativity is difficult even to well-trained designers. In addition, the requirement of rapid development of GUI design also aggravates designers' working load. So, the availability of various automated generated GUIs can help enhance the design personalization and specialization as they can cater to the taste of different designers. To assist designers, we develop a model tool to automatically generate GUI designs. Different from conventional image generation models based on image pixels, our tool is to reuse GUI components collected from existing mobile app GUIs for composing a new design which is similar to natural-language generation. Our tool is based on SeqGAN by modelling the GUI component style compatibility and GUI structure. The evaluation demonstrates that our model significantly outperforms the best of the baseline methods by 30.77% in Fr'echet Inception distance (FID) and 12.35% in 1-Nearest Neighbor Accuracy (1-NNA). Through a pilot user study, we provide initial evidence of the usefulness of our approach for generating acceptable brand new GUI designs.","Graphical User Interface, mobile application, GUI design, deep learning, Generative Adversarial Network","748, 760",,IEEE Conferences,2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE),IEEE
228,,Self-Checking Deep Neural Networks in Deployment,Y. Xiao; I. Beschastnikh; D. S. Rosenblum; C. Sun; S. Elbaum; Y. Lin; J. S. Dong,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402003,10.1109/ICSE43902.2021.00044,"The widespread adoption of Deep Neural Networks (DNNs) in important domains raises questions about the trustworthiness of DNN outputs. Even a highly accurate DNN will make mistakes some of the time, and in settings like self-driving vehicles these mistakes must be quickly detected and properly dealt with in deployment. Just as our community has developed effective techniques and mechanisms to monitor and check programmed components, we believe it is now necessary to do the same for DNNs. In this paper we present DNN self-checking as a process by which internal DNN layer features are used to check DNN predictions. We detail SelfChecker, a self-checking system that monitors DNN outputs and triggers an alarm if the internal layer features of the model are inconsistent with the final prediction. SelfChecker also provides advice in the form of an alternative prediction. We evaluated SelfChecker on four popular image datasets and three DNN models and found that SelfChecker triggers correct alarms on 60.56% of wrong DNN predictions, and false alarms on 2.04% of correct DNN predictions. This is a substantial improvement over prior work (SelfOracle, Dissector, and ConfidNet). In experiments with self-driving car scenarios, SelfChecker triggers more correct alarms than SelfOracle for two DNN models (DAVE-2 and Chauffeur) with comparable false alarms. Our implementation is available as open source.",deep learning;trustworthiness;deployment,"372, 384",,IEEE Conferences,2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE),IEEE
229,,DeepLV: Suggesting Log Levels Using Ordinal Based Neural Networks,Z. Li; H. Li; T. -H. Chen; W. Shang,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402068,10.1109/ICSE43902.2021.00131,"Developers write logging statements to generate logs that provide valuable runtime information for debugging and maintenance of software systems. Log level is an important component of a logging statement, which enables developers to control the information to be generated at system runtime. However, due to the complexity of software systems and their runtime behaviors, deciding a proper log level for a logging statement is a challenging task. For example, choosing a higher level (e.g., error) for a trivial event may confuse end users and increase system maintenance overhead, while choosing a lower level (e.g., trace) for a critical event may prevent the important execution information to be conveyed opportunely. In this paper, we tackle the challenge by first conducting a preliminary manual study on the characteristics of log levels. We find that the syntactic context of the logging statement and the message to be logged might be related to the decision of log levels, and log levels that are further apart in order (e.g., trace and error) tend to have more differences in their characteristics. Based on this, we then propose a deep-learning based approach that can leverage the ordinal nature of log levels to make suggestions on choosing log levels, by using the syntactic context and message features of the logging statements extracted from the source code. Through an evaluation on nine large-scale open source projects, we find that: 1) our approach outperforms the state-of-the-art baseline approaches; 2) we can further improve the performance of our approach by enlarging the training data obtained from other systems; 3) our approach also achieves promising results on cross-system suggestions that are even better than the baseline approaches on within-system suggestions. Our study highlights the potentials in suggesting log levels to help developers make informed logging decisions.",logs;deep learning;log level;empirical study,"1461, 1472",,IEEE Conferences,2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE),IEEE
230,,Prioritizing Test Inputs for Deep Neural Networks via Mutation Analysis,Z. Wang; H. You; J. Chen; Y. Zhang; X. Dong; W. Zhang,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402064,10.1109/ICSE43902.2021.00046,"Deep Neural Network (DNN) testing is one of the most widely-used ways to guarantee the quality of DNNs. However, labeling test inputs to check the correctness of DNN prediction is very costly, which could largely affect the efficiency of DNN testing, even the whole process of DNN development. To relieve the labeling-cost problem, we propose a novel test input prioritization approach (called PRIMA) for DNNs via intelligent mutation analysis in order to label more bug-revealing test inputs earlier for a limited time, which facilitates to improve the efficiency of DNN testing. PRIMA is based on the key insight: a test input that is able to kill many mutated models and produce different prediction results with many mutated inputs, is more likely to reveal DNN bugs, and thus it should be prioritized higher. After obtaining a number of mutation results from a series of our designed model and input mutation rules for each test input, PRIMA further incorporates learning-to-rank (a kind of supervised machine learning to solve ranking problems) to intelligently combine these mutation results for effective test input prioritization. We conducted an extensive study based on 36 popular subjects by carefully considering their diversity from five dimensions (i.e., different domains of test inputs, different DNN tasks, different network structures, different types of test inputs, and different training scenarios). Our experimental results demonstrate the effectiveness of PRIMA, significantly outperforming the state-of-the-art approaches (with the average improvement of 8.50%~131.01% in terms of prioritization effectiveness). In particular, we have applied PRIMA to the practical autonomous-vehicle testing in a large motor company, and the results on 4 real-world scene-recognition models in autonomous vehicles further confirm the practicability of PRIMA.",Test Prioritization;Deep Neural Network;Mutation;Label;Deep Learning Testing,"397, 409",,IEEE Conferences,2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE),IEEE
231,,Code Prediction by Feeding Trees to Transformers,S. Kim; J. Zhao; Y. Tian; S. Chandra,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402114,10.1109/ICSE43902.2021.00026,"Code prediction, more specifically autocomplete, has become an essential feature in modern IDEs. Autocomplete is more effective when the desired next token is at (or close to) the top of the list of potential completions offered by the IDE at cursor position. This is where the strength of the underlying machine learning system that produces a ranked order of potential completions comes into play. We advance the state-of-the-art in the accuracy of code prediction (next token prediction) used in autocomplete systems. Our work uses Transformers as the base neural architecture. We show that by making the Transformer architecture aware of the syntactic structure of code, we increase the margin by which a Transformer-based system outperforms previous systems. With this, it outperforms the accuracy of several state-of-the-art next token prediction systems by margins ranging from 14% to 18%. We present in the paper several ways of communicating the code structure to the Transformer, which is fundamentally built for processing sequence data. We provide a comprehensive experimental evaluation of our proposal, along with alternative design choices, on a standard Python dataset, as well as on Facebook internal Python corpus. Our code and data preparation pipeline will be available in open source.",code embedding;code prediction;autocomplete,"150, 162",,IEEE Conferences,2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE),IEEE
232,,SOAR: A Synthesis Approach for Data Science API Refactoring,A. Ni; D. Ramos; A. Z. H. Yang; I. Lynce; V. Manquinho; R. Martins; C. Le Goues,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402016,10.1109/ICSE43902.2021.00023,"With the growth of the open-source data science community, both the number of data science libraries and the number of versions for the same library are increasing rapidly. To match the evolving APIs from those libraries, open-source organizations often have to exert manual effort to refactor the APIs used in the code base. Moreover, due to the abundance of similar open-source libraries, data scientists working on a certain application may have an abundance of libraries to choose, maintain and migrate between. The manual refactoring between APIs is a tedious and error-prone task. Although recent research efforts were made on performing automatic API refactoring between different languages, previous work relies on statistical learning with collected pairwise training data for the API matching and migration. Using large statistical data for refactoring is not ideal because such training data will not be available for a new library or a new version of the same library. We introduce Synthesis for Open-Source API Refactoring (SOAR), a novel technique that requires no training data to achieve API migration and refactoring. SOAR relies only on the documentation that is readily available at the release of the library to learn API representations and mapping between libraries. Using program synthesis, SOAR automatically computes the correct configuration of arguments to the APIs and any glue code required to invoke those APIs. SOAR also uses the interpreter's error messages when running refactored code to generate logical constraints that can be used to prune the search space. Our empirical evaluation shows that SOAR can successfully refactor 80% of our benchmarks corresponding to deep learning models with up to 44 layers with an average run time of 97.23 seconds, and 90% of the data wrangling benchmarks with an average run time of 17.31 seconds.","software maintenance, program translation, program synthesis","112, 124",,IEEE Conferences,2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE),IEEE
233,,Shipwright: A Human-in-the-Loop System for Dockerfile Repair,J. Henkel; D. Silva; L. Teixeira; M. d’Amorim; T. Reps,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402069,10.1109/ICSE43902.2021.00106,"Docker is a tool for lightweight OS-level virtualization. Docker images are created by performing a build, controlled by a source-level artifact called a Dockerfile. We studied Dockerfiles on GitHub, and-to our great surprise-found that over a quarter of the examined Dockerfiles failed to build (and thus to produce images). To address this problem, we propose SHIPWRIGHT, a human-in-the-loop system for finding repairs to broken Dockerfiles. SHIPWRIGHT uses a modified version of the BERT language model to embed build logs and to cluster broken Dockerfiles. Using these clusters and a search-based procedure, we were able to design 13 rules for making automated repairs to Dockerfiles. With the aid of SHIPWRIGHT, we submitted 45 pull requests (with a 42.2% acceptance rate) to GitHub projects with broken Dockerfiles. Furthermore, in a ""time-travel"" analysis of broken Dockerfiles that were later fixed, we found that SHIPWRIGHT proposed repairs that were equivalent to human-authored patches in 22.77% of the cases we studied. Finally, we compared our work with recent, state-of-the-art, static Dockerfile analyses, and found that, while static tools detected possible build-failure-inducing issues in 20.6-33.8% of the files we examined, SHIPWRIGHT was able to detect possible issues in 73.25% of the files and, additionally, provide automated repairs for 18.9% of the files.",Docker;DevOps;Repair,"1148, 1160",,IEEE Conferences,2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE),IEEE
234,,Prioritize Crowdsourced Test Reports via Deep Screenshot Understanding,S. Yu; C. Fang; Z. Cao; X. Wang; T. Li; Z. Chen,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9401980,10.1109/ICSE43902.2021.00090,"Crowdsourced testing is increasingly dominant in mobile application (app) testing, but it is a great burden for app developers to inspect the incredible number of test reports. Many researches have been proposed to deal with test reports based only on texts or additionally simple image features. However, in mobile app testing, texts contained in test reports are condensed and the information is inadequate. Many screenshots are included as complements that contain much richer information beyond texts. This trend motivates us to prioritize crowdsourced test reports based on a deep screenshot understanding. In this paper, we present a novel crowdsourced test report prioritization approach, namely DeepPrior. We fifirstrst represent the crowdsourced test reports with a novelly introduced feature, namely DeepFeature, that includes all the widgets along with their texts, coordinates, types, and even intents based on the deep analysis of the app screenshots, and the textual descriptions in the crowdsourced test reports. DeepFeature includes the <i>Bug</i><i>Feature</i>, which directly describes the bugs, and the <i>Context</i><i>Feature</i>, which depicts the thorough context of the bug. The similarity of the DeepFeature is used to represent the test reports' similarity and prioritize the crowdsourced test reports. We formally define the similarity as DeepSimilarity. We also conduct an empirical experiment to evaluate the effectiveness of the proposed technique with a large dataset group. The results show that DeepPrior is promising, and it outperforms the state-of-the-art approach with less than half the overhead.",Crowdsourced testing;Mobile App Testing;Deep Screenshot Understanding,"946, 956",,IEEE Conferences,2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE),IEEE
235,,Representation of Developer Expertise in Open Source Software,T. Dey; A. Karnauch; A. Mockus,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9401957,10.1109/ICSE43902.2021.00094,"Background: Accurate representation of developer expertise has always been an important research problem. While a number of studies proposed novel methods of representing expertise within individual projects, these methods are difficult to apply at an ecosystem level. However, with the focus of software development shifting from monolithic to modular, a method of representing developers' expertise in the context of the entire OSS development becomes necessary when, for example, a project tries to find new maintainers and look for developers with relevant skills. Aim: We aim to address this knowledge gap by proposing and constructing the Skill Space where each API, developer, and project is represented and postulate how the topology of this space should reflect what developers know (and projects need). Method: we use the World of Code infrastructure to extract the complete set of APIs in the files changed by open source developers and, based on that data, employ Doc2Vec embeddings for vector representations of APIs, developers, and projects. We then evaluate if these embeddings reflect the postulated topology of the Skill Space by predicting what new APIs/projects developers use/join, and whether or not their pull requests get accepted. We also check how the developers' representations in the Skill Space align with their self-reported API expertise. Result: Our results suggest that the proposed embeddings in the Skill Space appear to satisfy the postulated topology and we hope that such representations may aid in the construction of signals that increase trust (and efficiency) of open source ecosystems at large and may aid investigations of other phenomena related to developer proficiency and learning.","Expertise, Developer Expertise, Vector Embedding, Doc2Vec, API, API embedding, Project embedding, Developer embedding, Skill Space, Machine Learning, Open Source, World of Code","995, 1007",,IEEE Conferences,2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE),IEEE
236,,Early Life Cycle Software Defect Prediction. Why? How?,S. N.C.; S. Majumder; T. Menzies,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9401968,10.1109/ICSE43902.2021.00050,"Many researchers assume that, for software analytics, ""more data is better."" We write to show that, at least for learning defect predictors, this may not be true. To demonstrate this, we analyzed hundreds of popular GitHub projects. These projects ran for 84 months and contained 3,728 commits (median values). Across these projects, most of the defects occur very early in their life cycle. Hence, defect predictors learned from the first 150 commits and four months perform just as well as anything else. This means that, at least for the projects studied here, after the first few months, we need not continually update our defect prediction models. We hope these results inspire other researchers to adopt a ""simplicity-first"" approach to their work. Some domains require a complex and data-hungry analysis. But before assuming complexity, it is prudent to check the raw data looking for ""short cuts"" that can simplify the analysis.","sampling, early, defect prediction, analytics","448, 459",,IEEE Conferences,2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE),IEEE
237,,An Empirical Study on Deployment Faults of Deep Learning Based Mobile Applications,Z. Chen; H. Yao; Y. Lou; Y. Cao; Y. Liu; H. Wang; X. Liu,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9401981,10.1109/ICSE43902.2021.00068,"Deep learning (DL) is moving its step into a growing number of mobile software applications. These software applications, named as DL based mobile applications (abbreviated as mobile DL apps) integrate DL models trained using large-scale data with DL programs. A DL program encodes the structure of a desirable DL model and the process by which the model is trained using training data. Due to the increasing dependency of current mobile apps on DL, software engineering (SE) for mobile DL apps has become important. However, existing efforts in SE research community mainly focus on the development of DL models and extensively analyze faults in DL programs. In contrast, faults related to the deployment of DL models on mobile devices (named as deployment faults of mobile DL apps) have not been well studied. Since mobile DL apps have been used by billions of end users daily for various purposes including for safety-critical scenarios, characterizing their deployment faults is of enormous importance. To fill in the knowledge gap, this paper presents the first comprehensive study to date on the deployment faults of mobile DL apps. We identify 304 real deployment faults from Stack Overflow and GitHub, two commonly used data sources for studying software faults. Based on the identified faults, we construct a fine-granularity taxonomy consisting of 23 categories regarding to fault symptoms and distill common fix strategies for different fault symptoms. Furthermore, we suggest actionable implications and research avenues that can potentially facilitate the deployment of DL models on mobile devices.","deep learning, mobile applications, deployment faults","674, 685",,IEEE Conferences,2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE),IEEE
238,,Testing Machine Translation via Referential Transparency,P. He; C. Meister; Z. Su,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402091,10.1109/ICSE43902.2021.00047,"Machine translation software has seen rapid progress in recent years due to the advancement of deep Neural Networks. People routinely use machine translation software in their daily lives for tasks such as ordering food in a foreign restaurant, receiving medical diagnosis and treatment from foreign doctors, and reading international political news online. However, due to the complexity and intractability of the underlying Neural Networks, modern machine translation software is still far from robust and can produce poor or incorrect translations; this can lead to misunderstanding, financial loss, threats to personal safety and health, and political conflicts. To address this problem, we introduce referentially transparent inputs (RTIs), a simple, widely applicable methodology for validating machine translation software. A referentially transparent input is a piece of text that should have similar translations when used in different contexts. Our practical implementation, Purity, detects when this property is broken by a translation. To evaluate RTI, we use Purity to test Google Translate and Bing Microsoft Translator with 200 unlabeled sentences, which detected 123 and 142 erroneous translations with high precision (79.3% and 78.3%). The translation errors are diverse, including examples of under-translation, over-translation, word/phrase mistranslation, incorrect modification, and unclear logic.",Testing;Machine translation;Referential transparency;Metamorphic testing,"410, 422",,IEEE Conferences,2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE),IEEE
239,,Identifying Key Features from App User Reviews,H. Wu; W. Deng; X. Niu; C. Nie,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402119,10.1109/ICSE43902.2021.00088,"Due to the rapid growth and strong competition of mobile application (app) market, app developers should not only offer users with attractive new features, but also carefully maintain and improve existing features based on users' feedbacks. User reviews indicate a rich source of information to plan such feature maintenance activities, and it could be of great benefit for developers to evaluate and magnify the contribution of specific features to the overall success of their apps. In this study, we refer to the features that are highly correlated to app ratings as key features, and we present KEFE, a novel approach that leverages app description and user reviews to identify key features of a given app. The application of KEFE especially relies on natural language processing, deep machine learning classifier, and regression analysis technique, which involves three main steps: 1) extracting feature-describing phrases from app description; 2) matching each app feature with its relevant user reviews; and 3) building a regression model to identify features that have significant relationships with app ratings. To train and evaluate KEFE, we collect 200 app descriptions and 1,108,148 user reviews from Chinese Apple App Store. Experimental results demonstrate the effectiveness of KEFE in feature extraction, where an average F-measure of 78.13% is achieved. The key features identified are also likely to provide hints for successful app releases, as for the releases that receive higher app ratings, 70% of features improvements are related to key features.","key features, feature extraction, user reviews, app store analysis","922, 932",,IEEE Conferences,2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE),IEEE
240,,Mining Software Repositories with a Collaborative Heuristic Repository,H. Babii; J. A. Prenner; L. Stricker; A. Karmakar; A. Janes; R. Robbes,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402255,10.1109/ICSE-NIER52604.2021.00030,"Many software engineering studies or tasks rely on categorizing software engineering artifacts. In practice, this is done either by defining simple but often imprecise heuristics, or by manual labelling of the artifacts. Unfortunately, errors in these categorizations impact the tasks that rely on them. To improve the precision of these categorizations, we propose to gather heuristics in a collaborative heuristic repository, to which researchers can contribute a large amount of diverse heuristics for a variety of tasks on a variety of SE artifacts. These heuristics are then leveraged by state-of-the-art weak supervision techniques to train high-quality classifiers, thus improving the categorizations. We present an initial version of the heuristic repository, which we applied to the concrete task of commit classification.",Mining Software Repositories;MSR Heuristics;Weak Supervision,"106, 110",,IEEE Conferences,2021 IEEE/ACM 43rd International Conference on Software Engineering: New Ideas and Emerging Results (ICSE-NIER),IEEE
241,,Robustness of on-Device Models: Adversarial Attack to Deep Learning Models on Android Apps,Y. Huang; H. Hu; C. Chen,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402124,10.1109/ICSE-SEIP52600.2021.00019,"Deep learning has shown its power in many applications, including object detection in images, natural-language understanding, and speech recognition. To make it more accessible to end users, many deep learning models are now embedded in mobile apps. Compared to offloading deep learning from smartphones to the cloud, performing machine learning on-device can help improve latency, connectivity, and power consumption. However, most deep learning models within Android apps can easily be obtained via mature reverse engineering, while the models' exposure may invite adversarial attacks. In this study, we propose a simple but effective approach to hacking deep learning models using adversarial attacks by identifying highly similar pre-trained models from TensorFlow Hub. All 10 real-world Android apps in the experiment are successfully attacked by our approach. Apart from the feasibility of the model attack, we also carry out an empirical study that investigates the characteristics of deep learning models used by hundreds of Android apps on Google Play. The results show that many of them are similar to each other and widely use fine-tuning techniques to pre-trained models on the Internet.","deep learning, mobile apps, Android, security, adversarial attack","101, 110",,IEEE Conferences,2021 IEEE/ACM 43rd International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP),IEEE
242,,NNStreamer: Efficient and Agile Development of On-Device AI Systems,M. Ham; J. Moon; G. Lim; J. Jung; H. Ahn; W. Song; S. Woo; P. Kapoor; D. Chae; G. Jang; Y. Ahn; J. Lee,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402062,10.1109/ICSE-SEIP52600.2021.00029,"We propose NNStreamer, a software system that handles neural networks as filters of stream pipelines, applying the stream processing paradigm to deep neural network applications. A new trend with the wide-spread of deep neural network applications is on-device AI. It is to process neural networks on mobile devices or edge/IoT devices instead of cloud servers. Emerging privacy issues, data transmission costs, and operational costs signify the need for on-device AI, especially if we deploy a massive number of devices. NNStreamer efficiently handles neural networks with complex data stream pipelines on devices, significantly improving the overall performance with minimal effort. Besides, NNStreamer simplifies implementations and allows reusing off-the-shelf media filters directly, which reduces developmental costs significantly. We are already deploying NNStreamer for a wide range of products and platforms, including the Galaxy series and various consumer electronic devices. The experimental results suggest a reduction in developmental costs and enhanced performance of pipeline architectures and NNStreamer. It is an open-source project incubated by Linux Foundation AI & Data, available to the public and applicable to various hardware and software platforms.",neural network;on-device AI;stream processing;pipe and filter architecture;open source software,"198, 207",,IEEE Conferences,2021 IEEE/ACM 43rd International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP),IEEE
243,,FIXME: Enhance Software Reliability with Hybrid Approaches in Cloud,J. Hwang; L. Shwartz; Q. Wang; R. Batta; H. Kumar; M. Nidd,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402017,10.1109/ICSE-SEIP52600.2021.00032,"With the promise of reliability in cloud, more enterprises are migrating to cloud. The process of continuous integration/deployment (CICD) in cloud connects developers who need to deliver value faster and more transparently with site reliability engineers (SREs) who need to manage applications reliably. SREs feed back development issues to developers, and developers commit fixes and trigger CICD to redeploy. The release cycle is more continuous than ever, thus the code to production is faster and more automated. To provide this higher level agility, the cloud platforms become more complex in the face of flexibility with deeper layers of virtualization. However, reliability does not come for free with all these complexities. Software engineers and SREs need to deal with wider information spectrum from virtualized layers. Therefore, providing correlated information with true positive evidences is critical to identify the root cause of issues quickly in order to reduce mean time to recover (MTTR), performance metrics for SREs. Similarity, knowledge, or statistics driven approaches have been effective, but with increasing data volume and types, an individual approach is limited to correlate semantic relations of different data sources. In this paper, we introduce FIXME to enhance software reliability with hybrid diagnosis approaches for enterprises. Our evaluation results show using hybrid diagnosis approach is about 17% better in precision. The results are helpful for both practitioners and researchers to develop hybrid diagnosis in the highly dynamic cloud environment.",event management;hybrid system;event correlation;localization;cloud,"228, 237",,IEEE Conferences,2021 IEEE/ACM 43rd International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP),IEEE
244,,Neural Knowledge Extraction From Cloud Service Incidents,M. Shetty; C. Bansal; S. Kumar; N. Rao; N. Nagappan; T. Zimmermann,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402085,10.1109/ICSE-SEIP52600.2021.00031,"The move from boxed products to services and the widespread adoption of cloud computing has had a huge impact on the software development life cycle and DevOps processes. Particularly, incident management has become critical for developing and operating large-scale services. Prior work on incident management has heavily focused on the challenges with incident triaging and de-duplication. In this work, we address the fundamental problem of structured knowledge extraction from service incidents. We have built SoftNER, a framework for unsupervised knowledge extraction from service incidents. We frame the knowledge extraction problem as a Named-Entity Recognition task for extracting factual information. SoftNER leverages structural patterns like key value pairs and tables for bootstrapping the training data. Further, we build a novel multitask learning based BiLSTM-CRF model which leverages not just the semantic context but also the data-types for named-entity extraction. We have deployed SoftNER at Microsoft, a major cloud service provider and have evaluated it on more than 2 months of cloud incidents. We show that the unsupervised machine learning pipeline has a high precision of 0.96. Our multi-task learning based deep learning model also outperforms the state of the art NER models. Lastly, using the knowledge extracted by SoftNER we are able to build significantly more accurate models for important downstream tasks like incident triaging.",Cloud Services;Service Incidents;Information Extraction;Knowledge Extraction;Deep Learning;Machine Learning,"218, 227",,IEEE Conferences,2021 IEEE/ACM 43rd International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP),IEEE
245,,"NLP for Requirements Engineering: Tasks, Techniques, Tools, and Technologies",A. Ferrari; L. Zhao; W. Alhoshan,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402628,10.1109/ICSE-Companion52605.2021.00137,"Requirements engineering (RE) is one of the most natural language-intensive fields within the software engineering area. Therefore, several works have been developed across the years to automate the analysis of natural language artifacts that are relevant for RE, including requirements documents, but also app reviews, privacy policies, and social media content related to software products. Furthermore, the recent diffusion of game-changing natural language processing (NLP) techniques and plat-forms has also boosted the interest of RE researchers. However, a reference framework to provide a holistic understanding of the field of NLP for RE is currently missing. Based on the results of a recent systematic mapping study, and stemming from a previous ICSE tutorial by one of the authors, this technical briefing gives an overview of NLP for RE tasks, available techniques, supporting tools and NLP technologies. It is oriented to both researchers and practitioners, and will gently guide the audience towards a clearer view of how NLP can empower RE, providing pointers to representative works and specialised tools.",NLP;Requirements Engineering;Software Engineering;Transfer Learning;Survey;Mapping Study;Empirical Studies;Tutorial,"322, 323",,IEEE Conferences,2021 IEEE/ACM 43rd International Conference on Software Engineering: Companion Proceedings (ICSE-Companion),IEEE
246,,Roosterize: Suggesting Lemma Names for Coq Verification Projects Using Deep Learning,P. Nie; K. Palmskog; J. J. Li; M. Gligoric,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402311,10.1109/ICSE-Companion52605.2021.00026,"Naming conventions are an important concern in large verification projects using proof assistants, such as Coq. In particular, lemma names are used by proof engineers to effectively understand and modify Coq code. However, providing accurate and informative lemma names is a complex task, which is currently often carried out manually. Even when lemma naming is automated using rule-based tools, generated names may fail to adhere to important conventions not specified explicitly. We demonstrate a toolchain, dubbed Roosterize, which automatically suggests lemma names in Coq projects. Roosterize leverages a neural network model trained on existing Coq code, thus avoiding manual specification of naming conventions. To allow proof engineers to conveniently access suggestions from Roosterize during Coq project development, we integrated the toolchain into the popular Visual Studio Code editor. Our evaluation shows that Roosterize substantially outperforms strong baselines for suggesting lemma names and is useful in practice. The demo video for Roosterize can be viewed at: https://youtu.be/HZ5ac7Q14rc.",Coq;lemma names;neural networks,"21, 24",,IEEE Conferences,2021 IEEE/ACM 43rd International Conference on Software Engineering: Companion Proceedings (ICSE-Companion),IEEE
247,,Testing Object Detection for Autonomous Driving Systems via 3D Reconstruction,J. Shao,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402601,10.1109/ICSE-Companion52605.2021.00052,"Object detection is to identify objects from images. In autonomous driving systems, object detection serves as an intermediate module, which is used as the input of autonomous decisions for vehicles. That is, the accuracy of autonomous decisions relies on the object detection. The state-of-the-art object detection modules are designed based on Deep Neural Networks (DNNs). It is difficult to employ white-box testing on DNNs since the output of a single neuron is inexplicable. Existing work conducted metamorphic testing for object detection via image synthesis: the detected object in the original image should be detected in the new synthetic image. However, a synthetic image may not look real from humans' perspective. Even the object detection module fails in detecting such synthetic image, the failure may not reflect the ability of object detection. In this paper, we propose an automatic approach to testing object detection via 3D reconstruction of vehicles in real photos. The 3D reconstruction is developed via vanishing point estimation in photos and heuristic based image insertion. Our approach adds new objects to blank spaces in photos to synthesize images. For example, a new vehicle can be added to a photo of a road and vehicles. In this approach, the output synthetic images are expected to be more natural-looking than randomly synthesizing images. The experiment is conducting on 500 driving photos from the Apollo autonomous driving dataset.","metamorphic testing, object detection system, vanishing point, image processing","117, 119",,IEEE Conferences,2021 IEEE/ACM 43rd International Conference on Software Engineering: Companion Proceedings (ICSE-Companion),IEEE
248,,RPT: Effective and Efficient Retrieval of Program Translations from Big Code,B. Chen; Z. Abedjan,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402662,10.1109/ICSE-Companion52605.2021.00117,"Program translation is a growing demand in software engineering. Manual program translation requires programming expertise in source and target language. One way to automate this process is to make use of the big data of programs, i.e., Big Code. However, existing code retrieval techniques lack the design to cover cross-language code retrieval. Other data-driven approaches require human efforts in constructing cross-language parallel datasets to train translation models. In this paper, we present RPT, a novel code translation retrieval system. We propose a lightweight but informative program representation, which can be generalized to all imperative PLs. Furthermore, we present our index structure and hierarchical filtering mechanism for efficient code retrieval from a Big Code database.",,"252, 253",,IEEE Conferences,2021 IEEE/ACM 43rd International Conference on Software Engineering: Companion Proceedings (ICSE-Companion),IEEE
249,,PyART: Python API Recommendation in Real-Time,X. He; L. Xu; X. Zhang; R. Hao; Y. Feng; B. Xu,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402592,10.1109/ICSE-Companion52605.2021.00114,"This is the research artifact of the paper titled 'PyART: Python API Recommendation in Real-Time'. PyART is a real-time API recommendation tool for Python, which includes two main functions: data-flow analysis and real-time API recommendation for both incomplete and complete Python code context. Compared to classical tools, PyART has two important particularities: it is able to work on real-time recommendation scenario, and it provides data-flow analysis and API recommendation for dynamic language. Classical tools often fail to make static analysis in real-time recommendation scenario, due to the incompletion of syntax. And the dynamic features of Python language also bring challenges to type inference and API recommendation. Different from classical tools, PyART derives optimistic data-flow that is neither sound nor complete but sufficient for API recommendation and cost-effective to collect, and provides real-time API recommendations based on novel candidate collection, context analysis and feature learning techniques. The artifact evaluation experiments of PyART include three main aspects: data-flow analysis, intra-project API recommendation and across-project API recommendation. We assume users of the artifact is able to use Linux Ubuntu Operating System.","API recommendation, context analysis, data flow analysis, real-time recommendation, Python","246, 247",,IEEE Conferences,2021 IEEE/ACM 43rd International Conference on Software Engineering: Companion Proceedings (ICSE-Companion),IEEE
250,,ProMal: Precise Window Transition Graphs for Android via Synergy of Program Analysis and Machine Learning,C. Liu,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402354,10.1109/ICSE-Companion52605.2021.00061,"Mobile apps have been an integral part in our daily life. As these apps become more complex, it is critical to provide automated analysis techniques to ensure the correctness, security, and performance of these apps. A key component for these automated analysis techniques is to create a graphical user interface (GUI) model of an app, i.e., a window transition graph (WTG), that models windows and transitions among the windows. While existing work has provided both static and dynamic analysis to build the WTG for an app, the constructed WTG misses many transitions or contains many infeasible transitions due to the coverage issues of dynamic analysis and over-approximation of the static analysis. We propose ProMal, a ""tribrid"" analysis that synergistically combines static analysis, dynamic analysis, and machine learning to construct a precise WTG. Specifically, ProMal first applies static analysis to build a static WTG, and then applies dynamic analysis to verify the transitions in the static WTG. For the unverified transitions, ProMal further provides machine learning techniques that leverage runtime information (i.e., screenshots, UI layouts, and text information) to predict whether they are feasible transitions. Our evaluations on 40 real-world apps demonstrate the superiority of ProMal in building WTGs over static analysis, dynamic analysis, and machine learning techniques when they are applied separately.",Android;Window Transition Graph;GUI Testing;Static Analysis;Dynamic Analysis;Machine Learning,"144, 146",,IEEE Conferences,2021 IEEE/ACM 43rd International Conference on Software Engineering: Companion Proceedings (ICSE-Companion),IEEE
251,,Scalable Quantitative Verification for Deep Neural Networks,T. Baluta; Z. L. Chua; K. S. Meel; P. Saxena,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402591,10.1109/ICSE-Companion52605.2021.00115,"Despite the functional success of deep neural networks (DNNs), their trustworthiness remains a crucial open challenge. To address this challenge, both testing and verification techniques have been proposed. But these existing techniques pro- vide either scalability to large networks or formal guarantees, not both. In this paper, we propose a scalable quantitative verification framework for deep neural networks, i.e., a test-driven approach that comes with formal guarantees that a desired probabilistic property is satisfied. Our technique performs enough tests until soundness of a formal probabilistic property can be proven. It can be used to certify properties of both deterministic and randomized DNNs. We implement our approach in a tool called PROVERO1 and apply it in the context of certifying adversarial robustness of DNNs. In this context, we first show a new attack- agnostic measure of robustness which offers an alternative to purely attack-based methodology of evaluating robustness being reported today. Second, PROVERO provides certificates of robustness for large DNNs, where existing state-of-the-art verification tools fail to produce conclusive results. Our work paves the way forward for verifying properties of distributions captured by real-world deep neural networks, with provable guarantees, even where testers only have black-box access to the neural network.",quantitative-verification;deep-neural-networks;robustness;probabilistic,"248, 249",,IEEE Conferences,2021 IEEE/ACM 43rd International Conference on Software Engineering: Companion Proceedings (ICSE-Companion),IEEE
252,,GNNMark: A Benchmark Suite to Characterize Graph Neural Network Training on GPUs,T. Baruah; K. Shivdikar; S. Dong; Y. Sun; S. A. Mojumder; K. Jung; J. L. Abellán; Y. Ukidave; A. Joshi; J. Kim; D. Kaeli,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9408205,10.1109/ISPASS51385.2021.00013,"Graph Neural Networks (GNNs) have emerged as a promising class of Machine Learning algorithms to train on non-euclidean data. GNNs are widely used in recommender systems, drug discovery, text understanding, and traffic forecasting. Due to the energy efficiency and high-performance capabilities of GPUs, GPUs are a natural choice for accelerating the training of GNNs. Thus, we want to better understand the architectural and system-level implications of training GNNs on GPUs. Presently, there is no benchmark suite available designed to study GNN training workloads. In this work, we address this need by presenting GNNMark, a feature-rich benchmark suite that covers the diversity present in GNN training workloads, datasets, and GNN frameworks. Our benchmark suite consists of GNN workloads that utilize a variety of different graph-based data structures, including homogeneous graphs, dynamic graphs, and heterogeneous graphs commonly used in a number of application domains that we mentioned above. We use this benchmark suite to explore and characterize GNN training behavior on GPUs. We study a variety of aspects of GNN execution, including both compute and memory behavior, highlighting major bottlenecks observed during GNN training. At the system level, we study various aspects, including the scalability of training GNNs across a multi-GPU system, as well as the sparsity of data, encountered during training. The insights derived from our work can be leveraged by both hardware and software developers to improve both the hardware and software performance of GNN training on GPUs.",graphs;benchmarks;GNN training;GPUs,"13, 23",,IEEE Conferences,2021 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS),IEEE
253,,AIBench Training: Balanced Industry-Standard AI Training Benchmarking,F. Tang; W. Gao; J. Zhan; C. Lan; X. Wen; L. Wang; C. Luo; Z. Cao; X. Xiong; Z. Jiang; T. Hao; F. Fan; F. Zhang; Y. Huang; J. Chen; M. Du; R. Ren; C. Zheng; D. Zheng; H. Tang; K. Zhan; B. Wang; D. Kong; M. Yu; C. Tan; H. Li; X. Tian; Y. Li; J. Shao; Z. Wang; X. Wang; J. Dai; H. Ye,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9408170,10.1109/ISPASS51385.2021.00014,"Earlier-stage evaluations of a new AI architecture/system need affordable AI benchmarks. Only using a few AI component benchmarks like MLPerf alone in the other stages may lead to misleading conclusions. Moreover, the learning dynamics are not well understood, and the benchmarks' shelf-life is short. This paper proposes a balanced benchmarking methodology. We use real-world benchmarks to cover the factors space that impacts the learning dynamics to the most considerable extent. After performing an exhaustive survey on Internet service AI domains, we identify and implement nineteen representative AI tasks with state-of-the-art models. For repeatable performance ranking (RPR subset) and workload characterization (WC subset), we keep two subsets to a minimum for affordability. We contribute by far the most comprehensive AI training benchmark suite. The evaluations show: (1) AIBench Training (v1.1) outperforms MLPerf Training (v0.7) in terms of diversity and representativeness of model complexity, computational cost, convergent rate, computation, and memory access patterns, and hotspot functions; (2) Against the AIBench full benchmarks, its RPR subset shortens the benchmarking cost by 64%, while maintaining the primary workload characteristics; (3) The performance ranking shows the single-purpose AI accelerator like TPU with the optimized TensorFlow framework performs better than that of GPUs while losing the latter's general support for various AI models. The specification, source code, and performance numbers are available from the AIBench homepage https://www.benchcouncil.org/aibench-training/index.html.",Benchmark;Deep Learning;Learning Dynamics;Training;Subsetting;Workload Characterization,"24, 35",,IEEE Conferences,2021 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS),IEEE
254,,Reducing BERT Computation by Padding Removal and Curriculum Learning,W. Zhang; W. Wei; W. Wang; L. Jin; Z. Cao,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9408202,10.1109/ISPASS51385.2021.00025,"BERT is very computationally expensive, which is a hurdle for its training and deployment. This work focuses on removing the unnecessary computation due to input padding in BERT. The input of BERT consists of two concatenated sentences. If the length of the two concatenated sentences is shorter than the maximum sequence length, padding must be added to the end of the sentences to fill the empty slots in the input. Because the lengths of sentences vary greatly, there can be a large amount of padding in input. For the English Wikipedia & BooksCorpus dataset, the percentage of padding among all the input tokens is 17% and 48%, respectively, when the max sequence length is set to 128 and 512. For the Chinese Wikipedia dataset, this percentage is 35% and 79%, respectively, when the max sequence length is 128 and 512. For SQuAD-v1.1 [2], padding accounts for 54% of the total input tokens when the max sequence length is 384. Thus, there is a lot of wasted computation on padding, which significantly increases the training and inference time.",,"90, 92",,IEEE Conferences,2021 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS),IEEE
255,,Comparative Code Structure Analysis using Deep Learning for Performance Prediction,T. Ramadan; T. Z. Islam; C. Phelps; N. Pinnow; J. J. Thiagarajan,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9408199,10.1109/ISPASS51385.2021.00032,"Performance analysis has always been an afterthought during the application development process, focusing on application correctness first. The learning curve of the existing static and dynamic analysis tools are steep, which requires understanding low-level details to interpret the findings for actionable optimizations. Additionally, application performance is a function of a number of unknowns stemming from the application-, runtime-, and interactions between the OS and underlying hardware, making it difficult to model using any deep learning technique, especially without a large labeled dataset. In this paper, we address both of these problems by presenting a large corpus of a labeled dataset for the community and take a comparative analysis approach to mitigate all unknowns except their source code differences between different correct implementations of the same problem. We put the power of deep learning to the test for automatically extracting information from the hierarchical structure of abstract syntax trees to represent source code. This paper aims to assess the feasibility of using purely static information (e.g., abstract syntax tree or AST) of applications to predict performance change based on the change in code structure. This research will enable performance-aware application development since every version of the application will continue to contribute to the corpora, which will enhance the performance of the model. We evaluate several deep learning-based representation learning techniques for source code. Our results show that tree-based Long Short-Term Memory (LSTM) models can leverage source code's hierarchical structure to discover latent representations. Specifically, LSTM-based predictive models built using a single problem and a combination of multiple problems can correctly predict if a source code will perform better or worse up to 84% and 73% of the time, respectively.",Comparative performance modeling;machine learning;Long Short-Term Memory Networks;Deep Graph Learning,"151, 161",,IEEE Conferences,2021 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS),IEEE
256,,News Text Classification and Recommendation Technology Based on Wide & Deep-Bert Model,W. Jing; Y. Bailong,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9404101,10.1109/ICICSE52190.2021.9404101,"With the rapid development of Internet technology, news information on various social platforms is growing wildly, generating large amounts of data. Since short text information such as news headlines, short messages, and newsletters has a small number of words and limited content, it is often difficult to extract effective information. The sparse feature information causes difficulties in text classification. If the news system cannot efficiently and accurately realize news classification and users preference recommendation, it will inevitably affect the experience and frequency of platform users. This paper mainly studies the application of deep learning in the field of text classification and users' personal recommendation. It uses multiple English text data sets to learn text features. Based on the Wide&Deep model, combined with the improved BERT pretraining model, the Wide&Deep-BERT model is designed. In addition, the corresponding news text classification and recommendation technology process is proposed, and the Tensorflow deep learning framework is used to experimentally verify the technology, which proves the effectiveness and practicability of the design technology.",wide&deep model;best model;text classification,"209, 216",,IEEE Conferences,2021 IEEE International Conference on Information Communication and Software Engineering (ICICSE),IEEE
257,,A Bearing Fault Diagnosis Method Based on L1 Regularization Transfer Learning and LSTM Deep Learning,D. Zhu; X. Song; J. Yang; Y. Cong; L. Wang,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9404081,10.1109/ICICSE52190.2021.9404081,"In the practical application of rail transit, it is difficult to obtain bearing fault data and the training data of fault diagnosis model is insufficient, which leads to the low accuracy and generalization ability of fault diagnosis model. In this paper, a new bearing fault diagnosis method based on transfer learning is proposed. Based on transfer learning, we introduce L1 regularization, then adds it to the Long Short-Term Memory (LSTM) classification model, and uses a small amount of target domain data to fine tune the parameters of the model, and finally constructs a bearing fault diagnosis model. In this paper, the bearing data set of Case Western Reserve University is used to test the bearing fault diagnosis model. Compared with the conventional LSTM, Gated Recurrent Unit (GRU) and Bi-LSTM, the model proposed in this paper has higher accuracy in fault diagnosis as well as certain reliability and generalization ability.",fault diagnosis;L1 regularization;transfer learning;LSTM,"308, 312",,IEEE Conferences,2021 IEEE International Conference on Information Communication and Software Engineering (ICICSE),IEEE
258,,Natural Language Processing through BERT for Identifying Gender-Based Violence Messages on Social Media,I. Soldevilla; N. Flores,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9404127,10.1109/ICICSE52190.2021.9404127,"The purpose of this article is to provide a natural language processing model to classify messages on social media that contain gender violence. The applied methodology considers performing a fine tune process to BERT, so as the output model can be trained with labeled messages -as violence and nonviolence-on the Reddit and Twitter platforms. The performance obtained in terms of area under the curve, accuracy, sensitivity and specificity were 0.9603, 0.8909, 0.8826 and 0.8989 respectively.",BERT;fine-tuning;natural language processing;text classification,"204, 208",,IEEE Conferences,2021 IEEE International Conference on Information Communication and Software Engineering (ICICSE),IEEE
259,,The Visualization of Cross-media Knowledge Graph of Tang and Song Poetry,W. Jiang; C. Li; C. Wu,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9403486,10.1109/SNPDWinter52325.2021.00022,"The special grammar and abstruse rhetoric of Tang and Song poetry doomed the diversity of poetry research. In order to help scholars explore the information space of poetry, we took into account the breadth and depth of learning, combined with heterogeneous data sources such as poetry, videos, textbooks, knowledge graph and other media sources, and carried out visual modeling. We provide a visual and informative platform to demonstrate the potential power of visualization. It supports comprehensive search capabilities for multiple types of data, online generation of poets' knowledge graphs. Relationship diagram, word cloud diagram, pie chart and other visual charts are displayed on the platform, so that users can complete exploratory visual analysis by dragging, clicking and typing.",visualization;heterogeneous network;broad learning;knowledge graph,"69, 73",,IEEE Conferences,"2021 21st ACIS International Winter Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD-Winter)",IEEE
260,,Learning to Find Bugs and Code Quality Problems - What Worked and What not?,V. Raychev,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9392977,10.1109/ICCQ51190.2021.9392977,"The recent growth of open source repositories and deep learning models brought big promises for the next generation of programming tools that can automate or significantly improve the software development process. Yet, such tools are still rare and the machine learning components in them are not always apparent to their users. The current most useful techniques in machine learning for code are also not coming from the organizations such as Microsoft, Google, DeepMind, Facebook, OpenAI or nVidia that invested the most in deep neural techniques such as huge neural networks. This probably means that either many of these coding problems are significantly different from other hot topics in deep learning such as image processing or that it is much more difficult to collect datasets that would result in similarly successful tools. In this work, we study the results in the literature on the topic and discuss ways to address these shortcomings.",,"1, 5",,IEEE Conferences,2021 International Conference on Code Quality (ICCQ),IEEE
261,,Choosing a Chatbot Development Tool,S. Perez-Soler; S. Juarez-Puerta; E. Guerra; J. de Lara,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9364349,10.1109/MS.2020.3030198,"Chatbots are programs that supply services to users via conversation in natural language, acting as virtual assistants within social networks or web applications. Here, we review the most representative chatbot development tools with a focus on technical and managerial aspects.",Software Engineering;Chatbots;Natural Language Processing,"94, 103",,IEEE Magazines,IEEE Software,IEEE
262,,Within-Project Defect Prediction of Infrastructure-as-Code Using Product and Process Metrics,S. Dalla Palma; D. Di Nucci; F. Palomba; D. A. Tamburri,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9321740,10.1109/TSE.2021.3051492,"Infrastructure-as-code (IaC) is the DevOps practice enabling management and provisioning of infrastructure through the definition of machine-readable files, hereinafter referred to as IaC scripts. Similarly to other source code artefacts, these files may contain defects that can preclude their correct functioning. In this paper, we aim at assessing the role of product and process metrics when predicting defective IaC scripts. We propose a fully integrated machine-learning framework for IaC Defect Prediction, that allows for repository crawling, metrics collection, model building, and evaluation. To evaluate it, we analyzed 104 projects and employed five machine-learning classifiers to compare their performance in flagging suspicious defective IaC scripts. The key results of the study report Random Forest as the best-performing model, with a median AUC-PR of 0.93 and MCC of 0.80. Furthermore, at least for the collected projects, product metrics identify defective IaC scripts more accurately than process metrics. Our findings put a baseline for investigating IaC Defect Prediction and the relationship between the product and process metrics, and IaC scripts' quality.",Infrastructure-as-code;Defect Prediction;Empirical Software Engineering,"1, 1",,IEEE Early Access Articles,IEEE Transactions on Software Engineering,IEEE
263,,Whence to Learn? Transferring Knowledge in Configurable Systems Using BEETLE,R. Krishna; V. Nair; P. Jamshidi; T. Menzies,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9050841,10.1109/TSE.2020.2983927,"As software systems grow in complexity and the space of possible configurations increases exponentially, finding the near-optimal configuration of a software system becomes challenging. Recent approaches address this challenge by learning performance models based on a sample set of configurations. However, collecting enough sample configurations can be very expensive since each such sample requires configuring, compiling, and executing the entire system using a complex test suite. When learning on new data is too expensive, it is possible to use <italic>Transfer Learning</italic> to “transfer” old lessons to the new context. Traditional transfer learning has a number of challenges, specifically, (a) learning from excessive data takes excessive time, and (b) the performance of the models built via transfer can deteriorate as a result of learning from a poor source. To resolve these problems, we propose a novel transfer learning framework called BEETLE, which is a “bellwether”-based transfer learner that focuses on identifying and learning from the most relevant source from amongst the old data. This paper evaluates BEETLE with 57 different software configuration problems based on five software systems (a video encoder, an SAT solver, a SQL database, a high-performance C-compiler, and a streaming data analytics tool). In each of these cases, BEETLE found configurations that are as good as or better than those found by other state-of-the-art transfer learners while requiring only a fraction (<inline-formula><tex-math notation=""LaTeX"">$\frac{1}{7}$</tex-math><alternatives><mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""><mml:mfrac><mml:mn>1</mml:mn><mml:mn>7</mml:mn></mml:mfrac></mml:math><inline-graphic xlink:href=""krishna-ieq1-2983927.gif"" xmlns:xlink=""http://www.w3.org/1999/xlink""/></alternatives></inline-formula>th) of the measurements needed by those other methods. Based on these results, we say that BEETLE is a new high-water mark in optimally configuring software.",Performance optimization;SBSE;transfer learning;bellwether,"2956, 2972",,IEEE Journals,IEEE Transactions on Software Engineering,IEEE
264,,An Exploratory Study of Machine Learning Model Stores,M. Xiu; Z. M. J. Jiang; B. Adams,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9003231,10.1109/MS.2020.2975159,Several organizations have introduced stores that provide public access to pretrained machine learning models and infrastructure. We examine three of them and compare the information they provide against two mobileapp stores and among themselves.,,"114, 122",,IEEE Magazines,IEEE Software,IEEE
265,,Checking Smart Contracts With Structural Code Embedding,Z. Gao; L. Jiang; X. Xia; D. Lo; J. Grundy,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8979435,10.1109/TSE.2020.2971482,"Smart contracts have been increasingly used together with blockchains to automate financial and business transactions. However, many bugs and vulnerabilities have been identified in many contracts which raises serious concerns about smart contract security, not to mention that the blockchain systems on which the smart contracts are built can be buggy. Thus, there is a significant need to better maintain smart contract code and ensure its high reliability. In this paper, we propose an automated approach to learn characteristics of smart contracts in Solidity, which is useful for clone detection, bug detection and contract validation on smart contracts. Our new approach is based on word embeddings and vector space comparison. We parse smart contract code into word streams with code structural information, convert code elements (e.g., statements, functions) into numerical vectors that are supposed to encode the code syntax and semantics, and compare the similarities among the vectors encoding code and known bugs, to identify potential issues. We have implemented the approach in a prototype, named S<sc>mart</sc>E<sc>mbed</sc>,<xref ref-type=""fn"" rid=""fn1""><sup>1</sup></xref><fn id=""fn1""><label>1.</label><p>The anonymous replication packages can be accessed at: <uri>https://drive.google.com/file/d/1kauLT3y2IiHPkUlVx4FSTda-dVAyL4za/view?usp=sharing</uri>.</p> </fn> and evaluated it with more than 22,000 smart contracts collected from the Ethereum blockchain. Results show that our tool can effectively identify many repetitive instances of Solidity code, where the clone ratio is around 90 percent. Code clones such as type-III or even type-IV semantic clones can also be detected accurately. Our tool can identify more than 1000 clone related bugs based on our bug databases efficiently and accurately. Our tool can also help to efficiently validate any given smart contract against a known set of bugs, which can help to improve the users’ confidence in the reliability of the contract.",Smart contract;code embedding;clone detection;bug detection;ethereum;blockchain,"2874, 2891",,IEEE Journals,IEEE Transactions on Software Engineering,IEEE
266,,An Empirical Study on Heterogeneous Defect Prediction Approaches,H. Chen; X. -Y. Jing; Z. Li; D. Wu; Y. Peng; Z. Huang,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8964460,10.1109/TSE.2020.2968520,"Software defect prediction has always been a hot research topic in the field of software engineering owing to its capability of allocating limited resources reasonably. Compared with cross-project defect prediction (CPDP), heterogeneous defect prediction (HDP) further relaxes the limitation of defect data used for prediction, permitting different metric sets to be contained in the source and target projects. However, there is still a lack of a holistic understanding of existing HDP studies due to different evaluation strategies and experimental settings. In this paper, we provide an empirical study on HDP approaches. We review the research status systematically and compare the HDP approaches proposed from 2014 to June 2018. Furthermore, we also investigate the feasibility of HDP approaches in CPDP. Through extensive experiments on 30 projects from five datasets, we have the following findings: (1) metric transformation-based HDP approaches usually result in better prediction effects, while metric selection-based approaches have better interpretability. Overall, the HDP approach proposed by Li <italic>et al.</italic> (CTKCCA) currently has the best performance. (2) Handling class imbalance problems can boost the prediction effects, but the improvements are usually limited. In addition, utilizing mixed project data cannot improve the performance of HDP approaches consistently since the label information in the target project is not used effectively. (3) HDP approaches are feasible for cross-project defect prediction in which the source and target projects have the same metric set.",Heterogeneous defect prediction;cross-project;empirical study;metric selection;metric transformation,"2803, 2822",,IEEE Journals,IEEE Transactions on Software Engineering,IEEE
267,,Where2Change: Change Request Localization for App Reviews,T. Zhang; J. Chen; X. Zhan; X. Luo; D. Lo; H. Jiang,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8924692,10.1109/TSE.2019.2956941,"Million of mobile apps have been released to the market. Developers need to maintain these apps so that they can continue to benefit end users. Developers usually extract useful information from user reviews to maintain and evolve mobile apps. One of the important activities that developers need to do while reading user reviews is to locate the source code related to requested changes. Unfortunately, this manual work is costly and time consuming since: (1) an app can receive thousands of reviews, and (2) a mobile app can consist of hundreds of source code files. To address this challenge, Palomba <italic>et al.</italic> recently proposed <monospace>CHANGEADVISOR</monospace> that utilizes user reviews to locate source code to be changed. However, we find that it cannot identify real source code to be changed for part of reviews. In this work, we aim to advance Palomba <italic>et al.</italic>'s work by proposing a novel approach that can achieve higher accuracy in change localization. Our approach first extracts the informative sentences (i.e., user feedback) from user reviews and identifies user feedback related to various problems and feature requests, and then cluster the corresponding user feedback into groups. Each group reports the similar users’ needs. Next, these groups are mapped to issue reports by using <inline-formula><tex-math notation=""LaTeX"">$Word2Vec$</tex-math><alternatives><mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""><mml:mrow><mml:mi>W</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mn>2</mml:mn><mml:mi>V</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href=""luo-ieq1-2956941.gif"" xmlns:xlink=""http://www.w3.org/1999/xlink""/></alternatives></inline-formula>. The resultant enriched text consisting of user feedback and their corresponding issue reports is used to identify source code classes that should be changed by using our novel <italic>weight selection</italic>-based cosine similarity metric. We have evaluated the new proposed change request localization approach (<monospace>Where2Change</monospace>) on 31,597 user reviews and 3,272 issue reports of 10 open source mobile apps. The experiments demonstrate that <monospace>Where2Change</monospace> can successfully locate more source code classes related to the change requests for more user feedback clusters than <monospace>CHANGEADVISOR</monospace> as demonstrated by higher Top-N and Recall values. The differences reach up to 17 for Top-1, 18.1 for Top-3, 17.9 for Top-5, and 50.08 percent for Recall. In addition, we also compare the performance of <monospace>Where2Change</monospace> and two previous Information Retrieval (IR)-based fault localization technologies:<monospace>BLUiR</monospace> and <monospace>BLIA</monospace>. The results showed that our approach performs better than them. As an important part of our work, we conduct an empirical study to investigate the value of using both user reviews and historical issue reports for change request localization; the results shown that historical issue reports can help to improve the performance of change localization.",User review;issue report;mobile app;change request localization;software maintenance,"2590, 2616",,IEEE Journals,IEEE Transactions on Software Engineering,IEEE
268,,Easy-to-Deploy API Extraction by Multi-Level Feature Embedding and Transfer Learning,S. Ma; Z. Xing; C. Chen; C. Chen; L. Qu; G. Li,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8865646,10.1109/TSE.2019.2946830,"Application Programming Interfaces (APIs) have been widely discussed on social-technical platforms (e.g., Stack Overflow). Extracting API mentions from such informal software texts is the prerequisite for API-centric search and summarization of programming knowledge. Machine learning based API extraction has demonstrated superior performance than rule-based methods in informal software texts that lack consistent writing forms and annotations. However, machine learning based methods have a significant overhead in preparing training data and effective features. In this paper, we propose a multi-layer neural network based architecture for API extraction. Our architecture automatically learns character-, word- and sentence-level features from the input texts, thus removing the need for manual feature engineering and the dependence on advanced features (e.g., API gazetteers) beyond the input texts. We also propose to adopt transfer learning to adapt a source-library-trained model to a target-library, thus reducing the overhead of manual training-data labeling when the software text of multiple programming languages and libraries need to be processed. We conduct extensive experiments with six libraries of four programming languages which support diverse functionalities and have different API-naming and API-mention characteristics. Our experiments investigate the performance of our neural architecture for API extraction in informal software texts, the importance of different features, the effectiveness of transfer learning. Our results confirm not only the superior performance of our neural architecture than existing machine learning based methods for API extraction in informal software texts, but also the easy-to-deploy characteristic of our neural architecture.",API extraction;CNN;word embedding;LSTM;transfer learning,"2296, 2311",,IEEE Journals,IEEE Transactions on Software Engineering,IEEE
269,,Metric-Based Fault Prediction for Spreadsheets,P. Koch; K. Schekotihin; D. Jannach; B. Hofer; F. Wotawa,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8859280,10.1109/TSE.2019.2944604,"Electronic spreadsheets are widely used in organizations for various data analytics and decision-making tasks. Even though faults within such spreadsheets are common and can have significant negative consequences, today's tools for creating and handling spreadsheets provide limited support for fault detection, localization, and repair. Being able to predict whether a certain part of a spreadsheet is faulty or not is often central for the implementation of such supporting functionality. In this work, we propose a novel approach to fault prediction in spreadsheet formulas, which combines an extensive catalog of spreadsheet metrics with modern machine learning algorithms. An analysis of the individual metrics from our catalog reveals that they are generally suited to discover a wide range of faults. Their predictive power is, however, limited when considered in isolation. Therefore, in our approach we apply supervised learning algorithms to obtain fault predictors that utilize all data provided by multiple spreadsheet metrics from our catalog. Experiments on different datasets containing faulty spreadsheets show that particularly Random Forests classifiers are often effective. As a result, the proposed method is in many cases able to make highly accurate predictions whether a given formula of a spreadsheet is faulty.<xref ref-type=""fn"" rid=""fn1""><sup>1</sup></xref><fn id=""fn1""><label>1.</label><p>Results of a preliminary study were published in <xref ref-type=""bibr"" rid=""ref1"">[1]</xref> .</p></fn>",Spreadsheets;fault prediction;machine learning,"2195, 2207",,IEEE Journals,IEEE Transactions on Software Engineering,IEEE
270,,FutureWare: Designing a Middleware for Anticipatory Mobile Computing,A. Mehrotra; V. Pejovic; M. Musolesi,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8847471,10.1109/TSE.2019.2943554,"Ubiquitous computing is moving from context-awareness to context-prediction. In order to build truly anticipatory systems developers have to deal with many challenges, from multimodal sensing to modeling context from sensed data, and, when necessary, coordinating multiple predictive models across devices. Novel expressive programming interfaces and paradigms are needed for this new class of mobile and ubiquitous applications. In this paper we present FutureWare, a middleware for seamless development of mobile applications that rely on context prediction. FutureWare exposes an expressive API to lift the burden of mobile sensing, individual and group behavior modeling, and future context querying, from an application developer. We implement FutureWare as an Android library, and through a scenario-based testing and a demo app we show that it represents an efficient way of supporting anticipatory applications, reducing the necessary coding effort by two orders of magnitude.",Anticipatory computing;mobile middleware;mobile sensing;prediction,"2107, 2124",,IEEE Journals,IEEE Transactions on Software Engineering,IEEE
271,,Which Variables Should I Log?,Z. Liu; X. Xia; D. Lo; Z. Xing; A. E. Hassan; S. Li,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8840982,10.1109/TSE.2019.2941943,"Developers usually depend on inserting logging statements into the source code to collect system runtime information. Such logged information is valuable for software maintenance. A logging statement usually prints one or more variables to record vital system status. However, due to the lack of rigorous logging guidance and the requirement of domain-specific knowledge, it is not easy for developers to make proper decisions about which variables to log. To address this need, in this work, we propose an approach to recommend logging variables for developers during development by learning from existing logging statements. Different from other prediction tasks in software engineering, this task has two challenges: 1) Dynamic labels - different logging statements have different sets of accessible variables, which means in this task, the set of possible labels of each sample is not the same. 2) Out-of-vocabulary words - identifiers' names are not limited to natural language words and the test set usually contains a number of program tokens which are out of the vocabulary built from the training set and cannot be appropriately mapped to word embeddings. To deal with the first challenge, we convert this task into a representation learning problem instead of a multi-label classification problem. Given a code snippet which lacks a logging statement, our approach first leverages a neural network with an RNN (recurrent neural network) layer and a self-attention layer to learn the proper representation of each program token, and then predicts whether each token should be logged through a unified binary classifier based on the learned representation. To handle the second challenge, we propose a novel method to map program tokens into word embeddings by making use of the pre-trained word embeddings of natural language tokens. We evaluate our approach on 9 large and high-quality Java projects. Our evaluation results show that the average MAP of our approach is over 0.84, outperforming random guess and an information-retrieval-based method by large margins.",Log;logging variable;word embedding;representation learning,"2012, 2031",,IEEE Journals,IEEE Transactions on Software Engineering,IEEE
272,,<sc>SequenceR</sc>: Sequence-to-Sequence Learning for End-to-End Program Repair,Z. Chen; S. Kommrusch; M. Tufano; L. -N. Pouchet; D. Poshyvanyk; M. Monperrus,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8827954,10.1109/TSE.2019.2940179,"This paper presents a novel end-to-end approach to program repair based on sequence-to-sequence learning. We devise, implement, and evaluate a technique, called SequenceR, for fixing bugs based on sequence-to-sequence learning on source code. This approach uses the copy mechanism to overcome the unlimited vocabulary problem that occurs with big code. Our system is data-driven; we train it on 35,578 samples, carefully curated from commits to open-source repositories. We evaluate SequenceR on 4,711 independent real bug fixes, as well on the Defects4J benchmark used in program repair research. SequenceR is able to perfectly predict the fixed line for 950/4,711 testing samples, and find correct patches for 14 bugs in Defects4J benchmark. SequenceR captures a wide range of repair operators without any domain-specific top-down design.",Program repair;machine learning,"1943, 1959",,IEEE Journals,IEEE Transactions on Software Engineering,IEEE
273,,Kernel Spectral Embedding Transfer Ensemble for Heterogeneous Defect Prediction,H. Tong; B. Liu; S. Wang,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8823052,10.1109/TSE.2019.2939303,"Cross-project defect prediction (CPDP) refers to predicting defects in the target project lacking of defect data by using prediction models trained on the historical defect data of other projects (i.e., source data). However, CPDP requires the source and target projects have common metric set (CPDP-CM). Recently, heterogeneous defect prediction (HDP) has drawn the increasing attention, which predicts defects across projects having heterogeneous metric sets. However, building high-performance HDP methods remains a challenge owing to several serious challenges including class imbalance problem, nonlinear, and the distribution differences between source and target datasets. In this paper, we propose a novel kernel spectral embedding transfer ensemble (KSETE) approach for HDP. KSETE first addresses the class-imbalance problem of the source data and then tries to find the latent common feature space for the source and target datasets by combining kernel spectral embedding, transfer learning, and ensemble learning. Experiments are performed on 22 public projects in both HDP and CPDP-CM scenarios in terms of multiple well-known performance measures such as, AUC, G-Measure, and MCC. The experimental results show that (1) KSETE improves the performance over previous HDP methods by at least 22.7, 138.9, and 494.4 percent in terms of AUC, G-Measure, and MCC, respectively. (2) KSETE improves the performance over previous CPDP-CM methods by at least 4.5, 30.2, and 17.9 percent in AUC, G-Measure, and MCC, respectively. It can be concluded that the proposed KSETE is very effective in both the HDP scenario and the CPDP-CM scenario.",Heterogeneous defect prediction;cross-project defect prediction;class imbalance learning;spectral embedding;transfer learning;ensemble learning;multiple kernel learning,"1886, 1906",,IEEE Journals,IEEE Transactions on Software Engineering,IEEE
274,,How does Machine Learning Change Software Development Practices?,Z. Wan; X. Xia; D. Lo; G. C. Murphy,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812912,10.1109/TSE.2019.2937083,"Adding an ability for a system to learn inherently adds uncertainty into the system. Given the rising popularity of incorporating machine learning into systems, we wondered how the addition alters software development practices. We performed a mixture of qualitative and quantitative studies with 14 interviewees and 342 survey respondents from 26 countries across four continents to elicit significant differences between the development of machine learning systems and the development of non-machine-learning systems. Our study uncovers significant differences in various aspects of software engineering (e.g., requirements, design, testing, and process) and work characteristics (e.g., skill variety, problem solving and task identity). Based on our findings, we highlight future research directions and provide recommendations for practitioners.",Software engineering;machine learning;practitioner;empirical study,"1857, 1871",,IEEE Journals,IEEE Transactions on Software Engineering,IEEE
275,,Deep Transfer Bug Localization,X. Huo; F. Thung; M. Li; D. Lo; S. -T. Shi,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8736995,10.1109/TSE.2019.2920771,"Many projects often receive more bug reports than what they can handle. To help debug and close bug reports, a number of bug localization techniques have been proposed. These techniques analyze a bug report and return a ranked list of potentially buggy source code files. Recent development on bug localization has resulted in the construction of effective supervised approaches that use historical data of manually localized bugs to boost performance. Unfortunately, as highlighted by Zimmermann et al., sufficient bug data is often unavailable for many projects and companies. This raises the need for cross-project bug localization - the use of data from a project to help locate bugs in another project. To fill this need, we propose a deep transfer learning approach for cross-project bug localization. Our proposed approach named TRANP-CNN extracts transferable semantic features from source project and fully exploits labeled data from target project for effective cross-project bug localization. We have evaluated TRANP-CNN on curated high-quality bug datasets and our experimental results show that TRANP-CNN can locate buggy files correctly at top 1, top 5, and top 10 positions for 29.9, 51.7, 61.3 percent of the bugs respectively, which significantly outperform state-of-the-art bug localization solution based on deep learning and several other advanced alternative solutions considering various standard evaluation metrics.",Cross-project bug localization;transfer learning;deep learning,"1368, 1380",,IEEE Journals,IEEE Transactions on Software Engineering,IEEE
276,,Research on Face Detection Method Based on Deep Learning,X. Sun,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9403826,10.1109/ICBASE51474.2020.00050,"Faster R-CNN is a popular method in object detection applications. Based on the similar characteristics of the category in face detection and the category label of target background in target detection, this paper improves the region suggestion network in Faster R-CNN method, and realizes multiscale face detection by adding multiple detectors. We train and test the multi-scale face detection network model on the Wider Face dataset. Through the experimental results, it can be obtained that the accuracy of the multi-scale face detection method is 89.6%, which has practical application value.",Face Detection;Convolutional Neural Network;Deep Learning;Faster R-CNN,"200, 203",,IEEE Conferences,2020 International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE),IEEE
277,,Insulator defect detection based on YOLO and SPP-Net,X. Zhang; Y. Zhang; M. Hu; X. Ju,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9403804,10.1109/ICBASE51474.2020.00092,"With the continuous construction of smart grids, drones are gradually being used in routine maintenance and inspection of transmission lines. Aiming at the problem of few defect insulator samples and complex background, a detection method of defect insulator based on YOLO and SPP-Net is proposed. This paper uses the original samples to train the YOLOv5s model, and the cropped samples to fine-tune the classification network composed of the pre-trained VGG16 network and SPP-Net, and then cascade the two models. After positioning and cutting the insulators, YOLOv5s sent them to the classification network for defect detection. The final insulator detection accuracy reached 89%.",defect detection;YOLO;insulator;transfer learning;fine tuning,"403, 407",,IEEE Conferences,2020 International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE),IEEE
278,,A Supernova Detection Implementation based on Faster R-CNN,T. Wu,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9403809,10.1109/ICBASE51474.2020.00089,"Object detection is the basis of many computer vision applications. It combines two tasks which are object classification and location. With the development of astronomical observation technology, using object detection methods to find more novae and supernova is becoming an interesting and practical issue. In this paper, a model-based supernova object detection framework including pseudo color image compositing technology is proposed by using faster R-CNN. And by balancing the uneven positive and negative sample, data enhancement, feature extraction, focal loss modification and neural network training, the model gives a suitable method for supernova detection. The deep learning network is realized by Tensorflow and the network test results show that the proposed method can achieve better accuracy under the given prediction standard calculation method. The training and testing sample are provided by the Popular Supernova Project(PSP).",Object Detection;Convolutional Neural;Pseudo-color;Data enhancement;Supernova Detection,"390, 393",,IEEE Conferences,2020 International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE),IEEE
279,,Research on Unconstrained Face Recognition Based on Deep Learning,Y. Wan; M. X. Zhang; Y. A. Zhang; L. Yao,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9403774,10.1109/ICBASE51474.2020.00054,"The emergence of deep learning has greatly promoted the development of the field of face recognition. The accuracy of face recognition in real scenes is affected by many factors. Among them, the problem of multiple poses is still an external factor that is difficult to overcome in face recognition. For the identification process in the Central African people face extreme attitude with the state led to the problem of low recognition accuracy, this paper proposes a gesture of deep space based correction feature to improve the recognition accuracy of multi-pose, first proposed in 2019 to use Google's lightweight network MobileNet for attitude correction in deep space, additionally employed ResNet18 verify and compare recognition results. This paper uses the VggFace2 dataset to train the two models in an end-to-end manner, and then test them on the CFP dataset, IJB-A dataset, and LFW dataset. The results show that the two backbone models proposed in the article are not much different from ResNet50 on the CFP. The face recognition on the IJB-A dataset is around 96%. The average recognition on the public data set LFW is about 96%. From the results of the test set, the model in the article is better than other methods. In addition, MobileNetV3 has a better recognition accuracy than ResNet18, and the amount of calculation is smaller.",deep learning;face recognition;ResNet18;multi-pose;MobileNetV3,"219, 227",,IEEE Conferences,2020 International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE),IEEE
280,,Wreckage Target Recognition in Side-scan Sonar Images Based on an Improved Faster R-CNN Model,T. Yulin; J. Shaohua; B. Gang; Z. Yonzhou; L. Fan,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9403838,10.1109/ICBASE51474.2020.00080,"Conventional object recognition approaches suffer from a number of problems such as difficulty in feature design, low detection accuracy and reliability, and weak generalization ability. This paper addresses these issues by proposing an improved Faster R-CNN model based on the VGG-16 convolutional neural network (CNN) for the automatic recognition of wreckage targets in side-scan sonar images. The object classification results of the Faster R-CNN model are improved by equalizing the number of anchor boxes in the region proposal network that either contain or do not contain wreckage targets and employing a balanced sampling of the image database for model training. The feasibility of the proposed model is demonstrated experimentally, and the results show that the average wreckage detection accuracy of the improved model is increased by 4.30% to 87.72% relative to that of the conventional Faster R-CNN model, while providing similar detection efficiency.",automatic recognition;wreckage target;side-scan sonar;Faster R-CNN model;anchor equalization;balanced sampling,"348, 354",,IEEE Conferences,2020 International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE),IEEE
281,,Research on Mass News Classification Algorithm Based on Spark,J. Wang; F. Ji; B. Liu; N. Wang; H. Yin; F. Zhang,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9403749,10.1109/ICBASE51474.2020.00093,"In recent years, with the explosion of the number of Internet news, people pay more and more attention to how to classify the mass of news. Therefore, this paper studies the mass news classification algorithm based on Spark, aiming at the problem of how to classify mass news data quickly and efficiently. In this paper, a large amount of news text is segmented based on Jieba segmentation tool, and several versions of stop words list are combined to remove stop words. Secondly, on the basis of traditional convolutional neural network, this paper proposes a news classification algorithm based on the combination of pre-trained Word2vec and improved CNN. In addition, the classification algorithm proposed in this paper is parallelized based on Spark, which improves the speed of mass news classification. In this paper, the standard data sets are used to compare and experiment the proposed news classification algorithm. The experimental results show that compared with the traditional algorithm, the news classification optimization algorithm designed in this paper has obvious improvement in multiple evaluation indexes such as accuracy, recall and F1. In addition, after parallel design of the algorithm proposed in this paper based on Spark, compared with the serial algorithm, the speed improvement effect is also more significant.",Spark;CNN;Classification;Word2Vec;Jieba,"408, 414",,IEEE Conferences,2020 International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE),IEEE
282,,A Context Enhanced Attention Network for Aspect-Based Sentiment Classification,Y. Zhu; W. Zheng; J. Zheng,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9403777,10.1109/ICBASE51474.2020.00031,"Aspect-based sentiment classification is a fine-grained task in sentiment analysis, it aims to detect the sentiment polarity of an aspect in a given context. Existing approaches mostly focused on the processing of aspect words, which often fail to adequately model the context via aspect terms. However, the surrounding words in context usually have an important impact on the sentiment polarity of aspect words. This paper proposes a context enhanced attention network (CE-ATT) to enhance context modeling for aspect-based sentiment classification. Firstly, the text sequence was divided into left and right contexts by a given aspect. Then, the Bidirectional Long Short Term Memory (BiLSTM) was used to model the left context and aspect, the right context and aspect, and both sides context and aspect, respectively. After summing these contextual representations, an attention mechanism was used to concentrate on the parts that have an important influence on the aspect word. The experimental results on three benchmark datasets demonstrate the effectiveness of the CE-ATT method.",Aspect-based sentiment classification;BiLSTM;Attention mechanism;Context;Aspect word,"108, 112",,IEEE Conferences,2020 International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE),IEEE
283,,Feature-oriented Design of Visual Analytics System for Interpretable Deep Learning based Intrusion Detection,C. Wu; A. Qian; X. Dong; Y. Zhang,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9405338,10.1109/TASE49443.2020.00019,"Deep Learning models have demonstrated significant performance on different tasks such as computer vision, natural language processing, etc. In recent years, these models have also achieved remarkable progress in Intrusion Detection Systems. However, the mechanism of these models is often hard to understand, especially for researchers in the domain of network security. In this paper, we propose a visual analytics system for interpretable deep learning based intrusion detection. During the design of this visual analytics system, we follow the requirements and features of explainable artificial intelligence for users in the domain of network security. The system allows users to select the best parameters to construct the model, to better understand the role of neurons in a deep learning model, to select instances and explore the detection mechanism of the model on these instances. We present multiple use cases to demonstrate the effectiveness of our system.",feature-oriented software;explainable artificial intelligence;intrusion detection system;deep learning,"73, 80",,IEEE Conferences,2020 International Symposium on Theoretical Aspects of Software Engineering (TASE),IEEE
284,,RKC-H: A Rich Knowledge Based Model for Multi-turn Dialogue Generation,F. Xu; G. Ding; W. Zhang; Audrey,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9405330,10.1109/TASE49443.2020.00013,"When conversational communication, people often draw upon their rich world knowledge in addition to the dialogue context. The commonsense world fact can facilitate natural language understanding. In the paper, we present a rich knowledge cognition hierarchical (RKC-H) multi-turn dialogue model in open-domain to improve language generation. Given the input, the model selects the corresponded seed-graphs and encodes the seed-graph nodes with a seed-graph attention mechanism. Then, the hierarchical encoder captures the multi-granularity of current utterance and history dialogue text features. We apply graph-to-sequence generator to the responses and provide Exponential Maximum Mutual Information loss function. Automatic and human evaluations show that the proposed model can complete rich meaning and coherent multi-turn dialogue. Our model outperforms over the baseline.",multi-turn dialogue system;knowledge graph;graph attention;hierarchical encoder,"25, 32",,IEEE Conferences,2020 International Symposium on Theoretical Aspects of Software Engineering (TASE),IEEE
285,,"Stress and Burnout in Open Source: Toward Finding, Understanding, and Mitigating Unhealthy Interactions",N. Raman; M. Cao; Y. Tsvetkov; C. Kästner; B. Vasilescu,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9397528,,"Developers from open-source communities have reported high stress levels from frequent demands for features and bug fixes and from the sometimes aggressive tone of these demands. Toxic conversations may demotivate and burn out developers, creating challenges for sustaining open source. We outline a path toward finding, understanding, and possibly mitigating such unhealthy interactions. We take a first step toward finding them, by developing and demonstrating a measurement instrument (an SVM classifier tailored for software engineering) to detect toxic discussions in GITHUB issues. We used our classifier to analyze trends over time and in different GITHUB communities, finding that toxicity varies by community and that toxicity decreased between 2012 and 2018.",toxicity;open source;github;community;sustainability;oss;classifier,"57, 60",,IEEE Conferences,2020 IEEE/ACM 42nd International Conference on Software Engineering: New Ideas and Emerging Results (ICSE-NIER),IEEE
286,,Where should I comment my code? A dataset and model for predicting locations that need comments,A. Louis; S. K. Dash; E. T. Barr; M. D. Ernst; C. Sutton,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9397521,,"Programmers should write code comments, but not on every line of code. We have created a machine learning model that suggests locations where a programmer should write a code comment. We trained it on existing commented code to learn locations that are chosen by developers. Once trained, the model can predict locations in new code. Our models achieved precision of 74% and recall of 13% in identifying comment-worthy locations. This first success opens the door to future work, both in the new where-to-comment problem and in guiding comment generation. Our code and data is available at http://groups.inf.ed.ac.uk/cup/comment-locator/.",NLP;natural language processing;comments,"21, 24",,IEEE Conferences,2020 IEEE/ACM 42nd International Conference on Software Engineering: New Ideas and Emerging Results (ICSE-NIER),IEEE
287,,Android Malware Detection using Convolutional Deep Neural Networks,F. Bourebaa; M. Benmohammed,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9380104,10.1109/ICAASE51408.2020.9380104,"Deep learning in general and convolutional architectures, in particular, have pushed the limits of the current state of the art in the field of computer vision and the processing of natural languages and speech. Recently, these techniques have been applied to detect mobile malware and have once again shown their ability to remedy this type of problem. However, the most suitable deep network architecture for malware detection remains an open issue. In this paper, we investigate the possibilities of convolutional neural networks for efficient detection of mobile malware. Specifically, we address the impact of using inception based and multichannel architectures on network performance. We achieve an accuracy of 92% using a multichannel model on a set of 50000 malware and 50000 benign applications.",Deep learning;Convolutional neural networks;Mobile security;Android malware detection,"1, 7",,IEEE Conferences,2020 International Conference on Advanced Aspects of Software Engineering (ICAASE),IEEE
288,,An Empirical Study on Robustness of DNNs with Out-of-Distribution Awareness,L. Zhou; B. Yu; D. Berend; X. Xie; X. Li; J. Zhao; X. Liu,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9359272,10.1109/APSEC51365.2020.00035,"The state-of-the-art deep neural network (DNN) achieves impressive performance on the input that is similar to training data. However, it fails to make reasonable decisions on the input that is quite different from training data, i.e., out-of-distribution (OOD) examples. Although many techniques have been proposed to detect OOD examples in recent years, it is still a lack of a systematic study about the effectiveness and robustness of different techniques as well as the performance of OOD-aware DNN models. In this paper, we conduct a comprehensive study to unveil the mystery of current OOD detection techniques, and investigate the differences between OOD-unaware/-aware DNNs in model performance, robustness, and uncertainty. We first compare the effectiveness of existing detection techniques and identify the best one. Then, evasion attacks are performed to evaluate the robustness of techniques. Furthermore, we compare the accuracy and robustness between OOD-unaware/-aware DNNs. At last, we study the uncertainty of different models on various kinds of data. Empirical results show OOD-aware detection modules have better performance and are more robust against random noises and evasion attacks. OOD-awareness seldom degrades the accuracy of DNN models in training/test datasets. In contrast, it makes the DNN model more robust against adversarial attacks and noisy inputs. Our study calls for attention to the development of OOD-aware DNN models and the necessity to take data distribution into account when robust and reliable DNN models are desired.",Deep Neural Network;Detection Performance;Evasion Attacks;Out-of-Distribution Awareness;Robustness,"266, 275",,IEEE Conferences,2020 27th Asia-Pacific Software Engineering Conference (APSEC),IEEE
289,,A Benchmark Study of the Contemporary Toxicity Detectors on Software Engineering Interactions,J. Sarker; A. K. Turzo; A. Bosu,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9359299,10.1109/APSEC51365.2020.00030,"Automated filtering of toxic conversations may help an Open-source software (OSS) community to maintain healthy interactions among the project participants. Although, several general purpose tools exist to identify toxic contents, those may incorrectly flag some words commonly used in the Software Engineering (SE) context as toxic (e.g., `junk', `kill', and `dump') and vice versa. To encounter this challenge, an SE specific tool has been proposed by the CMU Strudel Lab (referred as the `STRUDEL' hereinafter) by combining the output of the Perspective API with the output from a customized version of the Stanford's Politeness detector tool. However, since STRUDEL's evaluation was very limited with only 654 SE text, its practical applicability is unclear. Therefore, this study aims to empirically evaluate the Strudel tool as well as four state-of-the-art general purpose toxicity detectors on a large scale SE dataset. On this goal, we empirically developed a rubric to manually label toxic SE interactions. Using this rubric, we manually labeled a dataset of 6,533 code review comments and 4,140 Gitter messages. The results of our analyses suggest significant degradation of all tools' performances on our datasets. Those degradations were significantly higher on our dataset of formal SE communication such as code review than on our dataset of informal communication such as Gitter messages. Two of the models from our study showed significant performance improvements during 10-fold cross validations after we retrained those on our SE datasets. Based on our manual investigations of the incorrectly classified text, we have identified several recommendations for developing an SE specific toxicity detector.",toxicity;chat;code review;developer communication;benchmark;rubric,"218, 227",,IEEE Conferences,2020 27th Asia-Pacific Software Engineering Conference (APSEC),IEEE
290,,AI Deployment Architecture: Multi-Case Study for Key Factor Identification,M. M. John; H. H. Olsson; J. Bosch,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9359253,10.1109/APSEC51365.2020.00048,"Machine learning and deep learning techniques are becoming increasingly popular and critical for companies as part of their systems. However, although the development and prototyping of ML/DL systems are common across companies, the transition from prototype to production-quality deployment models are challenging. One of the key challenges is how to determine the selection of an optimal architecture for AI deployment. Based on our previous research, and to offer support and guidance to practitioners, we developed a framework in which we present five architectural alternatives for AI deployment ranging from centralized to fully decentralized edge architectures. As part of our research, we validated the framework in software-intensive embedded system companies and identified key challenges they face when deploying ML/DL models. In this paper, and to further advance our research on this topic, we identify factors that help practitioners determine what architecture to select for the ML/D L model deployment. For this, we conducted a follow-up study involving interviews and workshops in seven case companies in the embedded systems domain. Based on our findings, we identify three key factors and develop a framework in which we outline how prioritization and trade-offs between these results in certain architecture. The contribution of the paper is threefold. First, we identify key factors critical for AI system deployment. Second, we present the architecture selection framework that explains how prioritization and trade-offs between key factors result in the selection of a certain architecture. Third, we discuss additional factors that mayor may not influence the selection of an optimal architecture.",Artificial Intelligence;Machine Learning;Deep Learning;Edge;Cloud;Architecture;Deployment,"395, 404",,IEEE Conferences,2020 27th Asia-Pacific Software Engineering Conference (APSEC),IEEE
291,,Sia-RAE: A Siamese Network based on Recursive AutoEncoder for Effective Clone Detection,C. Feng; T. Wang; Y. Yu; Y. Zhang; Y. Zhang; H. Wang,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9359270,10.1109/APSEC51365.2020.00032,"Code clone helps improving programming productivity, while at the same time leads to many negative effects on software maintenance. Many approaches have been proposed to detect clones, but most of them fail on detecting low similarity code snippets. In this paper, we propose a Siamese network which links two recursive autoencoders (RAE) with a comparator network for clone detection. The unweighted recursive autoencoder is designed to learn code representation and then the comparator network is employed for similarity evaluation. In this Siamese network, it takes full advantages of lexical, semantic and structure information, and achieves high accuracy in revealing tiny similarity. We conduct comprehensive experiments on BigCloneBench using tagged clones as well as the whole repository respectively. The results suggest that our approach achieves good accuracy, and its recall reaches 93.02 % in WT3/T4, which outperforms state-of-the-art.",code clone detection;Siamese network;recursive auto encoder;learning model;open source software,"238, 246",,IEEE Conferences,2020 27th Asia-Pacific Software Engineering Conference (APSEC),IEEE
292,,Floating Point Accuracy Testing in Deep Neural Network Computations via Hypothesis Testing,C. Wang; J. Shen,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9359249,10.1109/APSEC51365.2020.00034,"Deep Learning (DL) has shown its success and convenience in our daily life, especially for the application in mobile devices. Because of the high quality demand for mobile applications from the consumers, the floating point accuracy of computations especially in deep neural networks (DNNs) has become particularly important. In this paper, we focus on testing the accuracy of floating point in DNN computations in the mobile devices from a statistical point of view. Specifically, two common hypothesis testing methods Z- Test and Student's T- Test are utilized, which cope with different situations. Z- Test is suitable for samples with known population variance and T-Test is suitable for samples without known population variance. Compared to existing scheme that pays more attention on single value comparison in industry, our proposed scheme in field of probability theory is more representative and reaches a more credible results with quantitative measurement, which can be seen from our experiment that based on well-known accelerator HiAI. The most important contribution of this paper is a novel perspective to test floating point accuracy. Future work is to study how to improve the accuracy based on the results.",Hypothesis Testing;Floating Point;Accuracy;Convolution,"257, 265",,IEEE Conferences,2020 27th Asia-Pacific Software Engineering Conference (APSEC),IEEE
293,,Principles for Re-architecting Software for Heterogeneous Platforms,H. Andrade; C. Berger; I. Crnkovic; J. Bosch,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9359288,10.1109/APSEC51365.2020.00049,"The demands on software continues to increase through the constant addition of functionalities and high expectations from users. In particular, performance has been the focus in many projects with the goal of fulfilling complex and hard requirements across a variety of domains. One way to achieve satisfactory levels of performance is through heterogeneous computing, i.e., systems that contain more than one type of processing unit, such as CPUs, GPUs, and FPGAs. However, applications are typically designed to be executed on CPUs, and re-architecting software for execution on such heterogeneous hardware architectures entails several challenges that must be addressed. In this paper, we propose a framework that supports engineers in the process of making architectural decisions to re-architect software for execution on heterogeneous platforms. We present several relevant aspects that should be addressed in the process, along with suggestions on how to create design solutions using different existing approaches. The framework was developed based on multiple interactions with three industrial partners and evaluated through a computer vision application in the automotive domain.",software engineering;software design;program processors,"405, 414",,IEEE Conferences,2020 27th Asia-Pacific Software Engineering Conference (APSEC),IEEE
294,,An Attentive Deep Supervision based Semantic Matching Framework For Tag Recommendation in Software Information Sites,X. Zheng; L. Li; D. Zhou,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9359291,10.1109/APSEC51365.2020.00062,"Tag recommendation in software information sites is a popular way to help developers classify software objects. Existing methods mostly consider tag recommendation as a multi-label classification task, which does not adequately leverage the semantic information of tags themselves. It is observed that the information granularity of tags is from abstract to specific and deep learning models have proven capable of automatically learning the features in different layers of an integrated network with different abstraction degrees. In this paper, we propose TagMatchRec, a deep semantic matching framework for tag recommendation instead of being based on classification. In our framework, multiple layers with different information granularities are directly connected to the output layer aiming at improving the quality of tag recommendation. Moreover, because the abstraction levels of semantic features learned by each layer may be different given different software objects and tags, an attentive deep supervision is introduced so that the dense connections from early layers to the output layer have directly weighted impact on loss function optimization. Comprehensive evaluations are conducted the datasets from four software information sites. The experimental results show that TagMatchRec has achieved better performance compared with the state-of-the-art approaches.",Software information site;Tag recommdation;Semantic matching;Multi-level feature,"490, 494",,IEEE Conferences,2020 27th Asia-Pacific Software Engineering Conference (APSEC),IEEE
295,,Boosting Component-based Synthesis with API Usage Knowledge,J. Liu; W. Dong; B. Liu,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9319302,10.1145/3417113.3423370,"Component-based synthesis is one of the hottest research areas in automated software engineering. It aims to generate programs from a collection of components like Java library. However, the program space constituted by all the components in the library is fairly large, which leads to a vast number of candidate programs generated for a long time. The intractability of the program space affects the synthesis efficiency of the program and the size of the program generated. In this paper, we propose ITAs, a framework of iterative program synthesis via API usage knowledge from the Internet, which can significantly improve the efficiency of program synthesis. ITAs aims to constrain the program space by combining two main ideas. First, narrow down the program space from the outside via the guidance of API usage knowledge. Second, expand the program space from the inside via iterative strategy based on knowledge. For evaluation, we collect a set of programming tasks and compare our approach with a program synthesis tool on synthesizing these tasks. The experiment results show that Itas can significantly improve the efficiency of program synthesis, which can reduce the synthesis time by 97.1 % than the original synthesizer.",Knowledge Search;Iterative Strategy;Component-based Synthesis,"91, 97",,IEEE Conferences,2020 35th IEEE/ACM International Conference on Automated Software Engineering Workshops (ASEW),IEEE
296,,Gesture Driven Smart Home Solution for Bedridden People,N. Jayaweera; B. Gamage; M. Samaraweera; S. Liyanage; S. Lokuliyana; T. Kuruppu,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9319303,10.1145/3417113.3422998,"Conversion of ordinary houses into smart homes has been a rising trend for past years. Smart house development is based on the enhancement of the quality of the daily activities of normal people. But many smart homes have not been designed in a way that is user friendly for differently-abled people such as immobile, bedridden (disabled people with at least one hand movable). Due to negligence and forgetfulness, there are cases where the electrical devices are left switched on, regardless of any necessity. It is one of the most occurred examples of domestic energy wastage. To overcome those challenges, this research represents the improved smart home design: MobiGO that uses cameras to capture gestures, smart sockets to deliver gesture-driven outputs to home appliances, etc. The camera captures the gestures done by the user and the system processes those images through advanced gesture recognition and image processing technologies. The commands relevant to the gesture are sent to the specific appliance through a specific IoT device attached to them. The basic literature survey content, which contains technical words, is analyzed using Deep Learning, Convolutional Neural Network (CNN), Image Processing, Gesture recognition, smart homes, IoT. Finally, the authors conclude that the MobiGO solution proposes a smart home system that is safer and easier for people with disabilities.",Deep Learning;Computer Vision;Gesture;Smart Appliances;Internet of Things,"152, 158",,IEEE Conferences,2020 35th IEEE/ACM International Conference on Automated Software Engineering Workshops (ASEW),IEEE
297,,Emotion Detection in Roman Urdu Text using Machine Learning,A. Majeed; H. Mujtaba; M. O. Beg,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9319147,10.1145/3417113.3423375,"Emotion detection is playing a very important role in our life. People express their emotions in different ways i.e face expression, gestures, speech, and text. This research focuses on detecting emotions from the Roman Urdu text. Previously, A lot of work has been done on different languages for emotion detection but there is limited work done in Roman Urdu. Therefore, there is a need to explore Roman Urdu as it is the most widely used language on social media platforms for communication. One major issue for the Roman Urdu is the absence of benchmark corpora for emotion detection from text because language assets are essential for different natural language processing (NLP) tasks. There are many useful applications of the emotional analysis of a text such as improving the quality of products, dialog systems, investment trends, mental health. In this research, to focus on the emotional polarity of the Roman Urdu sentence we develop a comprehensive corpus of 18k sentences that are gathered from different domains and annotate it with six different classes. We applied different baseline algorithms like KNN, Decision tree, SVM, and Random Forest on our corpus. After experimentation and evaluation, the results showed that the SVM model achieves a better F-measure score.",Datasets;Emotion detection;Text classification;Roman Urdu,"125, 130",,IEEE Conferences,2020 35th IEEE/ACM International Conference on Automated Software Engineering Workshops (ASEW),IEEE
298,,Towards Anomaly Detectors that Learn Continuously,A. Stocco; P. Tonella,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9307667,10.1109/ISSREW51248.2020.00073,"In this paper, we first discuss the challenges of adapting an already trained DNN-based anomaly detector with knowledge mined during the execution of the main system. Then, we present a framework for the continual learning of anomaly detectors, which records in-field behavioural data to determine what data are appropriate for adaptation. We evaluated our framework to improve an anomaly detector taken from the literature, in the context of misbehavior prediction for self-driving cars. Our results show that our solution can reduce the false positive rate by a large margin and adapt to nominal behaviour changes while maintaining the original anomaly detection capability.",AI Testing;Anomaly Detection;Autonomous Driving Systems;Continual Learning,"201, 208",,IEEE Conferences,2020 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW),IEEE
299,,Multi-label Classification of Commit Messages using Transfer Learning,M. U. Sarwar; S. Zafar; M. W. Mkaouer; G. S. Walia; M. Z. Malik,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9307651,10.1109/ISSREW51248.2020.00034,"Commit messages are used in the industry by developers to annotate changes made to the code. Accurate classification of these messages can help monitor the software evolution process and enable better tracking for various industrial stakeholders. In this paper, we present a state of the art method for commit message classification into categories as per Swanson's maintenance activities i.e. “Corrective”, “Perfective”, and “Adaptive”. This is a challenging task because not all commit messages are well written and informative. Existing approaches rely on keyword-based techniques to solve this problem. However, these approaches are oblivious to the full language model and do not recognize the contextual relationship between words. State of the art methodology in Natural Language Processing (NLP), is to train a context-aware neural network (Transformer) on a very large data set that encompasses the entire language and then fine-tunes it for a specific task. In this way, the model can learn the language, pay attention to the context, and then transfer that knowledge for better performance at the specific task. We use an off-the-shelf neural network called DistilBERT and fine-tune it for commit message classification task. This step is non-trivial because programming languages and commit messages have unique keywords, jargon, and idioms. This paper presents our effort in training this model and constructing the data set for this task. We describe the rules used to construct the data set. We validate our approach on industrial projects from GitHub, such as Kubernetes, Linux, TensorFlow, Spark, TypeScript, and PyTorch. We were able to achieve 87% F1-score for the commit message classification task, which is an order of magnitude accurate than previous studies.",software maintenance;commit message classification;software quality,"37, 42",,IEEE Conferences,2020 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW),IEEE
300,,Towards the implementation of an Attention-based Neural Machine Translation with artificial pronunciation for Nahuatl as a mobile application,S. K. Bello García; E. Sánchez Lucero; B. E. Pedroza Méndez; J. C. Hernández Hernández; E. Bonilla Huerta; J. F. Ramírez Cruz,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9307780,10.1109/CONISOFT50191.2020.00041,"There are great translation systems online. However, even though this technology is available for the majority of languages, it is not the case of Nahuatl [1]. For this reason, this paper outlines a master's degree thesis proposal which is aimed to use a Neural Network for Translation with an attention mechanism and Long Short-Term Memory (LSTM) like the one used by Google [2]. In addition, it seeks to implement an artificial Text To Speech (TTS) system trained with a Neural Network with a given dataset of Mel spectrograms in [3] from a person speaking in Nahuatl and it attempts to achieve a natural voice output as a spectrogram, then process it and obtain the sound desired. Finally, once trained, these models can be prepared for being used within mobile devices, and even taking advantage of the neural engine some of them are equipped with. In this way, this technology can reach more people and help to preserve and even spread the language. The early results showed how the limited resources of this language could cause a strong bias in the outputs and also how there could be some loss of information given the morphemes Nahuatl has, given its polysynthetic nature. This also highlights the way it can be tokenized, playing an important role in how the results turn out obtaining a BLEU score of 0.34 at best. Finally, this application and research can be an interesting framework of how a polysynthetic language can be manipulated to be used for fusional languages like Spanish or English. This research work was carried out at the “Tecnológico Nacional de México” (TecNM), campus of the “Instituto Tecnológico de Apizaco” (ITA).",Nahuatl;NMT;mobile;translation;attention;machine learning;CoreML;neural network;Mel spectrogram,"235, 244",,IEEE Conferences,2020 8th International Conference in Software Engineering Research and Innovation (CONISOFT),IEEE
301,,Long-Text Sentiment Analysis Based on Semantic Graph,L. Zhang; Y. Lei; Z. Wang,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9301570,10.1109/ICESS49830.2020.9301570,"The neural network of bilateral attention, especially Bidirectional Encoder Representations from Transformers (BERT), has achieved good results since its appearance, and has obtained high scores in various tasks of Natural Language Processing (NLP). However, Transformer completely adopts Attention mechanism and discards CNN and RNN, which also brings some problems. When analyzing long-text corpus, Transformer is usually limited by text length and cannot achieve good results. Due to the shortcoming, this paper proposed a long-text analyzing method called Semantic Graph Bidirectional Encoder Representations from Transformers (SG-BERT). SG-BERT obtains word features firstly, and then obtains sentence features, in corpus. After that, a semantic graph containing whole effective emotional will be generated, which can also be regarded as emotional labeling of long-text. Finally, semantic graph will be put into BERT to analysis sentiment. We experiment SG-BERT on Bag of Words Meets Bags of Popcorn (BoWMBoP) and online shopping 10 cats for sentiment analysis. In BoWMBoP and online shopping 10 cats 0~1000 length corpus, its ACC reached 96.3 and 95.9, respectively.",semantic graph;long-text;word features;sentence features,"1, 6",,IEEE Conferences,2020 IEEE International Conference on Embedded Software and Systems (ICESS),IEEE
302,,Automating Just-In-Time Comment Updating,Z. Liu; X. Xia; M. Yan; S. Li,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9286136,,"Code comments are valuable for program comprehension and software maintenance, and also require maintenance with code evolution. However, when changing code, developers sometimes neglect updating the related comments, bringing in inconsistent or obsolete comments (aka., bad comments). Such comments are detrimental since they may mislead developers and lead to future bugs. Therefore, it is necessary to fix and avoid bad comments. In this work, we argue that bad comments can be reduced and even avoided by automatically performing comment updates with code changes. We refer to this task as “Just-In-Time (JIT) Comment Updating” and propose an approach named CUP (Comment UPdater) to automate this task. CUP can be used to assist developers in updating comments during code changes and can consequently help avoid the introduction of bad comments. Specifically, CUP leverages a novel neural sequence-to-sequence model to learn comment update patterns from extant code-comment co-changes and can automatically generate a new comment based on its corresponding old comment and code change. Several customized enhancements, such as a special tokenizer and a novel co-attention mechanism, are introduced in CUP by us to handle the characteristics of this task. We build a dataset with over 108K comment-code co-change samples and evaluate CUP on it. The evaluation results show that CUP outperforms an information-retrieval-based and a rule-based baselines by substantial margins, and can reduce developers' edits required for JIT comment updating. In addition, the comments generated by our approach are identical to those updated by developers in 1612 (16.7%) test samples, 7 times more than the best-performing baseline.",Comment updating;Code-comment co-evolution;Seq2seq model,"585, 597",,IEEE Conferences,2020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
303,,BugPecker: Locating Faulty Methods with Deep Learning on Revision Graphs,J. Cao; S. Yang; W. Jiang; H. Zeng; B. Shen; H. Zhong,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9286037,,"Given a bug report of a project, the task of locating the faults of the bug report is called fault localization. To help programmers in the fault localization process, many approaches have been proposed, and have achieved promising results to locate faulty files. However, it is still challenging to locate faulty methods, because many methods are short and do not have sufficient details to determine whether they are faulty. In this paper, we present BugPecker, a novel approach to locate faulty methods based on its deep learning on revision graphs. Its key idea includes (1) building revision graphs and capturing the details of past fixes as much as possible, and (2) discovering relations inside our revision graphs to expand the details for methods and calculating various features to assist our ranking. We have implemented BugPecker, and evaluated it on three open source projects. The early results show that BugPecker achieves a mean average precision (MAP) of 0.263 and mean reciprocal rank (MRR) of 0.291, which improve the prior approaches significantly. For example, BugPecker improves the MAP values of all three projects by five times, compared with two recent approaches such as DNNLoc-m and BLIA 1.5.",bug localization;deep learning;revision graph,"1214, 1218",,IEEE Conferences,2020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
304,,Metamorphic Object Insertion for Testing Object Detection Systems,S. Wang; Z. Su,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9286114,,"Recent advances in deep neural networks (DNNs) have led to object detectors (ODs) that can rapidly process pictures or videos, and recognize the objects that they contain. Despite the promising progress by industrial manufacturers such as Amazon and Google in commercializing deep learning-based ODs as a standard computer vision service, ODs - similar to traditional software - may still produce incorrect results. These errors, in turn, can lead to severe negative outcomes for the users. For instance, an autonomous driving system that fails to detect pedestrians can cause accidents or even fatalities. However, despite their importance, principled, systematic methods for testing ODs do not yet exist. To fill this critical gap, we introduce the design and realization of Metaod, a metamorphic testing system specifically designed for ODs to effectively uncover erroneous detection results. To this end, we (1) synthesize natural-looking images by inserting extra object instances into background images, and (2) design metamorphic conditions asserting the equivalence of OD results between the original and synthetic images after excluding the prediction results on the inserted objects. Metaod is designed as a streamlined workflow that performs object extraction, selection, and insertion. We develop a set of practical techniques to realize an effective workflow, and generate diverse, natural-looking images for testing. Evaluated on four commercial OD services and four pretrained models provided by the TensorFlow API, Metaod found tens of thousands of detection failures. To further demonstrate the practical usage of Metaod, we use the synthetic images that cause erroneous detection results to retrain the model. Our results show that the model performance is significantly increased, from an mAP score of 9.3 to an mAP score of 10.5.",testing;computer vision;object detection;deep neural networks,"1053, 1065",,IEEE Conferences,2020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
305,,Synthesizing Smart Solving Strategy for Symbolic Execution,Z. Chen; Z. Chen; Z. Shuai; Y. Zhang; W. Pan,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9285993,,"Constraint solving is one of the challenges for symbolic execution. Modern SMT solvers allow users to customize the internal solving procedure by solving strategies. In this extended abstract, we report our recent progress in synthesizing a program-specific solving strategy for the symbolic execution of a program. We propose a two-stage procedure for symbolic execution. At the first stage, we synthesize a solving strategy by utilizing deep learning techniques. Then, the strategy will be used in the second stage to improve the performance of constraint solving. The preliminary experimental results indicate the promising of our method.",Symbolic Execution;SMT Solving Strategy;Synthesis,"1262, 1263",,IEEE Conferences,2020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
306,,Stay Professional and Efficient: Automatically Generate Titles for Your Bug Reports,S. Chen; X. Xie; B. Yin; Y. Ji; L. Chen; B. Xu,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9285994,,"Bug reports in a repository are generally organized line by line in a list-view, with their titles and other meta-data displayed. In this list-view, a concise and precise title plays an important role that enables project practitioners to quickly and correctly digest the core idea of the bug, without carefully reading the corresponding details. However, the quality of bug report titles varies in open-source communities, which may be due to the limited time and unprofessionalism of authors. To help report authors efficiently draft good-quality titles, we propose a method, named iTAPE, to automatically generate titles for their bug reports. iTAPE formulates title generation into a one-sentence summarization task. By properly tackling two domain-specific challenges (i.e. lacking off-the-shelf dataset and handling the low-frequency human-named tokens), iTAPE then generates titles using a Seq2Seq-based model. A comprehensive experimental study shows that iTAPE can obtain fairly satisfactory results, in terms of the comparison with three latest one-sentence summarization works, as well as the feedback from human evaluation.",issue title generation;one-sentence summarization;bug report quality;low-frequency token handling,"385, 397",,IEEE Conferences,2020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
307,,Dynamic Algorithm Selection for SMT,N. Pimpalkhare,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9286022,,"We describe an online approach to SMT solver selection using nearest neighbor classification and runtime estimation. We implement and evaluate our approach with MedleySolver, finding that it makes nearly optimal selections and evaluates a dataset of queries three times faster than any indivdual solver.",,"1376, 1378",,IEEE Conferences,2020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
308,,OCoR: An Overlapping-Aware Code Retriever,Q. Zhu; Z. Sun; X. Liang; Y. Xiong; L. Zhang,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9285661,,"Code retrieval helps developers reuse code snippets in the opensource projects. Given a natural language description, code retrieval aims to search for the most relevant code relevant among a set of code snippets. Existing state-of-the-art approaches apply neural networks to code retrieval. However, these approaches still fail to capture an important feature: overlaps. The overlaps between different names used by different people indicate that two different names may be potentially related (e.g., “message” and “msg”), and the overlaps between identifiers in code and words in natural language descriptions indicate that the code snippet and the description may potentially be related. To address this problem, we propose a novel neural architecture named OCoR<sup>1</sup>, where we introduce two specifically-designed components to capture overlaps: the first embeds names by characters to capture the overlaps between names, and the second introduces a novel overlap matrix to represent the degrees of overlaps between each natural language word and each identifier. The evaluation was conducted on two established datasets. The experimental results show that OCoR significantly outperforms the existing state-of-the-art approaches and achieves 13.1% to 22.3% improvements. Moreover, we also conducted several in-depth experiments to help understand the performance of the different components in OCoR.",Code Retrieval;Neural Network;Overlap,"883, 894",,IEEE Conferences,2020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
309,,GUI2WiRe: Rapid Wireframing with a Mined and Large-Scale GUI Repository using Natural Language Requirements,K. Kolthoff; C. Bartelt; S. P. Ponzetto,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9285988,,"High-fidelity Graphical User Interface (GUI) prototyping is a well-established and suitable method for enabling fruitful discussions, clarification and refinement of requirements formulated by customers. GUI prototypes can help to reduce misunderstandings between customers and developers, which may occur due to the ambiguity comprised in informal Natural Language (NL). However, a disadvantage of employing high-fidelity GUI prototypes is their time-consuming and expensive development. Common GUI prototyping tools are based on combining individual GUI components or manually crafted templates. In this work, we present GUI2WiRe, a tool that enables users to retrieve GUI prototypes from a semiautomatically created large-scale GUI repository for mobile applications matching user requirements specified in Natural Language (NLR). We extract multiple text segments from the GUI hierarchy data and employ various Information Retrieval (IR) models and Automatic Query Expansion (AQE) techniques to achieve ad-hoc GUI retrieval from NLR. Retrieved GUI prototypes mined from applications can be inserted in the graphical editor of GUI2WiRe to rapidly create wireframes. GUI components are extracted automatically from the GUI screenshots and basic editing functionality is provided to the user. Finally, a preview of the application is created from the wireframe to allow interactive exploration of the current design. We evaluated the applied IR and AQE approaches for their effectiveness in terms of GUI retrieval relevance on a manually annotated collection of NLR and discuss our planned user studies. Video presentation of GUI2WiRe: https://youtu.be/2nN-Xr2Hk7I.",Prototyping of Graphical User Interfaces from Natural Language Requirements;Information Retrieval for GUIs;Data-Driven Design,"1297, 1301",,IEEE Conferences,2020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
310,,How Incidental are the Incidents? Characterizing and Prioritizing Incidents for Large-Scale Online Service Systems,J. Chen; S. Zhang; X. He; Q. Lin; H. Zhang; D. Hao; Y. Kang; F. Gao; Z. Xu; Y. Dang; D. Zhang,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9286075,,"Although tremendous efforts have been devoted to the quality assurance of online service systems, in reality, these systems still come across many incidents (i.e., unplanned interruptions and outages), which can decrease user satisfaction or cause economic loss. To better understand the characteristics of incidents and improve the incident management process, we perform the first large-scale empirical analysis of incidents collected from 18 real-world online service systems in Microsoft. Surprisingly, we find that although a large number of incidents could occur over a short period of time, many of them actually do not matter, i.e., engineers will not fix them with a high priority after manually identifying their root cause. We call these incidents incidental incidents. Our qualitative and quantitative analyses show that incidental incidents are significant in terms of both number and cost. Therefore, it is important to prioritize incidents by identifying incidental incidents in advance to optimize incident management efforts. In particular, we propose an approach, called DeepIP (Deep learning based Incident Prioritization), to prioritizing incidents based on a large amount of historical incident data. More specifically, we design an attention-based Convolutional Neural Network (CNN) to learn a prediction model to identify incidental incidents. We then prioritize all incidents by ranking the predicted probabilities of incidents being incidental. We evaluate the performance of DeepIP using real-world incident data. The experimental results show that DeepIP effectively prioritizes incidents by identifying incidental incidents and significantly outperforms all the compared approaches. For example, the AUC of DeepIP achieves 0.808, while that of the best compared approach is only 0.624 on average.",• Software and its engineering →Maintaining software;Incidents;Online Service Systems;Prioritization,"373, 384",,IEEE Conferences,2020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
311,,Detecting and Explaining Self-Admitted Technical Debts with Attention-based Neural Networks,X. Wang; J. Liu; L. Li; X. Chen; X. Liu; H. Wu,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9286078,,"Self-Admitted Technical Debt (SATD) is a sub-type of technical debt. It is introduced to represent such technical debts that are intentionally introduced by developers in the process of software development. While being able to gain short-term benefits, the introduction of SATDs often requires to be paid back later with a higher cost, e.g., introducing bugs to the software or increasing the complexity of the software. To cope with these issues, our community has proposed various machine learning-based approaches to detect SATDs. These approaches, however, are either not generic that usually require manual feature engineering efforts or do not provide promising means to explain the predicted outcomes. To that end, we propose to the community a novel approach, namely HATD (Hybrid Attention-based method for self-admitted Technical Debt detection), to detect and explain SATDs using attention-based neural networks. Through extensive experiments on 445,365 comments in 20 projects, we show that HATD is effective in detecting SATDs on both in-the-lab and in-the-wild datasets under both within-project and cross-project settings. HATD also outperforms the state-of-the-art approaches in detecting and explaining SATDs.",Self-Admitted Technical Debt;Word Embedding;Attention-based Neural Networks;SATD,"871, 882",,IEEE Conferences,2020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
312,,Generating Concept based API Element Comparison Using a Knowledge Graph,Y. Liu; M. Liu; X. Peng; C. Treude; Z. Xing; X. Zhang,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9286015,,"Developers are concerned with the comparison of similar APIs in terms of their commonalities and (often subtle) differences. Our empirical study of Stack Overflow questions and API documentation confirms that API comparison questions are common and can often be answered by knowledge contained in API reference documentation. Our study also identifies eight types of API statements that are useful for API comparison. Based on these findings, we propose a knowledge graph based approach APIComp that automatically extracts API knowledge from API reference documentation to support the comparison of a pair of API classes or methods from different aspects. Our approach includes an offline phase for constructing an API knowledge graph, and an online phase for generating an API comparison result for a given pair of API elements. Our evaluation shows that the quality of different kinds of extracted knowledge in the API knowledge graph is generally high. Furthermore, the comparison results generated by APIComp are significantly better than those generated by a baseline approach based on heuristic rules and text similarity, and our generated API comparison results are useful for helping developers in API selection tasks.",API;Knowledge Graph;Documentation;Knowledge Extraction,"834, 845",,IEEE Conferences,2020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
313,,Evaluating Representation Learning of Code Changes for Predicting Patch Correctness in Program Repair,H. Tian; K. Liu; A. K. Kaboré; A. Koyuncu; L. Li; J. Klein; T. F. Bissyandé,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9286101,,"A large body of the literature of automated program repair develops approaches where patches are generated to be validated against an oracle (e.g., a test suite). Because such an oracle can be imperfect, the generated patches, although validated by the oracle, may actually be incorrect. While the state of the art explore research directions that require dynamic information or that rely on manually-crafted heuristics, we study the benefit of learning code representations in order to learn deep features that may encode the properties of patch correctness. Our empirical work mainly investigates different representation learning approaches for code changes to derive embeddings that are amenable to similarity computations. We report on findings based on embeddings produced by pre-trained and re-trained neural networks. Experimental results demonstrate the potential of embeddings to empower learning algorithms in reasoning about patch correctness: a machine learning predictor with BERT transformer-based embeddings associated with logistic regression yielded an AUC value of about 0.8 in the prediction of patch correctness on a deduplicated dataset of 1000 labeled patches. Our investigations show that learned representations can lead to reasonable performance when comparing against the state-of-the-art, PATCH-SIM, which relies on dynamic information. These representations may further be complementary to features that were carefully (manually) engineered in the literature.",Program Repair;Patch Correctness;Distributed Representation Learning;Machine learning;Embeddings,"981, 992",,IEEE Conferences,2020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
314,,A Deep Multitask Learning Approach for Requirements Discovery and Annotation from Open Forum,M. Li; L. Shi; Y. Yang; Q. Wang,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9286055,,"The ability in rapidly learning and adapting to evolving user needs is key to modern business successes. Existing methods are based on text mining and machine learning techniques to analyze user comments and feedback, and often constrained by heavy reliance on manually codified rules or insufficient training data. Multitask learning (MTL) is an effective approach with many successful applications, with the potential to address these limitations associated with requirements analysis tasks. In this paper, we propose a deep MTL-based approach, DEMAR, to address these limitations when discovering requirements from massive issue reports and annotating the sentences in support of automated requirements analysis. DEMAR consists of three main phases: (1) data augmentation phase, for data preparation and allowing data sharing beyond single task learning; (2) model construction phase, for constructing the MTL-based model for requirements discovery and requirements annotation tasks; and (3) model training phase, enabling eavesdropping by shared loss function between the two related tasks. Evaluation results from eight open-source projects show that, the proposed multitask learning approach outperforms two state-of-the-art approaches (CNC and FRA) and six common machine learning algorithms, with the precision of 91 % and the recall of 83% for requirements discovery task, and the overall accuracy of 83% for requirements annotation task. The proposed approach provides a novel and effective way to jointly learn two related requirements analysis tasks. We believe that it also sheds light on further directions of exploring multitask learning in solving other software engineering problems.",Requirements discovery;Requirements annotation;Multitask learning;Deep learning,"336, 348",,IEEE Conferences,2020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
315,,Problems and Opportunities in Training Deep Learning Software Systems: An Analysis of Variance,H. V. Pham; S. Qian; J. Wang; T. Lutellier; J. Rosenthal; L. Tan; Y. Yu; N. Nagappan,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9286042,,"Deep learning (DL) training algorithms utilize nondeterminism to improve models' accuracy and training efficiency. Hence, multiple identical training runs (e.g., identical training data, algorithm, and network) produce different models with different accuracies and training times. In addition to these algorithmic factors, DL libraries (e.g., TensorFlow and cuDNN) introduce additional variance (referred to as implementation-level variance) due to parallelism, optimization, and floating-point computation. This work is the first to study the variance of DL systems and the awareness of this variance among researchers and practitioners. Our experiments on three datasets with six popular networks show large overall accuracy differences among identical training runs. Even after excluding weak models, the accuracy difference is 10.8%. In addition, implementation-level factors alone cause the accuracy difference across identical training runs to be up to 2.9%, the per-class accuracy difference to be up to 52.4%, and the training time difference to be up to 145.3%. All core libraries (TensorFlow, CNTK, and Theano) and low-level libraries (e.g., cuDNN) exhibit implementation-level variance across all evaluated versions. Our researcher and practitioner survey shows that 83.8% of the 901 participants are unaware of or unsure about any implementation-level variance. In addition, our literature survey shows that only 19.5±3% of papers in recent top software engineering (SE), artificial intelligence (AI), and systems conferences use multiple identical training runs to quantify the variance of their DL approaches. This paper raises awareness of DL variance and directs SE researchers to challenging tasks such as creating deterministic DL implementations to facilitate debugging and improving the reproducibility of DL software and results.",deep learning;variance;nondeterminism,"771, 783",,IEEE Conferences,2020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
316,,Multi-task Learning based Pre-trained Language Model for Code Completion,F. Liu; G. Li; Y. Zhao; Z. Jin,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9285991,,"Code completion is one of the most useful features in the Integrated Development Environments (IDEs), which can accelerate software development by suggesting the next probable token based on the contextual code in real-time. Recent studies have shown that statistical language modeling techniques can improve the performance of code completion tools through learning from large-scale software repositories. However, these models suffer from two major drawbacks: a) Existing research uses static embeddings, which map a word to the same vector regardless of its context. The differences in the meaning of a token in varying contexts are lost when each token is associated with a single representation; b) Existing language model based code completion models perform poor on completing identifiers, and the type information of the identifiers is ignored in most of these models. To address these challenges, in this paper, we develop a multi-task learning based pre-trained language model for code understanding and code generation with a Transformer-based neural architecture. We pre-train it with hybrid objective functions that incorporate both code understanding and code generation tasks. Then we fine-tune the pre-trained model on code completion. During the completion, our model does not directly predict the next token. Instead, we adopt multi-task learning to predict the token and its type jointly and utilize the predicted type to assist the token prediction. Experiments results on two real-world datasets demonstrate the effectiveness of our model when compared with state-of-the-art methods.",code completion;multi-task learning;pre-trained language model;transformer networks,"473, 485",,IEEE Conferences,2020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
317,,Attend and Represent: A Novel View on Algorithm Selection for Software Verification,C. Richter; H. Wehrheim,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9286080,,"Today, a plethora of different software verification tools exist. When having a concrete verification task at hand, software developers thus face the problem of algorithm selection. Existing algorithm selectors for software verification typically use handpicked program features together with (1) either manually designed selection heuristics or (2) machine learned strategies. While the first approach suffers from not being transferable to other selection problems, the second approach lacks interpretability, i.e., insights into reasons for choosing particular tools. In this paper, we propose a novel approach to algorithm selection for software verification. Our approach employs representation learning together with an attention mechanism. Representation learning circumvents feature engineering, i.e., avoids the handpicking of program features. Attention permits a form of interpretability of the learned selectors. We have implemented our approach and have experimentally evaluated and compared it with existing approaches. The evaluation shows that representation learning does not only outperform manual feature engineering, but also enables transferability of the learning model to other selection tasks.",Software verification;algorithm selection;representation learning;attention,"1016, 1028",,IEEE Conferences,2020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
318,,Hybrid Deep Neural Networks to Infer State Models of Black-Box Systems,M. J. Mashhadi; H. Hemmati,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9286087,,"Inferring behavior model of a running software system is quite useful for several automated software engineering tasks, such as program comprehension, anomaly detection, and testing. Most existing dynamic model inference techniques are white-box, i.e., they require source code to be instrumented to get run-time traces. However, in many systems, instrumenting the entire source code is not possible (e.g., when using black-box third-party libraries) or might be very costly. Unfortunately, most black-box techniques that detect states over time are either univariate, or make assumptions on the data distribution, or have limited power for learning over a long period of past behavior. To overcome the above issues, in this paper, we propose a hybrid deep neural network that accepts as input a set of time series, one per input/output signal of the system, and applies a set of convolutional and recurrent layers to learn the non-linear correlations between signals and the patterns, over time. We have applied our approach on a real UAV auto-pilot solution from our industry partner with half a million lines of C code. We ran 888 random recent system-level test cases and inferred states, over time. Our comparison with several traditional time series change point detection techniques showed that our approach improves their performance by up to 102%, in terms of finding state change points, measured by F1 score. We also showed that our state classification algorithm provides on average 90.45% F1 score, which improves traditional classification algorithms by up to 17%.",Recurrent Neural Network;Convolutional Neural Network;Deep Learning;Specification Mining;Black-box Model Inference;Time series,"299, 311",,IEEE Conferences,2020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
319,,Marble: Model-based Robustness Analysis of Stateful Deep Learning Systems,X. Du; Y. Li; X. Xie; L. Ma; Y. Liu; J. Zhao,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9286028,,"State-of-the-art deep learning (DL) systems are vulnerable to adversarial examples, which hinders their potential adoption in safety-and security-critical scenarios. While some recent progress has been made in analyzing the robustness of feed-forward neural networks, the robustness analysis for stateful DL systems, such as recurrent neural networks (RNNs), still remains largely uncharted. In this paper, we propose Marble, a model-based approach for quantitative robustness analysis of real-world RNN-based DL systems. Marble builds a probabilistic model to compactly characterize the robustness of RNNs through abstraction. Furthermore, we propose an iterative refinement algorithm to derive a precise abstraction, which enables accurate quantification of the robustness measurement. We evaluate the effectiveness of Marble on both LSTM and GRU models trained separately with three popular natural language datasets. The results demonstrate that (1) our refinement algorithm is more efficient in deriving an accurate abstraction than the random strategy, and (2) Marble enables quantitative robustness analysis, in rendering better efficiency, accuracy, and scalability than the state-of-the-art techniques.",,"423, 435",,IEEE Conferences,2020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
320,,BiLO-CPDP: Bi-Level Programming for Automated Model Discovery in Cross-Project Defect Prediction,K. Li; Z. Xiang; T. Chen; K. C. Tan,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9285660,,"Cross-Project Defect Prediction (CPDP), which borrows data from similar projects by combining a transfer learner with a classifier, have emerged as a promising way to predict software defects when the available data about the target project is insufficient. However, developing such a model is challenge because it is difficult to determine the right combination of transfer learner and classifier along with their optimal hyper-parameter settings. In this paper, we propose a tool, dubbed BiLO-CPDP, which is the first of its kind to formulate the automated CPDP model discovery from the perspective of bi-level programming. In particular, the bi-level programming proceeds the optimization with two nested levels in a hierarchical manner. Specifically, the upper-level optimization routine is designed to search for the right combination of transfer learner and classifier while the nested lower-level optimization routine aims to optimize the corresponding hyper-parameter settings. To evaluate BiLO-CPDP, we conduct experiments on 20 projects to compare it with a total of 21 existing CPDP techniques, along with its single-level optimization variant and Auto-Sklearn, a state-of-the-art automated machine learning tool. Empirical results show that BiLO-CPDP champions better prediction performance than all other 21 existing CPDP techniques on 70% of the projects, while being overwhelmingly superior to Auto-Sklearn and its single-level optimization variant on all cases. Furthermore, the unique bi-level formalization in BiLO-CPDP also permits to allocate more budget to the upper-level, which significantly boosts the performance.",• Software and its engineering → Software creation and management;Software defect analysis;Cross-project defect prediction;transfer learning;classification techniques;automated parameter optimization;configurable software and tool,"573, 584",,IEEE Conferences,2020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
321,,Identifying and Describing Information Seeking Tasks,C. Satterfield; T. Fritz; G. C. Murphy,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9286003,,"A software developer works on many tasks per day, frequently switching between these tasks back and forth. This constant churn of tasks makes it difficult for a developer to know the specifics of when they worked on what task, complicating task resumption, planning, retrospection, and reporting activities. In a first step towards an automated aid to this issue, we introduce a new approach to help identify the topic of work during an information seeking task - one of the most common types of tasks that software developers face - that is based on capturing the contents of the developer's active window at regular intervals and creating a vector representation of key information the developer viewed. To evaluate our approach, we created a data set with multiple developers working on the same set of six information seeking tasks that we also make available for other researchers to investigate similar approaches. Our analysis shows that our approach enables: 1) segments of a developer's work to be automatically associated with a task from a known set of tasks with average accuracy of 70.6%, and 2) a word cloud describing a segment of work that a developer can use to recognize a task with average accuracy of 67.9%.",software development productivity;information seeking tasks,"797, 808",,IEEE Conferences,2020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
322,,Correctness-preserving Compression of Datasets and Neural Network Models,V. Joseph; N. Chalapathi; A. Bhaskara; G. Gopalakrishnan; P. Panchekha; M. Zhang,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9296941,10.1109/Correctness51934.2020.00006,"Neural networks deployed on edge devices must be efficient both in terms of their model size and the amount of data movement they cause when classifying inputs. These efficiencies are typically achieved through model compression: pruning a fully trained network model by zeroing out the weights. Given the overall challenge of neural network correctness, we argue that focusing on correctness preservation may allow the community to make measurable progress. We present a state-of-the-art model compression framework called Condensa around which we have launched correctness preservation studies. After presenting Condensa, we describe our initial efforts at understanding the effect of model compression in semantic terms, going beyond the top n% accuracy that Condensa is currently based on. We also take up the relatively unexplored direction of data compression that may help reduce data movement. We report preliminary results of learning from decompressed data to understand the effects of compression artifacts. Learning without decompressing input data also holds promise in terms of boosting efficiency, and we also report preliminary results in this regard. Our experiments centered around a state-of-the-art model compression framework called Condensa and two data compression algorithms, namely JPEG and ZFP, demonstrate the potential for employing model-and dataset compression without adversely affecting correctness.",Model Compression;Data Compression;Machine Learning;Correctness Verification,"1, 9",,IEEE Conferences,2020 IEEE/ACM 4th International Workshop on Software Correctness for HPC Applications (Correctness),IEEE
323,,TRADER: Trace Divergence Analysis and Embedding Regulation for Debugging Recurrent Neural Networks,G. Tao; S. Ma; Y. Liu; Q. Xu; X. Zhang,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284017,,"Recurrent Neural Networks (RNN) can deal with (textual) input with various length and hence have a lot of applications in software systems and software engineering applications. RNNs depend on word embeddings that are usually pre-trained by third parties to encode textual inputs to numerical values. It is well known that problematic word embeddings can lead to low model accuracy. In this paper, we propose a new technique to automatically diagnose how problematic embeddings impact model performance, by comparing model execution traces from correctly and incorrectly executed samples. We then leverage the diagnosis results as guidance to harden/repair the embeddings. Our experiments show that TRADER can consistently and effectively improve accuracy for real world models and datasets by 5.37% on average, which represents substantial improvement in the literature of RNN models.",,"986, 998",,IEEE Conferences,2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE),IEEE
324,,Translating Video Recordings of Mobile App Usages into Replayable Scenarios,C. Bernal-Cárdenas; N. Cooper; K. Moran; O. Chaparro; A. Marcus; D. Poshyvanyk,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283997,,"Screen recordings of mobile applications are easy to obtain and capture a wealth of information pertinent to software developers (e.g., bugs or feature requests), making them a popular mechanism for crowdsourced app feedback. Thus, these videos are becoming a common artifact that developers must manage. In light of unique mobile development constraints, including swift release cycles and rapidly evolving platforms, automated techniques for analyzing all types of rich software artifacts provide benefit to mobile developers. Unfortunately, automatically analyzing screen recordings presents serious challenges, due to their graphical nature, compared to other types of (textual) artifacts. To address these challenges, this paper introduces V2S, a lightweight, automated approach for translating video recordings of Android app usages into replayable scenarios. V2S is based primarily on computer vision techniques and adapts recent solutions for object detection and image classification to detect and classify user actions captured in a video, and convert these into a replayable test scenario. We performed an extensive evaluation of V2S involving 175 videos depicting 3,534 GUI-based actions collected from users exercising features and reproducing bugs from over 80 popular Android apps. Our results illustrate that V2S can accurately replay scenarios from screen recordings, and is capable of reproducing ≈ 89% of our collected videos with minimal overhead. A case study with three industrial partners illustrates the potential usefulness of V2S from the viewpoint of developers.",Bug Reporting;Screen Recordings;Object Detection,"309, 321",,IEEE Conferences,2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE),IEEE
325,,Repairing Deep Neural Networks: Fix Patterns and Challenges,M. J. Islam; R. Pan; G. Nguyen; H. Rajan,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284050,,"Significant interest in applying Deep Neural Network (DNN) has fueled the need to support engineering of software that uses DNNs. Repairing software that uses DNNs is one such unmistakable SE need where automated tools could be beneficial; however, we do not fully understand challenges to repairing and patterns that are utilized when manually repairing DNNs. What challenges should automated repair tools address? What are the repair patterns whose automation could help developers? Which repair patterns should be assigned a higher priority for building automated bug repair tools? This work presents a comprehensive study of bug fix patterns to address these questions. We have studied 415 repairs from Stack Overflow and 555 repairs from GitHub for five popular deep learning libraries Caffe, Keras, Tensorflow, Theano, and Torch to understand challenges in repairs and bug repair patterns. Our key findings reveal that DNN bug fix patterns are distinctive compared to traditional bug fix patterns; the most common bug fix patterns are fixing data dimension and neural network connectivity; DNN bug fixes have the potential to introduce adversarial vulnerabilities; DNN bug fixes frequently introduce new bugs; and DNN bug localization, reuse of trained model, and coping with frequent releases are major challenges faced by developers when fixing bugs. We also contribute a benchmark of 667 DNN (bug, repair) instances.",deep neural networks;bugs;bug fix;bug fix patterns,"1135, 1146",,IEEE Conferences,2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE),IEEE
326,,Big Code != Big Vocabulary: Open-Vocabulary Models for Source Code,R. -M. Karampatsis; H. Babii; R. Robbes; C. Sutton; A. Janes,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284032,,"Statistical language modeling techniques have successfully been applied to large source code corpora, yielding a variety of new software development tools, such as tools for code suggestion, improving readability, and API migration. A major issue with these techniques is that code introduces new vocabulary at a far higher rate than natural language, as new identifier names proliferate. Both large vocabularies and out-of-vocabulary issues severely affect Neural Language Models (NLMs) of source code, degrading their performance and rendering them unable to scale. In this paper, we address this issue by: 1) studying how various modelling choices impact the resulting vocabulary on a large-scale corpus of 13,362 projects; 2) presenting an open vocabulary source code NLM that can scale to such a corpus, 100 times larger than in previous work; and 3) showing that such models outperform the state of the art on three distinct code corpora (Java, C, Python). To our knowledge, these are the largest NLMs for code that have been reported. All datasets, code, and trained models used in this work are publicly available.",Naturalness of code;Neural Language Models;Byte-Pair Encoding,"1073, 1085",,IEEE Conferences,2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE),IEEE
327,,DeepBillboard: Systematic Physical-World Testing of Autonomous Driving Systems,H. Zhou; W. Li; Z. Kong; J. Guo; Y. Zhang; B. Yu; L. Zhang; C. Liu,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283977,,"Deep Neural Networks (DNNs) have been widely applied in autonomous systems such as self-driving vehicles. Recently, DNN testing has been intensively studied to automatically generate adversarial examples, which inject small-magnitude perturbations into inputs to test DNNs under extreme situations. While existing testing techniques prove to be effective, particularly for autonomous driving, they mostly focus on generating digital adversarial perturbations, e.g., changing image pixels, which may never happen in the physical world. Thus, there is a critical missing piece in the literature on autonomous driving testing: understanding and exploiting both digital and physical adversarial perturbation generation for impacting steering decisions. In this paper, we propose a systematic physical-world testing approach, namely DeepBillboard, targeting at a quite common and practical driving scenario: drive-by billboards. DeepBillboard is capable of generating a robust and resilient printable adversarial billboard test, which works under dynamic changing driving conditions including viewing angle, distance, and lighting. The objective is to maximize the possibility, degree, and duration of the steering-angle errors of an autonomous vehicle driving by our generated adversarial billboard. We have extensively evaluated the efficacy and robustness of DeepBillboard by conducting both experiments with digital perturbations and physical-world case studies. The digital experimental results show that DeepBillboard is effective for various steering models and scenes. Furthermore, the physical case studies demonstrate that DeepBillboard is sufficiently robust and resilient for generating physical-world adversarial billboard tests for real-world driving under various weather conditions, being able to mislead the average steering angle error up to 26.44 degrees. To the best of our knowledge, this is the first study demonstrating the possibility of generating realistic and continuous physical-world tests for practical autonomous driving systems; moreover, DeepBillboard can be directly generalized to a variety of other physical entities/surfaces along the curbside, e.g., a graffiti painted on a wall.",Autonomous Driving;Adversarial Machine Learning;Steering Model Testing,"347, 358",,IEEE Conferences,2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE),IEEE
328,,Importance-Driven Deep Learning System Testing,S. Gerasimou; H. F. Eniser; A. Sen; A. Cakan,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283940,,"Deep Learning (DL) systems are key enablers for engineering intelligent applications due to their ability to solve complex tasks such as image recognition and machine translation. Nevertheless, using DL systems in safety- and security-critical applications requires to provide testing evidence for their dependable operation. Recent research in this direction focuses on adapting testing criteria from traditional software engineering as a means of increasing confidence for their correct behaviour. However, they are inadequate in capturing the intrinsic properties exhibited by these systems. We bridge this gap by introducing DeepImportance, a systematic testing methodology accompanied by an Importance-Driven (IDC) test adequacy criterion for DL systems. Applying IDC enables to establish a layer-wise functional understanding of the importance of DL system components and use this information to assess the semantic diversity of a test set. Our empirical evaluation on several DL systems, across multiple DL datasets and with state-of-the-art adversarial generation techniques demonstrates the usefulness and effectiveness of DeepImportance and its ability to support the engineering of more robust DL systems.",Deep Learning Systems;Test Adequacy;Safety-Critical Systems,"702, 713",,IEEE Conferences,2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE),IEEE
329,,Structure-Invariant Testing for Machine Translation,P. He; C. Meister; Z. Su,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284002,,"In recent years, machine translation software has increasingly been integrated into our daily lives. People routinely use machine translation for various applications, such as describing symptoms to a foreign doctor and reading political news in a foreign language. However, the complexity and intractability of neural machine translation (NMT) models that power modern machine translation make the robustness of these systems difficult to even assess, much less guarantee. Machine translation systems can return inferior results that lead to misunderstanding, medical misdiagnoses, threats to personal safety, or political conflicts. Despite its apparent importance, validating the robustness of machine translation systems is very difficult and has, therefore, been much under-explored. To tackle this challenge, we introduce structure-invariant testing (SIT), a novel metamorphic testing approach for validating machine translation software. Our key insight is that the translation results of “similar” source sentences should typically exhibit similar sentence structures. Specifically, SIT (1) generates similar source sentences by substituting one word in a given sentence with semantically similar, syntactically equivalent words; (2) represents sentence structure by syntax parse trees (obtained via constituency or dependency parsing); (3) reports sentence pairs whose structures differ quantitatively by more than some threshold. To evaluate SIT, we use it to test Google Translate and Bing Microsoft Translator with 200 source sentences as input, which led to 64 and 70 buggy issues with 69.5% and 70% top-1 accuracy, respectively. The translation errors are diverse, including under-translation, over-translation, incorrect modification, word/phrase mistranslation, and unclear logic.",Metamorphic testing;Machine translation;Structural invariance,"961, 973",,IEEE Conferences,2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE),IEEE
330,,Caspar: Extracting and Synthesizing User Stories of Problems from App Reviews,H. Guo; M. P. Singh,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283933,,"A user's review of an app often describes the user's interactions with the app. These interactions, which we interpret as mini stories, are prominent in reviews with negative ratings. In general, a story in an app review would contain at least two types of events: user actions and associated app behaviors. Being able to identify such stories would enable an app's developer in better maintaining and improving the app's functionality and enhancing user experience. We present Caspar, a method for extracting and synthesizing user-reported mini stories regarding app problems from reviews. By extending and applying natural language processing techniques, Caspar extracts ordered events from app reviews, classifies them as user actions or app problems, and synthesizes action-problem pairs. Our evaluation shows that Caspar is effective in finding action-problem pairs from reviews. First, Caspar classifies the events with an accuracy of 82.0% on manually labeled data. Second, relative to human evaluators, Caspar extracts event pairs with 92.9% precision and 34.2% recall. In addition, we train an inference model on the extracted action-problem pairs that automatically predicts possible app problems for different use cases. Preliminary evaluation shows that our method yields promising results. Caspar illustrates the potential for a deeper understanding of app reviews and possibly other natural language artifacts arising in software engineering.",natural language processing;app review analysis;event extraction;event inference;requirements,"628, 640",,IEEE Conferences,2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE),IEEE
331,,An Investigation of Cross-Project Learning in Online Just-In-Time Software Defect Prediction,S. Tabassum; L. L. Minku; D. Feng; G. G. Cabral; L. Song,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283981,,"Just-In-Time Software Defect Prediction (JIT-SDP) is concerned with predicting whether software changes are defect-inducing or clean based on machine learning classifiers. Building such classifiers requires a sufficient amount of training data that is not available at the beginning of a software project. Cross-Project (CP) JIT-SDP can overcome this issue by using data from other projects to build the classifier, achieving similar (not better) predictive performance to classifiers trained on Within-Project (WP) data. However, such approaches have never been investigated in realistic online learning scenarios, where WP software changes arrive continuously over time and can be used to update the classifiers. It is unknown to what extent CP data can be helpful in such situation. In particular, it is unknown whether CP data are only useful during the very initial phase of the project when there is little WP data, or whether they could be helpful for extended periods of time. This work thus provides the first investigation of when and to what extent CP data are useful for JIT-SDP in a realistic online learning scenario. For that, we develop three different CP JIT-SDP approaches that can operate in online mode and be updated with both incoming CP and WP training examples over time. We also collect 2048 commits from three software repositories being developed by a software company over the course of 9 to 10 months, and use 19,8468 commits from 10 active open source GitHub projects being developed over the course of 6 to 14 years. The study shows that training classifiers with incoming CP+WP data can lead to improvements in G-mean of up to 53.90% compared to classifiers using only WP data at the initial stage of the projects. For the open source projects, which have been running for longer periods of time, using CP data to supplement WP data also helped the classifiers to reduce or prevent large drops in predictive performance that may occur over time, leading to up to around 40% better G-Mean during such periods. Such use of CP data was shown to be beneficial even after a large number of WP data were received, leading to overall G-means up to 18.5% better than those of WP classifiers.",Software defect prediction;cross-project learning;transfer learning;online learning;verification latency;concept drift;class imbalance,"554, 565",,IEEE Conferences,2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE),IEEE
332,,Understanding the Automated Parameter Optimization on Transfer Learning for Cross-Project Defect Prediction: An Empirical Study,K. Li; Z. Xiang; T. Chen; S. Wang; K. C. Tan,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284059,10.1145/3377811.3380360,"Data-driven defect prediction has become increasingly important in software engineering process. Since it is not uncommon that data from a software project is insufficient for training a reliable defect prediction model, transfer learning that borrows data/konwledge from other projects to facilitate the model building at the current project, namely cross-project defect prediction (CPDP), is naturally plausible. Most CPDP techniques involve two major steps, i.e., transfer learning and classification, each of which has at least one parameter to be tuned to achieve their optimal performance. This practice fits well with the purpose of automated parameter optimization. However, there is a lack of thorough understanding about what are the impacts of automated parameter optimization on various CPDP techniques. In this paper, we present the first empirical study that looks into such impacts on 62 CPDP techniques, 13 of which are chosen from the existing CPDP literature while the other 49 ones have not been explored before. We build defect prediction models over 20 real-world software projects that are of different scales and characteristics. Our findings demonstrate that: (1) Automated parameter optimization substantially improves the defect prediction performance of 77% CPDP techniques with a manageable computational cost. Thus more efforts on this aspect are required in future CPDP studies. (2) Transfer learning is of ultimate importance in CPDP. Given a tight computational budget, it is more cost-effective to focus on optimizing the parameter configuration of transfer learning algorithms (3) The research on CPDP is far from mature where it is `not difficult' to find a better alternative by making a combination of existing transfer learning and classification techniques. This finding provides important insights about the future design of CPDP techniques.",Cross-project defect prediction;transfer learning;classification techniques;automated parameter optimization,"566, 577",,IEEE Conferences,2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE),IEEE
333,,Detection of Hidden Feature Requests from Massive Chat Messages via Deep Siamese Network,L. Shi; M. Xing; M. Li; Y. Wang; S. Li; Q. Wang,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283914,,"Online chatting is gaining popularity and plays an increasingly significant role in software development. When discussing functionalities, developers might reveal their desired features to other developers. Automated mining techniques towards retrieving feature requests from massive chat messages can benefit the requirements gathering process. But it is quite challenging to perform such techniques because detecting feature requests from dialogues requires a thorough understanding of the contextual information, and it is also extremely expensive on annotating feature-request dialogues for learning. To bridge that gap, we recast the traditional text classification task of mapping single dialog to its class into the task of determining whether two dialogues are similar or not by incorporating few-shot learning. We propose a novel approach, named FRMiner, which can detect feature-request dialogues from chat messages via deep Siamese network. We design a BiLSTM-based dialog model that can learn the contextual information of a dialog in both forward and reverse directions. Evaluation on the realworld projects shows that our approach achieves average precision, recall and F1-score of 88.52%, 88.50% and 88.51%, which confirms that our approach could effectively detect hidden feature requests from chat messages, thus can facilitate gathering comprehensive requirements from the crowd in an automated way.",Feature Requests;Requirements Engineering;Deep Learning;Siamese Network,"641, 653",,IEEE Conferences,2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE),IEEE
334,,Unblind Your Apps: Predicting Natural-Language Labels for Mobile GUI Components by Deep Learning,J. Chen; C. Chen; Z. Xing; X. Xu; L. Zhut; G. Li; J. Wang,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284063,,"According to the World Health Organization(WHO), it is estimated that approximately 1.3 billion people live with some forms of vision impairment globally, of whom 36 million are blind. Due to their disability, engaging these minority into the society is a challenging problem. The recent rise of smart mobile phones provides a new solution by enabling blind users' convenient access to the information and service for understanding the world. Users with vision impairment can adopt the screen reader embedded in the mobile operating systems to read the content of each screen within the app, and use gestures to interact with the phone. However, the prerequisite of using screen readers is that developers have to add natural-language labels to the image-based components when they are developing the app. Unfortunately, more than 77% apps have issues of missing labels, according to our analysis of 10,408 Android apps. Most of these issues are caused by developers' lack of awareness and knowledge in considering the minority. And even if developers want to add the labels to UI components, they may not come up with concise and clear description as most of them are of no visual issues. To overcome these challenges, we develop a deep-learning based model, called Labeldroid, to automatically predict the labels of image-based buttons by learning from large-scale commercial apps in Google Play. The experimental results show thatour model can make accurate predictions and the generated labels are of higher quality than that from real Android developers.",Accessibility;neural networks;user interface;image-based buttons;content description,"322, 334",,IEEE Conferences,2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE),IEEE
335,,An Empirical Study on Program Failures of Deep Learning Jobs,R. Zhang; W. Xiao; H. Zhang; Y. Liu; H. Lin; M. Yang,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284109,10.1145/3377811.3380362,"Deep learning has made significant achievements in many application areas. To train and test models more efficiently, enterprise developers submit and run their deep learning programs on a shared, multi-tenant platform. However, some of the programs fail after a long execution time due to code/script defects, which reduces the development productivity and wastes expensive resources such as GPU, storage, and network I/O. This paper presents the first comprehensive empirical study on program failures of deep learning jobs. 4960 real failures are collected from a deep learning platform in Microsoft. We manually examine their failure messages and classify them into 20 categories. In addition, we identify the common root causes and bug-fix solutions on a sample of 400 failures. To better understand the current testing and debugging practices for deep learning, we also conduct developer interviews. Our major findings include: (1) 48.0% of the failures occur in the interaction with the platform rather than in the execution of code logic, mostly due to the discrepancies between local and platform execution environments; (2) Deep learning specific failures (13.5%) are mainly caused by inappropriate model parameters/structures and framework API misunderstanding; (3) Current debugging practices are not efficient for fault localization in many cases, and developers need more deep learning specific tools. Based on our findings, we further suggest possible research topics and tooling support that could facilitate future deep learning development.",Deep learning jobs;Program failures;Empirical study,"1159, 1170",,IEEE Conferences,2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE),IEEE
336,,Retrieval-based Neural Source Code Summarization,J. Zhang; X. Wang; H. Zhang; H. Sun; X. Liu,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284039,,"Source code summarization aims to automatically generate concise summaries of source code in natural language texts, in order to help developers better understand and maintain source code. Traditional work generates a source code summary by utilizing information retrieval techniques, which select terms from original source code or adapt summaries of similar code snippets. Recent studies adopt Neural Machine Translation techniques and generate summaries from code snippets using encoder-decoder neural networks. The neural-based approaches prefer the high-frequency words in the corpus and have trouble with the low-frequency ones. In this paper, we propose a retrieval-based neural source code summarization approach where we enhance the neural model with the most similar code snippets retrieved from the training set. Our approach can take advantages of both neural and retrieval-based techniques. Specifically, we first train an attentional encoder-decoder model based on the code snippets and the summaries in the training set; Second, given one input code snippet for testing, we retrieve its two most similar code snippets in the training set from the aspects of syntax and semantics, respectively; Third, we encode the input and two retrieved code snippets, and predict the summary by fusing them during decoding. We conduct extensive experiments to evaluate our approach and the experimental results show that our proposed approach can improve the state-of-the-art methods.",Source code summarization;Information retrieval;Deep neural network,"1385, 1397",,IEEE Conferences,2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE),IEEE
337,,Software Visualization and Deep Transfer Learning for Effective Software Defect Prediction,J. Chen; K. Hu; Y. Yu; Z. Chen; Q. Xuan; Y. Liu; V. Filkov,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284026,,"Software defect prediction aims to automatically locate defective code modules to better focus testing resources and human effort. Typically, software defect prediction pipelines are comprised of two parts: the first extracts program features, like abstract syntax trees, by using external tools, and the second applies machine learning-based classification models to those features in order to predict defective modules. Since such approaches depend on specific feature extraction tools, machine learning classifiers have to be custom-tailored to effectively build most accurate models. To bridge the gap between deep learning and defect prediction, we propose an end-to-end framework which can directly get prediction results for programs without utilizing feature-extraction tools. To that end, we first visualize programs as images, apply the self-attention mechanism to extract image features, use transfer learning to reduce the difference in sample distributions between projects, and finally feed the image files into a pre-trained, deep learning model for defect prediction. Experiments with 10 open source projects from the PROMISE dataset show that our method can improve cross-project and within-project defect prediction. Our code and data pointers are available at https://zenodo.org/record/3373409#.XV0Oy5Mza35.",Cross-project defect prediction;within-project defect prediction;deep transfer learning;self-attention;software visualization,"578, 589",,IEEE Conferences,2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE),IEEE
338,,How Android Developers Handle Evolution-induced API Compatibility Issues: A Large-scale Study,H. Xia; Y. Zhang; Y. Zhou; X. Chen; Y. Wang; X. Zhang; S. Cui; G. Hong; X. Zhang; M. Yang; Z. Yang,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284041,,"As Android platform evolves in a fast pace, API-related compatibility issues become a significant challenge for developers. To handle an incompatible API invocation, developers mainly have two choices: merely performing sufficient checks to avoid invoking incompatible APIs on platforms that do not support them, or gracefully providing replacement implementations on those incompatible platforms. As providing more consistent app behaviors, the latter one is more recommended and more challenging to adopt. However, it is still unknown how these issues are handled in the real world, do developers meet difficulties and what can we do to help them. In light of this, this paper performs the first large-scale study on the current practice of handling evolution-induced API compatibility issues in about 300,000 Android market apps, and more importantly, their solutions (if exist). Actually, it is in general very challenging to determine if developers have put in countermeasure for a compatibility issue, as different APIs have diverse behaviors, rendering various repair. To facilitate a large-scale study, this paper proposes RAPID, an automated tool to determine whether a compatibility issue has been addressed or not, by incorporating both static analysis and machine learning techniques. Results show that our trained classifier is quite effective by achieving a F1-score of 95.21% and 91.96% in the training stage and the validation stage respectively. With the help of RAPID, our study yields many interesting findings, e.g. developers are not willing to provide alternative implementations when handling incompatible API invocations (only 38.4%); for those incompatible APIs that Google gives replacement recommendations, the ratio of providing alternative implementations is significantly higher than those without recommendations; developers find more ways to repair compatibility issues than Google's recommendations and the knowledge acquired from these experienced developers would be extremely useful to novice developers and may significantly improve the current status of compatibility issue handling.",Compatibility Issues;API Evolution;Android App Analysis,"886, 898",,IEEE Conferences,2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE),IEEE
339,,HeteroRefactor: Refactoring for Heterogeneous Computing with FPGA,J. Lau; A. Sivaraman; Q. Zhang; M. A. Gulzar; J. Cong; M. Kim,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283938,,"Heterogeneous computing with field-programmable gate-arrays (FPGAs) has demonstrated orders of magnitude improvement in computing efficiency for many applications. However, the use of such platforms so far is limited to a small subset of programmers with specialized hardware knowledge. High-level synthesis (HLS) tools made significant progress in raising the level of programming abstraction from hardware programming languages to C/C++, but they usually cannot compile and generate accelerators for kernel programs with pointers, memory management, and recursion, and require manual refactoring to make them HLS-compatible. Besides, experts also need to provide heavily handcrafted optimizations to improve resource efficiency, which afΓects the maximum operating frequency, parallelization, and power efficiency. We propose a new dynamic invariant analysis and automated refactoring technique, called HeteroRefactor. First, HeteroRefactor monitors FPGA-specific dynamic invariants-the required bitwidth of integer and floating-point variables, and the size of recursive data structures and stacks. Second, using this knowledge of dynamic invariants, it refactors the kernel to make traditionally HLS-incompatible programs synthesizable and to optimize the accelerator's resource usage and frequency further. Third, to guarantee correctness, it selectively omoads the computation from CPU to FPGA, only if an input falls within the dynamic invariant. On average, for a recursive program of size 175 LOC, an expert FPGA programmer would need to write 185 more LOC to implement an HLS compatible version, while HETEROREFAcTOR automates such transformation. Our results on Xilinx FPGA show that HETEROREFAcTOR minimizes BRAM by 83% and increases frequency by 42% for recursive programs; reduces BRAM by 41% through integer bitwidth reduction; and reduces DSP by 50% through floating-point precision tuning.",heterogeneous computing;automated refactoring;FPGA;high-level synthesis;dynamic analysis,"493, 505",,IEEE Conferences,2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE),IEEE
340,,Taxonomy of Real Faults in Deep Learning Systems,N. Humbatova; G. Jahangirova; G. Bavota; V. Riccio; A. Stocco; P. Tonella,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284134,10.1145/3377811.3380395,"The growing application of deep neural networks in safety-critical domains makes the analysis of faults that occur in such systems of enormous importance. In this paper we introduce a large taxonomy of faults in deep learning (DL) systems. We have manually analysed 1059 artefacts gathered from GitHub commits and issues of projects that use the most popular DL frameworks (TensorFlow, Keras and PyTorch) and from related Stack Overflow posts. Structured interviews with 20 researchers and practitioners describing the problems they have encountered in their experience have enriched our taxonomy with a variety of additional faults that did not emerge from the other two sources. Our final taxonomy was validated with a survey involving an additional set of 21 developers, confirming that almost all fault categories (13/15) were experienced by at least 50% of the survey participants.",deep learning;real faults;software testing;taxonomy,"1110, 1121",,IEEE Conferences,2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE),IEEE
341,,Misbehaviour Prediction for Autonomous Driving Systems,A. Stocco; M. Weiss; M. Calzana; P. Tonella,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284027,,"Deep Neural Networks (DNNs) are the core component of modern autonomous driving systems. To date, it is still unrealistic that a DNN will generalize correctly to all driving conditions. Current testing techniques consist of offline solutions that identify adversarial or corner cases for improving the training phase. In this paper, we address the problem of estimating the confidence of DNNs in response to unexpected execution contexts with the purpose of predicting potential safety-critical misbehaviours and enabling online healing of DNN-based vehicles. Our approach SelfOracle is based on a novel concept of self-assessment oracle, which monitors the DNN confidence at runtime, to predict unsupported driving scenarios in advance. SelfOracle uses autoencoder- and time series-based anomaly detection to reconstruct the driving scenarios seen by the car, and to determine the confidence boundary between normal and unsupported conditions. In our empirical assessment, we evaluated the effectiveness of different variants of SelfOracle at predicting injected anomalous driving contexts, using DNN models and simulation environment from Udacity. Results show that, overall, SelfOracle can predict 77% misbehaviours, up to six seconds in advance, outperforming the online input validation approach of DeepRoad.",misbehaviour prediction;testing;deep learning;anomaly detection,"359, 371",,IEEE Conferences,2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE),IEEE
342,,Testing DNN Image Classifiers for Confusion & Bias Errors,Y. Tian; Z. Zhong; V. Ordonez; G. Kaiser; B. Ray,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284045,,"Image classifiers are an important component of today's software, from consumer and business applications to safety-critical domains. The advent of Deep Neural Networks (DNNs) is the key catalyst behind such wide-spread success. However, wide adoption comes with serious concerns about the robustness of software systems dependent on DNNs for image classification, as several severe erroneous behaviors have been reported under sensitive and critical circumstances. We argue that developers need to rigorously test their software's image classifiers and delay deployment until acceptable. We present an approach to testing image classifier robustness based on class property violations. We found that many of the reported erroneous cases in popular DNN image classifiers occur because the trained models confuse one class with another or show biases towards some classes over others. These bugs usually violate some class properties of one or more of those classes. Most DNN testing techniques focus on per-image violations, so fail to detect class-level confusions or biases. We developed a testing technique to automatically detect class-based confusion and bias errors in DNN-driven image classification software. We evaluated our implementation, DeepInspect, on several popular image classifiers with precision up to 100% (avg. 72.6%) for confusion errors, and up to 84.3% (avg. 66.8%) for bias errors. DeepInspect found hundreds of classification mistakes in widely-used models, many exposing errors indicating confusion or bias.",whitebox testing;deep learning;DNNs;image classifiers;bias,"1122, 1134",,IEEE Conferences,2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE),IEEE
343,,Towards Characterizing Adversarial Defects of Deep Learning Software from the Lens of Uncertainty,X. Zhang; X. Xie; L. Ma; X. Du; Q. Hu; Y. Liu; J. Zhao; M. Sun,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284139,10.1145/3377811.3380368,"Over the past decade, deep learning (DL) has been successfully applied to many industrial domain-specific tasks. However, the current state-of-the-art DL software still suffers from quality issues, which raises great concern especially in the context of safety- and security-critical scenarios. Adversarial examples (AEs) represent a typical and important type of defects needed to be urgently addressed, on which a DL software makes incorrect decisions. Such defects occur through either intentional attack or physical-world noise perceived by input sensors, potentially hindering further industry deployment. The intrinsic uncertainty nature of deep learning decisions can be a fundamental reason for its incorrect behavior. Although some testing, adversarial attack and defense techniques have been recently proposed, it still lacks a systematic study to uncover the relationship between AEs and DL uncertainty. In this paper, we conduct a large-scale study towards bridging this gap. We first investigate the capability of multiple uncertainty metrics in differentiating benign examples (BEs) and AEs, which enables to characterize the uncertainty patterns of input data. Then, we identify and categorize the uncertainty patterns of BEs and AEs, and find that while BEs and AEs generated by existing methods do follow common uncertainty patterns, some other uncertainty patterns are largely missed. Based on this, we propose an automated testing technique to generate multiple types of uncommon AEs and BEs that are largely missed by existing techniques. Our further evaluation reveals that the uncommon data generated by ourmethod is hard to be defended by the existing defense techniques with the average defense success rate reduced by 35%. Our results call for attention and necessity to generate more diverse data for evaluating quality assurance solutions of DL software.",Deep learning;uncertainty;adversarial attack;software testing,"739, 751",,IEEE Conferences,2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE),IEEE
344,,Character-word Double-dimensional Semantic Classification Model for Judging Illegal and Irregular Behaviors for Internet Food Safety,M. Zuo; S. -Y. He; Q. -C. Zhang; Q. -B. Wang,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9282692,10.1109/QRS-C51114.2020.00099,"It is difficult for Internet food safety supervision departments to automatically identify and classify the illegal and irregular behaviors in sampling inspection, an automatic semantic classification model was established to assist relevant departments in building an intelligent management system, so as to realize scientific decision-making. In this paper, Chinese word vectors and character vectors are used as the model input, CNN (Convolutional Neural Network) model is used to train the character vectors, and Positional Attention (PA) mechanism is introduced to BLSTM (bidirectional long short-term memory) network model to train the word vectors, to construct a character-word double-dimensional semantic classification model, CNN-PA-BLSTM, for judging illegal and irregular behaviors for internet food safety. The experimental results on the sampling inspection dataset show that the accuracy of the CNN-PA-BLSTM model is significantly higher than that of several commonly used deep neural network models, which verifies its rationality and effectiveness.",internet food;double-dimensional model;location awareness;attention;CNN;BLSTM,"571, 577",,IEEE Conferences,"2020 IEEE 20th International Conference on Software Quality, Reliability and Security Companion (QRS-C)",IEEE
345,,Effective Iterative Program Synthesis with Knowledge Searched from Internet,J. Liu; W. Dong; B. Liu; Y. Zhang; D. Wang,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9282699,10.1109/QRS-C51114.2020.00117,"This paper presents the ongoing work of studying the iterative program synthesis based on knowledge searched from the Internet, which can fairly reduce the scale of program space and improve the efficiency of synthesis. First, we implement a tool named Args(api Recommendation via General Search) to obtain the API knowledge from the Internet. Second, we propose an iterative method that incrementally constructs the program space to quickly approach the target program. The initial experimental result shows the effectiveness of our work.",,"678, 679",,IEEE Conferences,"2020 IEEE 20th International Conference on Software Quality, Reliability and Security Companion (QRS-C)",IEEE
346,,Depth Estimation and Object Detection for Monocular Semantic SLAM Using Deep Convolutional Network,C. Hou; X. Zhao; Y. Lin,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9282679,10.1109/QRS-C51114.2020.00051,"It is still challenging to efficiently construct semantic map with a monocular camera. In this paper, deep learning is introduced to combined with SLAM to realize semantic map production. We replace depth estimation module of SLAM with FCN which effectively solves the contradiction of triangulation. The Fc layers of FCN are modified to convolutional layers. Redundant calculation of Fc layers is avoided after optimization, and images can be input in any size. Besides, Faster RCNN, namely, a two-stage object detection network is utilized to obtain semantic information. We fine-tune RPN and Fc layers by transfer learning. The two algorithms are evaluated on official dataset. Results show that the average relative error of depth estimation is reduced by 12.6%, the accuracy of object detection is improved by 10.9%. The feasibility of the combination of deep learning and SLAM is verified.",monocular depth estimation;indoor object detection;deep learning;ORB-SLAM2,"256, 263",,IEEE Conferences,"2020 IEEE 20th International Conference on Software Quality, Reliability and Security Companion (QRS-C)",IEEE
347,,Evaluating Interaction Content in Online Learning Using Deep Learning for Quality Classification,L. Wu; D. Wu,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9282681,10.1109/QRS-C51114.2020.00041,Online learning is an important bridge between students and teachers for communication and learning. Online learning interaction content (OLIC) is created during information exchange. The evaluation of OLIC in the literature has been mainly focused on the perspective of quantitative instead of qualitative. The purpose of this study is to explore a new method for the qualitative evaluation of OLIC in online learning setting. A novel deep learning model is proposed for evaluating the quality of OLIC in the education domain. A multichannel of a framework based on bidirectional long short-term memory and attention mechanism (MFLBA) is used to achieve automatic evaluation. The results show that MFLBA takes the advantage of Word2Vector for evaluating the quality of OLIC. This study provides new horizon to analyze the nature of online interaction and monitors students' online learning process.,interaction content;content quality;online learning;domain concepts;deep learning,"198, 203",,IEEE Conferences,"2020 IEEE 20th International Conference on Software Quality, Reliability and Security Companion (QRS-C)",IEEE
348,,A Fusion of Java Domain Knowledge Base and Siamese Network for Java API Recommendation,H. Li; T. Li; S. Zhong; Y. Kang; T. Chen,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9282613,10.1109/QRS-C51114.2020.00074,"APIs play an important role in modern software development. Programmers need to frequently search for the appropriate APIs according to different tasks. With the development of the information industry, API reference documents have become larger and larger. Due to redundant and erroneous information on the Internet, traditional search methods can also cause inconvenience to programmers' queries. At the same time, there is a gap in terms of vocabulary and knowledge between the natural language description of the programming task and the description in the API documentation, so it is difficult to find a suitable API. To solve these problems, this paper proposes a Java API recommendation model by fusing the Java domain knowledge base and the Siamese Network to improve the accuracy of API recommendation. Experiments on the BIKER data set show that our method has better recommendation results than the state-of-art DeepAPI and BIKER model.",API recommendation;deep learning;Java;Stack Overflow;BERT,"398, 405",,IEEE Conferences,"2020 IEEE 20th International Conference on Software Quality, Reliability and Security Companion (QRS-C)",IEEE
349,,A Similarity Integration Method based Information Retrieval and Word Embedding in Bug Localization,S. Cheng; X. Yan; A. A. Khan,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9282791,10.1109/QRS51102.2020.00034,"To improve the performance of bug localization, there is necessity to solve the lexical mismatch between the natural language in the bug report and the programming language in the source file. A similarity integration method for bug localization is proposed, in which the similarity between bug report and source file is calculated by information retrieval (IR) and word embedding. More specifically, IR technique is used to collect the exact matches between bug report and source file. The terms in the bug report and the potential source files of different code tokens are connected by word embedding technique, which is used to complement with IR technique. Finally, deep neural network (DNN) is utilized to integrate extracted features to get the correlation between bug reports and source files. The experimental results show that the proposed approach outperforms several existing bug localization approaches in terms of Top N Rank, MAP, and MRR.",software bug localization;information retrieval;word embedding;similarity integration;bug report,"180, 187",,IEEE Conferences,"2020 IEEE 20th International Conference on Software Quality, Reliability and Security (QRS)",IEEE
350,,A Survey of the Use of Test Report in Crowdsourced Testing,S. Huang; H. Chen; Z. Hui; Y. Liu,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9282788,10.1109/QRS51102.2020.00062,"With the rise of crowdsourced software testing in recent years, the issuers of crowd test tasks can usually collect a large number of test reports after the end of the task. These reports have insufficient validity and completeness, and manual review often takes a lot of time and effort. The crowdsourced test task publisher hopes that after the crowdsourced platform collects the test report, it can analyze the validity and completeness of the report to determine the severity of the report and improve the efficiency of crowdsourced software testing. In the past ten years, researchers have used various technologies (such as natural language processing, information retrieval, machine learning, deep learning) to assist in analyzing reports to improve the efficiency of report review. We have summarized the relevant literature of report analysis in the past ten years, and then classified from report classification, duplicate report detection, report prioritization, report refactoring, and summarized the most important research work in each area. Finally, we propose research trends in these areas and analyze the challenges and opportunities facing crowdsourced test report analysis.",Survey;report classification;duplicate report;report prioritization;report refactoring;crowdsourced testing,"430, 441",,IEEE Conferences,"2020 IEEE 20th International Conference on Software Quality, Reliability and Security (QRS)",IEEE
351,,Quality Assurance for Machine Learning – an approach to function and system safeguarding,A. Poth; B. Meyer; P. Schlicht; A. Riel,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9282776,10.1109/QRS51102.2020.00016,"In an industrial context, high software quality is mandatory in order to avoid costly patching. We present a state of the art analysis of approaches to ensure that a specific Artificial Intelligence (AI) model is ready for release. We analyze the requirements a Machine Learning (ML) system has to fulfill in order to comply with the needs of an automotive OEM. The main implication for projects relying on ML is a holistic assessment of possible quality risks. These risks may stem from implemented ML models and spread into the delivery. We present a methodological quality assurance (QA) approach and its evaluation.",artificial intelligence;machine learning;quality management;quality assurance;risk management,"22, 29",,IEEE Conferences,"2020 IEEE 20th International Conference on Software Quality, Reliability and Security (QRS)",IEEE
352,,Cross-Project Dynamic Defect Prediction Model for Crowdsourced test,Y. Yao; Y. Liu; S. Huang; H. Chen; J. Liu; F. Yang,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9282773,10.1109/QRS51102.2020.00040,"By comparing the predicted number of defects with the number found in crowdsourced test in real time, people can dynamically assess the progress of crowdsourced test tasks. In this paper, we propose a cross-project dynamic defect prediction model (CPDDPM) for crowdsourced test to predict the number of defects in real time. In the construction of training dataset, we use density-based clustering method to select instances from the multiple source project datasets and build the initial training dataset. In the dynamic correction, CPDDPM iteratively corrects the prediction model using crowdsourced test reports and ability attributes of the crowdsourced testers until the predicted results converge. We collected project defect datasets on the crowdsourced test platform, and evaluated prediction accuracy of CPDDPM by using relative error and prediction at level l. The results show that CPDDPM can greatly improve the prediction performance of defect number.",crowdsourced test;defect number prediction;instance selection;dynamic correction,"223, 230",,IEEE Conferences,"2020 IEEE 20th International Conference on Software Quality, Reliability and Security (QRS)",IEEE
353,,Assessing Practitioner Beliefs about Software Defect Prediction,N. C. Shrikanth; T. Menzies,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9276610,,"Just because software developers say they believe in “X”, that does not necessarily mean that “X” is true. As shown here, there exist numerous beliefs listed in the recent Software Engineering literature which are only supported by small portions of the available data. Hence we ask what is the source of this disconnect between beliefs and evidence?. To answer this question we look for evidence for ten beliefs within 300,000+ changes seen in dozens of open-source projects. Some of those beliefs had strong support across all the projects; specifically, A commit that involves more added and removed lines is more bug-prone” and “Files with fewer lines contributed by their owners (who contribute most changes) are bug-prone”. Most of the widely-held beliefs studied are only sporadically supported in the data; i.e. large effects can appear in project data and then disappear in subsequent releases. Such sporadic support explains why developers believe things that were relevant to their prior work, but not necessarily their current work. Our conclusion will be that we need to change the nature of the debate with Software Engineering. Specifically, while it is important to report the effects that hold right now, it is also important to report on what effects change over time.CCS CONCEPTS Software and its engineering → Maintaining software.",defects;beliefs;practitioner;empirical software engineering,"182, 190",,IEEE Conferences,2020 IEEE/ACM 42nd International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP),IEEE
354,,Automated Bug Reproduction from User Reviews for Android Applications,S. Li; J. Guo; M. Fan; J. -G. Lou; Q. Zheng; T. Liu,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9276612,,"Bug-related user reviews of mobile applications have negative influence on their reputation and competence, and thus these reviews are highly regarded by developers. Before bug fixing, developers need to manually reproduce the bugs reported in user reviews, which is an extremely time-consuming and tedious task. Hence, it is highly expected to automate this process. However, it is challenging to do so since user reviews are hard to understand and poorly informative for bug reproduction (especially lack of reproduction steps). In this paper, we propose RepRev to automatically Reproduce Android application bugs from user Reviews. Specifically, RepRev leverages natural language processing techniques to extract valuable information for bug reproduction. Then, it ranks GUI components by semantic similarity with the user review and dynamically searches on apps with a novel one-step exploration technique. To evaluate RepRev, we construct a benchmark including 63 crash-related user reviews from Google Play, which have been reproduced successfully by three graduate students. On this benchmark, RepRev presents comparable performance with humans, which successfully reproduces 44 user reviews in our benchmark (about 70%) with 432.2 seconds average time. We make the implementation of our approach publicly available, along with the artifacts and experimental data we used [4].",Bug Reproduction;Android Applications;User Review Analysis,"51, 60",,IEEE Conferences,2020 IEEE/ACM 42nd International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP),IEEE
355,,EvalDNN: A Toolbox for Evaluating Deep Neural Network Models,Y. Tian; Z. Zeng; M. Wen; Y. Liu; T. -y. Kuo; S. -C. Cheung,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9270369,,"Recent studies have shown that the performance of deep learning models should be evaluated using various important metrics such as robustness and neuron coverage, besides the widely-used prediction accuracy metric. However, major deep learning frameworks currently only provide APIs to evaluate a model's accuracy. In order to comprehensively assess a deep learning model, framework users and researchers often need to implement new metrics by themselves, which is a tedious job. What is worse, due to the large number of hyper-parameters and inadequate documentation, evaluation results of some deep learning models are hard to reproduce, especially when the models and metrics are both new.To ease the model evaluation in deep learning systems, we have developed EvalDNN, a user-friendly and extensible toolbox supporting multiple frameworks and metrics with a set of carefully designed APIs. Using EvalDNN, evaluation of a pre-trained model with respect to different metrics can be done with a few lines of code. We have evaluated EvalDNN on 79 models from TensorFlow, Keras, GluonCV, and PyTorch. As a result of our effort made to reproduce the evaluation results of existing work, we release a performance benchmark of popular models, which can be a useful reference to facilitate future research. The tool and benchmark are available at https://github.com/yqtianust/EvalDNN and https://yqtianust.github.io/EvalDNN-benchmark/, respectively. A demo video of EvalDNN is available at: https://youtu.be/v69bNJN2bJc.",Deep Learning Model;Evaluation,"45, 48",,IEEE Conferences,2020 IEEE/ACM 42nd International Conference on Software Engineering: Companion Proceedings (ICSE-Companion),IEEE
356,,Importance-Driven Deep Learning System Testing,S. Gerasimou; H. F. Eniser; A. Sen; A. Cakan,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9270311,,"Deep Learning (DL) systems are key enablers for engineering intelligent applications. Nevertheless, using DL systems in safety- and security-critical applications requires to provide testing evidence for their dependable operation. We introduce DeepImportance, a systematic testing methodology accompanied by an Importance-Driven (IDC) test adequacy criterion for DL systems. Applying IDC enables to establish a layer-wise functional understanding of the importance of DL system components and use this information to assess the semantic diversity of a test set. Our empirical evaluation on several DL systems and across multiple DL datasets demonstrates the usefulness and effectiveness of DeepImportance.",Deep Neural Networks;Software Testing,"322, 323",,IEEE Conferences,2020 IEEE/ACM 42nd International Conference on Software Engineering: Companion Proceedings (ICSE-Companion),IEEE
357,,AppTestMigrator: A Tool for Automated Test Migration for Android Apps,F. Behrang; A. Orso,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9270392,,"The use of mobile apps is increasingly widespread, and much effort is put into testing these apps to make sure they behave as intended. In this demo, we present AppTestMigrator, a technique and tool for migrating test cases between apps with similar functionality. The intuition behind AppTestMigrator is that many apps share similarities in their functionality, and these similarities often result in conceptually similar user interfaces (through which that functionality is accessed). AppTestMigrator attempts to automatically transform the sequence of events and oracles in a test case for an app (source app) to events and oracles for another app (target app). The results of our preliminary evaluation show the effectiveness of AppTestMigrator in migrating test cases between mobile apps with similar functionality. Video URL: https://youtu.be/WQnfEcwYqa4.",GUI testing;test migration;mobile apps,"17, 20",,IEEE Conferences,2020 IEEE/ACM 42nd International Conference on Software Engineering: Companion Proceedings (ICSE-Companion),IEEE
358,,FeatureNET: Diversity-Driven Generation of Deep Learning Models,S. Ghamizi; M. Cordy; M. Papadakis; Y. Le Traon,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9270391,,"We present FeatureNET, an open-source Neural Architecture Search (NAS) tool <sup>1</sup> that generates diverse sets of Deep Learning (DL) models. FeatureNET relies on a meta-model of deep neural networks, consisting of generic configurable entities. Then, it uses tools developed in the context of software product lines to generate diverse (maximize the differences between the generated) DL models. The models are translated to Keras and can be integrated into typical machine learning pipelines. FeatureNET allows researchers to generate seamlessly a large variety of models. Thereby, it helps choosing appropriate DL models and performing experiments with diverse models (mitigating potential threats to validity). As a NAS method, FeatureNET successfully generates models performing equally well with handcrafted models.",Configuration search;NAS;Neural Architecture Search;AutoML,"41, 44",,IEEE Conferences,2020 IEEE/ACM 42nd International Conference on Software Engineering: Companion Proceedings (ICSE-Companion),IEEE
359,,DeepMutation: A Neural Mutation Tool,M. Tufano; J. Kimko; S. Wang; C. Watson; G. Bavota; M. Di Penta; D. Poshyvanyk,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9270362,,"Mutation testing can be used to assess the fault-detection capabilities of a given test suite. To this aim, two characteristics of mutation testing frameworks are of paramount importance: (i) they should generate mutants that are representative of real faults; and (ii) they should provide a complete tool chain able to automatically generate, inject, and test the mutants. To address the first point, we recently proposed an approach using a Recurrent Neural Network Encoder-Decoder architecture to learn mutants from ~787k faults mined from real programs. The empirical evaluation of this approach confirmed its ability to generate mutants representative of real faults. In this paper, we address the second point, presenting DEEPMUTATION, a tool wrapping our deep learning model into a fully automated tool chain able to generate, inject, and test mutants learned from real faults. Video: https://sites.google.com/view/learning-mutation/deepmutation.",software testing;mutation testing;neural networks,"29, 33",,IEEE Conferences,2020 IEEE/ACM 42nd International Conference on Software Engineering: Companion Proceedings (ICSE-Companion),IEEE
360,,Open-Vocabulary Models for Source Code (Extended Abstract),R. -M. Karampatsis; H. Babii; R. Robbes; C. Sutton; A. Janes,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9270359,,"Statistical language modeling techniques have successfully been applied to large source code corpora, yielding a variety of new software development tools, such as tools for code suggestion, improving readability, and API migration. A major issue with these techniques is that code introduces new vocabulary at a far higher rate than natural language, as new identifier names proliferate. Both large vocabularies and out-of-vocabulary issues severely affect Neural Language Models (NLMs) of source code, degrading their performance and rendering them unable to scale.In this paper, we address this issue by: 1) studying how various modelling choices impact the resulting vocabulary on a large-scale corpus of 13,362 projects; 2) presenting an open vocabulary source code NLM that can scale to such a corpus, 100 times larger than in previous work, and outperforms the state of the art. To our knowledge, this is the largest NLM for code that has been reported.",Naturalness of code;Neural Language Models;Byte-Pair Encoding,"294, 295",,IEEE Conferences,2020 IEEE/ACM 42nd International Conference on Software Engineering: Companion Proceedings (ICSE-Companion),IEEE
361,,FuRong: Fusing Report of Automated Android Testing on Multi-Devices,Y. Tian; S. Yu; C. Fang; P. Li,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9270319,,"Automated testing has been widely used to ensure the quality of Android applications. However, incomprehensible testing results make it difficult for developers to understand and fix potential bugs. This paper proposes FuRong, a novel tool, to fuse bug reports of high-readability and strong-guiding-ability via analyzing the automated testing results on multi-devices. FuRong builds a bug model with complete context information, such as screenshots, operation sequences, and logs from multi-devices, and then leverages pretrained Decision Tree classifier (with 18 bug category labels) to classify bugs. FuRong deduplicates the classified bugs via Levenshtein distance and finally generates the easy-to-understand report, not only context information of bugs, where possible causes and fix suggestions for each bug category are also provided. An empirical study of 8 open-source Android applications with automated testing on 20 devices has been conducted, the results show the effectiveness of FuRong, which has a bug classification precision of 93.4% and a bug classification accuracy of 87.9%. Video URL: https://youtu.be/LUkFTc32B6k.",Android Testing;Automated Testing;Bug Report;Bug Classification,"49, 52",,IEEE Conferences,2020 IEEE/ACM 42nd International Conference on Software Engineering: Companion Proceedings (ICSE-Companion),IEEE
362,,Variability Aware Requirements Reuse Analysis,M. Abbas,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9270333,,"Problem: The goal of a software product line is to aid quick and quality delivery of software products, sharing common features. Effectively achieving the above-mentioned goals requires reuse analysis of the product line features. Existing requirements reuse analysis approaches are not focused on recommending product line features, that can be reused to realize new customer requirements. Hypothesis: Given that the customer requirements are linked to product line features' description satisfying them: then the customer requirements can be clustered based on patterns and similarities, preserving the historic reuse information. New customer requirements can be evaluated against existing customer requirements and reuse of product line features can be recommended. Contributions: We treated the problem of feature reuse analysis as a text classification problem at the requirements-level. We use Natural Language Processing and clustering to recommend reuse of features based on similarities and historic reuse information. The recommendations can be used to realize new customer requirements.",software reuse;variability;product line;requirements;similarities,"190, 193",,IEEE Conferences,2020 IEEE/ACM 42nd International Conference on Software Engineering: Companion Proceedings (ICSE-Companion),IEEE
363,,Semantic Analysis of Issues on Google Play and Twitter,A. Yadav; F. H. Fard,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9270313,,"Mobile app users post their opinion about the apps, report bugs or request features on various platforms, the main one being App Stores. Previous research suggests that Twitter should be used as an additional resource to receive users' feedback, as app users tweet different issues. Although the classification and review summarization methods are developed previously for each platform separately, manual investigation of reviews or tweets is still required to identify the similar or different points that are discussed on App Store or Twitter. In this paper, we propose a framework to study the differences or similarities among app reviews from Google Play Store and tweets automatically by using the semantics of the words. The results from several experiments compared with expert evaluation, confirm that it can be applied to identify the similarities or differences among the extracted topics, n-grams, and users' comments.",app review analysis;Twitter;bug reports;semantic analysis,"308, 309",,IEEE Conferences,2020 IEEE/ACM 42nd International Conference on Software Engineering: Companion Proceedings (ICSE-Companion),IEEE
364,,Security of Edge Computing Based on Trusted Computing,B. Ma; Z. Ye; X. Zhang; J. Chen; Y. Zhou; Q. Xia,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9265904,10.1109/ISSSR51244.2020.00030,"With the development of information communication technology and Internet of Things technology, the number of devices connected to the network and the amount of data generated are exponentially increasing, thus resulting in a series of new application scenarios. The traditional centralized cloud computing big data processing model can no longer content the current data growth needs. Thus, born edge computing, a new computing model that migrates some or all of the computing tasks of the original cloud computing center to near the data source, gradually received extensive attention from all walks of life. According to the current cloud computing and edge computing application scenarios, analyze some security threats faced by edge computing in applications. Analyze and organize the security part of the existing edge computing reference architecture, and refine the edge computing security protection architecture and principles. By introducing trusted computing and blockchain technology, the credibility and adaptability of the edge computing security protection system is increased, making it more adaptable to the current practical application scenarios.",Edge computing;safety protection;Trusted Computing;Blockchain,"132, 137",,IEEE Conferences,2020 6th International Symposium on System and Software Reliability (ISSSR),IEEE
365,,Towards Detecting Inconsistent Comments in Java Source Code Automatically,N. Stulova; A. Blasi; A. Gorla; O. Nierstrasz,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9252010,10.1109/SCAM51674.2020.00012,"A number of tools are available to software developers to check consistency of source code during software evolution. However, none of these tools checks for consistency of the documentation accompanying the code. As a result, code and documentation often diverge, hindering program comprehension. This leads to errors in how developers use source code, especially in the case of APIs of reusable libraries. We propose a technique and a tool, upDoc, to automatically detect code-comment inconsistency during code evolution. Our technique builds a map between the code and its documentation, ensuring that changes in the code match the changes in respective documentation parts. We conduct a preliminary evaluation using inconsistency examples from an existing dataset of Java open source projects, showing that upDoc can successfully detect them. We present a roadmap for the further development of the technique and its evaluation.",Documentation;Natural Language Processing;Software Quality,"65, 69",,IEEE Conferences,2020 IEEE 20th International Working Conference on Source Code Analysis and Manipulation (SCAM),IEEE
366,,LogTransfer: Cross-System Log Anomaly Detection for Software Systems with Transfer Learning,R. Chen; S. Zhang; D. Li; Y. Zhang; F. Guo; W. Meng; D. Pei; Y. Zhang; X. Chen; Y. Liu,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251092,10.1109/ISSRE5003.2020.00013,"System logs, which describe a variety of events of software systems, are becoming increasingly popular for anomaly detection. However, for a large software system, current unsupervised learning-based methods are suffering from low accuracy due to the high diversity of logs, while the supervised learning methods are nearly infeasible to be used in practice because it is time-consuming and labor-intensive to obtain sufficient labels for different types of software systems. In this paper, we propose a novel framework, LogTransfer, which applies transfer learning to transfer the anomalous knowledge of one type of software system (source system) to another (target system). We represent every template using Glove, which considers both global word co-occurrence and local context information, to address the challenge that different types of software systems are different in log syntax while the semantics of logs should be reserved. We apply an LSTM network to extract the sequential patterns of logs, and propose a novel transfer learning method sharing fully connected networks between source and target systems, to minimize the impact of noises in anomalous log sequences. Extensive experiments have been performed on switch logs of different vendors collected from a top global cloud service provider. LogTransfer achieves an averaged 0.84 F1-score and outperforms the state-of-the-art supervised and unsupervised log-based anomaly detection methods, which are consistent with the experiments conducted on the public HDFS and Hadoop application datasets.",Transfer learning;system log;anomaly detection;word embedding;LSTM,"37, 47",,IEEE Conferences,2020 IEEE 31st International Symposium on Software Reliability Engineering (ISSRE),IEEE
367,,Cross-Project Aging-Related Bug Prediction Based on Joint Distribution Adaptation and Improved Subclass Discriminant Analysis,B. Xu; D. Zhao; K. Jia; J. Zhou; J. Tian; J. Xiang,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251079,10.1109/ISSRE5003.2020.00038,"Software aging, which is caused by Aging-Related Bugs (ARBs), refers to the phenomenon of performance degradation and eventual crash in long running systems. In order to discover and remove ARBs, ARB prediction is proposed. However, due to the low presence and reproducing difficulty of ARBs, it is usually difficult to collect sufficient ARB data within a project. Therefore, cross-project ARB prediction is proposed as a solution to build the target project's ARB predictor by using the labeled data from the source project. A key point for cross-project ARB prediction is to reduce distribution difference between source and target project. However, existing approaches mainly focus on the marginal distribution difference while somehow overlook the conditional distribution difference, and they mainly use random oversampling to alleviate the class imbalance which may lead to overfitting. To address these problems, we propose a new crossproject ARB prediction approach based on Joint Distribution Adaptation (JDA) and Improved Subclass Discriminant Analysis (ISDA), called JDA-ISDA. The key idea of JDA-ISDA is first to use JDA to reduce the marginal distribution and conditional distribution difference jointly and then apply ISDA to alleviate the severe class imbalance problem. A set of experiments are carried out on two large open-source projects with six different machine learning (ML) classifiers. The experimental results demonstrate that compared with the state-of-the-art Transfer Learning based Aging-related bug Prediction (TLAP) and Supervised Representation Learning Approach (SRLA), JDA-ISDA is much more robust to different ML classifiers than TLAP, and the average improvement in terms of the balance value can be achieved up to 31.8%, and JDA-ISDA also outperforms TLAP and SRLA on average when logistic regression is chosen as the classifier for best performance prediction.",software aging;aging-related bugs;cross-project ARB prediction;joint distribution adaptation;improved subclass discriminant analysis,"325, 334",,IEEE Conferences,2020 IEEE 31st International Symposium on Software Reliability Engineering (ISSRE),IEEE
368,,HINDBR: Heterogeneous Information Network Based Duplicate Bug Report Prediction,G. Xiao; X. Du; Y. Sui; T. Yue,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251082,10.1109/ISSRE5003.2020.00027,"Duplicate bug reports often exist in bug tracking systems (BTSs). Almost all the existing approaches for automatically detecting duplicate bug reports are based on text similarity. A recent study found that such approaches may become ineffective in detecting duplicates in bug reports submitted after the just-in-time (JIT) retrieval, which is now a built-in feature of modern BTSs (e.g., Bugzilla). This is mainly because the embedded JIT feature suggests possible duplicates in a bug database when a bug reporter types in the new summary field, therefore minimizing the submission of textually similar reports. Although JIT filtering seems effective, a number of bug report duplicates remain undetected. Our hypothesis is that we can detect them using a semantic similarity-based approach. This paper presents HINDBR, a novel deep neural network (DNN) that accurately detects semantically similar duplicate bug reports using a heterogeneous information network (HIN). Instead of matching text similarity alone, HINDBR embeds semantic relations of bug reports into a low-dimensional embedding space where two duplicate bug reports represented by two vectors are close to each other in the latent space. Results show that HINDBR is effective.",heterogeneous information network;duplicate bug report prediction;deep learning,"195, 206",,IEEE Conferences,2020 IEEE 31st International Symposium on Software Reliability Engineering (ISSRE),IEEE
369,,SwissLog: Robust and Unified Deep Learning Based Log Anomaly Detection for Diverse Faults,X. Li; P. Chen; L. Jing; Z. He; G. Yu,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251078,10.1109/ISSRE5003.2020.00018,"Log-based anomaly detection has been widely studied and achieves a satisfying performance on stable log data. But, the existing approaches still fall short meeting these challenges: 1) Log formats are changing continually in practice in those software systems under active development and maintenance. 2) Performance issues are latent causes that may not be detected by trivial monitoring tools. We thus propose SwissLog, namely a robust and unified deep learning based anomaly detection model for detecting diverse faults. SwissLog targets at those faults resulting in log sequence order changes and log time interval changes. To achieve that, an advanced log parser is introduced. Moreover, the semantic embedding and the time embedding approaches are combined to train a unified attention based BiLSTM model to detect anomalies. The experiments on real-world datasets and synthetic datasets show that SwissLog is robust to the changing log data and effective for diverse faults.",deep learning;log parsing;anomaly detection;BERT,"92, 103",,IEEE Conferences,2020 IEEE 31st International Symposium on Software Reliability Engineering (ISSRE),IEEE
370,,Fault Injection to Generate Failure Data for Failure Prediction: A Case Study,J. R. Campos; E. Costa,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251077,10.1109/ISSRE5003.2020.00020,"Due to the complexity of modern software, identifying every fault before deployment is extremely difficult or even not possible. Such residual faults can ultimately lead to failures, often incurring considerable risks or costs. Online Failure Prediction (OFP) is a fault-tolerance technique that attempts to predict the occurrence of failures in the near future and thus prevent/mitigate their consequences. Combined with recent technological developments, Machine Learning (ML) has been successfully used to create predictive models for OFP. However, as failures are rare events, failure data are often not available for building accurate models. Although fault injection has been accepted as a viable solution to generate realistic failure data, fault injectors are difficult to implement/update and thus research on Operating System (OS)-level OFP has become stale, with most works using data from outdated OSs. In this paper, we conduct a comprehensive fault injection campaign on an up-to-date Linux kernel and thoroughly study its behavior in the presence of faults. We then transform the data to explore and assess the predictive performance of various ML techniques for OFP. Finally, we study the influence of different OFP parameters (i.e., lead-time, prediction-window) and compare the results with existing related work. Results suggest that the various failures observed can be grouped into categories that can then be accurately predicted and distinguished by diverse ML models.",Dependability;Fault Injection;Failure Prediction;Machine Learning,"115, 126",,IEEE Conferences,2020 IEEE 31st International Symposium on Software Reliability Engineering (ISSRE),IEEE
371,,TensorFI: A Flexible Fault Injection Framework for TensorFlow Applications,Z. Chen; N. Narayanan; B. Fang; G. Li; K. Pattabiraman; N. DeBardeleben,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251063,10.1109/ISSRE5003.2020.00047,"As machine learning (ML) has seen increasing adoption in safety-critical domains (e.g., autonomous vehicles), the reliability of ML systems has also grown in importance. While prior studies have proposed techniques to enable efficient error-resilience (e.g., selective instruction duplication), a fundamental requirement for realizing these techniques is a detailed understanding of the application's resilience. In this work, we present TensorFI, a high-level fault injection (FI) framework for TensorFlow-based applications. TensorFI is able to inject both hardware and software faults in general TensorFlow programs. TensorFI is a configurable FI tool that is flexible, easy to use, and portable. It can be integrated into existing TensorFlow programs to assess their resilience for different fault types (e.g., faults in particular operators). We use TensorFI to evaluate the resilience of 12 ML programs, including DNNs used in the autonomous vehicle domain. The results give us insights into why some of the models are more resilient. We also present two case studies to demonstrate the usefulness of the tool. TensorFI is publicly available at https://github.com/DependableSystemsLab/TensorFI.",Fault Injection;Machine Learning;Resilience,"426, 435",,IEEE Conferences,2020 IEEE 31st International Symposium on Software Reliability Engineering (ISSRE),IEEE
372,,Towards Security Threats of Deep Learning Systems: A Survey,Y. He; G. Meng; K. Chen; X. Hu; J. He,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9252914,10.1109/TSE.2020.3034721,"Deep learning has gained tremendous success and great popularity in the past few years. However, deep learning systems are suffering several inherent weaknesses, which can threaten the security of learning models. Deep learning's wide use further magnifies the impact and consequences. To this end, lots of research has been conducted with the purpose of exhaustively identifying intrinsic weaknesses and subsequently proposing feasible mitigation. Yet few are clear about how these weaknesses are incurred and how effective these attack approaches are in assaulting deep learning. In order to unveil the security weaknesses and aid in the development of a robust deep learning system, we undertake an investigation on attacks towards deep learning, and analyze these attacks to conclude some findings in multiple views. In particular, we focus on four types of attacks associated with security threats of deep learning: model extraction attack, model inversion attack, poisoning attack and adversarial attack. For each type of attack, we construct its essential workflow as well as adversary capabilities and attack goals. Pivot metrics are devised for comparing the attack approaches, by which we perform quantitative and qualitative analyses. From the analysis, we have identified significant and indispensable factors in an attack vector, <i>e.g.</i>, how to reduce queries to target models, what distance should be used for measuring perturbation. We shed light on 18 findings covering these approaches' merits and demerits, success probability, deployment complexity and prospects. Moreover, we discuss other potential security weaknesses and possible mitigation which can inspire relevant research in this area.",deep learning;poisoning attack;adversarial attack;model extraction attack;model inversion attack,"1, 1",,IEEE Early Access Articles,IEEE Transactions on Software Engineering,IEEE
373,,Work-in-Progress: Enabling Edge-based Self-Navigation in Earthquake-Struck Zones,R. Zelek; V. K. Venkateshwar; S. K. Duggineni; R. Dighe; H. Jeon,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9244030,10.1109/CODESISSS51650.2020.9244030,"The role of unmanned vehicles for searching and localizing the victims in disaster impacted areas such as earthquake-struck zones is getting more important. Self-navigation on an earthquake zone has a unique challenge of detecting irregularly shaped obstacles such as cracks, puddles, and debris on the streets. In this paper, we present an edge-based self-navigation vehicle that can detect unique obstacles in earthquake-struck sites and discuss the performance and energy impact of various neural network structures, edge platforms, and optimizations. To enable vehicles to safely navigate earthquake-struck sites, we compiled a new image database of various earthquake impacted regions and developed semantic segmentation models that identify obstacles unique to earthquake-sites. The models are tested on an edge-based car platform. To our best knowledge, this is the first study that identifies unique challenges and discusses the performance and energy impact of edge-based self-navigation vehicles for earthquake-struck zones.",edge computing;autonomous navigation;convolutional neural network;semantic segmentation,"37, 39",,IEEE Conferences,2020 International Conference on Hardware/Software Codesign and System Synthesis (CODES+ISSS),IEEE
374,,Question Answering System based on Diease Knowledge Base,X. Wang; Z. Wang,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9237712,10.1109/ICSESS49938.2020.9237712,"In this article, our main concern is the question and answer based on Chinese disease knowledge base, which is a limited domain question. For this type of question and answer, there are mainly three steps: (1) First, identify the entity and attribute of the question raised by the user; (2) Then, turn the entity and attribute into a structured query; (3) Finally use the structure Inquiries can be found in the disease knowledge base. According to the above three steps, the problems we need to solve are: (1) Missing Chinese question database about diseases asked by users; (2) Chinese disease knowledge base is missing; (3) There is a cascading error in the current mainstream pipeline method. For questions 1 and 2, we used crawler technology to crawl relative disease questions and structured disease knowledge bases on disease question and answer websites and disease websites, respectively. We named the disease question database and knowledge database as DieaseQuestion and DieaseBase respectively. For problem 3, we propose a joint model to identify the entities and attributes in the question. In this article, we prove the effectiveness of our model on the DieaseQuestion dataset.",Knowledge Base question Answering System;Knowledge Graph;Information Extraction,"351, 354",,IEEE Conferences,2020 IEEE 11th International Conference on Software Engineering and Service Science (ICSESS),IEEE
375,,Transfer Learning on Natural YES/NO Questions,H. Yin; F. Zhou; X. Li; J. Zheng; K. Liu,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9237654,10.1109/ICSESS49938.2020.9237654,"Although neural network approaches achieve remarkable success on QA, many of them struggle to answer questions that require retrieving relevant factual information from the given paragraph. Inferring from a given paragraph and answering the question to be true or false is an essential part of natural language understanding. To improve the model's inferring ability on Natural YES/NO Question, we provide a simple and effective method. First, we find the tasks related to main task Natural YES/NO Question to fine-tune the model by multi-task learning, then we fine-tune the model on the main task. Results on dataset BoolQ show this method is competitive with other recently published methods, which means transferring from the related datasets through multi-task learning in first stage can save more beneficial information about main task Natural YES/NO. Further analysis show that this method can not only have benefit in this task, it also can be used to other tasks.",component;Transfer Learning;YES/NO QA;Multi-Task Learning,"1, 6",,IEEE Conferences,2020 IEEE 11th International Conference on Software Engineering and Service Science (ICSESS),IEEE
376,,ELMo+Gated Self-attention Network Based on BiDAF for Machine Reading Comprehension,W. Zhang; F. Ren,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9237663,10.1109/ICSESS49938.2020.9237663,"Machine reading comprehension (MRC) has always been a significant part of artificial intelligence and the focus in the field of natural language processing (NLP). Given context paragraph, to answer its query, we need to encode complex interaction between the question and the context. In the late years, with the rapid progress of neural network model and attention theory, MRC has made great advances. Especially, attention theory has been widely used in MRC. However, the accuracy of the previous classic baseline model has some upside potential and some of them did not take into account the long context dependence and polysemy. In this paper, for resolving the above problems and further improve the model, we introduce ELMo representations and add a gated self-attention layer to the Bi-Directional Attention Flow network (BIDAF). In addition, we employ the feature reuse method and modify the linear function of answer layer to further improve the performance. In the experiment of SQuAD, we prove this model greatly exceeds the baseline BIDAF model and its performance is close to the average level of human test, which proves the validity of this model.",Gated self-attention;ELMo;Machine reading comprehension,"1, 6",,IEEE Conferences,2020 IEEE 11th International Conference on Software Engineering and Service Science (ICSESS),IEEE
377,,Generate Adversarial Examples by Nesterov-momentum Iterative Fast Gradient Sign Method,J. Xu,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9237700,10.1109/ICSESS49938.2020.9237700,"At present, the security of neural networks has attracted more and more attention, and the emergence of adversarial examples is one of the problems. The gradient-based attack algorithm is a representative attack algorithm. Among the gradient attack algorithms, the momentum iterative fast gradient sign method (MI-FGSM) is currently an efficient and typical attack algorithm. However, this method will cause the gradient to advance too fast and accelerate too much. In this article, we propose an attack algorithm based on Nesterov-momentum called Nesterov-momentum iterative fast gradient sign method (NMI-FGSM). Nesterov-momentum makes a correction when the gradient is updated to avoid moving too fast. Experiments show that our algorithm performs well and has achieved a high success rate. At the same time, under the same attack success rate, the perturbation value of the adversarial examples generated by our algorithm is smaller.",Adversarial Examples;Artificial Intelligence Security;Fast Gradient Sign Method,"244, 249",,IEEE Conferences,2020 IEEE 11th International Conference on Software Engineering and Service Science (ICSESS),IEEE
378,,A Radar Filtering Model for Aerial Surveillance Base on Kalman Filter and Neural Network,Y. Jiang; X. Nong,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9237716,10.1109/ICSESS49938.2020.9237716,"Aerial surveillance information fusion is a key subsystem in air traffic control system. An excellent aerial surveillance information fusion algorithm can get more accurate position estimation on the aircraft. The commonly used algorithm for aerial surveillance information fusion is Kalman filter. The filtering accuracy of the Kalman filter algorithm is affected by accuracy of its parameters. When parameters are inaccurate, it may even cause the filter to diverge, so how to determine the parameters of Kalman filter is a key problem. This paper proposes a filtering model that integrates Back Propagation Neural Network, Generalized Regression Neural Network and Kalman filter. The parameters of Kalman filter are adjusted during filtering process dynamically by neural networks, so that the adaptability of traditional Kalman filter is enhanced. The actual radar measurement data is used for filtering experiments, and experimental results show effectiveness of this model.",Kalman filter;Back propagation neural network;Generalized regression neural network;Aerial surveillance,"57, 60",,IEEE Conferences,2020 IEEE 11th International Conference on Software Engineering and Service Science (ICSESS),IEEE
379,,A Performance Prediction Model Based on Combined Autoencoder,Y. Wu; Z. Zhang,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9237748,10.1109/ICSESS49938.2020.9237748,"Since most performance prediction models fail to make effective use of the essential characteristics of student performance data, this paper builds a combined autoencoder student performance prediction model, named HS-BP, which combines marginalized denoising autoencoder and stack sparse autoencoder to perform unsupervised feature learning on the student's historical performance data and behavior data, and connects the BP neural network on the top layer to achieve student performance prediction. Experimental results show that the proposed HS-BP model has higher prediction accuracy than other shallow models without feature learning.",Achievement prediction;Edge denoising autoencoder;Stack sparse autoencoder;BP neural network,"364, 368",,IEEE Conferences,2020 IEEE 11th International Conference on Software Engineering and Service Science (ICSESS),IEEE
380,,Application of Wavelet Packet Decomposition and Deep Belief Network for Rectal Function Diagnosis,P. Zan; R. Hong; B. Yang; L. Li; Z. OuYang; Z. Liu,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9237723,10.1109/ICSESS49938.2020.9237723,"Anal incontinence refers to as the loss of the ability of the body to store and control the liquid and solid contents and gases in the rectum, seriously affecting the normal life of patients. To accurately understand the health status of rectal function of the human body, a rapid diagnostic model for rectal function was proposed in this paper. Aiming at overcoming the deficiency of signal feature extraction, this paper used wavelet packet decomposition to extract the rectal pressure signals that was obtained in a real-time manner. Through the training and testing of the extracted feature parameters, the diagnosis results were reported in the form of classification. The experimental results showed that wavelet packet decomposition could well separate the characteristic parameters of rectal signal. The diagnosis model developed in this study can achieve the diagnosis of rectal function, with an average diagnosis rate of 92.1008%.",wavelet packet decomposition;feature extraction;deep belief network;classification processing;rectal function diagnosis,"416, 419",,IEEE Conferences,2020 IEEE 11th International Conference on Software Engineering and Service Science (ICSESS),IEEE
381,,Recurrent Graph Neural Networks for Text Classification,X. Wei; H. Huang; L. Ma; Z. Yang; L. Xu,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9237709,10.1109/ICSESS49938.2020.9237709,"Text classification is an essential and classical problem in natural language processing. Traditional text classifiers often rely on many human-designed features. With the rise of deep learning, Recurrent Neural Networks and Convolutional Neural Networks have widely applied into text classification. Meanwhile, the success of Graph Neural Networks (GNN) on structural data has attracted many researchers to apply GNN to traditional NLP applications. However, when these methods use the GNN, they commonly ignore the word order information of the sentence. In this work, we propose a model that uses a recurrent structure to capture contextual information as far as possible when learning word representations, which keeps word orders information compared to GNN-based networks. Then, we use the idea of GNN's message passing to aggregate the contextual information and update the word hidden representation. Like GNN's readout operation, we employ a max-pooling layer that automatically judges which words play key roles in text classification to capture the critical components in texts. We conduct experiments on four widely used datasets, and the experimental results show that our model achieves significant improvements against RNN-based model and GNN-based model.",Text Classification;Deep Learning;Graph Neural Networks,"91, 97",,IEEE Conferences,2020 IEEE 11th International Conference on Software Engineering and Service Science (ICSESS),IEEE
382,,Dynamic Gesture Recognition Based on 3D Separable Convolutional LSTM Networks,X. Zhang; Y. Tie; L. Qi,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9237672,10.1109/ICSESS49938.2020.9237672,"Dynamic gesture recognition, as an important component of human-computer interaction (HCI), has attracted the attention of many researchers. However, there are still many challenges in the actual gesture recognition task, such as the non-rigid properties of the hand, the acquisition of different semantic information through spatial and motion information at different locations, and the changes of light and background in the surrounding environment can ultimately affect the extraction of gesture features. Since the essence of dynamic gesture recognition is classification and recognition, how to extract the spatio-temporal features of dynamic gestures will have a significant impact on the final recognition and classification of gestures. In recent years, three-dimensional convolutional neural networks have achieved excellent performance in motion recognition, but traditional three-dimensional convolution is redundant in the feature extraction process. In order to reduce the resource consumption, this paper adopts 3D separable convolution as an alternative solution. To further extract semantic and action information of gestures, we combine attentional mechanisms and long- and short-term memory. To further extract semantic and action information of gestures, we combine attentional mechanisms and long- and short-term memory (LSTM) networks for gesture recognition in this paper. We conducted experiments on the ChaLearn Large-Scale Gesture Recognition Dataset (IsoGD), and the experimental results validate the effectiveness of our method.",3D Separable Convolution;LSTM;Dynamic Gesture Recognition;Attention Mechanism,"180, 183",,IEEE Conferences,2020 IEEE 11th International Conference on Software Engineering and Service Science (ICSESS),IEEE
383,,Aspect Based Sentiment Analysis with Self-Attention and Gated Convolutional Networks,J. Yang; J. Yang,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9237640,10.1109/ICSESS49938.2020.9237640,"Aspect based sentiment analysis (ABSA) is a fine-grained sentiment analysis task, whose main goal is to identify the sentiment polarity of an aspect in a sentence. A sentence may contain many different aspects, each of which may have different sentiment polarities. Based on the current researches in this area, ABSA can be divided into two subtasks: aspect-category sentiment analysis (ACSA) and aspect-term sentiment analysis (ATSA). In the past, more commonly used method is to adopt the time serial algorithm such as Long Short-Term Memory (LSTM) or Recurrent Neural Network (RNN), which usually needs more training time and has complex structures. Moreover, many previous models lack the abilities to effectively learn the internal structure features of sentences. For ABSA, sometimes the sentence structure may significantly affect the final classification results. However, we found the excellent performance of self-attention algorithm and gating mechanism in some other related researches. Therefore, to solve the problems above, we build a new model based on gating mechanism, combined with convolutional neural networks (CNN) and self-attention mechanism. First, we use self-attention to extract the structural feature of the input, and integrate it with the features of the original sentence extracted by CNN. On such basis, we further combine the aspect-category or aspect-term of the input sentence to form the final sentiment feature. Experiments on SemEval datasets show the performance of our models and the effectiveness of the model is proved.",deep learning;aspect based sentiment analysis;convolutional neural networks;self-Attention;gating mechanism,"146, 149",,IEEE Conferences,2020 IEEE 11th International Conference on Software Engineering and Service Science (ICSESS),IEEE
384,,Learning Code-Query Interaction for Enhancing Code Searches,W. Li; H. Qin; S. Yan; B. Shen; Y. Chen,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9240627,10.1109/ICSME46990.2020.00021,"Code search plays an important role in software development and maintenance. In recent years, deep learning (DL) has achieved a great success in this domain-several DL-based code search methods, such as DeepCS and UNIF, have been proposed for exploring deep, semantic correlations between code and queries; each method usually embeds source code and natural language queries into real vectors followed by computing their vector distances representing their semantic correlations. Meanwhile, deep learning-based code search still suffers from three main problems, i.e., the OOV (Out of Vocabulary) problem, the independent similarity matching problem, and the small training dataset problem. To tackle the above problems, we propose CQIL, a novel, deep learning-based code search method. CQIL learns code-query interactions and uses a CNN (Convolutional Neural Network) to compute semantic correlations between queries and code snippets. In particular, CQIL employs a hybrid representation to model code-query correlations, which solves the OOV problem. CQIL also deeply learns the code-query interaction for enhancing code searches, which solves the independent similarity matching and the small training dataset problems. We evaluate CQIL on two datasets (CODEnn and CosBench). The evaluation results show the strengths of CQIL-it achieves the MAP@1 values, 0.694 and 0.574, on CODEnn and CosBench, respectively. In particular, it outperforms DeepCS and UNIF, two state-of-the-art code search methods, by 13.6% and 18.1% in MRR, respectively, when the training dataset is insufficient.",code search;deep learning;code-query interaction;hybrid representation,"115, 126",,IEEE Conferences,2020 IEEE International Conference on Software Maintenance and Evolution (ICSME),IEEE
385,,Learning based and Context Aware Non-Informative Comment Detection,M. Liu; Y. Yang; X. Peng; C. Wang; C. Zhao; X. Wang; S. Xing,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9240658,10.1109/ICSME46990.2020.00115,"This report introduces the approach that we have designed and implemented for the DeClutter challenge of Doc-Gen2, which detects non-informative code comments. The approach combines both comment based text classification and code context based prediction. Based on the approach, our ""fduse"" team achieved the best F1 score (0.847) in the competition.",API Documentation;Text Classification;Deep Learning,"866, 867",,IEEE Conferences,2020 IEEE International Conference on Software Maintenance and Evolution (ICSME),IEEE
386,,Achieving Reliable Sentiment Analysis in the Software Engineering Domain using BERT,E. Biswas; M. E. Karabulut; L. Pollock; K. Vijay-Shanker,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9240599,10.1109/ICSME46990.2020.00025,"Researchers have shown that sentiment analysis of software artifacts can potentially improve various software engineering tools, including API and library recommendation systems, code suggestion tools, and tools for improving communication among software developers. However, sentiment analysis techniques applied to software artifacts still have not yet yielded very high accuracy. Recent adaptations of sentiment analysis tools to the software domain have reported some improvements, but the f-measures for the positive and negative sentences still remain in the 0.4-0.64 range, which deters their practical usefulness for software engineering tools.In this paper, we explore the potential effectiveness of customizing BERT, a language representation model, which has recently achieved very good results on various Natural Language Processing tasks on English texts, for the task of sentiment analysis of software artifacts. We describe our application of BERT to analyzing sentiments of sentences in Stack Overflow posts and compare the impact of a BERT sentiment classifier to state-of-the-art sentiment analysis techniques when used on a domain-specific data set created from Stack Overflow posts. We also investigate how the performance of sentiment analysis changes when using a much (3 times) larger data set than previous studies. Our results show that the BERT classifier achieves reliable performance for sentiment analysis of software engineering texts. BERT combined with the larger data set achieves an overall f-measure of 0.87, with the f-measures for the negative and positive sentences reaching 0.91 and 0.78 respectively, a significant improvement over the state-of-the-art.",Sentiment Analysis;Software Engineering;BERT,"162, 173",,IEEE Conferences,2020 IEEE International Conference on Software Maintenance and Evolution (ICSME),IEEE
387,,CrossASR: Efficient Differential Testing of Automatic Speech Recognition via Text-To-Speech,M. H. Asyrofi; F. Thung; D. Lo; L. Jiang,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9240600,10.1109/ICSME46990.2020.00066,"Automatic speech recognition (ASR) systems are ubiquitous parts of modern life. It can be found in our smartphones, desktops, and smart home systems. To ensure its correctness in recognizing speeches, ASR needs to be tested. Testing ASR requires test cases in the form of audio files and their transcribed texts. Building these test cases manually, however, is tedious and time-consuming. To deal with the aforementioned challenge, in this work, we propose CrossASR, an approach that capitalizes the existing Text-To-Speech (TTS) systems to automatically generate test cases for ASR systems. CrossASR is a differential testing solution that compares outputs of multiple ASR systems to uncover erroneous behaviors among ASRs. CrossASR efficiently generates test cases to uncover failures with as few generated tests as possible; it does so by employing a failure probability predictor to pick the texts with the highest likelihood of leading to failed test cases. As a black-box approach, CrossASR can generate test cases for any ASR, including when the ASR model is not available (e.g., when evaluating the reliability of various third-party ASR services).We evaluated CrossASR using 4 TTSes and 4 ASRs on the Europarl corpus. The experimented ASRs are Deepspeech, Deepspeech2, wav2letter, and wit. Our experiments on a randomly sampled 20,000 English texts showed that within an hour, CrossASR can produce, on average from 3 experiments, 130.34, 123.33, 47.33, and 8.66 failed test cases using Google, Respon-siveVoice, Festival, and Espeak TTSes, respectively. Moreover, when we run CrossASR on the entire 20,000 texts, it can generate 13,572, 13,071, 5,911, and 1,064 failed test cases using Google, ResponsiveVoice, Festival, and Espeak TTSes, respectively. Based on a manual verification carried out on statistically representative sample size, we found that most samples are actual failed test cases (audio understandable to humans but cannot be transcribed properly by an ASR), demonstrating that CrossASR is highly reliable in determining failed test cases. We also make the source code for CrossASR and evaluation data available at https://github.com/soarsmu/CrossASR.",Automatic Speech Recognition;Text-to-Speech;Test Case Generation;Differential Testing;Failure Probability Predictor,"640, 650",,IEEE Conferences,2020 IEEE International Conference on Software Maintenance and Evolution (ICSME),IEEE
388,,Automated Extraction of Requirement Entities by Leveraging LSTM-CRF and Transfer Learning,M. Li; Y. Yang; L. Shi; Q. Wang; J. Hu; X. Peng; W. Liao; G. Pi,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9240689,10.1109/ICSME46990.2020.00029,"Requirement entities, ""explicit specification of concepts that define the primary function objects"", play an important role in requirement analysis for software development and maintenance. It is a labor-intensive activity to extract requirement entities from textual requirements, which is typically done manually. A few existing studies propose automated methods to support key requirement concept extraction. However, they face two main challenges: lack of domain-specific natural language processing techniques and expensive labeling effort. To address the challenges, this study presents a novel approach named RENE, which employs LSTM-CRF model for requirement entity extraction and introduces the general knowledge to reduce the demands for labeled data. It consists of four phases: 1) Model construction, where RENE builds LSTM-CRF model and an isomorphic LSTM language model for transfer learning; 2) LSTM language model training, where RENE captures general knowledge and adapt to requirement context; 3) LSTM-CRF training, where RENE trains the LSTM-CRF model with the transferred layers; 4) Requirement entity extraction, where RENE applies the trained LSTM-CRF model to a new-coming requirement, and automatically extracts its requirement entities. RENE is evaluated using two methods: evaluation on historical dataset and user study. The evaluation on the historical dataset shows that RENE could achieve 79% precision, 81% recall, and 80% F1. The evaluation results from the user study also suggest that RENE could produce more accurate and comprehensive requirement entities, compared with those produced by engineers.",Requirement Entity;Sequence Tagging;LSTM-CRF;Transfer Learning,"208, 219",,IEEE Conferences,2020 IEEE International Conference on Software Maintenance and Evolution (ICSME),IEEE
389,,Few-Shot Guided Mix for DNN Repairing,X. Ren; B. Yu; H. Qi; F. Juefei-Xu; Z. Li; W. Xue; L. Ma; J. Zhao,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9240708,10.1109/ICSME46990.2020.00079,"Although deep neural networks (DNNs) achieve rather high performance in many cutting-edge applications (e.g., autonomous driving, medical diagnose), their trustworthiness on real-world scenarios still posts concerns, where some specific failure examples are often encountered during the real-world operational environment. With the limited failure examples collected during the practical operation, how to effectively leverage such failure cases to repair and enhance DNN so as to generalize to more potentially suspicious samples is challenging, but of great importance. In this paper, we formulate the failure-data-driven DNN repairing as a data augmentation problem, and design a novel augmentation-based repairing method, which to the best extent leverages limited failure cases. To realize the DNN repairing effects that generalize to specific failure examples, we originally propose few-shot guided mix (FSGMix) that augments training data with the guidance of failure examples. As a result, our method is able to achieve high generalization to the collected failure examples and other similar suspicious data. The preliminary evaluation on CIFAR-10 dataset demonstrates the potential of our proposed technique, which automatically learns to resolve the potential failure patterns in the DNN operational environment.",Deep neural network;repair;data augmentation,"717, 721",,IEEE Conferences,2020 IEEE International Conference on Software Maintenance and Evolution (ICSME),IEEE
390,,Sentiment Analysis for Software Engineering: How Far Can Pre-trained Transformer Models Go?,T. Zhang; B. Xu; F. Thung; S. A. Haryono; D. Lo; L. Jiang,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9240704,10.1109/ICSME46990.2020.00017,"Extensive research has been conducted on sentiment analysis for software engineering (SA4SE). Researchers have invested much effort in developing customized tools (e.g., SentiStrength-SE, SentiCR) to classify the sentiment polarity for Software Engineering (SE) specific contents (e.g., discussions in Stack Overflow and code review comments). Even so, there is still much room for improvement. Recently, pre-trained Transformer-based models (e.g., BERT, XLNet) have brought considerable breakthroughs in the field of natural language processing (NLP). In this work, we conducted a systematic evaluation of five existing SA4SE tools and variants of four state-of-the-art pre-trained Transformer-based models on six SE datasets. Our work is the first to fine-tune pre-trained Transformer-based models for the SA4SE task. Empirically, across all six datasets, our fine-tuned pre-trained Transformer-based models outperform the existing SA4SE tools by 6.5-35.6% in terms of macro/micro-averaged F1 scores.",Sentiment Analysis;Software Mining;Natural Language Processing;Pre-trained Models,"70, 80",,IEEE Conferences,2020 IEEE International Conference on Software Maintenance and Evolution (ICSME),IEEE
391,,A New Language Independent Strategy for Clickbait Detection,C. I. Coste; D. Bufnea; V. Niculescu,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9238342,10.23919/SoftCOM50211.2020.9238342,"Clickbait is a bad habit of today’s web publishers, which resort to such a technique in order to deceive web visitors and increase publishers’ page views and advertising revenue. Clickbait incidence is also an indicator for fake news and so, clickbait detection represents a mean in the fight against spreading false information. Recently, both the research community and the big actors on the WWW scene such as social networks and search engines, turn their attention towards this negative phenomenon that is more and more present in our everyday browsing experience. The detection techniques are usually based on intelligent classifiers, features selection being also of great importance. This paper aims to bring its own contributions in clickbait analysis and detection by presenting a new language independent strategy for clickbait detection that considers only general features that are non language specific. This approach is justified by the need for a higher level of abstractization in the clickbait detection, allowing its usability for articles written in different languages. A complex experiment on a real sample dataset was conducted and the obtained results are compared with the most relevant previous work results.",clickbait detection;features;intelligent classifier;natural language;accuracy,"1, 6",,IEEE Conferences,"2020 International Conference on Software, Telecommunications and Computer Networks (SoftCOM)",IEEE
392,,An NLP Approach to Estimating Effort in a Work Environment,I. Dan; R. Cătălin; O. Oliver,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9238219,10.23919/SoftCOM50211.2020.9238219,"Effort estimation in Software development is becoming increasingly hard to do correctly, this is in part due to the growing complexity of software projects, but also due to the higher amount of projects in total. Specialists in multiple fields are required to work together to obtain a realistic estimate, but even then, there is a good amount of risk involved when planning based on the estimate. This leads to cost increases both for the actual estimation process and the losses taken due to wrong estimations. There are already a few project estimation systems out there which take into account presumed system size, development cycles (design, develop, test) or other project related variables. We want to introduce a more granular estimation system, which uses the text descriptions of various tasks but also takes into account available metadata like seniority of the employee which will be working on said tasks, client and project/project type when estimating. The system is built on a deep neural network. The results we are getting are promising so far and we are working on establishing the human baseline accuracy while the tool is available for company employees to use.",effort;estimation;agile;task estimation,"1, 6",,IEEE Conferences,"2020 International Conference on Software, Telecommunications and Computer Networks (SoftCOM)",IEEE
393,,Evaluation of Generative Adversarial Network for Human Face Image Synthesis,I. Marin; S. Gotovac; M. Russo,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9238203,10.23919/SoftCOM50211.2020.9238203,"Meaningful and objective evaluation metric for fair model comparison is crucial for further scientific progress in the field of deep generative modeling. Despite the significant progress and impressive results obtained by Generative Adversarial Networks in recent years, the problem of their objective evaluation remains open. In this paper, we give an overview of qualitative and quantitative evaluation measures most frequently used to assess the quality of generated images and learned representations of an adversarial network together with the empirical comparison of their performance on the problem of human face image synthesis. It is shown that evaluation scores of the two most widely accepted quantitative metrics, Inception Score (IS) and Fréchet Inception Distance (FID), do not correlate. The IS is not an appropriate evaluation metric for a given problem, but FID shows good performance that correlates well with a visual inspection of generated samples. The qualitative evaluation can be used to complement results obtained with quantitative evaluation - to gain further insight into the learned data representation and detect possible overfitting.",Generative Adversarial Networks;Evaluation;Inception Score;Fréchet Inception Distance;Latent Space Exploration,"1, 6",,IEEE Conferences,"2020 International Conference on Software, Telecommunications and Computer Networks (SoftCOM)",IEEE
394,,Head Pose Estimation Using Lattice Computing Techniques,V. G. Kaburlasos; C. Lytridis; C. Bazinas; S. Chatzistamatis; K. Sotiropoulou; A. Najoua; M. Youssfi; O. Bouattane,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9238315,10.23919/SoftCOM50211.2020.9238315,"Visual stimuli are essential in many applications in human robot interaction. However, such tasks are usually computationally intensive. Also, data received from the various sensors on a robot require different data representation and processing techniques, which increases the complexity and makes the fusion of sensory data for decision making more difficult. An alternative approach is the use of the Lattice Computing (LC) paradigm for hybrid mathematical modelling based on mathematical lattice theory that unifies rigorously numerical data and non-numerical data. This paper presents an application of this approach, and more specifically a novel method for head pose estimation using LC techniques, as an initial step towards using LC as a unified methodology in social robot interaction applications.",Lattice Computing;social robots;pose estimation;tree structures,"1, 5",,IEEE Conferences,"2020 International Conference on Software, Telecommunications and Computer Networks (SoftCOM)",IEEE
395,,Syntax-aware Neural Semantic Role Labeling for Morphologically Rich Languages,D. Vasić; M. K. Vasić,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9238179,10.23919/SoftCOM50211.2020.9238179,"Semantic Role Labeling (SRL) is one of the most challenging tasks in Natural Language Processing (NLP). SRL is a task that consists of predicate identification, argument identification and argument classification. In this article we present novel approach for argument classification that is based on deep neural network architecture. Traditional discrete based SRL relies heavily on feature engineering that uses syntactic structures in contrast to deep learning approaches that encode whole sentences without taking into account syntactic features. We present an approach that uses combination of syntactic features and external word representations from FastText. The advantages of using FastText embeddings is the generation of better vector representations for rare words and FastText gives better results for words that are not within the dictionary. These attributes for vector representations give good results for morphologically rich languages. Most of the SRL approaches today are trained on resource rich languages. In this article we present novel neural architecture for SRL that is suitable for resource poor morphology rich languages. Experiments on hr500k corpus shows that our syntax-aware approach shows competitive results for argument classification. We present architecture for argument classification that is based on Bidirectional Long-Short Term Memory (Bi-LSTM) and Conditional Random Field (CRF) decoding for finding optimal sequence. Our approach showed results that are very close to benchmark results with F1 score of 72%.",semantic role labeling;deep learning;semantic parsing;morphologically rich languages,"1, 6",,IEEE Conferences,"2020 International Conference on Software, Telecommunications and Computer Networks (SoftCOM)",IEEE
396,,Deep Semantic Image Segmentation for UAV-UGV Cooperative Path Planning: A Car Park Use Case,M. K. Vasić; A. Drak; N. Bugarin; S. Kružić; J. Musić; C. Pomrehn; M. Schöbel; M. Johenneken; I. Stančić; V. Papić; R. Herpers,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9238313,10.23919/SoftCOM50211.2020.9238313,"Navigation of Unmanned Ground Vehicles (UGV) in unknown environments is an active area of research for mobile robotics. A main hindering factor for UGV navigation is the limited range of the on-board sensors that process only restricted areas of the environment at a time. In addition, most existing approaches process sensor information under the assumption of a static environment. This restrains the exploration capability of the UGV especially in time-critical applications such as search and rescue. The cooperation with an Unmanned Aerial Vehicle (UAV) can provide the UGV with an extended perspective of the environment which enables a better-suited path planning solution that can be adjusted on demand. In this work, we propose a UAV-UGV cooperative path planning approach for dynamic environments by performing semantic segmentation on images acquired from the UAV’s view via a deep neural network. The approach is evaluated in a car park scenario, with the goal of providing a path plan to an empty parking space for a ground-based vehicle. The experiments were performed on a created dataset of real-world car park images located in Croatia and Germany, in addition to images from a simulated environment. The segmentation results demonstrate the viability of the proposed approach in producing maps of the dynamic environment on demand and accordingly generating path plans for ground-based vehicles.",cooperative path planning;semantic image segmentation;neural networks;unmanned ground vehicle;unmanned aerial vehicle,"1, 6",,IEEE Conferences,"2020 International Conference on Software, Telecommunications and Computer Networks (SoftCOM)",IEEE
397,,Damage Analysis of Grassland from Aerial Images Applying Convolutional Neural Networks,M. Johenneken; A. Drak; R. Herpers,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9238230,10.23919/SoftCOM50211.2020.9238230,"Damage to grasslands is mainly caused by wild boar during foraging. Farmers in Germany thereby register yield losses and expenses for damage repair. This contribution analyzes the acquisition and processing of aerial images to orthomosaics and image segmentation to perform spatial measurements of damaged patches in grasslands. A sample set of manually annotated orthomosaics is analyzed. Preliminary classification results applying a convolutional neural network approach to segment damaged patches are presented. First results show the applicability of the applied methods in the detection of damage caused by wild boar and suggest that other damage causes (e.g., mole damage) should be considered to improve results.",UAV images;grassland;remote sensing;orthomosaics;convolutional neural networks;wild boar,"1, 6",,IEEE Conferences,"2020 International Conference on Software, Telecommunications and Computer Networks (SoftCOM)",IEEE
398,,Detecting Leaf Plant Diseases Using Deep Learning: A Review,H. B. Mureşan; A. M. Coroiu; A. D. Călin,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9238318,10.23919/SoftCOM50211.2020.9238318,"This paper presents the latest research in using deep learning methods for detecting plant disease using leaf images. It is estimated that early detection and treatment can save up to 40% of the agricultural product affected by pests. Various methods used by researchers obtain results between 62% and 98% accuracy for various types of diseases and images collected. We present the state of the art of the most relevant methods utilised, the datasets used, and the results yielded.",deep learning;smart agriculture;leaf disease detection;real image datasets;transfer learning,"1, 6",,IEEE Conferences,"2020 International Conference on Software, Telecommunications and Computer Networks (SoftCOM)",IEEE
399,,Orpheus: A New Deep Learning Framework for Easy Deployment and Evaluation of Edge Inference,P. Gibson; J. Cano,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9238597,10.1109/ISPASS48437.2020.00042,"Optimising deep learning inference across edge devices and optimisation targets such as inference time, memory footprint and power consumption is a key challenge due to the ubiquity of neural networks. Today, production deep learning frameworks provide useful abstractions to aid machine learning engineers and systems researchers. However, in exchange they can suffer from compatibility challenges (especially on constrained platforms), inaccessible code complexity, or design choices that otherwise limit research from a systems perspective. This paper presents Orpheus, a new deep learning framework for easy prototyping, deployment and evaluation of inference optimisations. Orpheus features a small codebase, minimal dependencies, and a simple process for integrating other third party systems. We present some preliminary evaluation results.",,"229, 230",,IEEE Conferences,2020 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS),IEEE
400,,AI on the Edge: Architectural Alternatives,M. M. John; H. Holmström Olsson; J. Bosch,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9226348,10.1109/SEAA51224.2020.00015,"Since the advent of mobile computing and IoT, a large amount of data is distributed around the world. Companies are increasingly experimenting with innovative ways of implementing edge/cloud (re)training of AI systems to exploit large quantities of data to optimize their business value. Despite the obvious benefits, companies face challenges as the decision on how to implement edge/cloud (re)training depends on factors such as the task intent, the amount of data needed for (re)training, edge-to-cloud data transfer, the available computing and memory resources. Based on action research in a software-intensive embedded systems company where we study multiple use cases as well as insights from our previous collaborations with industry, we develop a generic framework consisting of five architectural alternatives to deploy AI on the edge utilizing transfer learning. We validate the framework in four additional case companies and present the challenges they face in selecting the optimal architecture. The contribution of the paper is threefold. First, we develop a generic framework consisting of five architectural alternatives ranging from a centralized architecture where cloud (re)training is given priority to a decentralized architecture where edge (re)training is instead given priority. Second, we validate the framework in a qualitative interview study with four additional case companies. As an outcome of validation study, we present two variants to the architectural alternatives identified as part of the framework. Finally, we identify the key challenges that experts face in selecting an ideal architectural alternative.",Artificial Intelligence;Machine Learning;Deep Learning;Edge;Cloud;Transfer Learning;Action Research;Architectural alternatives,"21, 28",,IEEE Conferences,2020 46th Euromicro Conference on Software Engineering and Advanced Applications (SEAA),IEEE
401,,Trends in Software Engineering Processes using Deep Learning: A Systematic Literature Review,A. F. Del Carpio; L. B. Angarita,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9226311,10.1109/SEAA51224.2020.00077,"In recent years, several researchers have applied machine learning techniques to several knowledge areas achieving acceptable results. Thus, a considerable number of deep learning models are focused on a wide range of software processes. This systematic review investigates the software processes supported by deep learning models, determining relevant results for the software community. This research identified that the most extensively investigated sub-processes are software testing and maintenance. In such sub-processes, deep learning models such as CNN, RNN, and LSTM are widely used to process bug reports, malware classification, libraries and commits recommendations generation. Some solutions are oriented to effort estimation, classify software requirements, identify GUI visual elements, identification of code authors, the similarity of source codes, predict and classify defects, and analyze bug reports in testing and maintenance processes.",Deep Learning;Machine Learning;Software Processes;Systematic Review,"445, 454",,IEEE Conferences,2020 46th Euromicro Conference on Software Engineering and Advanced Applications (SEAA),IEEE
402,,An Observational Study on the Maintainability Characteristics of the Procedural and Object-Oriented Programming Paradigms,W. Brborich; B. Oscullo; J. E. Lascano; S. Clyde,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9206213,10.1109/CSEET49119.2020.9206213,"This paper presents an observational study that takes an initial step towards answering the question of how the selection of a programming paradigm affects the maintainability of a software system. Answers to this question are important because they can provide insight on how the maintainability characteristics of different programming paradigms could be leveraged to improve software-engineering education. The observational study compares developers' effectiveness and speed performing pre-determined maintenance tasks on two equivalent variations of a web-based application: one based on Procedural Programming and one based on Object-Oriented Programming. Presented within this paper are the study's design, metrics measuring maintenance effectiveness and speed, and a statistical analysis of the results, which provides evidence that the maintainability characteristics of the two paradigms are different within the context of this research. In addition, as an observational study, some of its important contributions are lessons learned about the experiment design process and ideas for new hypotheses. These are presented in preparation for a future, broader research, and larger-scale experiment. Finally, we discuss some forward-thinking ideas about how differences in the maintenance characteristics of programming paradigms may influence on software-engineering education.",programming paradigms;object-oriented programming;procedural programming paradigm;maintainability;software-engineering education,"1, 10",,IEEE Conferences,2020 IEEE 32nd Conference on Software Engineering Education and Training (CSEE&T),IEEE
403,,Software Curriculum @ Siemens — The Architecture of a Training Program for Architects,M. Backert; T. Blum; R. Kreuter; F. Paulisch; P. Zimmerer,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9206182,10.1109/CSEET49119.2020.9206182,"Siemens established a company-wide role-based qualification and certification curriculum, focusing on the topic of “architecture”. Architects play a central role in the product lifecycle of the complex systems that Siemens offers. Since the curriculum's start in 2006 with the “senior software architect” program, we have added programs for software, system, and test architects. This curriculum is meanwhile broadly established throughout Siemens and Siemens Healthineers and other associated Siemens companies. We meanwhile have almost 700 certified architects in almost 20 countries. This industrial experience report describes the curriculum briefly but will focus on how the “architecture” of the training program for architects enabled us to evolve and extend the curriculum over time. The evolution covers not only additional roles, but also keeps the technical content up to date and adapts to modern learning formats as we strive to achieve the most impact in the available time. Our experience may be a blueprint for other key role curricula.",software engineering education;software architecture training;software architecture;testing;system architecture;digitalization;product line engineering;training evolution,"1, 6",,IEEE Conferences,2020 IEEE 32nd Conference on Software Engineering Education and Training (CSEE&T),IEEE
404,,CNN-Based Model for Chinese Information Processing and Its Application in Large-Scale Book Purchasing,C. Guo; P. Yu; W. Guo; X. Wang,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9202782,10.1109/COMPSAC48688.2020.0-145,"The demand for library books has been growing year by year, and manual methods for purchasing books have been unable to keep with these ever-increasing Chinese book demands. As deep learning techniques have developed, they have achieved remarkable results in various fields. In this study, we applied a convolution neural network (CNN)-based model to the book purchasing task. We also built a Chinese book dataset based on historical data from the Wuhan University Library. Experimental results on this dataset show that our CNN-BOOK model can reach an accuracy of 83% on the book purchasing task. This method greatly improves the efficiency of book purchases and can provide new ideas for how library purchasing is conducted in the future.",Chinese information processing;Text classification;Book Acquisition;Convolutional neural network,"973, 978",,IEEE Conferences,"2020 IEEE 44th Annual Computers, Software, and Applications Conference (COMPSAC)",IEEE
405,,A Novel Tax Evasion Detection Framework via Fused Transaction Network Representation,Y. Wu; B. Dong; Q. Zheng; R. Wei; Z. Wang; X. Li,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9202523,10.1109/COMPSAC48688.2020.00039,"Tax evasion usually refers to the false declaration of taxpayers to reduce their tax obligations; this type of behavior leads to the loss of taxes and damage to the fair principle of taxation. Tax evasion detection plays a crucial role in reducing tax revenue loss. Currently, efficient auditing methods mainly include traditional data-mining-oriented methods, which cannot be well adapted to the increasingly complicated transaction relationships between taxpayers. Driven by this requirement, recent studies have been conducted by establishing a transaction network and applying the graphical pattern matching algorithm for tax evasion identification. However, such methods rely on expert experience to extract the tax evasion chart pattern, which is time-consuming and labor-intensive. More importantly, taxpayers' basic attributes are not considered and the dual identity of the taxpayer in the transaction network is not well retained. To address this issue, we have proposed a novel tax evasion detection framework via fused transaction network representation (TED-TNR), to detecting tax evasion based on fused transaction network representation, which jointly embeds transaction network topological information and basic taxpayer attributes into low-dimensional vector space, and considers the dual identity of the taxpayer in the transaction network. Finally, we conducted experimental tests on real-world tax data, revealing the superiority of our method, compared with state-of-the-art models.","Transaction network, Network representation learning, Tax evasion detection","235, 244",,IEEE Conferences,"2020 IEEE 44th Annual Computers, Software, and Applications Conference (COMPSAC)",IEEE
406,,Deep Learning for Visual Segmentation: A Review,J. Sun; Y. Li; H. Lu; T. Kamiya; S. Serikawa,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9202594,10.1109/COMPSAC48688.2020.00-84,"Big data-driven deep learning methods have been widely used in image or video segmentation. The main challenge is that a large amount of labeled data is required in training deep learning models, which is important in real-world applications. To the best of our knowledge, there exist few researches in the deep learning-based visual segmentation. To this end, this paper summarizes the algorithms and current situation of image or video segmentation technologies based on deep learning and point out the future trends. The characteristics of segmentation that based on semi-supervised or unsupervised learning, all of the recent novel methods are summarized in this paper. The principle, advantages and disadvantages of each algorithms are also compared and analyzed.","Deep learning, Image segmentation, Video segmentation","1256, 1260",,IEEE Conferences,"2020 IEEE 44th Annual Computers, Software, and Applications Conference (COMPSAC)",IEEE
407,,Simulating the Printed Circuit Board Assembly Process for Image Generation,J. Nau; J. Richter; D. Streitferdt; M. Kirchhoff,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9202457,10.1109/COMPSAC48688.2020.00040,"The inspection of printed circuit board assemblies gradually incorporates deep-learning-based classifiers. However, such classifiers require a vast dataset. To our knowledge, such a dataset is not available. This paper proposes a method to simulate the assembly process aiming at generating such a dataset. The simulation of the solder joint shape forming during reflow and the creation of a photorealistic rendering of the assembled board have the most significant impact on the visual appearance of the results. Therefore, this paper focuses on the simulation of these steps. The calculation of the solder joint shape requires minimizing the surface tension energy. For this, the algorithm discretizes the energy equations over a heightmap. The proposed software architecture for the simulation is highly extendable and facilitate future development. Experiments with the simulation of solder joints of a chip resistor show a remarkable similarity to real images from an automatic optical inspection machine.",Computer architecture;Simulation;Computational fluid dynamics;Soldering;Automatic optical inspection;Electronics manufacturing;Quality control;Training;Deep learning;Classification,"245, 254",,IEEE Conferences,"2020 IEEE 44th Annual Computers, Software, and Applications Conference (COMPSAC)",IEEE
408,,A Real-Time Feature Indexing System on Live Video Streams,A. Chakraborty; A. Pawar; H. Jang; S. Huang; S. Mishra; S. -H. Chen; Y. -H. Chang; G. K. Thiruvathukal; Y. -H. Lu,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9202837,10.1109/COMPSAC48688.2020.00016,"Most of the existing video storage systems rely on offline processing to support the feature-based indexing on video streams. The feature-based indexing technique provides an effective way for users to search video content through visual features, such as object categories (e.g., cars and persons). However, due to the reliance on offline processing, video streams along with their captured features cannot be searchable immediately after video streams are recorded. According to our investigation, buffering and storing live video steams are more time-consuming than the YOLO v3 object detector. Such observation motivates us to propose a real-time feature indexing (RTFI) system to enable instantaneous feature-based indexing on live video streams after video streams are captured and processed through object detectors. RTFI achieves its real-time goal via incorporating the novel design of metadata structure and data placement, the capability of modern object detector (i.e., YOLO v3), and the deduplication techniques to avoid storing repetitive video content. Notably, RTFI is the first system design for realizing real-time feature-based indexing on live video streams. RTFI is implemented on a Linux server and can improve the system throughput by upto 10.60x, compared with the base system without the proposed design. In addition, RTFI is able to make the video content searchable within 20 milliseconds for 10 live video streams after the video content is received by the proposed system, excluding the network transfer latency.",object detection;data storage;real-time;feature-bassed indexing,"42, 50",,IEEE Conferences,"2020 IEEE 44th Annual Computers, Software, and Applications Conference (COMPSAC)",IEEE
409,,ThermoCam: Smart Baby Monitoring Assistant,M. E. Akbiyik; C. S. Coban; E. Aygun; H. Z. Imamoglu; D. Gurgunoglu; D. Ider,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9202512,10.1109/COMPSAC48688.2020.0-185,"Fever is one of the main symptoms of various illnesses during infancy and it requires continuous supervision especially above 38 degree Celsius as fatal complications may develop. With the emerging IoT and e-Health technologies, a variety of different consumer products are being developed to facilitate the fever monitoring for parents. In this study, our aim is to develop a prototype that will continuously track the body temperature of an infant using an RGB and a thermal camera, and provide different notification capabilities for the users. The product is expected to work effectively from 1-2 meters of distance with approximately 0.4 degree Celsius accuracy. The real-time video display is included to the system with one second of latency for video, audio and alarm features. The user interface also provides room temperature and humidity data, along with measured statistics of baby body temperature. The system is also able to alarm the user upon the detection of loud noise in the environment, which can indicate interruptions in baby's sleep. The prototype is expected to provide an affordable, all-in-one solution for the parents as a baby monitoring device.","Baby monitor, home monitoring, thermal imaging, eHealth, IoT","636, 643",,IEEE Conferences,"2020 IEEE 44th Annual Computers, Software, and Applications Conference (COMPSAC)",IEEE
410,,ReRe: A Lightweight Real-Time Ready-to-Go Anomaly Detection Approach for Time Series,M. -C. Lee; J. -C. Lin; E. G. Gan,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9202855,10.1109/COMPSAC48688.2020.0-226,"Anomaly detection is an active research topic in many different fields such as intrusion detection, network monitoring, system health monitoring, IoT healthcare, etc. However, many existing anomaly detection approaches require either human intervention or domain knowledge, and may suffer from high computation complexity, consequently hindering their applicability in real-world scenarios. Therefore, a lightweight and ready-to-go approach that is able to detect anomalies in real-time is highly sought-after. Such an approach could be easily and immediately applied to perform time series anomaly detection on any commodity machine. The approach could provide timely anomaly alerts and by that enable appropriate countermeasures to be undertaken as early as possible. With these goals in mind, this paper introduces ReRe, which is a Real-time Ready-to-go proactive Anomaly Detection algorithm for streaming time series. ReRe employs two lightweight Long Short-Term Memory (LSTM) models to predict and jointly determine whether or not an upcoming data point is anomalous based on short-term historical data points and two long-term self-adaptive thresholds. Our experiment based on real-world time-series datasets demonstrates the good performance of ReRe in real-time anomaly detection without requiring human intervention or domain knowledge.","Time-series anomaly detection, lightweight LSTM, unsupervised learning, real time, self-adaptive threshold","322, 327",,IEEE Conferences,"2020 IEEE 44th Annual Computers, Software, and Applications Conference (COMPSAC)",IEEE
411,,TTED-PU:A Transferable Tax Evasion Detection Method Based on Positive and Unlabeled Learning,F. Zhang; B. Shi; B. Dong; Q. Zheng; X. Ji,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9202850,10.1109/COMPSAC48688.2020.00036,"Tax evasion usually refers to taxpayers making false declarations in order to reduce their tax obligations. One of the most common types of tax evasion is to lower the declared taxable amount. This kind of behavior will lead to the loss of tax revenues and damage the fairness of taxation. One of the main roles of the tax authorities is to conduct tax evasion testing through efficient auditing methods. At present, by using machine learning technology along with large amounts of labeled data, tax evasion detection models have achieved good results in specific areas. However, it is a long and costly process for tax experts to label large amounts of data. Since, the data distribution characteristics vary from region to region, models cannot be used across regions. In this paper, we propose a new method called a transferable tax evasion detection method based on positive and unlabeled learning (TTED-PU), which uses only semi-supervised techniques to detect tax evasion in the source domain. In addition, we use the idea of transfer to adapt to the domain to predict tax evasion behavior on the target domain where labeled tax data are unavailable. We evaluate our method on real-world tax data set. The experimental results show that our model can detect tax evasion in both the source and target domains.","tax evasion, transfer learning, positive and unlabeled learning","207, 216",,IEEE Conferences,"2020 IEEE 44th Annual Computers, Software, and Applications Conference (COMPSAC)",IEEE
412,,NEUD-TRI: Network Embedding Based on Upstream and Downstream for Transaction Risk Identification,J. An; Q. Zheng; R. Wei; B. Dong; X. Li,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9202459,10.1109/COMPSAC48688.2020.0-232,"Invoices serve as records of financial transactions of taxpayers and significant basis to controlling tax source and collection of tax, via analyzing which, we can discern diversified tasks of tax risk, such as industry identification, hidden transaction detection, and illegal behavior mining. Among all the existing studies related to the identification of tax risk, there are some weaknesses through the machine learning model and network analysis because of the dependence on tax knowledge. Different from the manual selection of indicators and the manual definitions mode with the guidance of tax knowledge in the past, in this paper, we propose a novel method, namely, network embedding based on upstream and downstream for tax risk identification (NEUD-TRI), which considers the taxpayers serving as both seller and purchaser. The method designs optimization functions respectively to capture local and global static network structures and dynamic network structure. In view of the significant discrepancy of weights in the transaction network, this paper normalizes the weight within the range of the upstream and downstream of the vertex. Negative sampling and edge sampling are adopted to deal with the large-scale trait of the transaction network. Empirical results on tax data-sets of Shanxi province substantiate the effectiveness of our models.",Taxpayer Representation;Transaction Network;Network Representation Learning;Tax Risk Identification,"277, 286",,IEEE Conferences,"2020 IEEE 44th Annual Computers, Software, and Applications Conference (COMPSAC)",IEEE
413,,A Comparative Evaluation of Heart Rate Estimation Methods using Face Videos,J. Hernandez-Ortega; J. Fierrez; A. Morales; D. Diaz,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9202804,10.1109/COMPSAC48688.2020.00-53,"This paper presents a comparative evaluation of methods for remote heart rate estimation using face videos, i.e., given a video sequence of the face as input, methods to process it to obtain a robust estimation of the subject's heart rate at each moment. Four alternatives from the literature are tested, three based in hand-crafted approaches and one based on deep learning. The methods are compared using RGB videos from the COHFACE database. Experiments show that the learning-based method achieves much better accuracy than the hand-crafted ones. The low error rate achieved by the learning-based model makes possible its application in real scenarios, e.g. in medical or sports environments.","Remote Plethysmography, Heart Rate, Face Biometrics, Hand-crafted, Deep Learning","1438, 1443",,IEEE Conferences,"2020 IEEE 44th Annual Computers, Software, and Applications Conference (COMPSAC)",IEEE
414,,An Improved Design Scheme for Perceptual Hashing Based on CNN for Digital Watermarking,Z. Meng; T. Morizumi; S. Miyata; H. Kinoshita,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9202519,10.1109/COMPSAC48688.2020.00048,"Digital watermarking technology is used extensively in the field of digital rights management. However, there are a few problems when it comes to making effective use of digital watermarking. First, for conventional digital watermarking, a digital image is used only as a carrier for embedded watermarking information, and as this information may be diverted to other images, the watermark information needs to be generated based on the original image. Second, after the original image is modified/edited, the watermark information needs to prove that it is from the original image. Third, multiple digital watermarks need to be stored and managed without depending on trusted third parties. In an earlier work, we proposed a digital rights management system based on digital watermarking, blockchain, and perceptual hashing to resolve these issues. However, because we used conventional perceptual hashing, we could not draw sufficient conclusions about the first and second problems. In order to obtain a stable digest message of an image for digital watermarking, we here propose a new construction method for perceptual hashing using a convolutional neural network (CNN). In the proposed method, we first construct a machine-learned CNN for accepting an image that we want to take the perceptual hash value. The perceptual hash value is the cryptographic hash value of the weights that make up the CNN. We then verify that the reconstructed CNN can guarantee the hash value used when obtaining the hash value, and confirm that the image to be verified is accepted and is the perceptual hash value of this image.",perceptual hashing;CNN;digital watermarking;digital rights management,"1789, 1794",,IEEE Conferences,"2020 IEEE 44th Annual Computers, Software, and Applications Conference (COMPSAC)",IEEE
415,,An Early Warning System for Hemodialysis Complications Utilizing Transfer Learning from HD IoT Dataset,C. Shih; L. Youchen; C. -h. Chen; W. C. -C. Chu,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9202848,10.1109/COMPSAC48688.2020.0-168,"According to the 2018 annual report of US Department of Kidney Data System (USRDS), Taiwan's dialysis rate and prevalence rate are the highest in the world due to population aging, diabetes and progresses in cardiovascular care. With the rise of artificial intelligence deep learning in recent years, various analytical software resources have gradually become easier to obtain. At the same time, wearable cyber physical sensors are becoming more and more popular. Measurements on vital signs such as heartbeat, electrocardiogram, and blood oxygenation blood pressure values are ubiquitous. We propose an integrated system that combines dialysis big data deep learning analysis with cross platform physiological sensing. We specifically tackle the early warning of dialysis discomfort such as hypotension, hypertension, cramps, etc., this requires a large amount of data collection, related training, data sources including dialysis treatment process and home physiological data. Although the Dialysis machine is able to produce huge amount of IoT data, the usable data for early warning system training is not as huge due to the limited physician labors devoted for labeling questionable samples. This generally leads to low accuracy for regular CNN training methods. We enhance the AI training performance via a transfer learning technique. The AI training accuracy reaches the value of 99% with the help of transfer learning, while that of an original CNN process on the HD data bears a low 60% accuracy. Given the high prediction accuracy of our AI engine, we are able to integrate the real time measurements from Dialysis machine with wearable devices such as ECG sensors and wrist health watches, and make precision prediction of incoming discomfort during the HD treatments. The ECG signal of the same group patients are also analyzed with the same technique. The same accuracy enhancement are also observed.","Hemodialysis complications, Deep learning, early warning system, mel-spectrogram, medical IOT, transfer learning","759, 767",,IEEE Conferences,"2020 IEEE 44th Annual Computers, Software, and Applications Conference (COMPSAC)",IEEE
416,,Dual Adversarial Networks for Land-Cover Classification,J. An; R. Wei; Z. Zha; B. Dong,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9202763,10.1109/COMPSAC48688.2020.0-216,"River basin scene classification as an important application in the field of land-cover recognition has been arousing extensive concern. Traditional land-cover classification methods with multi-feature extractions on specific scene perform well on a single river basin, however, poorly address inter-basin classification owing to the varied texture shown in satellite images cross river basins (e.g., topography and climates). Current transfer learning approaches with domain adaptation, which can shorten the discrepancies between two river basins, pay less attention to diversity of multi-feature extractions given by remote sensing images, which may lead to negative transfer. To better address the above challenges, this paper proposes a model known as Dual Adversarial Networks for Land-cover Classification (DANLC). Our DANLC architecture consists of two domain adversarial networks in a paralleled structure, namely RGB and texture networks, for multi-feature extractions, which are able to capture the underlying representation of satellite images from different perspectives and get invariable transfer component. Results demonstrate the outstanding performance of our model in both the classification effect and robustness compared with traditional methods and state-of-the-art transfer learning approaches.",,"394, 399",,IEEE Conferences,"2020 IEEE 44th Annual Computers, Software, and Applications Conference (COMPSAC)",IEEE
417,,Collaborative Filtering Recommendation Based on Multi-Domain Semantic Fusion,X. Li; J. He; N. Zhu; Z. Hou,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9202779,10.1109/COMPSAC48688.2020.00041,"Collaborative filtering based on single domains has become widely used in today's recommendation system. Nevertheless, it has two problems that need to be solved, i.e., the cold start problem and the data sparseness problem. As the result, cross-domain recommendation technology has emerged, which aims at integrating user preference characteristics from different domains. This paper proposes a collaborative filtering recommendation method based on multi-domain semantic fusion (CF-MDS). CF-MDS achieves cross-domain item similarity calculation through semantic analysis and ontology and integrates data from different domains iteratively based on domain relevance to rate users on target domain items and to produce a cross-domain user-item rating matrix. Collaborative filtering technology is then combined with multi-domain fusion recommendation algorithm. Experimental results show that the proposed method can deal effectively with the cold start problem and data sparsity problem that exist in traditional recommendation systems as well as can improve the diversity of recommendation. Compared to other cross-domain recommendation methods, the proposed method can better meet personal needs of users and also improve the accuracy of recommendation.",recommendation system;cross-domain;semantic analysis;item similarity;collaborative filtering,"255, 261",,IEEE Conferences,"2020 IEEE 44th Annual Computers, Software, and Applications Conference (COMPSAC)",IEEE
418,,SLA+: Narrowing the Difference between Data Sets in Heterogenous Cross-Project Defection Prediction,J. Wu; Y. Wu; M. Zhou; X. Jiang,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9202405,10.1109/COMPSAC48688.2020.00-88,"Different from existing cross-project defection prediction(CPDP) problems which assume that there is a close relation between the source data sets and the target data sets, in the heterogenous cross-project defection prediction(HCPDP) problem, the target data sets can be totally different from the source data sets. In order to narrow the difference between source data sets and target data sets, we implemented our own algorithm SLA + based on the selective learning algorithm . We select one of the multiple sources that have the highest similarity to the target data set as the source data set, and select one or more of the other source data sets that are similar to both the target data set and the source data set as an intermediate domain. We set up a bridge between the target domain and the source domain through the intermediate domain , breaking the large distribution gap for transferring knowledge between the source domain and the target domain. Besides, we achieve the purpose of dimensionality reduction by mining the potential relationship between features. We have done experiments on open source data sets, and the data sets used are all heterogeneous. The experiments prove that our method achieves comparable results compared with state-of-the-art HCPDP in most cases.","heterogenous cross-project defection prediction, intermediate domain, selective learning algorithm","1229, 1234",,IEEE Conferences,"2020 IEEE 44th Annual Computers, Software, and Applications Conference (COMPSAC)",IEEE
419,,An Approach to Improving the Effectiveness of Data Augmentation for Deep Neural Networks,S. Jang; K. Y. Lee; Y. Kim,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9202845,10.1109/COMPSAC48688.2020.00-78,"Nowadays, deep neural networks (DNNs) have many achievements in various fields like classification, clustering and regression. However, there is a major drawback: they are always affected by the dataset. Since the good quality and quantity of datasets were accompanied, DNNs were able to have much better performance. The evidence indicates that a good model should be supported by a well-organized dataset. In this respect, DNNs are also heavily influenced by the quality and quantity of the dataset used to train the model. Generally, many DNNs models achieve a great performance based on the assumption that the dataset represents all aspects of the problem well. However, in order to be applied in a real situation, the dataset may need to be modified or added for each specific application. Especially, it is difficult to add new classes into an already well-organized dataset. Adding data of completely new classes into the existing dataset is still a difficult challenge. In this paper we present a simple yet effective way to add data of new classes into the dataset especially when its amount is very small compared to the existing dataset. First, we have experimented on how performance is affected if a small amount of data of new classes comes into the dataset as it is. Then, in order to improve the performance of the model on that small amount of new data, we used an upsampling method to balance the newly added data and the original data. The upsampling method is one of the inflation methods used to balance data. The upsampling method maintains a balance of data with different amounts of objects so that less information is not forgotten. We describe and analyze our experiments on the MNIST (handwritten dataset) and MSCOCO (image captioning dataset) and explain how our upsampling approach can improve the performance of the models",Data Upsampling;Image Captioning;Data Augmentation;Deep neural network,"1290, 1295",,IEEE Conferences,"2020 IEEE 44th Annual Computers, Software, and Applications Conference (COMPSAC)",IEEE
420,,CHOCSLAT: Chinese Healthcare-Oriented Computerised Speech & Language Assessment Tools,D. Towey; L. Jin; H. Zhu; J. Zhu; K. Feng; H. Geng; J. Lu; T. Yu; Y. Wang,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9202505,10.1109/COMPSAC48688.2020.00-49,"This paper describes an on-going project to develop diagnostic profiling tools for healthcare professionals to identify potential speech and language developmental problems of Chinese-speaking children. The tools aim to provide a technical advance in helping children who may have speech impairment or language delay. The project is currently being carried out as a collaboration with Chinese healthcare professionals, and a multidisciplinary team including applied linguists, speech and language pathologists, and computer scientists (staff and students) at the first Sino-foreign higher education institution in China, University of Nottingham Ningbo China (UNNC). The paper presents the background, development, and current state of the project. Challenges to project completion, including difficulties encountered due to the Covid-19 outbreak and subsequent world-wide emergency and lock-down are also discussed.",Chinese Language;child language;language acquisition;language assessment;language development;language impairment;langugage profiling;Sino-foreign;staff-student collaboration;computerised assessment,"1460, 1465",,IEEE Conferences,"2020 IEEE 44th Annual Computers, Software, and Applications Conference (COMPSAC)",IEEE
421,,Generating Region of Interests for Invasive Breast Cancer in Histopathological Whole-Slide-Image,S. M. Patil; L. Tong; M. D. Wang,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9202549,10.1109/COMPSAC48688.2020.0-174,"The detection of the region of interests (ROIs) on Whole Slide Images (WSIs) is one of the primary steps in computer-aided cancer diagnosis and grading. Early and accurate identification of invasive cancer regions in WSI is critical in the improvement of breast cancer diagnosis and further improvements in patient survival rates. However, invasive cancer ROI segmentation is a challenging task on WSI because of the low contrast of invasive cancer cells and their high similarity in terms of appearance, to non-invasive region. In this paper, we propose a CNN based architecture for generating ROIs through segmentation. The network tackles the constraints of data-driven learning and working with very low-resolution WSI data in the detection of invasive breast cancer. Our proposed approach is based on transfer learning and the use of dilated convolutions. We propose a highly modified version of U-Net based auto-encoder, which takes as input an entire WSI with a resolution of 320x320. The network was trained on low-resolution WSI from four different data cohorts and has been tested for inter as well as intra-dataset variance. The proposed architecture shows significant improvements in terms of accuracy for the detection of invasive breast cancer regions.","Invasive Breast Cancer, Whole-Slide-Image, Deep Learning, ROI Segmentation, CNN","723, 728",,IEEE Conferences,"2020 IEEE 44th Annual Computers, Software, and Applications Conference (COMPSAC)",IEEE
422,,Detecting Malicious Web Requests Using an Enhanced TextCNN,L. Yu; L. Chen; J. Dong; M. Li; L. Liu; B. Zhao; C. Zhang,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9202428,10.1109/COMPSAC48688.2020.0-167,"This paper proposes an approach that combines a deep learning-based method and a traditional machine learning-based method to efficiently detect malicious requests Web servers received. The first few layers of Convolutional Neural Network for Text Classification (TextCNN) are used to automatically extract powerful semantic features and in the meantime transferable statistical features are defined to boost the detection ability, specifically Web request parameter tampering. The semantic features from TextCNN and transferable statistical features from artificially-designing are grouped together to be fed into Support Vector Machine (SVM), replacing the last layer of TextCNN for classification. To facilitate the understanding of abstract features in form of numerical data in vectors extracted by TextCNN, this paper designs trace-back functions that map max-pooling outputs back to words in Web requests. After investigating the current available datasets for Web attack detection, HTTP Dataset CSIC 2010 is selected to test and verify the proposed approach. Compared with other deep learning models, the experimental results demonstrate that the approach proposed in this paper is competitive with the state-of-the-art.","Web security, Malicious Web requests detection, Transferable statistical features, Deep learning, Convolutional Neural Network for Text Classification (TextCNN), Support vector machine (SVM)","768, 777",,IEEE Conferences,"2020 IEEE 44th Annual Computers, Software, and Applications Conference (COMPSAC)",IEEE
423,,Few-Shot Ontology Alignment Model with Attribute Attentions,J. Sun; S. Takeuchi; I. Yamasaki,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9202824,10.1109/COMPSAC48688.2020.00-90,"Nowadays, explosive growth of ontologies are used for managing data in various domains. They usually own different vocabularies and structures following different fashions. Ontology alignment finding semantic correspondences between elements of these ontologies can effectively facilitate the data communication and novel application creation in many practical scenarios. However, we noticed that, the traditional parametric ontology mapping methods still depend on individualistic abilities for setting proper parameters for mapping. When trying to utilize artificial neural networks for the automatic ontology mapping, the training data are found insufficient in most of the cases. This paper analyzes these problems, and proposes a few-shot ontology alignment model, which can automatically learn how to map two ontologies from only a few training links between their element pairs. The proposed model applies the Siamese neural network in computer vision on ontology alignment and designs an attention detection network learning the attention weights for different ontology attributes. A few experiments that conducted on the anatomy ontology alignment show that our model achieves good performance (94.3% of F-measure) with 200 training alignments without traditional parametric setting.","Ontology Alignment, Ontology Matching, Siamese neural network, Machine learning","1218, 1222",,IEEE Conferences,"2020 IEEE 44th Annual Computers, Software, and Applications Conference (COMPSAC)",IEEE
424,,Towards Identifying the Optimal Timing for Near Real-Time Smoking Interventions Using Commercial Wearable Devices,T. Weber; M. Ferrin; F. Sweeney; S. Ahmed; M. Sharmin,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9202679,10.1109/COMPSAC48688.2020.0-212,"Tobacco addiction is one of the most challenging behavioral health problems with successful cessation rates remaining in single digits. With increased availability of commercially available mobile and wearable devices we have the opportunity to infer lapse vulnerability and intervene in near real-time. In this work we present findings from a mixed-method study where we collected around 398.2 hours of physiological data from regular smokers in the field (N is 5) using commercial wearable devices along with contextual data using EMAs. We applied quantitative and qualitative analysis techniques to identify key factors contributing to smoking lapse and developed a statistical model capable of inferring lapse vulnerability from physiological and contextual signals collected from the natural environment. Our methodology and findings show promise in designing practical, near real-time smoking intervention systems that can be used by regular smokers in their everyday living situation.","just-in-time adaptive interventions, mHealth, p4 medicine, smoking cessation, wearable technology, human-computer interaction, predictive modeling","429, 438",,IEEE Conferences,"2020 IEEE 44th Annual Computers, Software, and Applications Conference (COMPSAC)",IEEE
425,,Data Analytics for the COVID-19 Epidemic,R. Wang; G. Hu; C. Jiang; H. Lu; Y. Zhang,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9202626,10.1109/COMPSAC48688.2020.00-83,"With the spread of COVID-19 worldwide, peo-plej<sup>-</sup>s production and life have been significantly affected. Artificial intelligence and big data technologies have been vigorously developed in recent years. It is very significant to use data science and technology to help humans in a timely and accurate manner to prevent and control the development of the epidemic, maintain social stability and assess the impact of the epidemic. This paper explores how data science can play a role from the perspectives of epidemiology, social networking, and economics. In particular, for the existing epidemic model SIR, we present a parameter learning method using particle swarm optimization (PSO) and the least squares method, and use it to predict the trend of the epidemic. Aiming at the social network data, we provide a specific method to realize sentiment analysis during the epidemic and propose an explainable fake news detection technique based on a variety of data mining methods.",Data science;epidemic;data mining;sentiment analysis;fake news detection,"1261, 1266",,IEEE Conferences,"2020 IEEE 44th Annual Computers, Software, and Applications Conference (COMPSAC)",IEEE
426,,Deep Learning for Hardware-Constrained Driverless Cars,B. Krishnaswami Sreedhar; N. Shunmugam,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9202838,10.1109/COMPSAC48688.2020.00013,"The field of self-driving cars is a fast-growing one, and numerous companies and organizations are working at the forefront of this technology. One of the major requirements for self-driving cars is the necessity of expensive hardware to run complex models. This project aims to identify a suitable deep learning model under hardware constraints. We obtain the results of a supervised model trained with data from a human driver and compare it to a reinforcement learning-based approach. Both models will be trained and tested on devices with low-end hardware, and their results visualized with the help of a driving simulator. The objective is to demonstrate that even a simple model with enough data augmentation can perform specific tasks and does not require much investment in time and money. We also aim to introduce portability to deep learning models by trying to deploy the model in a mobile device and show that it can work as a standalone module.","Self-driving cars, Deep learning, Behavioral cloning, Reinforcement learning","26, 29",,IEEE Conferences,"2020 IEEE 44th Annual Computers, Software, and Applications Conference (COMPSAC)",IEEE
427,,Cinematographic Shot Classification through Deep Learning,B. Vacchetti; T. Cerquitelli; R. Antonino,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9202417,10.1109/COMPSAC48688.2020.0-222,"Cinematographic shot classification assigns a category to each shot on the basis of the field size, which is determined by the portion of the subject and of the environment shown in the field of view of the camera. This task is very important in the context of the creative field and can help freelancers in their daily activities when it is performed automatically. Novel and effective approaches capable of processing large volumes of images/videos and analyzing them effectively are becoming increasingly important. This paper presents a data-driven methodology to automatically classify cinematographic shots through deep learning techniques. In our study, we consider four classes of film shots: full figure, half figure, half torso and close up and we discuss three different scenarios in which the proposed work can be helpful. A new dataset of images was created to evaluate performances of the proposed methodology and to compare them with state-of-the-art techniques. Experimental results demonstrate the effectiveness of the proposed approach in performing the classification task with good accuracy.","Machine learning, image classification, creative field.","345, 350",,IEEE Conferences,"2020 IEEE 44th Annual Computers, Software, and Applications Conference (COMPSAC)",IEEE
428,,Transfer Learning from Planar to Spherical Images,T. Takeda; K. Yoshida,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9202664,10.1109/COMPSAC48688.2020.00037,"In recent years, 360-degree cameras have been released, and making it easy to enjoy 360-degree (i.e., spherical) images. This new image format is accepted as a new personal recording medium. New businesses using it are also emerging. In such a background, 360-degree image search function increases its importance. In this paper, we propose a method for processing 360-degree images using a transfer learning framework. The reuse of network weights by transfer learning framework reduces the computing resources required for training period. The use of small image size reduces the computing resources required for test period. It also improves accuracy of image search function. This paper also discuss the relationship between this unexpected improvement and transfer learning method we used.",Deep learning;Spherical image;Transfer learning,"217, 224",,IEEE Conferences,"2020 IEEE 44th Annual Computers, Software, and Applications Conference (COMPSAC)",IEEE
429,,The Effectiveness of Supervised Machine Learning Algorithms in Predicting Software Refactoring,M. Aniche; E. Maziero; R. Durelli; V. Durelli,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9186715,10.1109/TSE.2020.3021736,"Refactoring is the process of changing the internal structure of software to improve its quality without modifying its external behavior. Empirical studies have repeatedly shown that refactoring has a positive impact on the understandability and maintainability of software systems. However, before carrying out refactoring activities, developers need to identify refactoring opportunities. Currently, refactoring opportunity identification heavily relies on developers' expertise and intuition. In this paper, we investigate the effectiveness of machine learning algorithms in predicting software refactorings. More specifically, we train six different machine learning algorithms (i.e., Logistic Regression, Naive Bayes, Support Vector Machine, Decision Trees, Random Forest, and Neural Network) with a dataset comprising over two million refactorings from 11,149 real-world projects from the Apache, F-Droid, and GitHub ecosystems. The resulting models predict 20 different refactorings at class, method, and variable-levels with an accuracy often higher than 90%. Our results show that (i) Random Forests are the best models for predicting software refactoring, (ii) process and ownership metrics seem to play a crucial role in the creation of better models, and (iii) models generalize well in different contexts.",software engineering;software refactoring;machine learning for software engineering,"1, 1",,IEEE Early Access Articles,IEEE Transactions on Software Engineering,IEEE
430,,Comparing Offline and Online Testing of Deep Neural Networks: An Autonomous Car Case Study,F. U. Haq; D. Shin; S. Nejati; L. C. Briand,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9159088,10.1109/ICST46399.2020.00019,"There is a growing body of research on developing testing techniques for Deep Neural Networks (DNNs). We distinguish two general modes of testing for DNNs: Offline testing where DNNs are tested as individual units based on test datasets obtained independently from the DNNs under test, and online testing where DNNs are embedded into a specific application and tested in a close-loop mode in interaction with the application environment. In addition, we identify two sources for generating test datasets for DNNs: Datasets obtained from real-life and datasets generated by simulators. While offline testing can be used with datasets obtained from either sources, online testing is largely confined to using simulators since online testing within real-life applications can be time consuming, expensive and dangerous. In this paper, we study the following two important questions aiming to compare test datasets and testing modes for DNNs: First, can we use simulator-generated data as a reliable substitute to real-world data for the purpose of DNN testing? Second, how do online and offline testing results differ and complement each other? Though these questions are generally relevant to all autonomous systems, we study them in the context of automated driving systems where, as study subjects, we use DNNs automating end-to-end control of cars' steering actuators. Our results show that simulator-generated datasets are able to yield DNN prediction errors that are similar to those obtained by testing DNNs with real-life datasets. Further, offline testing is more optimistic than online testing as many safety violations identified by online testing could not be identified by offline testing, while large prediction errors generated by offline testing always led to severe safety violations detectable by online testing.",DNN;ADS;testing;simulation,"85, 95",,IEEE Conferences,"2020 IEEE 13th International Conference on Software Testing, Validation and Verification (ICST)",IEEE
431,,An Empirical Evaluation of Mutation Operators for Deep Learning Systems,G. Jahangirova; P. Tonella,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9159089,10.1109/ICST46399.2020.00018,"Deep Learning (DL) is increasingly adopted to solve complex tasks such as image recognition or autonomous driving. Companies are considering the inclusion of DL components in production systems, but one of their main concerns is how to assess the quality of such systems. Mutation testing is a technique to inject artificial faults into a system, under the assumption that the capability to expose (kilt) such artificial faults translates into the capability to expose also real faults. Researchers have proposed approaches and tools (e.g., Deep-Mutation and MuNN) that make mutation testing applicable to deep learning systems. However, existing definitions of mutation killing, based on accuracy drop, do not take into account the stochastic nature of the training process (accuracy may drop even when re-training the un-mutated system). Moreover, the same mutation operator might be effective or might be trivial/impossible to kill, depending on its hyper-parameter configuration. We conducted an empirical evaluation of existing operators, showing that mutation killing requires a stochastic definition and identifying the subset of effective mutation operators together with the associated most effective configurations.",testing;mutation;deep learning,"74, 84",,IEEE Conferences,"2020 IEEE 13th International Conference on Software Testing, Validation and Verification (ICST)",IEEE
432,,Poster: Performance Testing Driven by Reinforcement Learning,M. H. Moghadam; M. Saadatmand; M. Borg; M. Bohlin; B. Lisper,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9159096,10.1109/ICST46399.2020.00048,"Performance testing remains a challenge, particularly for complex systems. Different application-, platform- and workload-based factors can influence the performance of software under test. Common approaches for generating platform- and workload-based test conditions are often based on system model or source code analysis, real usage modeling and use-case based design techniques. Nonetheless, creating a detailed performance model is often difficult, and also those artifacts might not be always available during the testing. On the other hand, test automation solutions such as automated test case generation can enable effort and cost reduction with the potential to improve the intended test criteria coverage. Furthermore, if the optimal way (policy) to generate test cases can be learnt by testing system, then the learnt policy can be reused in further testing situations such as testing variants, evolved versions of software, and different testing scenarios. This capability can lead to additional cost and computation time saving in the testing process. In this research, we present an autonomous performance testing framework which uses a model-free reinforcement learning augmented by fuzzy logic and self-adaptive strategies. It is able to learn the optimal policy to generate platform- and workload-based test conditions which result in meeting the intended testing objective without access to system model and source code. The use of fuzzy logic and self-adaptive strategy helps to tackle the issue of uncertainty and improve the accuracy and adaptivity of the proposed learning. Our evaluation experiments show that the proposed autonomous performance testing framework is able to generate the test conditions efficiently and in a way adaptive to varying testing situations.",performance testing;stress testing;load testing;machine learning;reinforcement learning,"402, 405",,IEEE Conferences,"2020 IEEE 13th International Conference on Software Testing, Validation and Verification (ICST)",IEEE
433,,Towards a Deep Learning Model for Vulnerability Detection on Web Application Variants,A. Fidalgo; I. Medeiros; P. Antunes; N. Neves,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9155596,10.1109/ICSTW50294.2020.00083,"Reported vulnerabilities have grown significantly over the recent years, with SQL injection (SQLi) being one of the most prominent, especially in web applications. For these, such increase can be explained by the integration of multiple software parts (e.g., various plugins and modules), often developed by different organizations, composing thus web application variants. Machine Learning has the potential to be a great ally on finding vulnerabilities, aiding experts by reducing the search space or even by classifying programs on their own. However, previous work usually does not consider SQLi or utilizes techniques hard to scale. Moreover, there is a clear gap in vulnerability detection with machine learning for PHP, the most popular server-side language for web applications. This paper presents a Deep Learning model able to classify PHP slices as vulnerable (or not) to SQLi. As slices can belong to any variant, we propose the use of an intermediate language to represent the slices and interpret them as text, resorting to well-studied Natural Language Processing (NLP) techniques. Preliminary results of the use of the model show that it can discover SQLi, helping programmers and precluding attacks that would eventually cost a lot to repair.",web application vulnerabilities;vulnerability detection;natural language processing;deep learning;software security,"465, 476",,IEEE Conferences,"2020 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)",IEEE
434,,Plant Leaf Diseases Detection and Classification Using Image Processing and Deep Learning Techniques,M. A. Jasim; J. M. AL-Tuwaijari,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9142097,10.1109/CSASE48920.2020.9142097,"Agricultural products are the primary need for every country. If plants are infected by diseases, this impacts the country's agricultural production and its economic resources. This paper presents a system that is used to classify and detect plant leaf diseases using deep learning techniques. The used images were obtained from (Plant Village dataset) website. In our work, we have taken specific types of plants; include tomatoes, pepper, and potatoes, as they are the most common types of plants in the world and in Iraq in particular. This Data Set contains 20636 images of plants and their diseases. In our proposed system, we used the convolutional neural network (CNN), through which plant leaf diseases are classified, 15 classes were classified, including 12 classes for diseases of different plants that were detected, such as bacteria, fungi, etc., and 3 classes for healthy leaves. As a result, we obtained excellent accuracy in training and testing, we have got an accuracy of (98.29%) for training, and (98.029%) for testing for all data set that were used.",Plant leaf disease;Deep Learning;CNN algorithm,"259, 265",,IEEE Conferences,2020 International Conference on Computer Science and Software Engineering (CSASE),IEEE
435,,Attention-Aware Convolutional Neural Network for Age-Related Macular Degeneration Classification,S. Li; Z. Quan,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139104,10.1109/ICCSN49894.2020.9139104,"Though age-related macular degeneration (AMD) poses an important personal and public health burden, studies on AMD is hampered by different approaches to classify AMD. In this paper, we propose convolutional neural networks (CNN) based models for fundus retinal images that classify four types of AMD automatically. We use deep residual network (ResNet50) to extract high-dimensional features and be trained end-to-end to classify AMD. Furthermore, we apply attention mechanism to deep residual network (Atten-ResNet) which enables to further select features adaptively. Experimental results show that comparing to HOG-SVM and Visual Geometry Group (VGG), the ResNet50 based method could achieve 17.2% and 12.1% overall classification accuracy improvement. The Atten-ResNet based method has more 0.4% accuracy improvement than ResNet50 based method.",ResNet;Attention Mechanism;OCT;AMD;Classification,"264, 269",,IEEE Conferences,2020 12th International Conference on Communication Software and Networks (ICCSN),IEEE
436,,Chinese Explanatory Opinion Relationship Recognition Based on Improved Target Attention Mechanism,X. Cao; C. Zhu; C. Lv,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9131389,10.1109/AEMCSE50948.2020.00126,"Opinion relationship recognition is an important part of the opinion mining task. Its main purpose is to extract the opinion element tuple from the user comment data and identify the relationship between them, such as evaluation object, evaluation content, opinion explanation, opinion object. Because the comments of the network having are characterized by randomness, diversity of opinions and different formats, it will become more difficult for the opinion mining task. If we can extract the interrelationships between the various explanatory opinion elements, it not only makes subsequent tasks easier but also applies its extracted results to other related tasks. For example, applying the opinion seven-tuple from the opinion extraction task to the text summary generation task can greatly improve the effectiveness of the text summary generation task. In this paper, we have improved on the traditional LSTM-Attention model and proposed an opinion relationship recognition framework based on improved Target Attention Mechanism. Also, we conducted experiments in two different domains, and the experimental results show that the performance has been effectively improved in two domains. We also explored two different pre-training strategies, Word2vec and Elmo, to further analyze the impact of pre-training on this experiment.",opinion mining;opinion relationship recognition;target attention mechanism,"578, 583",,IEEE Conferences,"2020 3rd International Conference on Advanced Electronic Materials, Computers and Software Engineering (AEMCSE)",IEEE
437,,Tumor Classification using MS Spectra Based on Deep Learning,H. Dong; K. Shu,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9131250,10.1109/AEMCSE50948.2020.00106,"Deep learning models plays a significant role in bioinformatics research, such as prediction of incidence, classification of disease samples, identification and detection of tumor areas. Mass spectrometry (MS) has been widely applied to protein research due to its high throughput and sensitivity. Tumor protein mass spectrometry data has high sample dimensions and low signal-to-noise ratio, which is difficult to extract features for classification. Here, we aim to develop a new method to extract features from tumor protein mass spectrometry data and classify proteomics data using deep learning models. Our results demonstrated that the deep learning models we proposed has a good performance and may provide ideas for researchers to classify other protein mass spectral data or similar data.",deep learning;mass spectrometry;protein;feature;classification,"464, 467",,IEEE Conferences,"2020 3rd International Conference on Advanced Electronic Materials, Computers and Software Engineering (AEMCSE)",IEEE
438,,A Protein Identification Algorithm Optimization for Mass Spectrometry Data using Deep Learning,R. Xu; M. Bai; K. Shu; Y. Liang; Y. Zhu; C. Chang,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9131377,10.1109/AEMCSE50948.2020.00110,"Protein sequence database search is one of the most commonly used methods for protein identification in shotgun proteomics. In tradition, searching a protein sequence database is usually required to construct the theoretical spectrum for each peptide at first, which only considers the information of mass-to-charge ratio at present. However, the information related to isotope peak intensity is neglected. Thanks to the rapid development of artificial intelligence technique in recent years, deep learning-based MS/MS spectrum prediction tools have showed a high accuracy and great potentials to improve the sensitivity and accuracy of protein sequence database searching. In this study, we used a deep learning model (pDeep2) to predict the theoretical mass spectrum of all peptides and applied it to a database searching tool (DeepNovo), thus improving the sensitivity and accuracy of peptide identification.",protein identification;peptide identification;spectrum prediction;deep learning,"482, 486",,IEEE Conferences,"2020 3rd International Conference on Advanced Electronic Materials, Computers and Software Engineering (AEMCSE)",IEEE
439,,Facial Ethnicity Recognition Based on Transfer Learning from Deep Convolutional Networks,S. Gao; C. Zeng; M. Bai; K. Shu,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9131205,10.1109/AEMCSE50948.2020.00073,"With the development of deep learning, computer face recognition has made significant progress. However, face ethnic characteristics information is rarely used in face recognition technology. The research of facial ethnicity recognition had not only been directly applied in daily life, but also avoided racial effects and improved model generalization performance. In the paper, we proposed a Chinese facial ethnicity recognition (CFER) model based on transfer learning from deep convolution networks. First, we collected 5 Chinese ethnic groups to build a face dataset containing ethnicity information; then we have applied CFER to recognize Chinese ethnicity characteristics and 10-fold cross validation method to estimate mainly the accuracy rate of the model. The average recognition rate of the model is 80.5%, meanwhile, the model also has good generalization performance. It's proved that deep learning method is feasible for facial ethnicity recognition.",convolutional networks;transfer learning;Facial ethnicity recognition,"310, 314",,IEEE Conferences,"2020 3rd International Conference on Advanced Electronic Materials, Computers and Software Engineering (AEMCSE)",IEEE
440,,A Multi-Neural Network Fusion Based Method for Financial Event Subject Extraction,Z. Wang; Z. Liu; L. Luo; X. Chen,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9131176,10.1109/AEMCSE50948.2020.00084,"Event extraction is a fundamental task in the domain of public opinion monitoring and financial risk control. Subject extraction of events with specific types is the kernel of event extraction. At present, there are some problems still existing in the mainstream event subject extraction methods, such as the inadequate use of semantic relationship between Chinese characters and the weak ability of feature learning. In order to solve these problems, this paper introduces the BERT (Bidirectional Encoder Representations from Transformers) pre-training model to enhance the semantic representation of characters, then proposes a novel event subject extraction method combing convolutional neural network (CNN) and long short-term memory (LSTM) to improve the ability of feature learning in the model. Experimental results show that the F1 score of the method proposed in this paper can reach 86.99%, which greatly improves the identification accuracy of the event subject in the financial domain.",event subject extraction;multi-neural network fusion;BERT,"360, 365",,IEEE Conferences,"2020 3rd International Conference on Advanced Electronic Materials, Computers and Software Engineering (AEMCSE)",IEEE
441,,A Review on Sentiment Analysis Model for Chinese Weibo Text,D. Wang; R. Alfred,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9131173,10.1109/AEMCSE50948.2020.00105,"the technology of sentiment analysis about Chinese Weibo text is a complex and systematic model. In general situation, it includes 3 parts: data washing, word segmentation and feature extraction. Weibo text is an unstructured text and there are many non-standard contents in it. Therefore, it should be thoroughly data washing before feature extraction. Due to emoticon in Weibo text are very useful in sentiment analysis, thus, in data washing, all of Non-Chinese, with ""@"",""#"" character should be removed except emoticon. In word segmentation, related algorithms can be divided into three categories: based on string matching, based on understand and based on statistics[1]. In feature extraction, the Lexicon-based Model, Machine learning Model and deep learning Model usually was used. Through literature search, the paper found that grammar characteristic in Chinese Weibo text was fully considered and solved by program of Lexicon-based Model, sentiment word, for example, adverb of degree, no word and all kinds of Chinese sentence patterns. But, due to characteristic of poor generalization, the performance of Lexicon-based Model in experiment is not good. Therefore, performance the model should be continued to improve. For traditional machine learning, there are 2 mainly aspects of innovation: Simultaneous classifier (Adoboost+SVM) and Improvement of classical classification algorithm. One worth noted is that the performance of the some improve classifier (SVM, P Naïve Bayes) has not been verified in Chinese Weibo classification. For deep learning, now, the innovation point is mainly focus on Convolution layer and input attention mechanism. For the next step, YuanHejin think should input ensemble learning and attention mechanism should be improve. LuXin argue that the recognition performance about irony sentence with context in Weibo needs to improve. GaoWeiju think that individual sentiment space for each user in EMCNN model should be build.",review;sentiment analysis;Chinese Weibo,"456, 463",,IEEE Conferences,"2020 3rd International Conference on Advanced Electronic Materials, Computers and Software Engineering (AEMCSE)",IEEE
442,,Architectural Patterns for Cross-Domain Personalised Automotive Functions,S. Kugele; C. Segler; T. Hubregtsen,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9101332,10.1109/ICSA47634.2020.00026,"Context: Future automotive customer functions will be highly personalisable and adapt their settings proactively in an intelligent way. Aim: We aim at designing generic architectural patterns for functional architectures containing machine learning components. Method: We first formalise a new architectural model. Based on this model, we present and discuss three alternative architectural patterns: (1) concurrent learning, (2) end-to-end learning, and (3) user shadow learning. For these patterns, three alternative integration approaches are discussed: (i) centralised holistic approach, (ii) domain-specific approach, and (iii) dedicated approach. Moreover, we conduct an evaluation using real car data for different customer functions. Conclusion: We propose the use of the user shadow learning pattern in the dynamic architectural model. The user shadow learning pattern is not affected by safety constraints, as is usually the case for integrating artificial intelligence, as it only models user behaviour while leaving the original function intact. To integrate the multitude of models, we propose a domain-specific approach. This approach provides a balance between the trade-offs in the dedicated approach and the holistic approach, being high computational overhead and design complexity, respectively.",architectural patterns;automotive;personalisation;intelligent functions;artificial intelligence,"191, 201",,IEEE Conferences,2020 IEEE International Conference on Software Architecture (ICSA),IEEE
443,,A Semantic-Based Framework for Analyzing App Users' Feedback,A. Yadav; R. Sharma; F. H. Fard,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9054843,10.1109/SANER48275.2020.9054843,"The competitive market of mobile apps requires app developers to consider the users' feedback frequently. This feedback, when comes from different resources, e.g. App Stores and Twitter, will provide a broader picture of the state of the app, as the users discuss different topics on each platform. Automated tools are developed to filter the informative comments for app developers. However, to integrate the feedbacks from different platforms, one should evaluate the similarities and/or differences of the text from each one. Different meaning of the words in various context, makes this evaluation a challenging task for automated processes. For example, Request night theme and Add dark mode are two comments that are requesting the same feature. This similarity cannot be identified automatically if the semantics of the words are not embedded in the analysis. In this paper, we propose a new framework to analyze the users' feedback by embedding their semantics. As a case study, we investigate whether our approach can identify the similar/different comments from Google Play Store and Twitter, in the two well studied classes of bug reports and feature requests from literature. The initial results, validated by expert evaluation and statistical analysis, shows that this framework can automatically measure the semantic differences among users' comments in both groups. The framework can be used to build intelligent tools to integrate the users' feedback from other platforms, as well as providing ways to analyze the reviews in more detail automatically.",app review analysis;user feedback analysis;reviews vs tweets;semantic analysis,"572, 576",,IEEE Conferences,"2020 IEEE 27th International Conference on Software Analysis, Evolution and Reengineering (SANER)",IEEE
444,,SpojitR: Intelligently Link Development Artifacts,M. Rath; M. T. Tomova; P. Mäder,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9054839,10.1109/SANER48275.2020.9054839,"Traceability has been acknowledged as an important part of the software development process and is considered relevant when performing tasks such as change impact and coverage analysis. With the growing popularity of issue tracking systems and version control systems developers began including the unique identifiers of issues to commit messages. The goal of this message tagging is to trace related artifacts and eventually establish project-wide traceability. However, the trace creation process is still performed manually and not free of errors, i. e. developers may forget to tag their commit with an issue id. The prototype spojitR is designed to assist developers in tagging commit messages and thus (semi-) automatically creating trace links between commits and an issue they are working on. When no tag is present in a commit message, spojitR offers the developer a short recommendation list of potential issue ids to tag the commit message. We evaluated our tool using an open-source project hosted by the Apache Software Foundation. The source code, a demonstration, and a video about spojitR is available online: https://github.com/SECSY-Group/spojitr.",software development;traceability;version control systems;issue tracking systems;workflow systems,"652, 656",,IEEE Conferences,"2020 IEEE 27th International Conference on Software Analysis, Evolution and Reengineering (SANER)",IEEE
445,,Automatically Learning Patterns for Self-Admitted Technical Debt Removal,F. Zampetti; A. Serebrenik; M. Di Penta,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9054868,10.1109/SANER48275.2020.9054868,"Technical Debt (TD) expresses the need for improvements in a software system, e.g., to its source code or architecture. In certain circumstances, developers “self-admit” technical debt (SATD) in their source code comments. Previous studies investigate when SATD is admitted, and what changes developers perform to remove it. Building on these studies, we present a first step towards the automated recommendation of SATD removal strategies. By leveraging a curated dataset of SATD removal patterns, we build a multi-level classifier capable of recommending six SATD removal strategies, e.g., changing API calls, conditionals, method signatures, exception handling, return statements, or telling that a more complex change is needed. SARDELE (SAtd Removal using DEep LEarning) combines a convolutional neural network trained on embeddings extracted from the SATD comments with a recurrent neural network trained on embeddings extracted from the SATD-affected source code. Our evaluation reveals that SARDELE is able to predict the type of change to be applied with an average precision of ~55%, recall of ~ 57%, and AUC of 0.73, reaching up to 73% precision, 63% recall, and 0.74 AUC for certain categories such as changes to method calls. Overall, results suggest that SATD removal follows recurrent patterns and indicate the feasibility of supporting developers in this task with automated recommenders.",Self-Admitted Technical Debt;Recommender Systems;Deep Learning;Neural Networks,"355, 366",,IEEE Conferences,"2020 IEEE 27th International Conference on Software Analysis, Evolution and Reengineering (SANER)",IEEE
446,,We Are Family: Analyzing Communication in GitHub Software Repositories and Their Forks,S. Brisson; E. Noei; K. Lyons,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9054834,10.1109/SANER48275.2020.9054834,"GitHub facilitates software development practices that encourage collaboration and communication. Part of GitHub's model includes forking, which enables users to make changes on a copy of the base repository. The process of forking opens avenues of communication between the users from the base repository and the users from the forked repositories. Since forking on GitHub is a common mechanism for initiating repositories, we are interested in how communication between a repository and its forks (forming a software family) relates to stars. In this paper, we study communications within 385 software families comprised of 13,431 software repositories. We find that the fork depth, the number of users who have contributed to multiple repositories in the same family, the number of followers from outside the family, familial pull requests, and reported issues share a statistically significant relationship with repository stars. Due to the importance of issues and pull requests, we identify and compare common topics in issues and pull requests from inside the repository (via branching) and within the family (via forking). Our results offer insights into the importance of communication within a software family, and how this leads to higher individual repository star counts.",Empirical study;Open source software;Data mining;Software maintenance;Software family,"59, 69",,IEEE Conferences,"2020 IEEE 27th International Conference on Software Analysis, Evolution and Reengineering (SANER)",IEEE
447,,Leveraging Machine Learning for Software Redocumentation,V. Geist; M. Moser; J. Pichler; S. Beyer; M. Pinzger,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9054838,10.1109/SANER48275.2020.9054838,"Source code comments contain key information about the underlying software system. Many redocumentation approaches, however, cannot exploit this valuable source of information. This is mainly due to the fact that not all comments have the same goals and target audience and can therefore only be used selectively for redocumentation. Performing a required classification manually, e.g. in the form of heuristic rules, is usually time-consuming and error-prone and strongly dependent on programming languages and guidelines of concrete software systems. By leveraging machine learning, it should be possible to classify comments and thus transfer valuable information from the source code into documentation with less effort but the same quality. We applied different machine learning techniques to a COBOL legacy system and compared the results with industry-strength heuristic classification. As a result, we found that machine learning outperforms the heuristics in number of errors and less effort.",software redocumentation;legacy system;comment classification pipeline;heuristic rules;machine learning;NLP;CNNs,"622, 626",,IEEE Conferences,"2020 IEEE 27th International Conference on Software Analysis, Evolution and Reengineering (SANER)",IEEE
448,,Cross-Dataset Design Discussion Mining,A. Mahadi; K. Tongay; N. A. Ernst,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9054792,10.1109/SANER48275.2020.9054792,"Being able to identify software discussions that are primarily about design—which we call design mining—can improve documentation and maintenance of software systems. Existing design mining approaches have good classification performance using natural language processing (NLP) techniques, but the conclusion stability of these approaches is generally poor. A classifier trained on a given dataset of software projects has so far not worked well on different artifacts or different datasets. In this study, we replicate and synthesize these earlier results in a meta—analysis. We then apply recent work in transfer learning for NLP to the problem of design mining. However, for our datasets, these deep transfer learning classifiers perform no better than less complex classifiers. We conclude by discussing some reasons behind the transfer learning approach to design mining.",replication;mining software design;empirical software engineering;transfer learning,"149, 160",,IEEE Conferences,"2020 IEEE 27th International Conference on Software Analysis, Evolution and Reengineering (SANER)",IEEE
449,,Req2Lib: A Semantic Neural Model for Software Library Recommendation,Z. Sun; Y. Liu; Z. Cheng; C. Yang; P. Che,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9054865,10.1109/SANER48275.2020.9054865,"Third-party libraries are crucial to the development of software projects. To get suitable libraries, developers need to search through millions of libraries by filtering, evaluating, and comparing. The vast number of libraries places a barrier for programmers to locate appropriate ones. To help developers, researchers have proposed automated approaches to recommend libraries based on library usage pattern. However, these prior studies can not sufficiently match user requirements and suffer from cold-start problem. In this work, we would like to make recommendations based on requirement descriptions to avoid these problems. To this end, we propose a novel neural approach called Req2Lib which recommends libraries given descriptions of the project requirement. We use a Sequence-to-Sequence model to learn the library linked-usage information and semantic information of requirement descriptions in natural language. Besides, we apply a domain-specific pre-trained word2vec model for word embedding, which is trained over textual corpus from Stack Overflow posts. In the experiment, we train and evaluate the model with data from 5,625 java projects. Our preliminary evaluation demonstrates that Req2Lib can recommend libraries accurately.",Library Recommendation;Deep Learning;GitHub,"542, 546",,IEEE Conferences,"2020 IEEE 27th International Conference on Software Analysis, Evolution and Reengineering (SANER)",IEEE
450,,CORE: Automating Review Recommendation for Code Changes,J. K. Siow; C. Gao; L. Fan; S. Chen; Y. Liu,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9054794,10.1109/SANER48275.2020.9054794,"Code review is a common process that is used by developers, in which a reviewer provides useful comments or points out defects in the submitted source code changes via pull request. Code review has been widely used for both industry and open-source projects due to its capacity in early defect identification, project maintenance, and code improvement. With rapid updates on project developments, code review becomes a non-trivial and labor-intensive task for reviewers. Thus, an automated code review engine can be beneficial and useful for project development in practice. Although there exist prior studies on automating the code review process by adopting static analysis tools or deep learning techniques, they often require external sources such as partial or full source code for accurate review suggestion. In this paper, we aim at automating the code review process only based on code changes and the corresponding reviews but with better performance. The hinge of accurate code review suggestion is to learn good representations for both code changes and reviews. To achieve this with limited source, we design a multi-level embedding (i.e., word embedding and character embedding) approachto represent the semantics provided by code changes and reviews. The embeddings are then well trained through a proposed attentional deep learning model, as a whole named CORE. We evaluate the effectiveness of CORE on code changes and reviews collected from 19 popular Java projects hosted on Github. Experimental results show that our model CORE can achieve significantly better performance than the state-of-the-art model (DeepMem), with an increase of 131.03% in terms of Recall@10 and 150.69% in terms of Mean Reciprocal Rank. Qualitative general word analysis among project developers also demonstrates the performance of CORE in automating code review.",,"284, 295",,IEEE Conferences,"2020 IEEE 27th International Conference on Software Analysis, Evolution and Reengineering (SANER)",IEEE
451,,Automating Intention Mining,Q. Huang; X. Xia; D. Lo; G. C. Murphy,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8493285,10.1109/TSE.2018.2876340,"Developers frequently discuss aspects of the systems they are developing online. The comments they post to discussions form a rich information source about the system. Intention mining, a process introduced by Di Sorbo et al., classifies sentences in developer discussions to enable further analysis. As one example of use, intention mining has been used to help build various recommenders for software developers. The technique introduced by Di Sorbo et al. to categorize sentences is based on linguistic patterns derived from two projects. The limited number of data sources used in this earlier work introduces questions about the comprehensiveness of intention categories and whether the linguistic patterns used to identify the categories are generalizable to developer discussion recorded in other kinds of software artifacts (e.g., issue reports). To assess the comprehensiveness of the previously identified intention categories and the generalizability of the linguistic patterns for category identification, we manually created a new dataset, categorizing 5,408 sentences from issue reports of four projects in GitHub. Based on this manual effort, we refined the previous categories. We assess Di Sorbo et al.'s patterns on this dataset, finding that the accuracy rate achieved is low (0.31). To address the deficiencies of Di Sorbo et al.'s patterns, we propose and investigate a convolution neural network (CNN)-based approach to automatically classify sentences into different categories of intentions. Our approach optimizes CNN by integrating batch normalization to accelerate the training speed, and an automatic hyperparameter tuning approach to tune appropriate hyperparameters of CNN. Our approach achieves an accuracy of 0.84 on the new dataset, improving Di Sorbo et al.'s approach by 171 percent. We also apply our approach to improve an automated software engineering task, in which we use our proposed approach to rectify misclassified issue reports, thus reducing the bias introduced by such data to other studies. A case study on four open source projects with 2,076 issue reports shows that our approach achieves an average AUC score of 0.687, which improves other baselines by at least 16 percent.",Deep Learning;Intention;Issue Report;Empirical Study,"1098, 1119",,IEEE Journals,IEEE Transactions on Software Engineering,IEEE
452,,Metamorphic Relations for Enhancing System Understanding and Use,Z. Q. Zhou; L. Sun; T. Y. Chen; D. Towey,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8493260,10.1109/TSE.2018.2876433,"Modern information technology paradigms, such as online services and off-the-shelf products, often involve a wide variety of users with different or even conflicting objectives. Every software output may satisfy some users, but may also fail to satisfy others. Furthermore, users often do not know the internal working mechanisms of the systems. This situation is quite different from bespoke software, where developers and users typically know each other. This paper proposes an approach to help users to better understand the software that they use, and thereby more easily achieve their objectives-even when they do not fully understand how the system is implemented. Our approach borrows the concept of metamorphic relations from the field of metamorphic testing (MT), using it in an innovative way that extends beyond MT. We also propose a “symmetry” metamorphic relation pattern and a “change direction” metamorphic relation input pattern that can be used to derive multiple concrete metamorphic relations. Empirical studies reveal previously unknown failures in some of the most popular applications in the world, and show how our approach can help users to better understand and better use the systems. The empirical results provide strong evidence of the simplicity, applicability, and effectiveness of our methodology.",Metamorphic exploration;symmetry;metamorphic testing;metamorphic relation;metamorphic relation pattern;metamorphic relation input pattern;change direction;oracle problem;user experience;user countermeasure;software validation,"1120, 1154",,IEEE Journals,IEEE Transactions on Software Engineering,IEEE
453,,Finding Faster Configurations Using <sc>FLASH</sc>,V. Nair; Z. Yu; T. Menzies; N. Siegmund; S. Apel,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8469102,10.1109/TSE.2018.2870895,"Finding good configurations of a software system is often challenging since the number of configuration options can be large. Software engineers often make poor choices about configuration or, even worse, they usually use a sub-optimal configuration in production, which leads to inadequate performance. To assist engineers in finding the better configuration, this article introduces Flash, a sequential model-based method that sequentially explores the configuration space by reflecting on the configurations evaluated so far to determine the next best configuration to explore. Flash scales up to software systems that defeat the prior state-of-the-art model-based methods in this area. Flash runs much faster than existing methods and can solve both single-objective and multi-objective optimization problems. The central insight of this article is to use the prior knowledge of the configuration space (gained from prior runs) to choose the next promising configuration. This strategy reduces the effort (i.e., number of measurements) required to find the better configuration. We evaluate Flash using 30 scenarios based on 7 software systems to demonstrate that Flash saves effort in 100 and 80 percent of cases in single-objective and multi-objective problems respectively by up to several orders of magnitude compared to state-of-the-art techniques.",Performance prediction;search-based SE;configuration;multi-objective optimization;sequential model-based methods,"794, 811",,IEEE Journals,IEEE Transactions on Software Engineering,IEEE
454,,Machine Learning-Based Prototyping of Graphical User Interfaces for Mobile Apps,K. Moran; C. Bernal-Cárdenas; M. Curcio; R. Bonett; D. Poshyvanyk,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8374985,10.1109/TSE.2018.2844788,"It is common practice for developers of user-facing software to transform a mock-up of a graphical user interface (GUI) into code. This process takes place both at an application's inception and in an evolutionary context as GUI changes keep pace with evolving features. Unfortunately, this practice is challenging and time-consuming. In this paper, we present an approach that automates this process by enabling accurate prototyping of GUIs via three tasks: detection, classification, and assembly. First, logical components of a GUI are detected from a mock-up artifact using either computer vision techniques or mock-up metadata. Then, software repository mining, automated dynamic analysis, and deep convolutional neural networks are utilized to accurately classify GUI-components into domain-specific types (e.g., toggle-button). Finally, a data-driven, K-nearest-neighbors algorithm generates a suitable hierarchical GUI structure from which a prototype application can be automatically assembled. We implemented this approach for Android in a system called ReDraw. Our evaluation illustrates that ReDraw achieves an average GUI-component classification accuracy of 91 percent and assembles prototype applications that closely mirror target mock-ups in terms of visual affinity while exhibiting reasonable code structure. Interviews with industrial practitioners illustrate ReDraw's potential to improve real development workflows.",GUI;CNN;mobile;prototyping;machine-learning;mining software repositories,"196, 221",,IEEE Journals,IEEE Transactions on Software Engineering,IEEE
455,,Securing PKES: Integrating Artificial Intelligence Into PKES Forcefield For Anomaly Detection,S. Rizvi; J. Imler; L. Ritchey; M. Tokar,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9307677,10.1109/ICSSA48308.2019.00008,"Passive Keyless Entry and Start System (PKES) is a convenient feature found in newer cars that allows the user to enter their car without having to press a button on a key fob. Users can simply walk up to their vehicle and once they are in close enough proximity, it unlocks and the user is granted access. However, the convenience of PKES comes at a price; vulnerability to relay attacks and amplified relay attacks. Our proposed method, PKES Forcefield, would protect the user from these attacks without sacrificing any convenience. PKES Forcefield utilizes coordinate tracing and multifactor authentication to secure the user. To add more security to PKES Forcefield, we propose AI integration to construct a profile for each user of the vehicle. This profile will store data on the habits of the user (location, walking speed, time, etc.). Once each user has a profile built from their input data, our system will be able to detect any behaviors that differ from the user's profile (anomalies) and then proper security measures will be taken depending on if the anomaly is considered to be a Low Risk Anomaly or a High Risk Anomaly.",Passive Keyless Entry and Start System;artificial intelligence;CAR security;PKES Forcefield;anomaly detection,"10, 15",,IEEE Conferences,2019 International Conference on Software Security and Assurance (ICSSA),IEEE
456,,Aspect Category Detection on Indonesian E-commerce Mobile Application Review,D. F. Nasiri; I. Budi,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9092619,10.1109/ICoDSE48700.2019.9092619,"E-commerce platform has a great influence on the growth of digital economy in Indonesia. This promising sector creates a fierce competition between e-commerce platforms. User reviews can be utilized to discover useful information for both developers and users. Developers use them for enhancing their application, while users take them as a consideration for using the application. We perform aspect category detection to retrieve the important aspects in reviews. We gather and analyze any aspect categories related to e-commerce and find that new potential aspects can be obtained from mobile application reviews, such as promos and payment. For identifying the aspects, we employ two different approaches, one-vs-all and single model, for 3748 annotated reviews. In one-vs-all we compare Naive Bayes, SVM, and the effect on using class-weights in SVM, since the distribution in the dataset is imbalanced. Meanwhile, we implement neural networks architecture for single model. We compare CNN to GRU+CNN architecture. GRU+CNN successfully achieves overall best result on both example-based and label-based performance metrics for multi-label task in our dataset. In example-based we use Hamming score to calculate the accuracy where GRU+CNN gets 71%, and it obtains 78% samples average F1-score for labelbased metric.",e-commerce;application review;aspect detection;multi-label;Indonesian review,"1, 6",,IEEE Conferences,2019 International Conference on Data and Software Engineering (ICoDSE),IEEE
457,,Rule-based Approach for English-Indonesian Code-switching Acoustic Model,R. Hartanto; D. P. Lestari,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9092620,10.1109/ICoDSE48700.2019.9092620,"The usage of English in daily conversation and many aspects in everyday life is increasing in Indonesia. It affects the usage of Indonesian as the main language. People often communicate using both English and Indonesian (code-switching). English-Indonesian code-switching causes degradation of the performance of Indonesian automatic speech recognition system. Other than out of vocabulary of foreign words, the system fails to recognize the foreign phonemes and predicts them as the most similar Indonesian phonemes. An investigation is conducted to improve the Indonesian automatic speech recognition system. To support the investigation, an English-Indonesian code-switching corpus is built. Rule-based approaches for English-Indonesian code-switching acoustic model are developed and experimented. Those approaches are English to Indonesian phone mapping and International Phonetic Alphabets based phone merging. The phoneme mapping approach succeeds to decrease the word error rate from the baseline system. This method outperformed the phone merging approach.",code-switching;automatic speech recognition system;acoustic model,"1, 5",,IEEE Conferences,2019 International Conference on Data and Software Engineering (ICoDSE),IEEE
458,,Sequential Multi-Kernel Convolutional Recurrent Network for Sentiment Classification,A. Oluwasanmi; S. Akeem; J. Jehoaida; M. U. Aftab; N. Hundera; B. Kumeda; E. Baagere; Z. Qin,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9040746,10.1109/ICSESS47205.2019.9040746,"The emergence of deep learning as a commanding technique for learning heterogeneous layers of feature representations have consequently substituted traditional machine learning algorithms which are generally poor in analyzing compound sentences. Additionally, convolutional and recurrent neural networks have auspiciously yielded state-of-the-art results in sentiment classification and Natural Language Processing (NLP). In this paper, a deep sentiment representation model through the combination of multiple Convolutional Neural Networks (CNN) kernels with Long Short-Term Memory (LSTM) is proposed for sentiment classification. Our model gains word vector representation using pre-trained Global Vectors for Word Representation (GloVe) embeddings, thereafter used as input to the CNN layer which extracts higher local text representations. Finally, Bidirectional LSTM (biLSTM) generates sentiment classification of sentence representation based on context dependent features. Our combined approach of CNN and biLSTM was experimented using the Stanford Large Movie Review Dataset (IMDB) and Stanford Sentiment Treebank Dataset (SSTB) for binary classification. The evaluation achieves outstanding results in outperforming several existing approaches with 90.4% accuracy on the Stanford Sentiment Treebank dataset and 94.8% accuracy on the Stanford Large Movie Review dataset. These results are achieved with a drastic reduction of model parameters and without a pooling layer in the CNN architecture, helping to retain local and structural information in comparison to other existing deep neural network frameworks.",component;sentiment analysis;natral language processing;convolutional neural network;recurrent neural network: word embeddings.,"1, 5",,IEEE Conferences,2019 IEEE 10th International Conference on Software Engineering and Service Science (ICSESS),IEEE
459,,Semantic Segmentation of Intracranial Hemorrhages in Head CT Scans,Y. Qiu; C. S. Chang; J. L. Yan; L. Ko; T. S. Chang,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9040733,10.1109/ICSESS47205.2019.9040733,"This paper presents a semantic segmentation method that can distinguish six different types of intracranial hemorrhage and calculate the amount of blood loss. The major challenge of medical image segmentation are the lack of enough data due to the difficulty of data collection and labeling. In this paper, we propose to adopt a pretrained U-Net model with fine tuning to solve this problem. The best final test accuracy can reach 94.1%, which is 10.5% higher than the model training from scratch, proving its advantages in dealing with relatively complex datasets with a small amount of data, and the success of the proposed segmentation method.",segmentation;intracranial hemorrhage;U-Net;pretrained;blood loss,"112, 115",,IEEE Conferences,2019 IEEE 10th International Conference on Software Engineering and Service Science (ICSESS),IEEE
460,,Research on Chinese Naming Recognition Model Based on BERT Embedding,Q. Cai,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9040736,10.1109/ICSESS47205.2019.9040736,"Named entity recognition (NER) is one of the foundations of natural language processing(NLP). In the method of Chinese named entity recognition based on neural network, the vector representation of words is an important step. Traditional word embedding method map words or chars into a single vector, which can not represent the polysemy of words. To solve this problem, a named entity recognition method based on BERT Embedding model is proposed. The method enhances the semantic representation of words by BERT(Bidirectional Encoder Representations from Transformers) pre-trained language model. BERT can generates the semantic vectors dynamically according to the context of the words, and then inputs the word vectors into BiGRU-CRF for training. The whole model can be trained during training. It is also possible to fix the BERT and train only the BiGRU-CRF part. Experiments show that the two training methods of the model reach 95.43% F1 and 94.18% F1 in MSRA corpus, respectively, which are better than the current optimal Lattice-LSTM model.",component;chinese NER;BERT;BiGRU;Pre-trained Language Model;CRF,"1, 4",,IEEE Conferences,2019 IEEE 10th International Conference on Software Engineering and Service Science (ICSESS),IEEE
461,,Images’ Partially Blurred Part Location and Restoration based on the Calculation Model and GAN Algorithm,T. Wu; H. Mao; K. Xie; Y. Xie,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9040737,10.1109/ICSESS47205.2019.9040737,"Digital images would be blurred due to defocus and motion of objects. A lot of researches have been done on the restoration of motion blurred images. Unlike the global motion blurred image, the local blurred image needs different methods to recover as there is both clear and blurred part in an image. In this paper, we proposed a method combining calculation model and Generative Adversarial Network (GAN), which can automatically identify the local blurred region and deblurring. Also, the blurred part can be filled back correctly. Firstly, the gradient image of the complete image is calculated based on the Sobel operator. The boundary information of the fuzzy region and the Gauss function variance of the blurred and clear region are obtained. And the power spectral gradient of the whole image and each local pixel block are further compared with the variance of the Gauss function. After the analysis is accomplished, a more accurate fixed position is achieved. After finishing extracting with clipping pixels, the extracted fuzzy region is input into the pre-trained Generative Adversarial Network to restore the local image. Finally, the alpha channel algorithm is used to calculate the RGB component without the alpha channel of the two images. The simulation results show the feasibility of this method.",partial blurred images;sobel operator;power spectral gradient;Generative Adversarial Network,"171, 175",,IEEE Conferences,2019 IEEE 10th International Conference on Software Engineering and Service Science (ICSESS),IEEE
462,,Vehicle Type Recognition Based on Radon-CDT Hybrid Transfer Learning,S. Guan; B. Liao; Y. Du; X. Yin,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9040687,10.1109/ICSESS47205.2019.9040687,"Recognition of vehicle type is an important and challenging tasks. To improve the accuracy and reduce the computational complexity of model for vehicle type recognition, this paper proposes the vehicle type recognition method based on Radon-CDT hybrid transfer learning. Deep features of images are obtained by the pre-trained CNNs, then Radon-CDT are used to capture the non-linearly property of the features. This method makes full use of the excellent feature extraction ability of convolution layer and the property to make data linearly separable of Radon-CDT. Experimental results show that the proposed method is a very promising alternative to deal with the recognition of vehicle type, which obtains a better recognition accuracy than other popular models (AlexNet, VGG and ResNet).",vehicle type recognition;convolution neural network;Radon-CDT;transfer learning,"1, 4",,IEEE Conferences,2019 IEEE 10th International Conference on Software Engineering and Service Science (ICSESS),IEEE
463,,A Chinese Question Answering System based on GPT,S. Liu; X. Huang,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9040807,10.1109/ICSESS47205.2019.9040807,"The Chinese question-answering system needs to select the most appropriate answer from the answer library for user according to the given question on the natural language form. Previous question-answering systems required modeling for specific task characteristics and designing multiple modules. This paper first proposes to use the Generative Pre-trained Transformer (GPT) to implement the Chinese question-answering system. To optimize and improve the model, this Chinese model pays more attention to the contextual content and semantic characteristics, and we designed a new method to train this model. This model reduces the number of modules in the question-answering system. This paper evaluates the model on the Document-Based Chinese Question and Answer (DBQA) dataset and achieves a 2.5% improvement in MRR/MAP over the latest lattice convolutional neural networks (Lattice CNNs).",Chinese Question-Answering System;GPT;Transformer;Self-Attention (key words),"533, 537",,IEEE Conferences,2019 IEEE 10th International Conference on Software Engineering and Service Science (ICSESS),IEEE
464,,DeepDroid: Feature Selection approach to detect Android malware using Deep Learning,A. Mahindru; A. L. Sangal,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9040821,10.1109/ICSESS47205.2019.9040821,"Smartphones are now able to use for various purposes such as online banking, social networking, web browsing, ubiquitous services, MMS, and more daily essential needs through various apps. However, these apps are highly vulnerable to various types of malware attacks attributed to their open nature and high popularity in the market. The fault lies in the underneath permission model of Android apps. These apps need several sensitive permissions during their installation and runtime, which enables possible security breaches by malware. Hence, there is a requirement to develop a malware detection that can provide an effective solution to defense the mobile user from any malicious threat. In this paper, we proposed a framework which works on the principals of feature selection methods and Deep Neural Network (DNN) as a classifier. In this study, we empirically evaluate 1,20,000 Android apps and applied five different feature selection techniques. Out of which by using a set of features formed by Principal component analysis (PCA)can able to detect 94% Android malware from real-world apps.",Android apps;Machine learning;Feature selection;Malware detection,"16, 19",,IEEE Conferences,2019 IEEE 10th International Conference on Software Engineering and Service Science (ICSESS),IEEE
465,,An Efficient End-to-End Channel Level Pruning Method for Deep Neural Networks Compression,L. Zeng; S. Chen; S. Zeng,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9040742,10.1109/ICSESS47205.2019.9040742,"Deep neural networks (DNNS) have obtained compelling performance among many visual tasks by a significant increase in the computation and memory consumption, which severely impede their applications on resource-constrained systems like smart mobiles or embedded devices. To solve these problems, recent efforts toward compressing DNNS have received increased focus. In this paper, we proposed an effective end-to-end channel pruning approach to compress DNNS. To this end, firstly, we introduce additional auxiliary classifiers to enhance the discriminative power of shallow and intermediate layers. Secondly, we impose Ll-regularization on the scaling factors and shifting factors in batch normalization (BN) layer, and adopt the fast and iterative shrinkage-thresholding algorithm (FISTA) to effectively prune the redundant channels. Finally, by forcing selected factors to zero, we can prune the corresponding unimportant channels safely, thus obtaining a compact model. We empirically reveal the prominent performance of our approach with several state-of-theart DNNS architectures, including VGGNet, and MobileNet, on different datasets. For instance, on cifar10 dataset, the pruned MobileNet achieves 26. 9x reduction in model parameters and 3. 9x reduction in computational operations with only 0.04% increase of classification error.",component;DNNS;network pruning;channel pruning;classification,"43, 46",,IEEE Conferences,2019 IEEE 10th International Conference on Software Engineering and Service Science (ICSESS),IEEE
466,,An Adaptive Pooling Model for Aesthetic Quality Assessment Based on Convolutional Neural Network,M. Han; P. Shi; X. Bao,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9040714,10.1109/ICSESS47205.2019.9040714,"It is a requirement to find well-photographed and attractive photos from a large number of images, but image aesthetic quality assessment (IAQA) is challenging because of the subjectivity of aesthetic criteria. In this paper, we propose an adaptive pooling model for IAQA based on convolutional neural network (CAP) which consists of three parts: feature extraction, feature adaptive pooling, and obtaining quality scores. Through experimental comparison, we choose ResNet50 network as the image feature extraction network. In order to reduce the fixed input size limit, we introduce Spatial Pyramid Pooling (SPP) in model. The final score is weighted summation of the score distribution. The experimental results show that the proposed model achieves the state-of-the-art performance.",image aesthetic quality assessment;convolutional neural nenvork;adaptive pooling,"358, 361",,IEEE Conferences,2019 IEEE 10th International Conference on Software Engineering and Service Science (ICSESS),IEEE
467,,HDCNN-CRF for Biomedical Text Named Entity Recognition,M. Gao; H. Wei; F. Chen; W. Qu; M. Lu,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9040749,10.1109/ICSESS47205.2019.9040749,"Biomedical named entity recognition (BNER) is one of the most basic and important tasks of biomedical text mining. LSTM does not take full advantage of parallelism, making recognition slower. This paper focuses on improving the model structure and proposes a HDCNN-CRF method which combines hybrid dilated convolutional neural network (HDCNN) and conditional random field (CRF). It can not only avoid the expensive cost of human participation in feature construction, but also greatly improve the speed compared with LSTM method in named entity recognition (NER). We use Adam for optimization during model training and the IOBES tagging method for labeling the sequence. The HDCNN-CRF model that does not rely on any costly feature engineering has shown good performances on the NCBI-disease corpus. Due to its high degree of parallelism, the model speed is four times higher than BLSTM.",component;biomedical text;conditional random field;hybrid dilated convolutional neural network;named entity recognition,"191, 194",,IEEE Conferences,2019 IEEE 10th International Conference on Software Engineering and Service Science (ICSESS),IEEE
468,,Remote Sensing Image Aircraft Detection Based on Feature Fusion across Deep Learning Framework,W. Wei; J. Zhang; C. Xu,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9040808,10.1109/ICSESS47205.2019.9040808,"The detection of remote sensing image aircraft targets based on deep learning has practical and important significance in the fields of military reconnaissance and disaster rescue. As a typical representative of the two mainstream detection algorithms, YOLOv3 and Faster_R_CNN have good detection effects on remote sensing image aircraft targets. However, for low quality remote sensing images, the two detection algorithms also have the phenomenons of omission and false detection. To deal with this, this paper proposes a target detection algorithm (YF_R_CNN) for ""Separate training, joint detection"", which realizes the cross-platform detection feature fusion of YOLOv3 based on darknet framework and Faster_R_CNN based on tensorflow framework, effectively alleviating the problems of existing algorithms. The experimental results show that the detection accuracy of YF_R_CNN algorithm reaches 94.8%, which is 3.7% and 3.1% higher than YOLOv3 and Faster_R_CNN respectively. The detection accuracy is obviously improved, and the algorithm has better flexibility and robustness.",component;remote sensing image;aircraft detection;feature fusion,"1, 5",,IEEE Conferences,2019 IEEE 10th International Conference on Software Engineering and Service Science (ICSESS),IEEE
469,,Low-cost 3D Building Modeling via Image Processing,Y. Li; Z. Ding; W. Miao; M. Zhang; W. Li; W. Ye,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9040755,10.1109/ICSESS47205.2019.9040755,"In the era of Big Data, 3D electronic maps and GIS are widely used in geographic information analysis and urban computing. However, the production cost of electronic maps is expensive, especially that of 3D modeling of buildings that are important geographic objects. In this paper, we propose a fast and low-cost method for 3D modeling of buildings by using building images provided by online electronic maps. The method employs edge detection to identify the bottom contours of buildings in 2D building images; it then takes DNN-based object detection and semantic segmentation to spill a single building from 3D building images, and calculates the height of the building facades. We use this approach to model 22,740 3D buildings in the urban area of Nanjing, and take a 5m high-resolution electronic map as the benchmark to evaluate the modeling accuracy. The results show that image-based 3D building modeling is able to provide acceptable-precision 3D geometry information of buildings.",3D modeling;edge detection;object detection;semantic segmentation,"331, 335",,IEEE Conferences,2019 IEEE 10th International Conference on Software Engineering and Service Science (ICSESS),IEEE
470,,Jointly Temporal Pooling Networks and Multi-loss Fusion for Video-based Person Re-Identification,H. Xu; X. Sun,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9040764,10.1109/ICSESS47205.2019.9040764,"With the increasing demand in surveillance and camera networks, video-based person re-identification is an important task in the field of computer vision. The video contains rich samples of person's appearances, how to effectively use this information is a challenge. We propose a hierarchical network that joint temporal pooling and multi-loss fusion function to obtain the representation of videos. The network can be trained efficiently with end-to-end way. We conducted experiments on the large-scale MARS, iLIDS-VID and PRID-2011 datasets to confirm the effectiveness of our proposed method.",component;hierarchical network;person reidentification;temporal pooling;multi-loss,"1, 4",,IEEE Conferences,2019 IEEE 10th International Conference on Software Engineering and Service Science (ICSESS),IEEE
471,,Learning Static and Dynamic Features for Collaborative Filtering,X. Yang; H. Jiang,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9040854,10.1109/ICSESS47205.2019.9040854,"User preferences are influenced by the purchased products, and ratings of products are also related to theirs public praises. Dynamic latent representations can be learned from these sequence information. Researches show that learning such dynamic features is helpful to build model-based collaborative filtering. However, static features also play an irreplaceable role in recommendations by reason of inherent characteristics of users/items. Ratings of users on products directly represent user preferences and qualities of products. A neural network model for learning both static and dynamic features is proposed in this paper. Autoencoder is adopted as a static model focusing on explicit feedback i.e. ratings, and gated recurrent unit is adopted as a dynamic model focusing on implicit feedback i.e. sequences. Features learned from static and dynamic models are combined to make predictions. Experiments on two real-word datasets i.e. Baby of Amazon dataset and MovieLens 10M show improvement of our proposed model over the state-of-the-art methods.",autoencoder;gated recurrent unit;collaborative filtering,"304, 308",,IEEE Conferences,2019 IEEE 10th International Conference on Software Engineering and Service Science (ICSESS),IEEE
472,,Facial Expression Recognition Based on Convolutional Neural Network,Z. Yue; F. Yanyan; Z. Shangyou; P. Bing,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9040730,10.1109/ICSESS47205.2019.9040730,"Facial expression recognition is an important field of pattern recognition research. Traditional machine learning methods extract features manually. It has insufficient generalization ability and poor stability. Moreover, its accuracy is difficult to improve. In order to achieve better facial expression recognition, this paper designs a modular multi-channel deep convolutional neural network. To avoid overfitting, the network output uses a global average layer. Data enhancement on the dataset before training can improve the generalization ability of the model. Test the performance of network on the FER2013 emoticon dataset. The accuracy of expression recognition is 68.4%. It performs a prediction for about 0.12s. Compared to other recognition algorithms, network has certain advantages. Finally, a real-time facial expression recognition system is constructed by using the trained recognition model. The experimental results show that the system can effectively recognize facial expressions in real time.",convolutional neural network;network performance;real-time;expression recognition,"410, 413",,IEEE Conferences,2019 IEEE 10th International Conference on Software Engineering and Service Science (ICSESS),IEEE
473,,Research on Liveness Detection Algorithms Based on Deep Learning,Y. Fan; Y. Shi; X. Wang; H. Yi,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9040795,10.1109/ICSESS47205.2019.9040795,"Face liveness detection is designed to prevent the face recognition system from using the attacking image as the user's real image. At present, although there are many liveness detection methods, most of the methods have poor generalization ability in practical application. Because these methods are based on datasets, and these data cannot well simulate the data distribution in the real environment. In order to improve the generalization ability of the model, in this paper, we build model using the deep neural network and the dataset collected by ourselves. The experimental results show that our model has achieved significant improvement in the realistic scenario.",Face Recognition;Liveness Detect;Deep Neural Network,"1, 6",,IEEE Conferences,2019 IEEE 10th International Conference on Software Engineering and Service Science (ICSESS),IEEE
474,,Software Framework for Data Fault Injection to Test Machine Learning Systems,J. K. Nurminen; T. Halvari; J. Harviainen; J. Mylläri; A. Röyskö; J. Silvennoinen; T. Mikkonen,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8990189,10.1109/ISSREW.2019.00087,"Data-intensive systems are sensitive to the quality of data. Data often has problems due to faulty sensors or network problems, for instance. In this work, we develop a software framework to emulate faults in data and use it to study how machine learning (ML) systems work when the data has problems. We aim for flexibility: users can use predefined or their own dedicated fault models. Likewise, different kind of data (e.g. text, time series, video) can be used and the system under test can vary from a single ML model to a complicated software system. Our goal is to show how data faults can be emulated and how that can be used in the study and development of ML solutions.",Testing;machine learning;fault injection,"294, 299",,IEEE Conferences,2019 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW),IEEE
475,,TamperNN: Efficient Tampering Detection of Deployed Neural Nets,E. L. Merrer; T. Gilles,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8987572,10.1109/ISSRE.2019.00049,"Neural networks are powering the deployment of embedded devices and Internet of Things. Applications range from personal assistants to critical ones such as self-driving cars. It has been shown recently that models obtained from neural nets can be trojaned; an attacker can then trigger an arbitrary model behavior facing crafted inputs. This has a critical impact on the security and reliability of those deployed devices. We introduce novel algorithms to detect the tampering with deployed models, classifiers in particular. In the remote interaction setup we consider, the proposed strategy is to identify markers of the model input space that are likely to change class if the model is attacked, allowing a user to detect a possible tampering. This setup makes our proposal compatible with a wide range of scenarios, such as embedded models, or models exposed through prediction APIs. We experiment those tampering detection algorithms on the canonical MNIST dataset, over three different types of neural nets, and facing five different attacks (trojaning, quantization, fine-tuning, compression and watermarking). We then validate over five large models (VGG16, VGG19, ResNet, MobileNet, DenseNet) with a state of the art dataset (VGGFace2), and report results demonstrating the possibility of an efficient detection of model tampering.",Tampering detection;neural networks;decision boundaries;black box interaction model,"424, 434",,IEEE Conferences,2019 IEEE 30th International Symposium on Software Reliability Engineering (ISSRE),IEEE
476,,Supervised Representation Learning Approach for Cross-Project Aging-Related Bug Prediction,X. Wan; Z. Zheng; F. Qin; Y. Qiao; K. S. Trivedi,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8987459,10.1109/ISSRE.2019.00025,"Software aging, which is caused by Aging-Related Bugs (ARBs), tends to occur in long-running systems and may lead to performance degradation and increasing failure rate during software execution. ARB prediction can help developers discover and remove ARBs, thus alleviating the impact of software aging. However, ARB-prone files occupy a small percentage of all the analyzed files. It is usually difficult to gather sufficient ARB data within a project. To overcome the limited availability of training data, several researchers have recently developed cross-project models for ARB prediction. A key point for cross-project models is to learn a good representation for instances in different projects. Nevertheless, most of the previous approaches neither consider the reconstruction property of new representation nor encode source samples' label information in learning representation. To address these shortcomings, we propose a Supervised Representation Learning Approach (SRLA), which is based on double encoding-layer autoencoder, to perform cross-project ARB prediction. Moreover, we present a transfer cross-validation framework to select the hyper-parameters of cross-project models. Experiments on three large open-source projects demonstrate the effectiveness and superiority of our approach compared with the state-of-the-art approach TLAP.","software aging, aging-related bug, cross-project bug prediction, supervised representation learning approach","163, 172",,IEEE Conferences,2019 IEEE 30th International Symposium on Software Reliability Engineering (ISSRE),IEEE
477,,Identifying Crashing Fault Residence Based on Cross Project Model,Z. Xu; T. Zhang; Y. Zhang; Y. Tang; J. Liu; X. Luo; J. Keung; X. Cui,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8987488,10.1109/ISSRE.2019.00027,"Analyzing the crash reports recorded upon software crashes is a critical activity for software quality assurance. Predicting whether or not the fault causing the crash (crashing fault for short) resides in the stack traces of crash reports can speed-up the program debugging process and determine the priority of the debugging efforts. Previous work mostly collected label information from bug-fixing logs, and extracted crash features from stack traces and source code to train classification models for the Identification of Crashing Fault Residence (ICFR) of newly-submitted crashes. However, labeled data are not always fully available in real applications. Hence the classifier training is not always feasible. In this work, we make the first attempt to develop a cross project ICFR model to address the data scarcity problem. This is achieved by transferring the knowledge from external projects to the current project via utilizing a state-of-the-art Balanced Distribution Adaptation (BDA) based transfer learning method. BDA not only combines both marginal distribution and conditional distribution across projects but also assigns adaptive weights to the two distributions for better adjusting specific cross project pair. The experiments on 7 software projects show that BDA is superior to 9 baseline methods in terms of 6 indicators overall.",crashing fault;stack trace;transfer learning;cross project model,"183, 194",,IEEE Conferences,2019 IEEE 30th International Symposium on Software Reliability Engineering (ISSRE),IEEE
478,,Textout: Detecting Text-Layout Bugs in Mobile Apps via Visualization-Oriented Learning,Y. Wang; H. Xu; Y. Zhou; M. R. Lyu; X. Wang,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8987514,10.1109/ISSRE.2019.00032,"Layout bugs commonly exist in mobile apps. Due to the fragmentation issues of smartphones, a layout bug may occur only on particular versions of smartphones. It is quite challenging to detect such bugs for state-of-the-art commercial automated testing platforms, although they can test an app with thousands of different smartphones in parallel. The main reason is that typical layout bugs neither crash an app nor generate any error messages. In this paper, we present our work for detecting text-layout bugs, which account for a large portion of layout bugs. We model text-layout bug detection as a classification problem. This then allows us to address it with sophisticated image processing and machine learning techniques. To this end, we propose an approach which we call Textout. Textout takes screenshots as its input and adopts a specifically-tailored text detection method and a convolutional neural network (CNN) classifier to perform automatic text-layout bug detection. We collect 33,102 text-region images as our training dataset and verify the effectiveness of our tool with 1,481 text-region images collected from real-world apps. Textout achieves an AUC (area under the curve) of 0.956 on the test dataset and shows an acceptable overhead. The dataset is open-source released for follow-up research.",mobile application testing;GUI testing;GUI bug detection;text-layout bug detection;deep learning,"239, 249",,IEEE Conferences,2019 IEEE 30th International Symposium on Software Reliability Engineering (ISSRE),IEEE
479,,A Deep Learning Approach to Tumour Identification in Fresh Frozen Tissues,H. Ugail; M. Alzorgani; A. Bukar; H. Hussain; C. Burn; T. M. Sein; S. Betmouni,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8982508,10.1109/SKIMA47702.2019.8982508,"The demand for pathology services are significantly increasing whilst the numbers of pathologists are significantly decreasing. In order to overcome these challenges, a growing interest in faster and efficient diagnostic methods such as computer-aided diagnosis (CAD) have been observed. An increase in the use of CAD systems in clinical settings has subsequently led to a growing interest in machine learning. In this paper, we show the use of machine learning algorithms in the prediction of tumour content in Fresh Frozen (FF) histological samples of head and neck. More specifically, we explore a pre-trained convolutional neural network (CNN), namely the AlexNet, to build two common machine learning classifiers. For the first classifier, the pre-trained AlexNet network is used to extract features from the activation layer and then Support Vector Machine (SVM) based classifier is trained by using these extracted features. In the second case, we replace the last three layers of the pre-trained AlexNet network and then fine tune these layers on the FF histological image samples. The results of our experiments are very promising. We have obtained percentage classification rates in the high 90s, and our results show there is little difference between SVM and transfer learning. Thus, the present study show that an AlexNet driven CNN with SVM and fine-tuned classifiers are a suitable choice for accurate discrimination between tumour and non-tumour histological samples from the head and neck.",Tumour Identification;Deep Learning;Convolutional Neural Networks;AlexNet;Histopathological Diagnosis;Fresh Frozen Tissue Analysis,"1, 6",,IEEE Conferences,"2019 13th International Conference on Software, Knowledge, Information Management and Applications (SKIMA)",IEEE
480,,Illumination-Based Data Augmentation for Robust Background Subtraction,D. Sakkos; H. P. H. Shum; E. S. L. Ho,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8982527,10.1109/SKIMA47702.2019.8982527,"A core challenge in background subtraction (BGS) is handling videos with sudden illumination changes in consecutive frames. In this paper, we tackle the problem from a data point-of-view using data augmentation. Our method performs data augmentation that not only creates endless data on the fly, but also features semantic transformations of illumination which enhance the generalisation of the model. It successfully simulates flashes and shadows by applying the Euclidean distance transform over a binary mask generated randomly. Such data allows us to effectively train an illumination-invariant deep learning model for BGS. Experimental results demonstrate the contribution of the synthetics in the ability of the models to perform BGS even when significant illumination changes take place.",Background subtraction;convolutional neural networks;synthetics;data augmentation;illumination-invariant,"1, 8",,IEEE Conferences,"2019 13th International Conference on Software, Knowledge, Information Management and Applications (SKIMA)",IEEE
481,,A Design and Implementation of Performance Dashboard for the Work Integrated Learning Unit,P. N. Lumpoon; P. Thiengburanathum,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8982514,10.1109/SKIMA47702.2019.8982514,"At present, the higher education institutes focus on the learning outcomes that curriculums can produce graduates with qualified employability skills. Work Integrated Learning (WIL) center is a unit in academic faculty that aims to develop students' competencies at workplace and classroom based on the integrated learning outcomes. However, measuring the performances of the WIL program is a difficult task such that the related key performance indicators are not identified. In this paper, we developed a novel model of the KPIs of WIL activities and the relevant employability skills. Additionally, design and implementation of the dashboards to display and monitor the defined KPIs are presented. As a result, users can gain insightful information and provide confidence for decision makers.",KPIs;performance dashboard;employability skills;Work Integrated Learning;cooperative education,"1, 6",,IEEE Conferences,"2019 13th International Conference on Software, Knowledge, Information Management and Applications (SKIMA)",IEEE
482,,Assay Type Detection Using Advanced Machine Learning Algorithms,M. H. Tania; K. T. Lwin; A. M. Shabut; K. J. Abu-Hassan; M. S. Kaiser; M. A. Hossain,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8982449,10.1109/SKIMA47702.2019.8982449,"The colourimetric analysis has been used in diversified fields for years. This paper provides a unique overview of colourimetric tests from the perspective of computer vision by describing different aspects of a colourimetric test in the context of image processing, followed by an investigation into the development of a colorimetric assay type detection system using advanced machine learning algorithms. To the best of our knowledge, this is the first attempt to define colourimetric assay types from the eyes of a machine and perform any colorimetric test using deep learning. This investigation utilizes the state-of-the-art pre-trained models of Convolutional Neural Network (CNN) to perform the assay type detection of an enzyme-linked immunosorbent assay (ELISA) and lateral flow assay (LFA). The ELISA dataset contains images of both positive and negative samples, prepared for the plasmonic ELISA based TB-antigen specific antibody detection. The LFA dataset contains images of the universal pH indicator paper of eight pH levels. It is noted that the pre-trained models offered 100% accurate visual recognition for the assay type detection. Such detection can assist novice users to initiate a colorimetric test using his/her personal digital devices. The assay type detection can also aid in calibrating an image-based colorimetric classification.",computer vision;deep learning;transfer learning;colorimetric test;point-of-care system;diagnosis,"1, 8",,IEEE Conferences,"2019 13th International Conference on Software, Knowledge, Information Management and Applications (SKIMA)",IEEE
483,,Deep Learning with Convolutional Neural Network and Long Short-Term Memory for Phishing Detection,M. A. Adebowale; K. T. Lwin; M. A. Hossain,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8982427,10.1109/SKIMA47702.2019.8982427,"Phishers sometimes exploit users' trust of a known website's appearance by using a similar page that looks like the legitimate site. In recent times, researchers have tried to identify and classify the issues that can contribute to the detection of phishing websites. This study focuses on design and development of a deep learning based phishing detection solution that leverages the Universal Resource Locator and website content such as images and frame elements. A Convolutional Neural Network (CNN) and the Long Short-Term Memory (LSTM) algorithm were used to build a classification model. The experimental results showed that the proposed model achieved an accuracy rate of 93.28%.",Phishing detection;Cybercrime;Deep learning (DL);Convolutional Neural Network (CNN);Long Short-Term Memory (LSTM);Big data;Universal Resource Locator (URL),"1, 8",,IEEE Conferences,"2019 13th International Conference on Software, Knowledge, Information Management and Applications (SKIMA)",IEEE
484,,An automatic cluster-based approach for depth estimation of single 2D images,M. A. Shoukat; A. B. Sargano; Z. Habib; L. You,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8982472,10.1109/SKIMA47702.2019.8982472,"In this paper, the problem of single 2D image depth estimation is considered. This is a very important problem due to its various applications in the industry. Previous learning-based methods are based on a key assumption that color images having photometric resemblance are likely to present similar depth structure. However, these methods search the whole dataset for finding corresponding images using handcrafted features, which is quite cumbersome and inefficient process. To overcome this, we have proposed a clustering-based algorithm for depth estimation of a single 2D image using transfer learning. To realize this, images are categorized into clusters using K-means clustering algorithm and features are extracted through a pre-trained deep learning model i.e., ResNet-50. After clustering, an efficient step of replacing feature vector is embedded to speedup the process without compromising on accuracy. After then, images with similar structure as an input image, are retrieved from the best matched cluster based on their correlation values. Then, retrieved candidate depth images are employed to initialize prior depth of a query image using weighted-correlation-average (WCA). Finally, the estimated depth is improved by removing variations using cross-bilateral-filter. In order to evaluate the performance of proposed algorithm, experiments are conducted on two benchmark datasets, NYU v2 and Make3D.",Depth estimation;transfer learning;2D to 3D conversion;K-means clustering,"1, 8",,IEEE Conferences,"2019 13th International Conference on Software, Knowledge, Information Management and Applications (SKIMA)",IEEE
485,,The Computer Nose Best,S. Jilani; H. Ugail; A. Logan,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8982474,10.1109/SKIMA47702.2019.8982474,"The nose is the most central feature on the face which is known to exhibit both gender and ethnic differences. It is a robust feature, invariant to expression and known to contain depth information. In this paper we address the topic of binary ethnicity classificiation from images of the nose, using a novel dataset of South Asian, Pakistani images. To the best of our knowledge, we are one of the first to attempt demographic (ethnicity) based identification based solely on information from the nose. A two-category (Pakistani vs Non-Pakistani) task was used in combination with Deep learning (ResNet) based and VGG-based pre-trained models. A series of experiments were conducted using ResNet-50, ResNet-101, ResNet-152, VGG-Face, VGG-16 and VGG-19, for feature extraction and a Linear Support Vector Machine for classification. The experimental results demonstrate ResNet-50 achieves the highest performance accuracy of 94.1%. In comparison, the highest score for the VGG-based models (VGG-16) was 90.8%. These results demonstrate that information from the nose is sufficient for deep learning models to achieve >90% accuracy on judgements of ethnicity.",Deep Residual Neural Networks;Ethnicity;Nose;Pakistani Face Database;Human Attributes;Feature Extraction,"1, 6",,IEEE Conferences,"2019 13th International Conference on Software, Knowledge, Information Management and Applications (SKIMA)",IEEE
486,,Deep Learning and Cultural Heritage: The CEPROQHA Project Case Study,A. Belhi; H. Gasmi; A. K. Al-Ali; A. Bouras; S. Foufou; X. Yu; H. Zhang,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8982520,10.1109/SKIMA47702.2019.8982520,"Cultural heritage takes an important part of the history of humankind as it is one of the most powerful tools for the transfer and preservation of moral identity. As a result, these cultural assets are considered highly valuable and sometimes priceless. Digital technologies provided multiple tools that address challenges related to the promotion and information access in the cultural context. However, the large data collections of cultural information have more potential to add value and address current challenges in this context with the recent progress in artificial intelligence (AI) with deep learning and data mining tools. Through the present paper, we investigate several approaches that are used or can potentially be used to promote, curate, preserve and value cultural heritage through new and evolutionary techniques based on deep learning tools. The deep learning approaches entirely developed by our team are intended to classify and annotate cultural data, complete missing data, or map existing data schemes and information to standardized schemes with language processing tools.",Cultural Heritage;Digital Heritage;Deep Learning;CEPROQHA Project;Artificial Intelligence,"1, 5",,IEEE Conferences,"2019 13th International Conference on Software, Knowledge, Information Management and Applications (SKIMA)",IEEE
487,,Image Tag Refinement with Self Organizing Maps,T. Üstünkök; O. C. Acar; M. Karakaya,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8965477,10.1109/UBMYK48245.2019.8965477,"Nowadays, data sharing has become faster than ever. This speed demands novel search methods. Most popular way of accessing the data is to search its tag. Therefore, creating tags, captions from an image is a research area that gains reputation rapidly. In this study, we aim to refine image captions by utilizing Self Organizing Maps. We extract image and caption pairs as feature vectors and then cluster those vectors. Vectors with similar content clustered close to each other. With the help of those clusters, we hope to get some relevant tags that do not exist in the original tags. We performed extensive experiments and presented our initial results. According to these results, the proposed model performs reasonably well with a 54% precision score. Finally, we conclude our work by providing a list of future work.",image tagging;tag refinement;self organizing maps;SOM;clustering,"1, 6",,IEEE Conferences,2019 1st International Informatics and Software Engineering Conference (UBMYK),IEEE
488,,Vibration Signal Processing Based Bearing Defect Diagnosis with Transfer Learning,C. Tastimur; M. Karakose; E. Akin,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8965451,10.1109/UBMYK48245.2019.8965451,"It is very important to diagnose the fault condition of the bearing machines in order to make the machine run healthier. There are many successful studies in the detection of failures in the bearing machine made by conventional machine learning methods. However, these studies produce successful results in cases where the machines operate under the same condition and feature space is the same. Therefore, a deep learning-based diagnostic has been proposed for changing machine operating conditions. In the scope of our study, Keras and Tensorflow libraries, CNN network from scratch, VGG16 model and VGG19 deep learning models have been used for the classification of vibration images. Freeze the weights for transfer learning and remove the last fully connected layer to update the network in a problem-specific manner. The number of iterations and batch size has been determined by experimental studies. In this study, four faulty conditions have been successfully classified. While the accuracy rate of CNN network from scratch is 25%, the accuracy rate obtained by the VGG16 transfer learning method is 93% and the loss rate is 0.17% and the accuracy rate obtained by the VGG19 transfer learning method is 95% and the loss rate is 0.13%.",Bearing;Deep learning;Fault diagnosis;Transfer Learning;Vibration signal,"1, 5",,IEEE Conferences,2019 1st International Informatics and Software Engineering Conference (UBMYK),IEEE
489,,"Brain Hemorrhage Detection based on Heat Maps, Autoencoder and CNN Architecture",M. Toğaçar; Z. Cömert; B. Ergen; Ü. Budak,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8965576,10.1109/UBMYK48245.2019.8965576,"Brain hemorrhage refers to hemorrhage within the brain tissue or between the surrounding bone. Therefore, head hemorrhage can lead to many dangerous consequences, especially brain hemorrhage. Early and correct intervention by experts in such cases is important for the patient's life. In this study, computed tomography images of brain hemorrhage are classified by AlexNet which is one of the convolutional neural network models used recently in the biomedical field. In this scope, the data set is restructured with the autoencoder network model and heat maps of each image in the data set are extracted to improve the classification success. The number of images in the data set is then increased by approximately 10 times using the data augmentation technique. The classification process is performed using support vector machines. As a result, the best success rate in the classification was 98.57%. In conclusion, the proposed approach contributed to the classification of cerebral hemorrhage images.",Biomedical image processing;brain hemorrhage;autoencoder network;heat map;deep learning,"1, 5",,IEEE Conferences,2019 1st International Informatics and Software Engineering Conference (UBMYK),IEEE
490,,FPGA-Based Optimized Convolutional Neural Network Framework for Handwritten Digit Recognition,H. Madadum; Y. Becerikli,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8965628,10.1109/UBMYK48245.2019.8965628,"The size of ConNN (Convolutional neural network) tends to increase every year. Thus, it consumes plenty of power to be able to run it. To reduce the power consumption, choosing the right hardware platform is important. Field programmable gate area (FPGA) is also known as low power platform for low level digital design. Therefore, this paper proposes an implementation of ConNN using FPGA in order to reduce the power consumption. We describe the simple ConNN which is optimized ConNN framework compatible with FPGA. Moreover, we implement the handwritten digit recognition to evaluate the performance of our framework.",Convolutional Neural Network;FPGA;Lenet-5;Embedded system design,"1, 6",,IEEE Conferences,2019 1st International Informatics and Software Engineering Conference (UBMYK),IEEE
491,,Prostate Segmentation via Fusing the Nested-V-net3d and V-net2d,H. Öcal; N. Barışçı,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8965456,10.1109/UBMYK48245.2019.8965456,"Prostate cancer is caused by uncontrolled growth of cells in the prostate gland. Prostate cancer, unlike benign prostate gland enlargement, is not from the center of the prostate, but from the site of the tumor to the center of the capsule. Therefore, the patient experiences urinary tract complaints at the last stage. Therefore, until the last stage, the patient does not have any finding. For this reason, Magnetic Resonance Imaging (MRI) is used in regular examinations after a certain age or in various diagnostic imaging methods of patients diagnosed with this disease. Proper localization of the prostate is an important step in assisting diagnosis and treatment, such as guiding the biopsy procedure and radiation therapy. However, manual segmentation of the prostate is tedious and time-consuming. It also varies in inter-rater evaluation. The two main challenges for correct MR prostate localization are; nonhomogeneous and inconsistent appearance around the prostate border, wide prostate shape variability in different patients. In this study, Fusing the Nested 3D Dimensional Volumetric Convolutional Neural Network (Nested-Vnet3d) and 2D Volumetric Convolutional Neural Network (V-net2d) models are compared with other V-net based models. In the training conducted on the PROMISE12 dataset, 0.92 validation dice score was achieved. This study showed that the proposed model is a robust deep learning model for prostate segmentation.",Prostate segmentation;Volumetric CNN;Nested-V-net-3d;PROMISE12,"1, 4",,IEEE Conferences,2019 1st International Informatics and Software Engineering Conference (UBMYK),IEEE
492,,A Novel Application based on Spectrogram and Convolutional Neural Network for ECG Classification,A. Diker; Z. Cömert; E. Avcı; M. Toğaçar; B. Ergen,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8965506,10.1109/UBMYK48245.2019.8965506,"Electrocardiogram (ECG) is a biomedical signal which represents the electrical activity of the human heart. Various cardiac diseases have been detected using the outputs of ECG devices. Recently, advances in signal processing techniques bring out a new horizon for processing the ECG signals. In this scope, a novel application based on the spectrogram, which is a graphical representation of time-frequency information of the signal, and the convolutional neural network (CNN) is proposed so as to distinguish ECG signals. To this aim, a publicly available data set in Physionet was utilized. Firstly, the spectrograms of each signal were obtained. Then, these colorful spectrogram images were applied as the input to CNNs that are AlexNet, VGG-16, and ResNet-18. The transfer learning and fine-tuning approach were used for training and validation of the models. As a result, the most efficient results were provided by AlexNet with an accuracy of 83.82%. The experimental results of this study show that the proposed model ensures promising results for the ECG signal classification.",Biomedical signal processing;decision-making support system;convolutional neural network;classification,"1, 6",,IEEE Conferences,2019 1st International Informatics and Software Engineering Conference (UBMYK),IEEE
493,,Improving the Decision-Making Process of Self-Adaptive Systems by Accounting for Tactic Volatility,J. Palmerino; Q. Yu; T. Desell; D. Krutz,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952178,10.1109/ASE.2019.00092,"When self-adaptive systems encounter changes withintheir surrounding environments, they enacttacticsto performnecessary adaptations. For example, a self-adaptive cloud-basedsystem may have a tactic that initiates additional computingresources when response time thresholds are surpassed, or theremay be a tactic to activate a specific security measure when anintrusion is detected. In real-world environments, these tacticsfrequently experiencetactic volatilitywhich is variable behaviorduring the execution of the tactic.Unfortunately, current self-adaptive approaches do not accountfor tactic volatility in their decision-making processes, and merelyassume that tactics do not experience volatility. This limitationcreates uncertainty in the decision-making process and mayadversely impact the system's ability to effectively and efficientlyadapt. Additionally, many processes do not properly account forvolatility that may effect the system's Service Level Agreement(SLA). This can limit the system's ability to act proactively, especially when utilizing tactics that contain latency.To address the challenge of sufficiently accounting for tacticvolatility, we propose aTactic Volatility Aware(TVA) solution.Using Multiple Regression Analysis (MRA), TVA enables self-adaptive systems to accurately estimate the cost and timerequired to execute tactics. TVA also utilizesAutoregressiveIntegrated Moving Average(ARIMA) for time series forecasting, allowing the system to proactively maintain specifications.","Artificial Intelligence, Self-Adaptation, Machine Learning","949, 961",,IEEE Conferences,2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
494,,Lancer: Your Code Tell Me What You Need,S. Zhou; B. Shen; H. Zhong,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952168,10.1109/ASE.2019.00137,"Programming is typically a difficult and repetitive task. Programmers encounter endless problems during programming, and they often need to write similar code over and over again. To prevent programmers from reinventing wheels thus increase their productivity, we propose a context-aware code-to-code recommendation tool named Lancer. With the support of a Library-Sensitive Language Model (LSLM) and the BERT model, Lancer is able to automatically analyze the intention of the incomplete code and recommend relevant and reusable code samples in real-time. A video demonstration of Lancer can be found at https://youtu.be/tO9nhqZY35g. Lancer is open source and the code is available at https://github.com/sfzhou5678/Lancer.",Code recommendation;Code reuse;Language model,"1202, 1205",,IEEE Conferences,2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
495,,AutoFocus: Interpreting Attention-Based Neural Networks by Code Perturbation,N. D. Q. Bui; Y. Yu; L. Jiang,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952269,10.1109/ASE.2019.00014,"Despite being adopted in software engineering tasks, deep neural networks are treated mostly as a black box due to the difficulty in interpreting how the networks infer the outputs from the inputs. To address this problem, we propose AutoFocus, an automated approach for rating and visualizing the importance of input elements based on their effects on the outputs of the networks. The approach is built on our hypotheses that (1) attention mechanisms incorporated into neural networks can generate discriminative scores for various input elements and (2) the discriminative scores reflect the effects of input elements on the outputs of the networks. This paper verifies the hypotheses by applying AutoFocus on the task of algorithm classification (i.e., given a program source code as input, determine the algorithm implemented by the program). AutoFocus identifies and perturbs code elements in a program systematically, and quantifies the effects of the perturbed elements on the network's classification results. Based on evaluation on more than 1000 programs for 10 different sorting algorithms, we observe that the attention scores are highly correlated to the effects of the perturbed code elements. Such a correlation provides a strong basis for the uses of attention scores to interpret the relations between code elements and the algorithm classification results of a neural network, and we believe that visualizing code elements in an input program ranked according to their attention scores can facilitate faster program comprehension with reduced code.","attention mechanisms, neural networks, algorithm classification, interpretability, explainability, code perturbation, program comprehension","38, 41",,IEEE Conferences,2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
496,,Automating App Review Response Generation,C. Gao; J. Zeng; X. Xia; D. Lo; M. R. Lyu; I. King,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952476,10.1109/ASE.2019.00025,"Previous studies showed that replying to a user review usually has a positive effect on the rating that is given by the user to the app. For example, Hassan et al. found that responding to a review increases the chances of a user updating their given rating by up to six times compared to not responding. To alleviate the labor burden in replying to the bulk of user reviews, developers usually adopt a template-based strategy where the templates can express appreciation for using the app or mention the company email address for users to follow up. However, reading a large number of user reviews every day is not an easy task for developers. Thus, there is a need for more automation to help developers respond to user reviews. Addressing the aforementioned need, in this work we propose a novel approach RRGen that automatically generates review responses by learning knowledge relations between reviews and their responses. RRGen explicitly incorporates review attributes, such as user rating and review length, and learns the relations between reviews and corresponding responses in a supervised way from the available training data. Experiments on 58 apps and 309,246 review-response pairs highlight that RRGen outperforms the baselines by at least 67.4% in terms of BLEU-4 (an accuracy measure that is widely used to evaluate dialogue response generation systems). Qualitative analysis also confirms the effectiveness of RRGen in generating relevant and accurate responses.",App reviews;response generation;neural machine translation,"163, 175",,IEEE Conferences,2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
497,,"Discovering, Explaining and Summarizing Controversial Discussions in Community Q&A Sites",X. Ren; Z. Xing; X. Xia; G. Li; J. Sun,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952216,10.1109/ASE.2019.00024,"Developers often look for solutions to programming problems in community Q&A sites like Stack Overflow. Due to the crowdsourcing nature of these Q&A sites, many user-provided answers are wrong, less optimal or out-of-date. Relying on community-curated quality indicators (e.g., accepted answer, answer vote) cannot reliably identify these answer problems. Such problematic answers are often criticized by other users. However, these critiques are not readily discoverable when reading the posts. In this paper, we consider the answers being criticized and their critique posts as controversial discussions in community Q&A sites. To help developers notice such controversial discussions and make more informed choices of appropriate solutions, we design an automatic open information extraction approach for systematically discovering and summarizing the controversies in Stack Overflow and exploiting official API documentation to assist the understanding of the discovered controversies. We apply our approach to millions of java/android-tagged Stack overflow questions and answers and discover a large scale of controversial discussions in Stack Overflow. Our manual evaluation confirms that the extracted controversy information is of high accuracy. A user study with 18 developers demonstrates the usefulness of our generated controversy summaries in helping developers avoid the controversial answers and choose more appropriate solutions to programming questions.","Controversial discussion, Stack Overflow, Open information extraction, Sentence embedding","151, 162",,IEEE Conferences,2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
498,,Emotions Extracted from Text vs. True Emotions–An Empirical Evaluation in SE Context,Y. Wang,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952437,10.1109/ASE.2019.00031,"Emotion awareness research in SE context has been growing in recent years. Currently, researchers often rely on textual communication records to extract emotion states using natural language processing techniques. However, how well these extracted emotion states reflect people's real emotions has not been thoroughly investigated. In this paper, we report a multi-level, longitudinal empirical study with 82 individual members in 27 project teams. We collected their self-reported retrospective emotion states on a weekly basis during their year-long projects and also extracted corresponding emotions from the textual communication records. We then model and compare the dynamics of these two types of emotions using multiple statistical and time series analysis methods. Our analyses yield a rich set of findings. The most important one is that the dynamics of emotions extracted using text-based algorithms often do not well reflect the dynamics of self-reported retrospective emotions. Besides, the extracted emotions match self-reported retrospective emotions better at the team-level. Our results also suggest that individual personalities and the team's emotion display norms significantly impact the match/mismatch. Our results should warn the research community about the limitations and challenges of applying text-based emotion recognition tools in SE research.",emotion recognition;emotion dynamics;text based NLP techniques;time series analysis;personality;organizational norms,"230, 242",,IEEE Conferences,2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
499,,Property Inference for Deep Neural Networks,D. Gopinath; H. Converse; C. Pasareanu; A. Taly,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952519,10.1109/ASE.2019.00079,"We present techniques for automatically inferring formal properties of feed-forward neural networks. We observe that a significant part (if not all) of the logic of feed forward networks is captured in the activation status (on or off) of its neurons. We propose to extract patterns based on neuron decisions as preconditions that imply certain desirable output property e.g., the prediction being a certain class. We present techniques to extract input properties, encoding convex predicates on the input space that imply given output properties and layer properties, representing network properties captured in the hidden layers that imply the desired output behavior. We apply our techniques on networks for the MNIST and ACASXU applications. Our experiments highlight the use of the inferred properties in a variety of tasks, such as explaining predictions, providing robustness guarantees, simplifying proofs, and network distillation.",Deep Neural Networks;Explainability;Property Inference;Data Mining,"797, 809",,IEEE Conferences,2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
500,,Apricot: A Weight-Adaptation Approach to Fixing Deep Learning Models,H. Zhang; W. K. Chan,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952197,10.1109/ASE.2019.00043,"A deep learning (DL) model is inherently imprecise. To address this problem, existing techniques retrain a DL model over a larger training dataset or with the help of fault injected models or using the insight of failing test cases in a DL model. In this paper, we present Apricot, a novel weight-adaptation approach to fixing DL models iteratively. Our key observation is that if the deep learning architecture of a DL model is trained over many different subsets of the original training dataset, the weights in the resultant reduced DL model (rDLM) can provide insights on the adjustment direction and magnitude of the weights in the original DL model to handle the test cases that the original DL model misclassifies. Apricot generates a set of such reduced DL models from the original DL model. In each iteration, for each failing test case experienced by the input DL model (iDLM), Apricot adjusts each weight of this iDLM toward the average weight of these rDLMs correctly classifying the test case and/or away from that of these rDLMs misclassifying the same test case, followed by training the weight-adjusted iDLM over the original training dataset to generate a new iDLM for the next iteration. The experiment using five state-of-the-art DL models shows that Apricot can increase the test accuracy of these models by 0.87%-1.55% with an average of 1.08%. The experiment also reveals the complementary nature of these rDLMs in Apricot.",Deep Neural Networks;Optimization;Model Evolution;Debugging;Model Fixing;Model Repair,"376, 387",,IEEE Conferences,2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
501,,Machine Learning Based Recommendation of Method Names: How Far are We,L. Jiang; H. Liu; H. Jiang,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952208,10.1109/ASE.2019.00062,"High quality method names are critical for the readability and maintainability of programs. However, constructing concise and consistent method names is often challenging, especially for inexperienced developers. To this end, advanced machine learning techniques have been recently leveraged to recommend method names automatically for given method bodies/implementation. Recent large-scale evaluations also suggest that such approaches are accurate. However, little is known about where and why such approaches work or don't work. To figure out the state of the art as well as the rationale for the success/failure, in this paper we conduct an empirical study on the state-of-the-art approach code2vec. We assess code2vec on a new dataset with more realistic settings. Our evaluation results suggest that although switching to new dataset does not significantly influence the performance, more realistic settings do significantly reduce the performance of code2vec. Further analysis on the successfully recommended method names also reveals the following findings: 1) around half (48.3%) of the accepted recommendations are made on getter/setter methods; 2) a large portion (19.2%) of the successfully recommended method names could be copied from the given bodies. To further validate its usefulness, we ask developers to manually score the difficulty in naming methods they developed. Code2vec is then applied to such manually scored methods to evaluate how often it works in need. Our evaluation results suggest that code2vec rarely works when it is really needed. Finally, to intuitively reveal the state of the art and to investigate the possibility of designing simple and straightforward alternative approaches, we propose a heuristics based approach to recommending method names. Evaluation results on large-scale dataset suggest that this simple heuristics-based approach significantly outperforms the state-of-the-art machine learning based approach, improving precision and recall by 65.25% and 22.45%, respectively. The comparison suggests that machine learning based recommendation of method names may still have a long way to go.",Code Recommendation;Machine Learning,"602, 614",,IEEE Conferences,2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
502,,Assessing the Generalizability of Code2vec Token Embeddings,H. J. Kang; T. F. Bissyandé; D. Lo,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952475,10.1109/ASE.2019.00011,"Many Natural Language Processing (NLP) tasks, such as sentiment analysis or syntactic parsing, have benefited from the development of word embedding models. In particular, regardless of the training algorithms, the learned embeddings have often been shown to be generalizable to different NLP tasks. In contrast, despite recent momentum on word embeddings for source code, the literature lacks evidence of their generalizability beyond the example task they have been trained for. In this experience paper, we identify 3 potential downstream tasks, namely code comments generation, code authorship identification, and code clones detection, that source code token embedding models can be applied to. We empirically assess a recently proposed code token embedding model, namely code2vec's token embeddings. Code2vec was trained on the task of predicting method names, and while there is potential for using the vectors it learns on other tasks, it has not been explored in literature. Therefore, we fill this gap by focusing on its generalizability for the tasks we have identified. Eventually, we show that source code token embeddings cannot be readily leveraged for the downstream tasks. Our experiments even show that our attempts to use them do not result in any improvements over less sophisticated methods. We call for more research into effective and general use of code embeddings.",Code Embeddings;Distributed Representations;Big Code,"1, 12",,IEEE Conferences,2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
503,,ACTGAN: Automatic Configuration Tuning for Software Systems with Generative Adversarial Networks,L. Bao; X. Liu; F. Wang; B. Fang,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952456,10.1109/ASE.2019.00051,"Complex software systems often provide a large number of parameters so that users can configure them for their specific application scenarios. However, configuration tuning requires a deep understanding of the software system, far beyond the abilities of typical system users. To address this issue, many existing approaches focus on exploring and learning good performance estimation models. The accuracy of such models often suffers when the number of available samples is small, a thorny challenge under a given tuning-time constraint. By contrast, we hypothesize that good configurations often share certain hidden structures. Therefore, instead of trying to improve the performance estimation of a given configuration, we focus on capturing the hidden structures of good configurations and utilizing such learned structure to generate potentially better configurations. We propose ACTGAN to achieve this goal. We have implemented and evaluated ACTGAN using 17 workloads with eight different software systems. Experimental results show that ACTGAN outperforms default configurations by 76.22% on average, and six state-of-the-art configuration tuning algorithms by 6.58%-64.56%. Furthermore, the ACTGAN-generated configurations are often better than those used in training and show certain features consisting with domain knowledge, both of which supports our hypothesis.",software system;automatic configuration tuning;generative adversarial networks,"465, 476",,IEEE Conferences,2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
504,,RENN: Efficient Reverse Execution with Neural-Network-Assisted Alias Analysis,D. Mu; W. Guo; A. Cuevas; Y. Chen; J. Gai; X. Xing; B. Mao; C. Song,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952186,10.1109/ASE.2019.00090,"Reverse execution and coredump analysis have long been used to diagnose the root cause of software crashes. Each of these techniques, however, face inherent challenges, such as insufficient capability when handling memory aliases. Recent works have used hypothesis testing to address this drawback, albeit with high computational complexity, making them impractical for real world applications. To address this issue, we propose a new deep neural architecture, which could significantly improve memory alias resolution. At the high level, our approach employs a recurrent neural network (RNN) to learn the binary code pattern pertaining to memory accesses. It then infers the memory region accessed by memory references. Since memory references to different regions naturally indicate a non-alias relationship, our neural architecture can greatly reduce the burden of doing hypothesis testing to track down non-alias relation in binary code. Different from previous researches that have utilized deep learning for other binary analysis tasks, the neural network proposed in this work is fundamentally novel. Instead of simply using off-the-shelf neural networks, we designed a new recurrent neural architecture that could capture the data dependency between machine code segments. To demonstrate the utility of our deep neural architecture, we implement it as RENN, a neural network-assisted reverse execution system. We utilize this tool to analyze software crashes corresponding to 40 memory corruption vulnerabilities from the real world. Our experiments show that RENN can significantly improve the efficiency of locating the root cause for the crashes. Compared to a state-of-the-art technique, RENN has 36.25% faster execution time on average, detects an average of 21.35% more non-alias pairs, and successfully identified the root cause of 12.5% more cases.","Reverse Execution, Deep Learning, Memory Alias","924, 935",,IEEE Conferences,2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
505,,CLCDSA: Cross Language Code Clone Detection using Syntactical Features and API Documentation,K. W. Nafi; T. S. Kar; B. Roy; C. K. Roy; K. A. Schneider,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952189,10.1109/ASE.2019.00099,"Software clones are detrimental to software maintenance and evolution and as a result many clone detectors have been proposed. These tools target clone detection in software applications written in a single programming language. However, a software application may be written in different languages for different platforms to improve the application's platform compatibility and adoption by users of different platforms. Cross language clones (CLCs) introduce additional challenges when maintaining multi-platform applications and would likely go undetected using existing tools. In this paper, we propose CLCDSA, a cross language clone detector which can detect CLCs without extensive processing of the source code and without the need to generate an intermediate representation. The proposed CLCDSA model analyzes different syntactic features of source code across different programming languages to detect CLCs. To support large scale clone detection, the CLCDSA model uses an action filter based on cross language API call similarity to discard non-potential clones. The design methodology of CLCDSA is two-fold: (a) it detects CLCs on the fly by comparing the similarity of features, and (b) it uses a deep neural network based feature vector learning model to learn the features and detect CLCs. Early evaluation of the model observed an average precision, recall and F-measure score of 0.55, 0.86, and 0.64 respectively for the first phase and 0.61, 0.93, and 0.71 respectively for the second phase which indicates that CLCDSA outperforms all available models in detecting cross language clones.",Code Clone;API documentation;Word2Vector;Source Code Syntax,"1026, 1037",,IEEE Conferences,2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
506,,Automatic Generation of Graphical User Interface Prototypes from Unrestricted Natural Language Requirements,K. Kolthoff,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952477,10.1109/ASE.2019.00148,"High-fidelity GUI prototyping provides a meaningful manner for illustrating the developers' understanding of the requirements formulated by the customer and can be used for productive discussions and clarification of requirements and expectations. However, high-fidelity prototypes are time-consuming and expensive to develop. Furthermore, the interpretation of requirements expressed in informal natural language is often error-prone due to ambiguities and misunderstandings. In this dissertation project, we will develop a methodology based on Natural Language Processing (NLP) for supporting GUI prototyping by automatically translating Natural Language Requirements (NLR) into a formal Domain-Specific Language (DSL) describing the GUI and its navigational schema. The generated DSL can be further translated into corresponding target platform prototypes and directly provided to the user for inspection. Most related systems stop after generating artifacts, however, we introduce an intelligent and automatic interaction mechanism that allows users to provide natural language feedback on generated prototypes in an iterative fashion, which accordingly will be translated into respective prototype changes.",Graphical;User;Interface;Automatic;GUI;Generation;Processing;Natural;Language;Requirements;Intelligent;Interaction;Prototyping,"1234, 1237",,IEEE Conferences,2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
507,,Continuous Incident Triage for Large-Scale Online Service Systems,J. Chen; X. He; Q. Lin; H. Zhang; D. Hao; F. Gao; Z. Xu; Y. Dang; D. Zhang,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952483,10.1109/ASE.2019.00042,"In recent years, online service systems have become increasingly popular. Incidents of these systems could cause significant economic loss and customer dissatisfaction. Incident triage, which is the process of assigning a new incident to the responsible team, is vitally important for quick recovery of the affected service. Our industry experience shows that in practice, incident triage is not conducted only once in the beginning, but is a continuous process, in which engineers from different teams have to discuss intensively among themselves about an incident, and continuously refine the incident-triage result until the correct assignment is reached. In particular, our empirical study on 8 real online service systems shows that the percentage of incidents that were reassigned ranges from 5.43% to 68.26% and the number of discussion items before achieving the correct assignment is up to 11.32 on average. To improve the existing incident triage process, in this paper, we propose DeepCT, a Deep learning based approach to automated Continuous incident Triage. DeepCT incorporates a novel GRU-based (Gated Recurrent Unit) model with an attention-based mask strategy and a revised loss function, which can incrementally learn knowledge from discussions and update incident-triage results. Using DeepCT, the correct incident assignment can be achieved with fewer discussions. We conducted an extensive evaluation of DeepCT on 14 large-scale online service systems in Microsoft. The results show that DeepCT is able to achieve more accurate and efficient incident triage, e.g., the average accuracy identifying the responsible team precisely is 0.641~0.729 with the number of discussion items increasing from 1 to 5. Also, DeepCT statistically significantly outperforms the state-of-the-art bug triage approach.",Incident Triage;Online Service Systems;Deep Learning,"364, 375",,IEEE Conferences,2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
508,,CocoQa: Question Answering for Coding Conventions Over Knowledge Graphs,T. Du; J. Cao; Q. Wu; W. Li; B. Shen; Y. Chen,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952314,10.1109/ASE.2019.00108,"Coding convention plays an important role in guaranteeing software quality. However, coding conventions are usually informally presented and inconvenient for programmers to use. In this paper, we present CocoQa, a system that answers programmer's questions about coding conventions. CocoQa answers questions by querying a knowledge graph for coding conventions. It employs 1) a subgraph matching algorithm that parses the question into a SPARQL query, and 2) a machine comprehension algorithm that uses an end-to-end neural network to detect answers from searched paragraphs. We have implemented CocoQa, and evaluated it on a coding convention QA dataset. The results show that CocoQa can answer questions about coding conventions precisely. In particular, CocoQa can achieve a precision of 82.92% and a recall of 91.10%. Repository: https://github.com/14dtj/CocoQa/ Video: https://youtu.be/VQaXi1WydAU.",Coding convention;question answering;knowledge graph,"1086, 1089",,IEEE Conferences,2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
509,,Predicting Rate Control Target Through A Learning Based Content Adaptive Model,H. Xing; Z. Zhou; J. Wang; H. Shen; D. He; F. Li,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8954541,10.1109/PCS48520.2019.8954541,"Rate Control (RC) plays an important role in video encoding. Traditional solutions are using fixed rate or fixed quantization parameters as the unified rate-control targets for all videos in one given video application. However, unified rate-control targets tend to have some bad encoding cases because of applying wrong rate for the video content. In this paper, we propose one content-adaptive rate control solution. We employ one neural-network based model which can end-to-end learn the optimal rate-control target appropriate to the content characteristics. The experimental results show that the proposed model can predict the optimal rate-factor value with the accuracy up to 77.637%. With this model, the proposed video-encoding method can significantly decrease the encoding quality fluctuation.",,"1, 5",,IEEE Conferences,2019 Picture Coding Symposium (PCS),IEEE
510,,Bypassing Depth Maps Transmission For Immersive Video Coding,P. Garus; J. Jung; T. Maugey; C. Guillemot,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8954543,10.1109/PCS48520.2019.8954543,"This paper addresses several downsides of the system under development in MPEG-I for coding and transmission of immersive media. We present a solution, which enables Depth-Image-Based Rendering for immersive video applications, while lifting the requirement of transmitting depth information. Instead, we estimate the depth information on the client-side from the transmitted views. The approach leads to an impressive rate saving (37.3% in average). Preserving perceptual quality in terms of MS-SSIM of synthesized views, it yields to 24.6% rate reduction for the same quality of reconstructed views after residue transmission under the MPEG-I common test conditions. Simultaneously, the required pixel rate, i.e. the number of pixels processed per second by the decoder, is reduced by 50% for any test sequence. To the author's knowledge, this is the first time that such an approach is under consideration in the context of immersive video coding.",MPEG;immersive video coding;depth estimation,"1, 5",,IEEE Conferences,2019 Picture Coding Symposium (PCS),IEEE
511,,Deep Scalable Image Compression via Hierarchical Feature Decorrelation,Z. Guo; Z. Zhang; Z. Chen,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8954536,10.1109/PCS48520.2019.8954536,"Scalable image compression allows reconstructing complete images through partially decoding. It plays an important role for image transmission and storage. In this paper, we study the problem of feature decorrelation for Deep Neural Network (DNN) based image codec. Inspired by self-attention mechanism [1], we design a transformer-based decorrelation unit (DU) and adopt it in our scalable image compression framework to reduce the redundancy of feature representations at different levels. Experimental results demonstrate that proposed framework outperforms the state-of-the-art DNN-based scalable image codec and conventional scalable image codecs in terms of MS-SSIM. We also conduct ablation experiments which explicitly verify the effectiveness of decorrelation unit in our scheme.",image compression;scalable coding;feature decorrelation,"1, 5",,IEEE Conferences,2019 Picture Coding Symposium (PCS),IEEE
512,,A Novel Deep Progressive Image Compression Framework,C. Cai; L. Chen; X. Zhang; G. Lu; Z. Gao,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8954500,10.1109/PCS48520.2019.8954500,"In Internet applications, compressing the image without perceptually distinguishable distortions and loading the images without notable delays in the client end can significantly improve the user experience. Compressing the image at high bit rates can maintain the high quality of the decoded image but in cost of long transmitting and decoding time, resulting in bad user experience. The progressive coding scheme can resolve the conflict between the high quality requirement and the large loading delay. This paper proposes a novel efficient progressive image coding framework based on deep convolutional neural networks. The proposed framework is composed of a uniform encoder network and two progressive decoder networks. The encoder network decomposes the input image into two scales of representations, that can be transmitted and reconstructed progressively into a basic quality preview image and a high-quality image by two individual decoder networks respectively. All the networks are jointly learned when achieving the rate distortion optimization of both scales. Experiments results show that the proposed method has much better coding performance than the commercial codecs WebP and JPEG, which are commonly used in Internet applications. Meanwhile, the proposed codec consumes much less time to load the image compared with WebP.",High quality image compression;Progressive image coding;Convolutional neural networks;Rate distortion optimization,"1, 5",,IEEE Conferences,2019 Picture Coding Symposium (PCS),IEEE
513,,Visibility Metric for Visually Lossless Image Compression,N. Ye; M. Pérez-Ortiz; R. K. Mantiuk,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8954560,10.1109/PCS48520.2019.8954560,"Encoding images in a visually lossless manner helps to achieve the best trade-off between image compression performance and quality and so that compression artifacts are invisible to the majority of users. Visually lossless encoding can often be achieved by manually adjusting compression quality parameters of existing lossy compression methods, such as JPEG or WebP. But the required compression quality parameter can also be determined automatically using visibility metrics. However, creating an accurate visibility metric is challenging because of the complexity of the human visual system and the effort needed to collect the required data. In this paper, we investigate how to train an accurate visibility metric for visually lossless compression from a relatively small dataset. Our experiments show that prediction error can be reduced by 40% compared with the state-of-theart, and that our proposed method can save between 25%-75% of storage space compared with the default quality parameter used in commercial software. We demonstrate how the visibility metric can be used for visually lossless image compression and for benchmarking image compression encoders.",Visually lossless image compression;visibility metric;deep learning;transfer learning,"1, 5",,IEEE Conferences,2019 Picture Coding Symposium (PCS),IEEE
514,,Beyond Coding: Detection-driven Image Compression with Semantically Structured Bit-stream,T. He; S. Sun; Z. Guo; Z. Chen,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8954525,10.1109/PCS48520.2019.8954525,"With the development of 5G and edge computing, it is increasingly important to offload intelligent media computing to edge device. Traditional media coding scheme codes the media into one binary stream without a semantic structure, which prevents many important intelligent applications from operating directly in bit-stream level, including semantic analysis, parsing specific content, media editing, etc. Therefore, in this paper, we propose a learning based Semantically Structured Coding (SSC) framework to generate Semantically Structured Bit-stream (SSB), where each part of bit-stream represents a certain object and can be directly used for aforementioned tasks. Specifically, we integrate an object detection module in our compression framework to locate and align the object in feature domain. After applying quantization and entropy coding, the features are re-organized according to detected and aligned objects to form a bit-stream. Besides, different from existing learning-based compression schemes that individually train models for specific bit-rate, we share most of model parameters among various bit-rates to significantly reduce model size for variable-rate compression. Experimental results demonstrate that only at the cost of negligible overhead, objects can be completely reconstructed from partial bit-stream. We also verified that classification and pose estimation can be directly performed on partial bit-stream without performance degradation.",compression;semantically structured bit-stream;neural networks,"1, 5",,IEEE Conferences,2019 Picture Coding Symposium (PCS),IEEE
515,,Work-in-Progress: EAST-DNN: Expediting Architectural SimulaTions Using Deep Neural Networks,A. Dutt; G. Narasimman; L. Jie; V. R. Chandrasekhar; M. M. Sabry,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8949906,10.1145/3349567.3351728,"A rapid and accurate architectural simulator is a cornerstone for an emcient design-space exploration of computing systems. In this paper, we introduce EAST-DNN, a feed-forward deep neural network, to accelerate architectural simulations. EAST-DNN achieves > 10<sup>6</sup>× speedup with an average prediction error of 4.3% over the baseline simulator. It also achieves an average of 2× better accuracy with at least 2.3× speedup compared to state-of-the-art.",Deep Neural Network;Hardware Accelerators,"1, 2",,IEEE Conferences,2019 International Conference on Hardware/Software Codesign and System Synthesis (CODES+ISSS),IEEE
516,,Work-in-Progress: BPNet: Branch-pruned Conditional Neural Network for Systematic Time-accuracy Tradeoff in DNN Inference,K. Park; Y. Yi,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8949902,10.1145/3349567.3351721,"Recently, there have been attempts to execute the neural network conditionally with auxiliary classifiers allowing early termination depending on the difficulty of the input, which can reduce the execution time or energy consumption without any or with negligible accuracy decrease. However, these studies do not consider how many or where the auxiliary classifiers, or branches, should be added in a systematic fashion. In this paper, we propose Branch-pruned Conditional Neural Network (BPNet) and its methodology in which the time-accuracy tradeoff for the conditional neural network can be found systematically. We applied BPNet to SqueezeNet, ResNet-20, and VGG-16 with CIFAR-10 and 100. BPNet achieves on average 2.0× of speedups without any accuracy drop on average compared to the base network.",,"1, 2",,IEEE Conferences,2019 International Conference on Hardware/Software Codesign and System Synthesis (CODES+ISSS),IEEE
517,,Deep Semantic Feature Learning with Embedded Static Metrics for Software Defect Prediction,G. Fan; X. Diao; H. Yu; K. Yang; L. Chen,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8946058,10.1109/APSEC48747.2019.00041,"Software defect prediction, which locates defective code snippets, can assist developers in finding potential bugs and assigning their testing efforts. Traditional defect prediction features are static code metrics, which only contain statistic information of programs and fail to capture semantics in programs, leading to the degradation of defect prediction performance. To take full advantage of the semantics and static metrics of programs, we propose a framework called Defect Prediction via Attention Mechanism (DP-AM) in this paper. Specifically, DPAM first extracts vectors which are then encoded as digital vectors by mapping and word embedding from abstract syntax trees (ASTs) of programs. Then it feeds these numerical vectors into Recurrent Neural Network to automatically learn semantic features of programs. After that, it applies self-attention mechanism to further build relationship among these features. Furthermore, it employs global attention mechanism to generate significant features among them. Finally, we combine these semantic features with traditional static metrics for accurate software defect prediction. We evaluate our method in terms of F1-measure on seven open-source Java projects in Apache. Our experimental results show that DP-AM improves F1-measure by 11% in average, compared with the state-of-the-art methods.",software defect prediction;Abstract Syntax Trees;static code metrics;global attention mechanism;self-attention mechanism,"244, 251",,IEEE Conferences,2019 26th Asia-Pacific Software Engineering Conference (APSEC),IEEE
518,,Anatomizing Android Malwares,A. Tirkey; R. K. Mohapatra; L. Kumar,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8945606,10.1109/APSEC48747.2019.00067,"Android OS being the popular choice of majority users also faces the constant risk of breach of confidentiality, integrity and availability (CIA). Effective mitigation efforts needs to identified in order to protect and uphold the CIA triad model, within the android ecosystem. In this paper, we propose a novel method of android malware classification using Object-Oriented Software Metrics and machine learning algorithms. First, android apps are decompiled and Object-Oriented Metrics are obtained. VirusTotal service is used to tag an app either as malware or benign. Object-Oriented Metrics and malware tag are clubbed together into a dataset. Eighty different machine-learned models are trained over five thousand seven hundred and seventy four android apps. We evaluate the performance and stability of these models using it's malware classification accuracy and AUC (area under ROC curve) values. Our method yields an accuracy and AUC of 99.83% and 1.0 respectively.",android;malware detection;machine learning;object oriented metrics,"450, 457",,IEEE Conferences,2019 26th Asia-Pacific Software Engineering Conference (APSEC),IEEE
519,,Detecting Duplicate Questions in Stack Overflow via Deep Learning Approaches,L. Wang; L. Zhang; J. Jiang,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8945690,10.1109/APSEC48747.2019.00074,"Stack Overflow is a popular question and answer website based on the software programming. Different users often ask the same questions in different ways, resulting in a large number of duplicate questions in Stack Overflow. Generally, the users with high reputation manually analyze and mark duplicate questions, which is time consuming and low efficiency. Therefore, the automatic duplicate question detection approach is demanded. We first investigate the application of deep learning models to software engineering task. Then, three deep learning models (i.e., CNN, RNN and LSTM) are applied to demonstrate whether they are effective to duplicate question detection task in Stack Overflow. In this paper, we explore three deep learning approaches DQ-CNN, DQ-RNN and DQ-LSTM based on CNN, RNN and LSTM to detect duplicate questions. The effectiveness of DQ-CNN, DQ-RNN and DQ-LSTM is evaluated by six different question groups. The experimental results show that DQ-LSTM outperforms DupPredictor, Dupe, DupePredictorRep-T and DupeRep in terms of recall-rate@5, recall-rate@10 and recall-rate@20 except for Ruby question group.","Stack Overflow, duplicate questions, CNN, RNN, LSTM","506, 513",,IEEE Conferences,2019 26th Asia-Pacific Software Engineering Conference (APSEC),IEEE
520,,Implantation Process of Enterprise IT Application in a Medium-Sized Enterprise,I. Reascos Paredes; J. A. Carvalho,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8940370,10.1109/ICI2ST.2019.00021,"Nowadays, for micro, small, and medium enterprises (SMEs), acquiring generic enterprise IT applications existing in the market, it is more convenient than developing a custom one. Now, the challenge is to search, select, hire, implanting (go-live) and provide continuous maintenance of the enterprise IT application that meets the needs of the enterprise. The problem is the lack of methodologies that cover this process (especially for SMEs), and those that exist are rarely available to the public. The objective of this article is to identify the process of implantation of an enterprise IT application in medium-sized enterprises. For this, an exploratory case study was carried out in an enterprise that changed the application. In this study, we interview the personnel involved in the implantation process and analyse the documentation generated. The result of this exploratory case study identifies the main motivations, difficulties and lessons learned. Also, emerges an implantation process of an enterprise IT application. This process consists of three phases (pre-implantation, implantation, and post-implantation) and a set of cross-cutting concerns (leadership & communication, change management, and project management). The phases have 13 stages (pre-implantation - 5, implantation - 6, and post-implantation - 2). This article ends with a discussion of it.",implantation;implementation;enterprise application;case study;ERP;CRM;SME,"100, 107",,IEEE Conferences,2019 International Conference on Information Systems and Software Technologies (ICI2ST),IEEE
521,,Development of Machine Learning Model for Mobile Advanced Driver Assistance (ADA),Y. Torres-Berru; P. Torres-Carrion,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8940428,10.1109/ICI2ST.2019.00030,"Every day around 3500 people die on the roads and tens of millions suffer injuries or disabilities each year; vehicle manufacturers spend millions of dollars annually on research for the prevention of traffic accidents in development of Advanced Driving Assistance Systems (ADAS). To solve this problem, the development of a model based on decision trees and supervised neural networks is proposed; it is focusing on the four main risk factors of traffic accidents: a) drive inattentive to traffic conditions, b) not respecting the regulatory traffic signals, c) driving in a state of drowsiness or poor physical condition, d) drive vehicle over the Maximum Speed Limits. The Machine Learning system requires three models: a) neural network for object recognition; b) algorithm for sleepiness detection; and, c) decision tree to issue factor-related alerts: driver status (sleepiness or distraction), speed and traffic signal alert. Finally, the model is put into production in a low cost ADAS mobile application, and it was tested in the laboratory using recordings obtained in real driving tests (n = 200, success>=90%) in ideal luminosity environments.","ADAS, Artificial Vision, Artificial Neural Networks, Machine Learning","162, 167",,IEEE Conferences,2019 International Conference on Information Systems and Software Technologies (ICI2ST),IEEE
522,,Toward Optimal Selection of Information Retrieval Models for Software Engineering Tasks,M. M. Rahman; S. Chakraborty; G. Kaiser; B. Ray,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8930841,10.1109/SCAM.2019.00022,"Information Retrieval (IR) plays a pivotal role in diverse Software Engineering (SE) tasks, e.g., bug localization and triaging, bug report routing, code retrieval, requirements analysis, etc. SE tasks operate on diverse types of documents including code, text, stack-traces, and structured, semi-structured and unstructured meta-data that often contain specialized vocabularies. As the performance of any IR-based tool critically depends on the underlying document types, and given the diversity of SE corpora, it is essential to understand which models work best for which types of SE documents and tasks. We empirically investigate the interaction between IR models and document types for two representative SE tasks (bug localization and relevant project search), carefully chosen as they require a diverse set of SE artifacts (mixtures of code and text), and confirm that the models' performance varies significantly with mix of document types. Leveraging this insight, we propose a generalized framework, SRCH, to automatically select the most favorable IR model(s) for a given SE task. We evaluate SRCH w.r.t. these two tasks and confirm its effectiveness. Our preliminary user study shows that SRCH's intelligent adaption of the IR model(s) to the task at hand not only improves precision and recall for SE tasks but may also improve users' satisfaction.",Information retrieval;IR metrics;project recommendation;bug localization,"127, 138",,IEEE Conferences,2019 19th International Working Conference on Source Code Analysis and Manipulation (SCAM),IEEE
523,,Performance-Influence Model for Highly Configurable Software with Fourier Learning and Lasso Regression,H. Ha; H. Zhang,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8919029,10.1109/ICSME.2019.00080,"Many software systems are highly configurable, which provide a large number of configuration options for users to choose from. During the maintenance and operation of these configurable systems, it is important to estimate the system performance under any specific configurations and understand the performance-influencing configuration options. However, it is often not feasible to measure the system performance under all the possible configurations as the combination of configurations could be exponential. In this paper, we propose PerLasso, a performance modeling and prediction method based on Fourier Learning and Lasso (Least absolute shrinkage and selection operator) regression techniques. Using a small sample of measured performance values of a configurable system, PerLasso produces a performance-influence model, which can 1) predict system performance under a new configuration; 2) explain the influence of the individual features and their interactions on the software performance. Besides, to reduce the number of Fourier coefficients to be estimated for large-scale systems, we also design a novel dimension reduction algorithm. Our experimental results on four synthetic and six real-world datasets confirm the effectiveness of our approach. Compared to the existing performance-influence models, our models have higher or comparable prediction accuracy.",performance influence model;software performance prediction;configurable systems,"470, 480",,IEEE Conferences,2019 IEEE International Conference on Software Maintenance and Evolution (ICSME),IEEE
524,,Synthesizing Program Execution Time Discrepancies in Julia Used for Scientific Software,E. Farhana; N. Imtiaz; A. Rahman,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8918949,10.1109/ICSME.2019.00083,"Scientific software is defined as software that is used to analyze data to investigate unanswered research questions in the scientific community. Developers use programming languages such as Julia to build scientific software. When programming with Julia, developers experience program execution time discrepancy i.e. not obtaining desired program execution time, which hinders them to efficiently complete their tasks. The goal of this paper is to help developers in achieving desired program execution time for Julia by identifying the causes of why program execution time discrepancies happen with an empirical study of Stack Overflow posts. We conduct an empirical study with 263 Julia-related posts collected from Stack Overflow, and apply qualitative analysis on the collected 263 posts. We identify 9 categories of program execution time discrepancies for Julia, which include discrepancies related to data structures usage such as, arrays and dictionaries. We also identify 10 causes that explain why the program execution time discrepancies happen. For example, we identify program execution time discrepancy to happen when developers unnecessarily allocate memory by using array comprehension.","Julia, programming language, stack overflow","496, 500",,IEEE Conferences,2019 IEEE International Conference on Software Maintenance and Evolution (ICSME),IEEE
525,,Ticket Tagger: Machine Learning Driven Issue Classification,R. Kallis; A. Di Sorbo; G. Canfora; S. Panichella,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8918993,10.1109/ICSME.2019.00070,"Software maintenance is crucial for software projects evolution and success: code should be kept up-to-date and error-free, this with little effort and continuous updates for the end-users. In this context, issue trackers are essential tools for creating, managing and addressing the several (often hundreds of) issues that occur in software systems. A critical aspect for handling and prioritizing issues involves the assignment of labels to them (e.g., for projects hosted on GitHub), in order to determine the type (e.g., bug report, feature request and so on) of each specific issue. Although this labeling process has a positive impact on the effectiveness of issue processing, the current labeling mechanism is scarcely used on GitHub. In this demo, we introduce a tool, called Ticket Tagger, which leverages machine learning strategies on issue titles and descriptions for automatically labeling GitHub issues. Ticket Tagger automatically predicts the labels to assign to issues, with the aim of stimulating the use of labeling mechanisms in software projects, this to facilitate the issue management and prioritization processes. Along with the presentation of the tool's architecture and usage, we also evaluate its effectiveness in performing the issue labeling/classification process, which is critical to help maintainers to keep control of their workloads by focusing on the most critical issue tickets.",Software maintenance and evolution;Issue Processing;Unstructured Data Labeling,"406, 409",,IEEE Conferences,2019 IEEE International Conference on Software Maintenance and Evolution (ICSME),IEEE
526,,Learning to Identify Security-Related Issues Using Convolutional Neural Networks,D. N. Palacio; D. McCrystal; K. Moran; C. Bernal-Cárdenas; D. Poshyvanyk; C. Shenefiel,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8919098,10.1109/ICSME.2019.00024,"Software security is becoming a high priority for both large companies and start-ups alike due to the increasing potential for harm that vulnerabilities and breaches carry with them. However, attaining robust security assurance while delivering features requires a precarious balancing act in the context of agile development practices. One path forward to help aid development teams in securing their software products is through the design and development of security-focused automation. Ergo, we present a novel approach, called SecureReqNet, for automatically identifying whether issues in software issue tracking systems describe security-related content. Our approach consists of a two-phase neural net architecture that operates purely on the natural language descriptions of issues. The first phase of our approach learns high dimensional word embeddings from hundreds of thousands of vulnerability descriptions listed in the CVE database and issue descriptions extracted from open source projects. The second phase then utilizes the semantic ontology represented by these embeddings to train a convolutional neural network capable of predicting whether a given issue is security-related. We evaluated SecureReqNet by applying it to identify security-related issues from a dataset of thousands of issues mined from popular projects on GitLab and GitHub. In addition, we also applied our approach to identify security-related requirements from a commercial software project developed by a major telecommunication company. Our preliminary results are encouraging, with SecureReqNet achieving an accuracy of 96% on open source issues and 71.6% on industrial requirements.",deep learning;cnn;security issues;sdl,"140, 144",,IEEE Conferences,2019 IEEE International Conference on Software Maintenance and Evolution (ICSME),IEEE
527,,Tracing with Less Data: Active Learning for Classification-Based Traceability Link Recovery,C. Mills; J. Escobar-Avila; A. Bhattacharya; G. Kondyukov; S. Chakraborty; S. Haiduc,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8919048,10.1109/ICSME.2019.00020,"Previous work has established both the importance and difficulty of establishing and maintaining adequate software traceability. While it has been shown to support essential maintenance and evolution tasks, recovering traceability links between related software artifacts is a time consuming and error prone task. As such, substantial research has been done to reduce this barrier to adoption by at least partially automating traceability link recovery. In particular, recent work has shown that supervised machine learning can be effectively used for automating traceability link recovery, as long as there is sufficient data (i.e., labeled traceability links) to train a classification model. Unfortunately, the amount of data required by these techniques is a serious limitation, given that most software systems rarely have traceability information to begin with. In this paper we address this limitation of previous work and propose an approach based on active learning, which substantially reduces the amount of training data needed by supervised classification approaches for traceability link recovery while maintaining similar performance.",software traceability;machine learning;active learning;classification,"103, 113",,IEEE Conferences,2019 IEEE International Conference on Software Maintenance and Evolution (ICSME),IEEE
528,,Losing Confidence in Quality: Unspoken Evolution of Computer Vision Services,A. Cummaudo; R. Vasa; J. Grundy; M. Abdelrazek; A. Cain,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8919078,10.1109/ICSME.2019.00051,The following topics are dealt with: software maintenance; public domain software; program testing; source code (software); program debugging; software quality; program diagnostics; learning (artificial intelligence); mobile computing; data mining.,machine learning;intelligent service;computer vision;quality assurance;evolution risk;documentation,"333, 342",,IEEE Conferences,2019 IEEE International Conference on Software Maintenance and Evolution (ICSME),IEEE
529,,Deep Learning Anti-Patterns from Code Metrics History,A. Barbez; F. Khomh; Y. Guéhéneuc,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8919037,10.1109/ICSME.2019.00021,"Anti-patterns are poor solutions to recurring design problems. Number of empirical studies have highlighted the negative impact of anti-patterns on software maintenance which motivated the development of various detection techniques. Most of these approaches rely on structural metrics of software systems to identify affected components while others exploit historical information by analyzing co-changes occurring between code components. By relying solely on one aspect of software systems (i.e., structural or historical), existing approaches miss some precious information which limits their performances. In this paper, we propose CAME (Convolutional Analysis of code Metrics Evolution), a deep-learning based approach that relies on both structural and historical information to detect anti-patterns. Our approach exploits historical values of structural code metrics mined from version control systems and uses a Convolutional Neural Network classifier to infer the presence of anti-patterns from this information. We experiment our approach for the widely know God Class anti-pattern and evaluate its performances on three software systems. With the results of our study, we show that: (1) using historical values of source code metrics allows to increase the precision; (2) CAME outperforms existing static machine-learning classifiers; and (3) CAME outperforms existing detection tools.",Anti-patterns;Deep learning;Mining Software Repositories,"114, 124",,IEEE Conferences,2019 IEEE International Conference on Software Maintenance and Evolution (ICSME),IEEE
530,,Know-How in Programming Tasks: From Textual Tutorials to Task-Oriented Knowledge Graph,J. Sun; Z. Xing; R. Chu; H. Bai; J. Wang; X. Peng,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8919028,10.1109/ICSME.2019.00039,"Accomplishing a program task usually involves performing multiple activities in a logical order. Task-solving activities may have different relationships, such as subactivityof, precede-follow, and different attributes, such as location, condition, API, code. We refer to task-solving activities and their relationships and attributes as know-how knowledge. Programming task know-how knowledge is commonly documented in semi-structured textual tutorials. A formative study of the 20 top-viewed Android-tagged how-to questions on Stack Overflow suggests that developers are faced with three information barriers (incoherent modeling of task intent, tutorial information overload and unstructured task activity description) for effectively discovering and understanding task-solving knowledge in textual tutorials. Knowledge graph has been shown to be effective in representing relational knowledge and supporting knowledge search in a structured way. Unfortunately, existing knowledge graphs extract only know-what information (e.g., APIs, API caveats and API dependencies) from software documentation. In this paper, we devise open information extraction (OpenIE) techniques to extract candidates for task activities, activity attributes and activity relationships from programming task tutorials. The resulting knowledge graph, TaskKG, includes a hierarchical taxonomy of activities, three types of activities relationships and five types of activity attributes, and enables activity-centric knowledge search. As a proof-of-concept, we apply our approach to Android Developer Guide. A comprehensive evaluation of TaskKG shows high accuracy of our OpenIE techniques. A user study shows that TaskKG is promising in helping developers finding correct answers to programming how-to questions.",Knowledge Graph;Task Search;Programming Tutorials,"257, 268",,IEEE Conferences,2019 IEEE International Conference on Software Maintenance and Evolution (ICSME),IEEE
531,,Approaches to Enhancing Transfer of Training using Adaptive Instructional Systems,J. T. Folsom-Kovarik; B. Mostafavi; R. A. Sottilare; I. Davidson; R. Perez; P. B. Walker,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8903774,10.23919/SOFTCOM.2019.8903774,"This paper examines approaches to improve training technology design with the goal of enhancing transfer of training, the effect that knowledge and skills acquired during training have on problem solving or knowledge acquisition in work or operational environments. Specifically, we address the design of adaptive instructional systems (AISs), a category of learning technology that includes intelligent tutoring systems (ITSs), intelligent mentors and other recommender systems that tailor instruction to more efficiently meet the needs of individual learners and teams. Since tailored training requires more content and instructional decisions and therefore more authoring time than non-tailored computer-based training, we also examine how AIS design and transfer of training might be influenced by AIS authoring processes and interfaces. Design challenges are also discussed and include factors such as the ease of authoring that influence transfer of training.",adaptive instructional systems (AISs);intelligent tutoring systems (ITSs);transfer of learning;transfer of training,"1, 6",,IEEE Conferences,"2019 International Conference on Software, Telecommunications and Computer Networks (SoftCOM)",IEEE
532,,Automatic Categorization of Educational Videos According to Learning Styles,M. A. Ciurez; M. C. Mihaescu; M. Giménez; S. Heras; J. Palanca; V. Julian,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8903601,10.23919/SOFTCOM.2019.8903601,"A learning style consists of a series of personal characteristics with which a person is born and develops as they grow. It determines, among other things, through which activities and senses a person tends to absorb information more efficiently. Learning styles can be used in the process of recommending those Learning Objects (LO) most appropriate to the student's learning style. In this paper, a method to automatically tag videos from a repository of LOs is proposed. The idea is to classify, applying machine learning techniques, a dataset of LOs for the different learning styles with which students are identified. The proposed approach is being evaluated using a repository of videos from the Universitat Politècnica de València.",learning objects;learning styles;automatic video classification,"1, 6",,IEEE Conferences,"2019 International Conference on Software, Telecommunications and Computer Networks (SoftCOM)",IEEE
533,,Superpixel Segmentation of Breast Cancer Pathology Images Based on Features Extracted from the Autoencoder,J. Zhou; J. Ruan; C. Wu; G. Ye; Z. Zhu; J. Yue; Y. Zhang,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8905358,10.1109/ICCSN.2019.8905358,"In order to identify the breast cancer region, it is necessary to discriminate the pathological image of breast cancer pixel by pixel. This is a very huge work for machine learning. Therefore, the preprocessing of superpixel segmentation of breast cancer pathology images is necessary for reducing the number of pixels that need to be discriminated. In this paper: 1. We have trained serveral kinds of autoencoder networks and evaluated their performance in images clustering. 2. In order to enhance the image clustering effect, we adopt the clustering loss which is defined in Deep Embedded Clustering (DEC). 3. In order to enhance the ability of neural networks of extracting features, we added inception-like block, Sequeeze and Excitation (SE) block to the network. 4. We improved the performance of current Simple Linear Iterative Clustering (SLIC) algorithm to achieve superpixel segmentation of high-dimensional features.",autoencoder;feature extraction;SLIC,"366, 370",,IEEE Conferences,2019 IEEE 11th International Conference on Communication Software and Networks (ICCSN),IEEE
534,,Video Data Hierarchical Retrieval via Deep Hash Method,Y. Suo; C. Zhang; X. Xi; X. Wang; Z. Zou,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8905258,10.1109/ICCSN.2019.8905258,"Video retrieval technology faces a series of challenges with the tremendous growth in the number of videos. In order to improve the retrieval performance in efficiency and accuracy, a novel deep hash method for video data hierarchical retrieval is proposed in this paper. The approach first uses cluster-based method to extract key frames, which reduces the workload of subsequent work. On the basis of this, high-level semantical features are extracted from VGG16, a widely used deep convolutional neural network (deep CNN) model. Then we utilize a hierarchical retrieval strategy to improve the retrieval performance, roughly can be categorized as coarse search and fine search. In coarse search, we modify simHash to learn hash codes for faster speed, and in fine search, we use the Euclidean distance to achieve higher accuracy. Finally, we compare our approach with other two methods through practical experiments on two videos, and the results demonstrate that our approach has better retrieval effect.",video hierarchical retrieval;key frame extraction;feature extraction;deep convolutional neural network;simHash,"709, 714",,IEEE Conferences,2019 IEEE 11th International Conference on Communication Software and Networks (ICCSN),IEEE
535,,Full Convolutional Color Constancy with Adding Pooling,T. Yuan; X. Li,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8905344,10.1109/ICCSN.2019.8905344,"Traditional methods of color constancy have solved the problem by modeling the statistical laws of natural objects and illumination colors. With the development of convolutional neural networks (CNN), the improvements of color constancy have greatly arisen. At first, the CNN-based color constancy algorithms use the images by image patches. However, the illumination information and object information contained in the local image patches may not be enough to estimate the scene illumination color. The full convolutional neural networks (FCNs) architecture can use the entire image as input without thinking about the size if images, which in turn improves the training and testing quality of network. FCNs also allow for end-to-end training to achieve higher efficiency and accuracy. We improve a color constancy algorithm which is based on full convolutional neural network and a new pooling layer named adding pooling. On standard benchmarks, our network outperforms the previous state-of-the-art algorithms.",color constancy;FCN;adding pooling,"666, 671",,IEEE Conferences,2019 IEEE 11th International Conference on Communication Software and Networks (ICCSN),IEEE
536,,Image Restoration of Agricultural History Based on Neural Patch Synthesis,Y. Chen; Y. Cui; J. Wu,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8905400,10.1109/ICCSN.2019.8905400,"In recent years, the study of agricultural history has been constantly promoted. In the process of developing agricultural modernization, more and more attention has been paid to the origin of agriculture. With the development of digital image restoration technology and neural network, this study proposes to repair agricultural propaganda paintings of new China by means of neural patch synthesis. First, use the structure of the encoder and decoder to repair the overall structure of the image, and use the idea of the discriminator to make the overall image realistic. Secondly, the texture information of the globally restored image is further optimized by using the feature layer of VGG network, resulting in better results of both global and local restoration. Finally, the actual discriminator is used for realistic processing. The experimental results show that the restored images produced by this method are lifelike, without obvious artificial boundary, and the restored parts are not blurred, which is suitable for the restoration of agricultural history images.",image restoration;VGG;Nerve patch;Historical images of agriculture,"401, 405",,IEEE Conferences,2019 IEEE 11th International Conference on Communication Software and Networks (ICCSN),IEEE
537,,Multi-source Log Comprehensive Feature Extraction Method Based on Restricted Boltzmann Machine in Power Information System,D. Liu; H. Yu; W. Wang; H. Zhang; X. Zhao; Y. Zhao; J. Chen; D. Li,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8905373,10.1109/ICCSN.2019.8905373,"In order to excavate security threats in power grid by making full use of heterogeneous data sources in power information system, this paper proposes a multi-source log comprehensive feature extraction method based on restricted boltzmann machine (RBM). Firstly, the restricted boltzmann machine neural network is used to normalize coding all kinds of log information. Then, the contrast divergence fast learning method is used to optimize the network weight, and the stochastic gradient rise method is used to maximize the logarithmic likelihood function for the training and learning of the RBM model. The data dimension reduction is realized by processing the normalized coded log information. At the same time, the comprehensive features are obtained, which can effectively solve the problems caused by the heterogeneity of log data. The experimental environment was set up in the power information system, and the comprehensive feature extraction and algorithm verification of the security log were carried out. Experimental results show that the proposed method can be applied to all kinds of security analysis, such as clustering analysis, anomaly detection, etc., and it can effectively improve the speed and accuracy of power information system security situation prediction.",power information system;restricted boltzmann machine;feature extraction;neural network;comprehensive feature,"503, 508",,IEEE Conferences,2019 IEEE 11th International Conference on Communication Software and Networks (ICCSN),IEEE
538,,Comparing Insights From Inductive Qualitative Analysis Versus Automated NLP Algorithms For Analyzing Feedback In Digital Randomized Controlled Trials,P. L. Li; C. Yang; S. Liu; M. Hu,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8906676,10.1109/SEAA.2019.00060,"Randomized controlled trials (i.e. A/B testing) is the gold standard for evaluating improvements and accelerating innovations in the digital space. Prior work has shown that qualitative user feedback is an effective and important tool in the analysis of A/B tests. However, manual inductive qualitative analysis of feedback-best practices today-is expensive and unscalable, which may lead to the omission of important insights about quality and user experiences. Prior work has shown various automated NLP algorithms to be effective in extracting insights from feedback in the digital domain. But we lack understanding of the differences in insights gained from a manual inductive qualitative analysis versus automated NLP algorithms, for analyzing digital randomized controlled trials (where a key objective is understanding differences between control and treatment conditions). In this paper, we compare insights from manual inductive qualitative analyses and from six automated NLP algorithms, using data from large-scale real-world digital randomized controlled trials. We find that collocation algorithms (notable, trigrams) are promising, providing similar insights as manual analyses with substantially lower cost; however, issues remain and improvements are needed. We discuss implications for future research and for operationalization.",a/b testing;experimentation;text analysis;qualitative analysis;NLP;randomized controlled trials,"347, 354",,IEEE Conferences,2019 45th Euromicro Conference on Software Engineering and Advanced Applications (SEAA),IEEE
539,,Word Embeddings for Comment Coherence,A. Cimasa; A. Corazza; C. Coviello; G. Scanniello,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8906527,10.1109/SEAA.2019.00046,"During the evolution of software, it could happen that the information in the comments and in the associated source code are not aligned, so hampering the execution of software evolution and maintenance tasks. This kind of misalignment is known as lack of coherence and it can happen for several reasons, e.g., programmers modify the intent of source code while executing a maintenance task without updating its comment accordingly. We study the problem of detecting a lack of coherence between comments and source code by exploiting Word Embeddings (WEs). We present four models based on WE and tested these models using six different WE variants through an experiment conducted on a publicly available dataset. Results are compared against a baseline. The most important outcome is: the considered models and WE variants are more efficient in terms of execution time while maintaining performance very close to the baseline. The explanation for such an improvement is that WEs are able to concentrate the important information in a more compact input representation.",Comment Coherence;Word Embeddings,"244, 251",,IEEE Conferences,2019 45th Euromicro Conference on Software Engineering and Advanced Applications (SEAA),IEEE
540,,Data Brushes: Interactive Style Transfer for Data Art,M. Dubey; J. Otto; A. G. Forbes,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8900858,10.1109/VISAP.2019.8900858,"This paper introduces Data Brushes, an interactive web application to explore neural style transfer using models trained on data visualizations. Our application includes two distinct modes that invite casual creators to engage with deep convolutional neural networks to co-create custom artworks. The first mode, `magic markers', mimics painting with a brush on a canvas, enabling users to paint a style onto selected areas of an image. The second mode, `compositing stamps', uses a real-time method for applying style filters to selected portions of an image. Specifically, we focus on style transfer networks created from canonical and contemporary works of data visualization and data art in order to investigate the versatility and flexibility of the algorithm. In addition to enabling a novel creative workflow, the process of interactively modifying an image via multiple style transfer networks reveals meaningful features encoded within the networks, and provides insight into the effects particular networks have on different images, or different regions within a single image. To evaluate Data Brushes, we gathered expert feedback from participants of a data science symposium and ran an observational study, finding that our application facilitates the creative exploration of neural style transfer for data art and enhances user intuition regarding the expressive range of style transfer features.",Style transfer;data art;casual creators;creative AI,"1, 9",,IEEE Conferences,2019 IEEE VIS Arts Program (VISAP),IEEE
541,,Towards Standardizing and Improving Classification of Bug-Fix Commits,S. Zafar; M. Z. Malik; G. S. Walia,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8870174,10.1109/ESEM.2019.8870174,"Background: Open source software repositories like GitHub are mined to gain useful empirical software engineering insights and answer critical research questions. However, the present state of the art mining approaches suffers from high error rate in the labeling of data that is used for such analysis. This is particularly true when labels are automatically generated from the commit message, and seriously undermines the results of these studies. Aim: Our goal is to label commit comments with high accuracy automatically. In this work, we focus on classifying a commit as a “Bug-Fix commit” or not. Method: Traditionally, researchers have utilized keyword-based approaches to identify bug fix commits that leads to a significant increase in the error rate. We present an alternative methodology leveraging a deep neural network model called Bidirectional Encoder Representations from Transformers (BERT) that can understand the context of the commit message. We provide the rules for semantic interpretation of commit comments. We construct a hand-labeled dataset from real GitHub commits according to these rules and fine-tune BERT for classification. Results: Our initial evaluation shows that our approach significantly reduces the error rate, with up to 10% relative improvement in classification over keyword-based approaches. Future Direction: We plan on extending our dataset to cover more corner cases and reduce programming language specific biases. We also plan on refining the semantic rules. In this work, we have only considered a simple binary classification problem (Bug-Fix or not), which we plan to extend to other classes and extend the approach to consider multiclass problems. Conclusion: The rules, data, and the model proposed in this paper have the potential to be used by people analyzing open source repositories to improve the labeling of data used in their analysis.",Software Maintenance;Mining Software Repositories;Predictive Models;Human Factors,"1, 6",,IEEE Conferences,2019 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM),IEEE
542,,IVAA: Intelligent Vehicle Accident Analysis System,K. Thonglek; N. Urailertprasert; P. Pattiyathanee; C. Chantrapornchai,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8864186,10.1109/JCSSE.2019.8864186,"Intelligent Vehicle Accident Analysis (IVAA) system provides an artificial intelligence as a service (AIaaS) for building a system that can automatically evaluate vehicle parts' damage from the accident. Particularly, IVAA can recognize the damaged vehicle part and the severity level facilitating the insurance company's claiming process. There are four main elements in the system which support four stakeholders in an insurance company: insurance experts, data scientists, operators and field employees. Insurance experts utilize the data labeling tool to label damaged parts of a vehicle in a given image as a training data building process. Data scientists iterate to the deep learning model building process for continuous model update. Operators monitor the visualization system for daily statistics related to the number of accidents based on locations. Field employees use LINE Official integration to take a photo the damaged on the vehicle at the accident site and retrieve the repair estimation. IVAA is built on the docker image which can scale-in or scale-out the system depend on utilization efficiently. We deploy the Faster Region-based convolutional neural network, along with residual Inception network to localize the damage region and classify into 5 damage levels for a vehicle. The accuracy of the localization is 93.28 percents and the accuracy of the classification is 98.47 percents.",Artificial Intelligence as a Service;Faster Regions with Convolutional Neural Network;Image Processing,"85, 90",,IEEE Conferences,2019 16th International Joint Conference on Computer Science and Software Engineering (JCSSE),IEEE
543,,Information Extraction based on Named Entity for Tourism Corpus,C. Chantrapornchai; A. Tunsakul,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8864166,10.1109/JCSSE.2019.8864166,"Tourism information is scattered around nowadays. To search for the information, it is usually time consuming to browse through the results from search engine, select and view the details of each accommodation. In this paper, we present a methodology to extract particular information from full text returned from the search engine to facilitate the users. The approach is based on name entity recognition (NER). The main steps are 1) building training data and 2) building the model. The key task is the building training data: First, the tourism data are gathered and the vocabularies are built. Several minor steps include sentence extraction, relation and name entity extraction for tagging purpose. Then, the recognition model of a given entity type can be built. From the experiments, given hotel description, the model can extract the desired entity,i.e, name, location, facility as well as relation type. The extracted data can further be stored as a structured information, e.g., in the ontology format, for future querying and inference. The model for automatic named entity identification, based on machine learning, yields the error ranging 5%-25%.",,"187, 192",,IEEE Conferences,2019 16th International Joint Conference on Computer Science and Software Engineering (JCSSE),IEEE
544,,An Open-source Based Automatic Car Detection System using IoT,A. Khurat; N. Siriphun; J. Saingthong; J. Sriwiphasathit,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8864188,10.1109/JCSSE.2019.8864188,"Cars have been employed as a tool to commit a crime such as for escaping, carrying drugs, or bombs to attack especially in the three southern borders of Thailand. To detect a suspicious car from a blacklist, government officers normally check this manually which may have some errors. This blacklist can be updated time by time. In addition, to avoid being detected, the criminal may change the car license plate. Therefore, it is better to have an automatic system that can detect suspicious cars. Though there are some open-source programs that can detect car such as by its license plate, they have many limitations; for instance, cannot recognize Thai license plate, cannot detect car logo; especially there is no system that can perform these two functions together. This project thus proposes an automatic car detection system that can recognize a Thai license plate and car logo in one system. We aim to build this system to be portable using low-cost IoT devices and to develop it with open-source programs. In addition, we will publish the Thai license plate recognition and logo detection models for the benefit of Thai society.",Image Processing;OpenALPR;YOLO;Raspberry Pi;Tesseract;Cloud Server;IoT,"283, 288",,IEEE Conferences,2019 16th International Joint Conference on Computer Science and Software Engineering (JCSSE),IEEE
545,,Classification of Nutrient Deficiency in Black Gram Using Deep Convolutional Neural Networks,K. A. M. Han; U. Watchareeruetai,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8864224,10.1109/JCSSE.2019.8864224,"This paper investigates the use of various deep convolutional neural networks (CNNs) with transfer learning to identify nutrient deficiencies from a leaf image. Experiments were conducted with a dataset containing 4,088 images of black gram (Vigna mungo) leaves grown under seven different treatments, i.e., complete nutrient treatment and six nutrient deficiency treatment, including calcium (Ca), iron (Fe), magnesium (Mg), nitrogen (N), potassium (K), and phosphorus (P) deficiencies. Experimental results indicate that a deep CNN model known as ResNet50 was the best among all experimented models with a test accuracy of 65.44% and a F-measure of 66.15%. In addition, We found that the ResNet50 model obviously outperformed a block-based method and the human performance reported in a literature.",transfer learning;pretrained network;deep learning;convolutional neural network;black gram,"277, 282",,IEEE Conferences,2019 16th International Joint Conference on Computer Science and Software Engineering (JCSSE),IEEE
546,,Convolutional Neural Networks Using MobileNet for Skin Lesion Classification,W. Sae-Lim; W. Wettayaprasit; P. Aiyarak,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8864155,10.1109/JCSSE.2019.8864155,"Skin lesion classification is a particular interesting area of research in dermatoscopic lesion image processing. In this paper, we present a skin lesion classification approach based on the light weight deep Convolutional Neural Networks (CNNs), called MobileNet. We employed MobileNet and proposed the modified MobileNet for skin lesion classification. For the evaluation of our model, we had used the official dataset of Human Against Machine with 10,000 training images (HAM 10000) which was a collection of multisource dermatoscopic images. Data up-sampling and data augmentation method were used in our study for improving the efficiency of the classifier. The comparison results showed that our modified model had achieved higher accuracy, specificity, sensitivity, and F1-score than the traditional MobileNet.",deep neural networks;convolutional neural networks;image processing;skin lesion classification;MobileNet,"242, 247",,IEEE Conferences,2019 16th International Joint Conference on Computer Science and Software Engineering (JCSSE),IEEE
547,,Natural Language Contents Evaluation System for Detecting Fake News using Deep Learning,Y. Ahn; C. Jeong,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8864171,10.1109/JCSSE.2019.8864171,"This Recently, a lot of information is spreading rapidly on SNS. Inaccurate communication of news media includes fears about unreliable sources and fake news that lacks confirmation of facts. Fake news is spread through SNS, causing social confusion and further economic loss. The purpose of the news is accurate information transmission. In this regard, it is very important to judge the discrepancies in the contents of the text and the distorted reports. We try to solve the problem of judging whether the sentence to be verified is correct after collecting the facts. This paper defines the problem of extracting the related sentences from the input sentence in Fact Data Corpus which is assumed to be fact and judging whether the extracted sentence and the input sentence are true or false. In the various NLP tasks, we create a Korean-specific pre-training model using state-of-the-art BERT. Using this model, fine-tuning is performed to match the data set detected by Korean fake news. The AUROC score of 83.8% is derived from the test set generated using the fine-tuned model.",NLP;Fake news Detection;BERT,"289, 292",,IEEE Conferences,2019 16th International Joint Conference on Computer Science and Software Engineering (JCSSE),IEEE
548,,Evaluation of Judicial Imprisonment Term Prediction Model Based on Text Mutation,S. Zhang; G. Yan; Y. Li; J. Liu,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8859511,10.1109/QRS-C.2019.00025,"In recent years, artificial intelligence has witnessed great advancement, and its application in the legal field has experienced more than 60 years. The use of ""machine learning"" technology to aid the decision-making of legal intelligence systems is no longer far away. However, no comprehensive evaluation methods for predictive models of judicial cases can be found. The performance of the machine learning prediction model not only related to the accuracy but also should be measured in many different aspects. Mutation is a common means of traditional software testing, which can be borrowed in the evaluating of prediction models. This paper introduces the text mutation method, to evaluate the robustness of the judicial case prediction model. The following three evaluation methods are adopted: Classification preference test, Word order variation test and Noise variation test. This paper applies the proposed evaluation method to the judicial imprisonment term prediction model. We use the fastText, TextCNN, and Multi-layer LSTM models. Using the proposed evaluation method to test the above prediction model, and evaluate the robustness of the judicial case prediction model in different aspects.",Text Mutation;Judicial Imprisonment Term;Prediction Model,"62, 65",,IEEE Conferences,"2019 IEEE 19th International Conference on Software Quality, Reliability and Security Companion (QRS-C)",IEEE
549,,Improving GAN-Based Calligraphy Character Generation using Graph Matching,M. Li; J. Wang; Y. Yang; W. Huang; W. Du,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8859439,10.1109/QRS-C.2019.00062,"The Chinese character generation with specific style is the key of personal calligraphy font generation. In recent years, with the development of artificial intelligence, researchers have used deep learning to solve the calligraphy generation problem, which can automatically generate personal calligraphy fonts. However, the end-to-end approaches possibly generate wrong results in terms of glyph structure of Chinese character because of lack of constraints on glyph structures. This paper proposes a method that adds constraints of glyph structure to deep neural network in order to improve the correctness of generation. This paper represents the glyph structure of Chinese character with glyph nodes extracted by object detection method. We compute the glyph structure loss between inputs and the generated results by the deep learning-based graph matching method. Experiments show that our method significantly increases the accuracy of Chinese character generation. For showing our method more clearly, we use the print as the original style samples in this paper.",font style transfer;generative models;graph matching;deep learning;personalized Chinese character,"291, 295",,IEEE Conferences,"2019 IEEE 19th International Conference on Software Quality, Reliability and Security Companion (QRS-C)",IEEE
550,,Research on Security Detection and Data Analysis for Industrial Internet,J. Lin; L. Liu,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8859457,10.1109/QRS-C.2019.00089,"Industrial Internet platform needs to solve a series of problems, such as access of multi-type industrial equipment, multi-source industrial data integration, massive data management and processing, industrial Internet security and so on. This paper builds industrial big data analysis algorithm library based on domain knowledge modeling and big data analysis of industrial data. Through the analysis of the behavior characteristics of industrial internet network traffic data, this paper studies the method of selecting traffic characteristics of events in the industrial Internet; establishes the propagation and evolution model of security events in the industrial Internet, and builds a traceability map of security event propagation; This study combines the characteristics of large data volume and centralized control of future industrial Internet to reduce the complexity of security event detection and analysis. It has reference value for industrial Internet controller to formulate node routing strategy.","Industrial Internet, Future network, Big Data, Security Detection","466, 470",,IEEE Conferences,"2019 IEEE 19th International Conference on Software Quality, Reliability and Security Companion (QRS-C)",IEEE
551,,An Exploratory Study on Judicial Image Quality Assessment Based on Deep Learning,Q. Gu; W. Cai; S. Yu; Z. Chen,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8854694,10.1109/QRS.2019.00046,"Images are important judicial materials. With the deepening of intelligent systems in the judicial area, image quality plays a vital role in the result of many judicial applications. This paper firstly introduces deep learning into judicial image quality assessment. Pre-trained convolutional neural network (CNN) models are fine-tuned and then used to extract image features. Based on the features extracted from CNN models, we convert them into specific numbers representing the quality. A preliminary experiment has been designed and conducted on three types of judicial images. The experimental results show that our approach can outperform the existing image processing technique. Images used as investigation materials are more distinctive than the other two types, and they need an independent model for analyzing.","Image Quality Assessment, Judicial Image, Deep Learning","300, 305",,IEEE Conferences,"2019 IEEE 19th International Conference on Software Quality, Reliability and Security (QRS)",IEEE
552,,On the Investigation of Essential Diversities for Deep Learning Testing Criteria,Z. Zhang; X. Xie,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8854700,10.1109/QRS.2019.00056,"Recent years, more and more testing criteria for deep learning systems has been proposed to ensure system robustness and reliability. These criteria were defined based on different perspectives of diversity. However, there lacks comprehensive investigation on what are the most essential diversities that should be considered by a testing criteria for deep learning systems. Therefore, in this paper, we conduct an empirical study to investigate the relation between test diversities and erroneous behaviors of deep learning models. We define five metrics to reflect diversities in neuron activities, and leverage metamorphic testing to detect erroneous behaviors. We investigate the correlation between metrics and erroneous behaviors. We also go further step to measure the quality of test suites under the guidance of defined metrics. Our results provided comprehensive insights on the essential diversities for testing criteria to exhibit good fault detection ability.",metamorphic testing;deep learning testing;testing criteria;essential metrics,"394, 405",,IEEE Conferences,"2019 IEEE 19th International Conference on Software Quality, Reliability and Security (QRS)",IEEE
553,,A Transfer Learning Based Interpretable User Experience Model on Small Samples,Q. Yu; X. Che; Y. Yang; L. Wang,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8854695,10.1109/QRS.2019.00035,"User experience (UX) is a key factor that affects software survival time. A rich line of research has studied the relationships between UX and software factors to modify software and improve user satisfaction. However, the existing machine learning models for predicting UX on small data set is not accurate enough, and research with traditional statistical methods only obtained indistinct relations among UX, user characteristics and software factors. With the goal of improving the accuracy of UX model and obtaining sufficient UX relationships, we propose Transfer in Cart (TrCart) algorithm and Transfer Adaboost in Cart (TrAdaBoostCart) algorithm. To verify this approach, we present the UX study on a desktop game and an android game. According to the experimental results, we find that the TrAdaBoostCart has better accuracy and interpretable results. Hence, the proposed approach provides important guidelines for the design process of mobile applications.",user experience;transfer learning;decision tree;mobile application;user characteristics,"186, 196",,IEEE Conferences,"2019 IEEE 19th International Conference on Software Quality, Reliability and Security (QRS)",IEEE
554,,Automatic Detection of Latent Software Component Relationships from Online Q&A Sites,S. Karthik; N. Medvidovic,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8823731,10.1109/RAISE.2019.00011,"Modern software system stacks are composed of large numbers of software components. These components may include a broad range of entities such as services, libraries, and frameworks, all intended to address specific requirements. It is not only necessary that these components satisfy respective functional and non-functional concerns, but also that the combinations of selected components work well together. The space of component combinations to explore is huge. Together with the almost universal lack of formal documentation suggesting desirable combinations and cautioning against undesirable ones, this renders the proper selection of combinations very challenging. For this reason, software engineers often solicit advice and document their experience on online forums such as community Q&A sites. In this paper, we show that these Q&A sites contain valuable knowledge about inter-component relations. We develop an approach using information extraction techniques to automatically identify three different types of compatibility relations from unstructured text on Q&A site postings. Our work demonstrates that identifying such relations is valuable for the design of component-based systems and that automatic relation extraction is a promising technique to systematically harness such community knowledge.",Component Based Software Development;Relation Extraction;Information Extraction;Deep Learning,"15, 21",,IEEE Conferences,2019 IEEE/ACM 7th International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering (RAISE),IEEE
555,,Building Sankie: An AI Platform for DevOps,R. Kumar; C. Bansal; C. Maddila; N. Sharma; S. Martelock; R. Bhargava,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8823620,10.1109/BotSE.2019.00020,"There has been a fundamental shift amongst software developers and engineers in the past few years. The software development life cycle (SDLC) for a developer has increased in complexity and scale. Changes that were developed and deployed over a matter of days or weeks are now deployed in a matter of hours. Due to greater availability of compute, storage, better tooling, and the necessity to react, developers are constantly looking to increase their velocity and throughput of developing and deploying changes. Consequently, there is a great need for more intelligent and context sensitive DevOps tools and services that help developers increase their efficiency while developing and debugging. Given the vast amounts of heterogeneous data available from the SDLC, such intelligent tools and services can now be built and deployed at a large scale to help developers achieve their goals and be more productive. In this paper, we present Sankie, a scalable and general service that has been developed to assist and impact all stages of the modern SDLC. Sankie provides all the necessary infrastructure (back-end and front-end bots) to ingest data from repositories and services, train models based on the data, and eventually perform decorations or provide information to engineers to help increase the velocity and throughput of changes, bug fixes etc. This paper discusses the architecture as well as some of the key observations we have made from wide scale deployment of Sankie within Microsoft.",DevOps;empirical software engineering;machine learning;bot;software development life cycle;infrastructure;scale;azure;pull request,"48, 53",,IEEE Conferences,2019 IEEE/ACM 1st International Workshop on Bots in Software Engineering (BotSE),IEEE
556,,Ideas on Improving Software Artifact Reuse via Traceability and Self-Awareness,C. Tinnes; A. Biesdorf; U. Hohenstein; F. Matthes,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8823790,10.1109/SST.2019.00013,We describe our vision towards automatic software and system development and argue that reusing knowledge from existing projects as well as traceability between corresponding artifacts are important steps towards this vision. We furthermore list barriers that are currently experienced with software artifact reuse and traceability in industry and suggest some ideas to overcome these barriers.,Software Reuse;Traceability;Self Aware Systems;Natural Language Processing;Architecture Knowledge Management,"13, 16",,IEEE Conferences,2019 IEEE/ACM 10th International Symposium on Software and Systems Traceability (SST),IEEE
557,,Learning Units-of-Measure from Scientific Code,M. Danish; M. Allamanis; M. Brockschmidt; A. Rice; D. Orchard,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8823677,10.1109/SE4Science.2019.00013,"CamFort is our multi-purpose tool for lightweight analysis and verification of scientific Fortran code. One core feature provides units-of-measure verification (dimensional analysis) of programs, where users partially annotate programs with units-of-measure from which our tool checks consistency and infers any missing specifications. However, many users find it onerous to provide units-of-measure information for existing code, even in part. We have noted however that there are often many common patterns and clues about the intended units-of-measure contained within variable names, comments, and surrounding code context. In this work-in-progress paper, we describe how we are adapting our approach, leveraging machine learning techniques to reconstruct units-of-measure information automatically thus saving programmer effort and increasing the likelihood of adoption.",units-of-measure;verification;machine learning,"43, 46",,IEEE Conferences,2019 IEEE/ACM 14th International Workshop on Software Engineering for Science (SE4Science),IEEE
558,,Cross-Language Clone Detection by Learning Over Abstract Syntax Trees,D. Perez; S. Chiba,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8816761,10.1109/MSR.2019.00078,"Clone detection across programs written in the same programming language has been studied extensively in the literature. On the contrary, the task of detecting clones across multiple programming languages has not been studied as much, and approaches based on comparison cannot be directly applied. In this paper, we present a clone detection method based on semi-supervised machine learning designed to detect clones across programming languages with similar syntax. Our method uses an unsupervised learning approach to learn token-level vector representations and an LSTM-based neural network to predict whether two code fragments are clones. To train our network, we present a cross-language code clone dataset - which is to the best of our knowledge the first of its kind - containing around 45,000 code fragments written in Java and Python. We evaluate our approach on the dataset we created and show that our method gives promising results when detecting similarities between code fragments written in Java and Python.","clone detection, machine learning, source code representation","518, 528",,IEEE Conferences,2019 IEEE/ACM 16th International Conference on Mining Software Repositories (MSR),IEEE
559,,Semantic Source Code Models Using Identifier Embeddings,V. Efstathiou; D. Spinellis,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8816775,10.1109/MSR.2019.00015,"The emergence of online open source repositories in the recent years has led to an explosion in the volume of openly available source code, coupled with metadata that relate to a variety of software development activities. As an effect, in line with recent advances in machine learning research, software maintenance activities are switching from symbolic formal methods to data-driven methods. In this context, the rich semantics hidden in source code identifiers provide opportunities for building semantic representations of code which can assist tasks of code search and reuse. To this end, we deliver in the form of pretrained vector space models, distributed code representations for six popular programming languages, namely, Java, Python, PHP, C, C++, and C#. The models are produced using fastText, a state-of-the-art library for learning word representations. Each model is trained on data from a single programming language; the code mined for producing all models amounts to over 13.000 repositories. We indicate dissimilarities between natural language and source code, as well as variations in coding conventions in between the different programming languages we processed. We describe how these heterogeneities guided the data preprocessing decisions we took and the selection of the training parameters in the released models. Finally, we propose potential applications of the models and discuss limitations of the models.",fastText;Code Semantics;Vector Space Models;Semantic Similarity,"29, 33",,IEEE Conferences,2019 IEEE/ACM 16th International Conference on Mining Software Repositories (MSR),IEEE
560,,Exploring Word Embedding Techniques to Improve Sentiment Analysis of Software Engineering Texts,E. Biswas; K. Vijay-Shanker; L. Pollock,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8816816,10.1109/MSR.2019.00020,"Sentiment analysis (SA) of text-based software artifacts is increasingly used to extract information for various tasks including providing code suggestions, improving development team productivity, giving recommendations of software packages and libraries, and recommending comments on defects in source code, code quality, possibilities for improvement of applications. Studies of state-of-the-art sentiment analysis tools applied to software-related texts have shown varying results based on the techniques and training approaches. In this paper, we investigate the impact of two potential opportunities to improve the training for sentiment analysis of SE artifacts in the context of the use of neural networks customized using the Stack Overflow data developed by Lin et al. We customize the process of sentiment analysis to the software domain, using software domain-specific word embeddings learned from Stack Overflow (SO) posts, and study the impact of software domain-specific word embeddings on the performance of the sentiment analysis tool, as compared to generic word embeddings learned from Google News. We find that the word embeddings learned from the Google News data performs mostly similar and in some cases better than the word embeddings learned from SO posts. We also study the impact of two machine learning techniques, oversampling and undersampling of data, on the training of a sentiment classifier for handling small SE datasets with a skewed distribution. We find that oversampling alone, as well as the combination of oversampling and undersampling together, helps in improving the performance of a sentiment classifier.",Sentiment Analysis;Software Engineering;Word Embeddings,"68, 78",,IEEE Conferences,2019 IEEE/ACM 16th International Conference on Mining Software Repositories (MSR),IEEE
561,,A Novel Neural Source Code Representation Based on Abstract Syntax Tree,J. Zhang; X. Wang; H. Zhang; H. Sun; K. Wang; X. Liu,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812062,10.1109/ICSE.2019.00086,"Exploiting machine learning techniques for analyzing programs has attracted much attention. One key problem is how to represent code fragments well for follow-up analysis. Traditional information retrieval based methods often treat programs as natural language texts, which could miss important semantic information of source code. Recently, state-of-the-art studies demonstrate that abstract syntax tree (AST) based neural models can better represent source code. However, the sizes of ASTs are usually large and the existing models are prone to the long-term dependency problem. In this paper, we propose a novel AST-based Neural Network (ASTNN) for source code representation. Unlike existing models that work on entire ASTs, ASTNN splits each large AST into a sequence of small statement trees, and encodes the statement trees to vectors by capturing the lexical and syntactical knowledge of statements. Based on the sequence of statement vectors, a bidirectional RNN model is used to leverage the naturalness of statements and finally produce the vector representation of a code fragment. We have applied our neural network based source code representation method to two common program comprehension tasks: source code classification and code clone detection. Experimental results on the two tasks indicate that our model is superior to state-of-the-art approaches.","Abstract Syntax Tree, source code representation, neural network, code classification, code clone detection","783, 794",,IEEE Conferences,2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE),IEEE
562,,On Learning Meaningful Code Changes Via Neural Machine Translation,M. Tufano; J. Pantiuchina; C. Watson; G. Bavota; D. Poshyvanyk,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811910,10.1109/ICSE.2019.00021,"Recent years have seen the rise of Deep Learning (DL) techniques applied to source code. Researchers have exploited DL to automate several development and maintenance tasks, such as writing commit messages, generating comments and detecting vulnerabilities among others. One of the long lasting dreams of applying DL to source code is the possibility to automate non-trivial coding activities. While some steps in this direction have been taken (e.g., learning how to fix bugs), there is still a glaring lack of empirical evidence on the types of code changes that can be learned and automatically applied by DL. Our goal is to make this first important step by quantitatively and qualitatively investigating the ability of a Neural Machine Translation (NMT) model to learn how to automatically apply code changes implemented by developers during pull requests. We train and experiment with the NMT model on a set of 236k pairs of code components before and after the implementation of the changes provided in the pull requests. We show that, when applied in a narrow enough context (i.e., small/medium-sized pairs of methods before/after the pull request changes), NMT can automatically replicate the changes implemented by developers during pull requests in up to 36% of the cases. Moreover, our qualitative analysis shows that the model is capable of learning and replicating a wide variety of meaningful code changes, especially refactorings and bug-fixing activities. Our results pave the way for novel research in the area of DL on code, such as the automatic learning and applications of refactoring.",Neural-Machine Translation;Empirical Study,"25, 36",,IEEE Conferences,2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE),IEEE
563,,Resource-Aware Program Analysis Via Online Abstraction Coarsening,K. Heo; H. Oh; H. Yang,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812143,10.1109/ICSE.2019.00027,"We present a new technique for developing a resource-aware program analysis. Such an analysis is aware of constraints on available physical resources, such as memory size, tracks its resource use, and adjusts its behaviors during fixpoint computation in order to meet the constraint and achieve high precision. Our resource-aware analysis adjusts behaviors by coarsening program abstraction, which usually makes the analysis consume less memory and time until completion. It does so multiple times during the analysis, under the direction of what we call a controller. The controller constantly intervenes in the fixpoint computation of the analysis and decides how much the analysis should coarsen the abstraction. We present an algorithm for learning a good controller automatically from benchmark programs. We applied our technique to a static analysis for C programs, where we control the degree of flow-sensitivity to meet a constraint on peak memory consumption. The experimental results with 18 real-world programs show that our algorithm can learn a good controller and the analysis with this controller meets the constraint and utilizes available memory effectively.",static analysis;resource constraint;learning,"94, 104",,IEEE Conferences,2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE),IEEE
564,,CRADLE: Cross-Backend Validation to Detect and Localize Bugs in Deep Learning Libraries,H. V. Pham; T. Lutellier; W. Qi; L. Tan,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812095,10.1109/ICSE.2019.00107,"Deep learning (DL) systems are widely used in domains including aircraft collision avoidance systems, Alzheimer's disease diagnosis, and autonomous driving cars. Despite the requirement for high reliability, DL systems are difficult to test. Existing DL testing work focuses on testing the DL models, not the implementations (e.g., DL software libraries) of the models. One key challenge of testing DL libraries is the difficulty of knowing the expected output of DL libraries given an input instance. Fortunately, there are multiple implementations of the same DL algorithms in different DL libraries. Thus, we propose CRADLE, a new approach that focuses on finding and localizing bugs in DL software libraries. CRADLE (1) performs cross-implementation inconsistency checking to detect bugs in DL libraries, and (2) leverages anomaly propagation tracking and analysis to localize faulty functions in DL libraries that cause the bugs. We evaluate CRADLE on three libraries (TensorFlow, CNTK, and Theano), 11 datasets (including ImageNet, MNIST, and KGS Go game), and 30 pre-trained models. CRADLE detects 12 bugs and 104 unique inconsistencies, and highlights functions relevant to the causes of inconsistencies for all 104 unique inconsistencies.",deep learning software testing;cross-implementation testing;bugs detection;software testing,"1027, 1038",,IEEE Conferences,2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE),IEEE
565,,NL2Type: Inferring JavaScript Function Types from Natural Language Information,R. S. Malik; J. Patra; M. Pradel,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811893,10.1109/ICSE.2019.00045,"JavaScript is dynamically typed and hence lacks the type safety of statically typed languages, leading to suboptimal IDE support, difficult to understand APIs, and unexpected runtime behavior. Several gradual type systems have been proposed, e.g., Flow and TypeScript, but they rely on developers to annotate code with types. This paper presents NL2Type, a learning-based approach for predicting likely type signatures of JavaScript functions. The key idea is to exploit natural language information in source code, such as comments, function names, and parameter names, a rich source of knowledge that is typically ignored by type inference algorithms. We formulate the problem of predicting types as a classification problem and train a recurrent, LSTM-based neural model that, after learning from an annotated code base, predicts function types for unannotated code. We evaluate the approach with a corpus of 162,673 JavaScript files from real-world projects. NL2Type predicts types with a precision of 84.1% and a recall of 78.9% when considering only the top-most suggestion, and with a precision of 95.5% and a recall of 89.6% when considering the top-5 suggestions. The approach outperforms both JSNice, a state-of-the-art approach that analyzes implementations of functions instead of natural language information, and DeepTyper, a recent type prediction approach that is also based on deep learning. Beyond predicting types, NL2Type serves as a consistency checker for existing type annotations. We show that it discovers 39 inconsistencies that deserve developer attention (from a manual analysis of 50 warnings), most of which are due to incorrect type annotations.",JavaScript;deep learning;type inference;comments;identifiers,"304, 315",,IEEE Conferences,2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE),IEEE
566,,Learning to Spot and Refactor Inconsistent Method Names,K. Liu; D. Kim; T. F. Bissyandé; T. Kim; K. Kim; A. Koyuncu; S. Kim; Y. Le Traon,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812134,10.1109/ICSE.2019.00019,"To ensure code readability and facilitate software maintenance, program methods must be named properly. In particular, method names must be consistent with the corresponding method implementations. Debugging method names remains an important topic in the literature, where various approaches analyze commonalities among method names in a large dataset to detect inconsistent method names and suggest better ones. We note that the state-of-the-art does not analyze the implemented code itself to assess consistency. We thus propose a novel automated approach to debugging method names based on the analysis of consistency between method names and method code. The approach leverages deep feature representation techniques adapted to the nature of each artifact. Experimental results on over 2.1 million Java methods show that we can achieve up to 15 percentage points improvement over the state-of-the-art, establishing a record performance of 67.9% F1- measure in identifying inconsistent method names. We further demonstrate that our approach yields up to 25% accuracy in suggesting full names, while the state-of-the-art lags far behind at 1.1% accuracy. Finally, we report on our success in fixing 66 inconsistent method names in a live study on projects in the wild.","Code refactoring, inconsistent method names, deep neural networks, code embedding","1, 12",,IEEE Conferences,2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE),IEEE
567,,Guiding Deep Learning System Testing Using Surprise Adequacy,J. Kim; R. Feldt; S. Yoo,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812069,10.1109/ICSE.2019.00108,"Deep Learning (DL) systems are rapidly being adopted in safety and security critical domains, urgently calling for ways to test their correctness and robustness. Testing of DL systems has traditionally relied on manual collection and labelling of data. Recently, a number of coverage criteria based on neuron activation values have been proposed. These criteria essentially count the number of neurons whose activation during the execution of a DL system satisfied certain properties, such as being above predefined thresholds. However, existing coverage criteria are not sufficiently fine grained to capture subtle behaviours exhibited by DL systems. Moreover, evaluations have focused on showing correlation between adversarial examples and proposed criteria rather than evaluating and guiding their use for actual testing of DL systems. We propose a novel test adequacy criterion for testing of DL systems, called Surprise Adequacy for Deep Learning Systems (SADL), which is based on the behaviour of DL systems with respect to their training data. We measure the surprise of an input as the difference in DL system's behaviour between the input and the training data (i.e., what was learnt during training), and subsequently develop this as an adequacy criterion: a good test input should be sufficiently but not overtly surprising compared to training data. Empirical evaluation using a range of DL systems from simple image classifiers to autonomous driving car platforms shows that systematic sampling of inputs based on their surprise can improve classification accuracy of DL systems against adversarial examples by up to 77.5% via retraining.",Test Adequacy;Coverage Criteria;Deep Learning Systems,"1039, 1049",,IEEE Conferences,2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE),IEEE
568,,One-Size-Fits-None? Improving Test Generation Using Context-Optimized Fitness Functions,G. Gay,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812217,10.1109/SBST.2019.000-1,"Current approaches to search-based test case generation have yielded limited results in terms of human-competitiveness. However, effective search-based test generation relies on the selection of the correct fitness functions-feedback mechanisms-for a chosen goal. We propose that the key to overcoming these limitations lies in infusing domain knowledge and context into the fitness functions used to guide the search and the ability to automatically optimize the fitness functions used when generating tests for a given class, goal, and algorithm.","Automated Test Generation, Search-Based Software Testing, Search-Based Software Engineering","3, 4",,IEEE Conferences,2019 IEEE/ACM 12th International Workshop on Search-Based Software Testing (SBST),IEEE
569,,Structural Coverage Criteria for Neural Networks Could Be Misleading,Z. Li; X. Ma; C. Xu; C. Cao,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8805667,10.1109/ICSE-NIER.2019.00031,"There is a dramatically increasing interest in the quality assurance for DNN-based systems in the software engineering community. An emerging hot topic in this direction is structural coverage criteria for testing neural networks, which are inspired by coverage metrics used in conventional software testing. In this short paper, we argue that these criteria could be misleading because of the fundamental differences between neural networks and human written programs. Our preliminary exploration shows that (1) adversarial examples are pervasively distributed in the finely divided space defined by such coverage criteria, while available natural samples are very sparse, and as a consequence, (2) previously reported fault-detection ""capabilities"" conjectured from high coverage testing are more likely due to the adversary-oriented search but not the real ""high"" coverage.",Software Testing;Neural Networks;Coverage Criteria,"89, 92",,IEEE Conferences,2019 IEEE/ACM 41st International Conference on Software Engineering: New Ideas and Emerging Results (ICSE-NIER),IEEE
570,,Leveraging Small Software Engineering Data Sets with Pre-Trained Neural Networks,R. Robbes; A. Janes,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8805726,10.1109/ICSE-NIER.2019.00016,"Many software engineering data sets, particularly those that demand manual labelling for classification, are necessarily small. As a consequence, several recent software engineering papers have cast doubt on the effectiveness of deep neural networks for classification tasks, when applied to these data sets. We provide initial evidence that recent advances in Natural Language Processing, that allow neural networks to leverage large amount of unlabelled data in a pre-training phase, can significantly improve performance.",Data sets;Deep learning;Transfer learning,"29, 32",,IEEE Conferences,2019 IEEE/ACM 41st International Conference on Software Engineering: New Ideas and Emerging Results (ICSE-NIER),IEEE
571,,Software Engineering for Machine Learning: A Case Study,S. Amershi; A. Begel; C. Bird; R. DeLine; H. Gall; E. Kamar; N. Nagappan; B. Nushi; T. Zimmermann,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8804457,10.1109/ICSE-SEIP.2019.00042,"Recent advances in machine learning have stimulated widespread interest within the Information Technology sector on integrating AI capabilities into software and services. This goal has forced organizations to evolve their development processes. We report on a study that we conducted on observing software teams at Microsoft as they develop AI-based applications. We consider a nine-stage workflow process informed by prior experiences developing AI applications (e.g., search and NLP) and data science tools (e.g. application diagnostics and bug reporting). We found that various Microsoft teams have united this workflow into preexisting, well-evolved, Agile-like software engineering processes, providing insights about several essential engineering challenges that organizations may face in creating large-scale AI solutions for the marketplace. We collected some best practices from Microsoft teams to address these challenges. In addition, we have identified three aspects of the AI domain that make it fundamentally different from prior software application domains: 1) discovering, managing, and versioning the data needed for machine learning applications is much more complex and difficult than other types of software engineering, 2) model customization and model reuse require very different skills than are typically found in software teams, and 3) AI components are more difficult to handle as distinct modules than traditional software components - models may be ""entangled"" in complex ways and experience non-monotonic error behavior. We believe that the lessons learned by Microsoft teams will be valuable to other organizations.",artifical intelligence;machine learning;software engineering;process;data,"291, 300",,IEEE Conferences,2019 IEEE/ACM 41st International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP),IEEE
572,,DeepConcolic: Testing and Debugging Deep Neural Networks,Y. Sun; X. Huang; D. Kroening; J. Sharp; M. Hill; R. Ashmore,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8802786,10.1109/ICSE-Companion.2019.00051,"Deep neural networks (DNNs) have been deployed in a wide range of applications. We introduce a DNN testing and debugging tool, called DeepConcolic, which is able to detect errors with sufficient rigour so as to be applicable to the testing of DNNs in safety-related applications. DeepConcolic is the first tool that implements a concolic testing technique for DNNs, and the first testing tool that provides users with the functionality of investigating particular parts of a DNN. The tool has been made publicly available and a demo video can be found at https://youtu.be/rliynbhoNLM.",deep neural networks;concolic testing;test coverage criteria,"111, 114",,IEEE Conferences,2019 IEEE/ACM 41st International Conference on Software Engineering: Companion Proceedings (ICSE-Companion),IEEE
573,,Towards Zero Knowledge Learning for Cross Language API Mappings,N. Bui,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8802774,10.1109/ICSE-Companion.2019.00054,"Programmers often need to migrate programs from one language or platform to another in order to implement functionality, instead of rewriting the code from scratch. However, most techniques proposed to identify API mappings across languages and facilitate automated program translation require manually curated parallel corpora that contain already mapped API seeds or functionally-equivalent code using the APIs in two different languages so that the techniques can have an anchor to map APIs. To alleviate the need of curating parallel data and to generalize the applicability of program translation techniques, we develop a new automated approach for identifying API mappings across languages based on the idea of unsupervised domain adaption via Generative Adversarial Network (GAN) and an additional refinement procedure that can transform two vector spaces to align the API vectors in the two spaces without the need of manually provided anchors. We show that our approach can identify API mappings more accurately than Api2Api without the need of curated parallel seeds.",generative adversarial network;adversarial learning;cross language;skip gram,"123, 125",,IEEE Conferences,2019 IEEE/ACM 41st International Conference on Software Engineering: Companion Proceedings (ICSE-Companion),IEEE
574,,An Artificial Intelligence-Based Model-Driven Approach for Exposing Off-Nominal Behaviors,K. Madala,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8802759,10.1109/ICSE-Companion.2019.00085,"With an increase in the automation of cyber-physical systems (e.g., automated vehicles and robots), quality problems such as off-nominal behaviors (ONBs) have also increased. While there are techniques that can find ONBs at the requirements engineering stage as it reduces the cost of addressing defects early in development, they do not meet the current industrial needs and often ignore functional safety. These techniques suffer from limitations such as scalability, need for significant human effort and inability to detect overlooked or unknown ONBs. To address these limitations we need a technique that analyzes requirements with respect to functional safety, but with less human effort. To achieve this, we propose our artificial intelligence-based model-driven methodology that provides a means to find ONBs during requirements engineering with minimal human effort. Our methodology utilizes existing approaches such as causal component model (CCM) and systems theoretic process analysis (STPA). We describe the details of each step of our approach and how our approach would support finding ONBs. Using our research and the results of our studies, we intend to provide empirical evidence that considering ONBs during requirements engineering stage and analyzing requirements with respect to functional safety can help create more robust designs and higher-quality products.",Off Nominal Behaviors;Cyber Physical Systems;Requirements Analysis;Functional Safety,"214, 217",,IEEE Conferences,2019 IEEE/ACM 41st International Conference on Software Engineering: Companion Proceedings (ICSE-Companion),IEEE
575,,Machine Learning Meets Quantitative Planning: Enabling Self-Adaptation in Autonomous Robots,P. Jamshidi; J. Cámara; B. Schmerl; C. Käestner; D. Garlan,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8787014,10.1109/SEAMS.2019.00015,"Modern cyber-physical systems (e.g., robotics systems) are typically composed of physical and software components, the characteristics of which are likely to change over time. Assumptions about parts of the system made at design time may not hold at run time, especially when a system is deployed for long periods (e.g., over decades). Self-adaptation is designed to find reconfigurations of systems to handle such run-time inconsistencies. Planners can be used to find and enact optimal reconfigurations in such an evolving context. However, for systems that are highly configurable, such planning becomes intractable due to the size of the adaptation space. To overcome this challenge, in this paper we explore an approach that (a) uses machine learning to find Pareto-optimal configurations without needing to explore every configuration and (b) restricts the search space to such configurations to make planning tractable. We explore this in the context of robot missions that need to consider task timeliness and energy consumption. An independent evaluation shows that our approach results in high-quality adaptation plans in uncertain and adversarial environments.","Machine learning, artificial intelligence, quantitative planning, self-adaptive systems, robotics systems","39, 50",,IEEE Conferences,2019 IEEE/ACM 14th International Symposium on Software Engineering for Adaptive and Self-Managing Systems (SEAMS),IEEE
576,,Is Adaptivity a Core Property of Intelligent Systems? It Depends,A. ElSaid; T. Desell; D. Krutz,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8787199,10.1109/SEAMS.2019.00027,"Autonomous systems rely upon intelligence to enable them to function without the need for human intervention. In many cases, these systems must adapt to properly react to numerous scenarios. This adaptivity could be a simple action such as activating an additional server, or could be incredibly complex in creating a completely new adaptation strategy. In the following work, we address the question ""Is Adaptability a Core Property of Intelligent Systems?"" We determine that due to the ambiguity of the term 'intelligent system' and the wide range of such systems, that there is no clear and definitive answer to this question. This paper details the deliberation of three AI/Self-Adaptive researchers in addressing this question. We use several scenarios of intelligent systems ranging from a simple motion detector to a complex 'humanoid' futuristic artificial system to demonstrate the complexity of this question.","intelligent system (265), adaptive intelligent system, neural network, self driving car, core property, core principle, adaptive system","153, 154",,IEEE Conferences,2019 IEEE/ACM 14th International Symposium on Software Engineering for Adaptive and Self-Managing Systems (SEAMS),IEEE
577,,All Versus One: An Empirical Comparison on Retrained and Incremental Machine Learning for Modeling Performance of Adaptable Software,T. Chen,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8787029,10.1109/SEAMS.2019.00029,"Given the ever-increasing complexity of adaptable software systems and their commonly hidden internal information (e.g., software runs in the public cloud), machine learning based performance modeling has gained momentum for evaluating, understanding and predicting software performance, which facilitates better informed self-adaptations. As performance data accumulates during the run of the software, updating the performance models becomes necessary. To this end, there are two conventional modeling methods: the retrained modeling that always discard the old model and retrain a new one using all available data; or the incremental modeling that retains the existing model and tunes it using one newly arrival data sample. Generally, literature on machine learning based performance modeling for adaptable software chooses either of those methods according to a general belief, but they provide insufficient evidences or references to justify their choice. This paper is the first to report on a comprehensive empirical study that examines both modeling methods under distinct domains of adaptable software, 5 performance indicators, 8 learning algorithms and settings, covering a total of 1,360 different conditions. Our findings challenge the general belief, which is shown to be only partially correct, and reveal some of the important, statistically significant factors that are often overlooked in existing work, providing evidence-based insights on the choice.","Performance modeling, self-adaptive system, machine learning, software runtime","157, 168",,IEEE Conferences,2019 IEEE/ACM 14th International Symposium on Software Engineering for Adaptive and Self-Managing Systems (SEAMS),IEEE
578,,On Learning in Collective Self-Adaptive Systems: State of Practice and a 3D Framework,M. D'Angelo; S. Gerasimou; S. Ghahremani; J. Grohmann; I. Nunes; E. Pournaras; S. Tomforde,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8787033,10.1109/SEAMS.2019.00012,"Collective self-adaptive systems (CSAS) are distributed and interconnected systems composed of multiple agents that can perform complex tasks such as environmental data collection, search and rescue operations, and discovery of natural resources. By providing individual agents with learning capabilities, CSAS can cope with challenges related to distributed sensing and decision-making and operate in uncertain environments. This unique characteristic of CSAS enables the collective to exhibit robust behaviour while achieving system-wide and agent-specific goals. Although learning has been explored in many CSAS applications, selecting suitable learning models and techniques remains a significant challenge that is heavily influenced by expert knowledge. We address this gap by performing a multifaceted analysis of existing CSAS with learning capabilities reported in the literature. Based on this analysis, we introduce a 3D framework that illustrates the learning aspects of CSAS considering the dimensions of autonomy, knowledge access, and behaviour, and facilitates the selection of learning techniques and models. Finally, using example applications from this analysis, we derive open challenges and highlight the need for research on collaborative, resilient and privacy-aware mechanisms for CSAS.","self-adaptive systems, learning, distributed systems, autonomic systems, taxonomy, multi-agent systems","13, 24",,IEEE Conferences,2019 IEEE/ACM 14th International Symposium on Software Engineering for Adaptive and Self-Managing Systems (SEAMS),IEEE
579,,Using Unstructured Data to Improve the Continuous Planning of Critical Processes Involving Humans,C. Paterson; R. Calinescu; S. Manandhar; D. Wang,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8787082,10.1109/SEAMS.2019.00013,"The success of processes executed in uncertain and changing environments is reliant on the dependable use of relevant information to support continuous planning at runtime. At the core of this planning is a model which, if incorrect, can lead to failures and, in critical processes such as evacuation and disaster relief operations, to harm to humans. Obtaining reliable and timely estimations of model parameters is often difficult, and considerable research effort has been expended to derive methods for updating models at run-time. Typically, these methods use data sources such as system logs, run-time events and sensor readings, which are well structured. However, in many critical processes, the most relevant data are produced by human participants to, and observers of, the process and its environment (e.g., through social media) and is unstructured. For such scenarios we propose COPE, a work-in-progress method for the continuous planning of critical processes involving humans and carried out in uncertain, changing environments. COPE uses a combination of runtime natural-language processing (to update a stochastic model of the target process based on unstructured data) and stochastic model synthesis (to generate Pareto-optimal plans for the process). Preliminary experiments indicate that COPE can support continuous planning effectively for a simulated evacuation operation after a natural disaster.",natural-language processing;stochastic model synthesis;probabilistic model checking,"25, 31",,IEEE Conferences,2019 IEEE/ACM 14th International Symposium on Software Engineering for Adaptive and Self-Managing Systems (SEAMS),IEEE
580,,Evasion Attacks Against Watermarking Techniques found in MLaaS Systems,D. Hitaj; B. Hitaj; L. V. Mancini,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8768572,10.1109/SDS.2019.8768572,"Deep neural networks have had enormous impact on various domains of computer science applications, considerably outperforming previous state-of-the-art machine learning techniques. To achieve this performance, neural networks need large quantities of data and huge computational resources, which heavily increase their costs. The increased cost of building a good deep neural network model gives rise to a need for protecting this investment from potential copyright infringements. Legitimate owners of a machine learning model want to be able to reliably track and detect a malicious adversary that tries to steal the intellectual property related to the model. This threat is very relevant to Machine Learning as a Service (MLaaS) systems, where a provider supplies APIs to clients, allowing them to interact with their trained proprietary deep learning models. Recently, this problem was tackled by introducing in deep neural networks the concept of watermarking, which allows a legitimate owner to embed some secret information (watermark) in a given model. Through the use of this watermark, the legitimate owners, remotely interacting with a model through input queries, are able to detect a copyright infringement, and prove the ownership of their models that were stolen/copied illegally. In this paper, we focus on assessing the robustness and reliability of state-of-the-art deep neural network watermarking schemes. In particular we show that, a malicious adversary, even in scenarios where the watermark is difficult to remove, can still evade the verification of copyright infringements from the legitimate owners, thus avoiding the detection of the model theft.",Security and Privacy;Watermarking;Deep Neural Networks;Backdoors;Machine Learning as a Service,"55, 63",,IEEE Conferences,2019 Sixth International Conference on Software Defined Systems (SDS),IEEE
581,,Identification of Cybersecurity Specific Content Using the Doc2Vec Language Model,O. Mendsaikhan; H. Hasegawa; Y. Yamaguchi; H. Shimada,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8753992,10.1109/COMPSAC.2019.00064,"It has become more challenging for the security analysts to identify cyber threat related content on the Internet because of the vast amount of publicly available digital texts. In this research, we proposed building an autonomous system for extracting cyber threat information from publicly available information sources. We tested a neural embedding method called doc2vec as a natural language filter for the proposed system. With cybersecurity-specific training data and custom preprocessing, we were able to train a doc2vec model and evaluate its performance. According to our evaluation, the natural language filter was able to identify cybersecurity specific natural language text with 83% accuracy.",text mining;cyber threat;document embedding;doc2vec,"396, 401",,IEEE Conferences,2019 IEEE 43rd Annual Computer Software and Applications Conference (COMPSAC),IEEE
582,,AILiveSim: An Extensible Virtual Environment for Training Autonomous Vehicles,J. Leudet; F. Christophe; T. Mikkonen; T. Männistö,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8754253,10.1109/COMPSAC.2019.00074,"Virtualization technologies have become common-place both in software development as well as engineering in a more general sense. Using virtualization offers other benefits than simulation and testing as a virtual environment can often be more liberally configured than the corresponding physical environment. This, in turn, introduces new possibilities for education and training, including both for humans and artificial intelligence (AI). To this end, we are developing a simulation platform AILiveSim. The platform is built on top of the Unreal Engine (www.unrealengine.com) game development system, and it is dedicated to training and testing autonomous systems, their sensors and their algorithms in a simulated environment. In this paper, we describe the elements that we have built on top of the engine to realize a Virtual Environment (VE) useful for the design, implementation, application and analysis of autonomous systems. We present the architecture that we have put in place to transform our simulation platform from automotive specific to be domain agnostic and support two new domains of applications: autonomous ships and autonomous mining machines. We describe the important specificity of each domain in regard to simulation.In addition, we also report the challenges encountered when simulating those applications, and the decisions taken to overcome these challenges.","Virtual Environment, Autonomous Systems, Autonomous Vehicles, Simulation, Training, Validation","479, 488",,IEEE Conferences,2019 IEEE 43rd Annual Computer Software and Applications Conference (COMPSAC),IEEE
583,,Chinese Social Media Entity Linking Based on Effective Context with Topic Semantics,C. Ma; Y. Sha; J. Tan; L. Guo; H. Peng,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8754254,10.1109/COMPSAC.2019.00063,"On social media, entity linking is very important for natural language processing tasks, such as Sentiment Analysis, Question Answering (QA) and Machine Translation. Compared to English-oriented entity linking, Chinese entity linking has its special difficulties. Just like the entity linking for short text, Chinese microblogs have lots of noise and the mention lacks effective context information. In order to solve these problems, we present a new model for Chinese microblogs entity linking. Entity linking usually includes two steps: candidate entities generation and candidate entities ranking. First, based on the characteristics of Chinese, we put forward multi-method fusion strategies for candidate generation to improve the recall rate of candidate entities. Second, we propose a new neural network model called TAS (Topic attention Siamese) for candidate entities ranking. In TAS model, we add effective topic semantics on Siamese network to learn representations of context, mention and entity, and rank the mention-entity similarity. The representation of mention incorporates information from multiple sentences on the same topic, which can effectively solve the problem of the lack of contextual information. We also use Character-enhanced Word Embedding model (CWE) to pre-train both word embedding and characters embedding to work out noise and word segmentation impact. Experimental results demonstrate that our method significantly outperforms the state-of-the-art results for entity linking on Chinese social media.","entity linking, Chinese microblogs, Siamese network, topic semantics, effective contexts","386, 395",,IEEE Conferences,2019 IEEE 43rd Annual Computer Software and Applications Conference (COMPSAC),IEEE
584,,Increasing Self-Adaptation in a Hybrid Decision-Making and Planning System with Reinforcement Learning,C. -E. Hrabia; P. M. Lehmann; S. Albayrak,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8754021,10.1109/COMPSAC.2019.00073,"Task-level decision-making and AI planning are used to control autonomous robots from a high-level, mission-oriented perspective. The dynamic selection of most suitable actions allows the system to adapt to changes in the environment as well as its own state. Nevertheless, decision-making and AI planning often require a priori definitions of capabilities, rules, decision models, or world knowledge. Due to the challenge of handling the uncertainty of robot applications in dynamic and uncontrolled environments such definitions or descriptions are always incomplete, hence the possible adaptation capabilities are limited. In this paper, we present how the self-adaptation of a robot planning and decision-making system can be improved by incorporating reinforcement learning. Particularly, we show our approach of integrating deep reinforcement learning into the ROS Hybrid Behaviour Planner (RHBP).","decision-making, planning, reinforcement learning, self-adaptation, autonomous robots","469, 478",,IEEE Conferences,2019 IEEE 43rd Annual Computer Software and Applications Conference (COMPSAC),IEEE
585,,View-Adaptive Weighted Deep Transfer Learning for Distributed Time-Series Classification,S. Das Bhattacharjee; W. J. Tolone; A. Mahabal; M. Elshambakey; I. Cho; G. Djorgovski,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8753914,10.1109/COMPSAC.2019.00061,"In this paper, we propose an effective, multi-view, deep, transfer learning framework for multivariate time-series data. Though widely used for tasks such as computer vision, the application of transfer learning to time-series classification problems (e.g., classification of light curves) is underexplored. The proposed framework makes several important contributions to facilitate knowledge sharing, while simultaneously ensuring an effective solution for domain specific fine-level categorizations. First, in contrast to the traditional approaches, the proposed framework describes pairwise view similarity by identifying a smaller subset of source-view samples that closely resemble the target data patterns. Second, by means of two-phase learning, a generic baseline model is learned on a larger source data collection and later fine-tuned on a smaller target data collection, precisely approximating the target data patterns. Third, an effective view-adaptive timestamp weighting scheme evaluates the relative importance of each timestamp in a more data-driven manner, which enables a more flexible yet discriminative feature representation scheme in the presence of evolving data characteristics. As shown by experiments, compared to the existing approaches, our proposed deep transfer learning framework improves classification performance by around 2-3% in the UCI multi-view activity recognition dataset, while also showing a robust, generalized representation capacity in classifying several large-scale multi-view light curve collections.",LSTM;RNN;Mult-iview Classification;Transfer Learning;Distributed Time-Series Analysi;Deep Learning;Minimum Description Length,"373, 381",,IEEE Conferences,2019 IEEE 43rd Annual Computer Software and Applications Conference (COMPSAC),IEEE
586,,Alchemy: Stochastic Feature Regeneration for Malicious Network Traffic Classification,B. Hu; A. Kumagai; K. Kamiya; K. Takahashi; D. Dalek; O. Soderstrom; K. Okada; Y. Sekiya; A. Nakao,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8754288,10.1109/COMPSAC.2019.00057,"As signature-based techniques have ever more difficulty detecting increasing and varying malicious activities through network traffic, machine learning has become a promising approach in network security. Many previous studies have aggregated traffic data into groups by hosts or flows for generating features and training detection models. However, two problems degrade detection performance. One is the scarcity of training sets due to the rarity of new types of malicious traffic, and the other is variations in feature values generated from incomplete data due to limited observed traffic. In this paper, we propose a stochastic method called Alchemy that regenerates a set of feature vectors by randomly resampling raw traffic data of each bag into several subsets. Alchemy can increase training sets and represent raw traffic robustly to correct the influence of variations in feature vectors, regardless of types of traffic data and classifiers. We evaluated Alchemy with real-world traffic data of network flows, passive DNS records, and HTTP logs, and demonstrated that it improves detection performance of various classifiers more effectively than the conventional methods in all three types of traffic data.",malicious traffic;machine learning;feature generation,"346, 351",,IEEE Conferences,2019 IEEE 43rd Annual Computer Software and Applications Conference (COMPSAC),IEEE
587,,Topic Shift Detection in Online Discussions using Structural Context,Y. Sun; K. Loparo,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8754029,10.1109/COMPSAC.2019.00155,"Topic shift occurs frequently in online discussions, and automatically detecting topic shift can help to better capture the main clues and obtain relevant answers from large number of comments. Traditional topic-shift detection methods calculate text similarity and have limited success because they ignore semantic relatedness. In this paper, we propose a new topic shift detection model that uses conversational structure to enrich the context information and word embedding to build the semantic associations for each comment - post pair. Experiments show that the proposed model leads to better performance in terms of precision, recall, and F1 score.",topic shift;online discussions;structural context,"948, 949",,IEEE Conferences,2019 IEEE 43rd Annual Computer Software and Applications Conference (COMPSAC),IEEE
588,,Improving Classification of Breast Cancer by Utilizing the Image Pyramids of Whole-Slide Imaging and Multi-scale Convolutional Neural Networks,L. Tong; Y. Sha; M. D. Wang,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8754277,10.1109/COMPSAC.2019.00105,"Whole-slide imaging (WSI) is the digitization of conventional glass slides. Automatic computer-aided diagnosis (CAD) based on WSI enables digital pathology and the integration of pathology with other data like genomic biomarkers. Numerous computational algorithms have been developed for WSI, with most of them taking the image patches cropped from the highest resolution as the input. However, these models exploit only the local information within each patch and lost the connections between the neighboring patches, which may contain important context information. In this paper, we propose a novel multi-scale convolutional network (ConvNet) to utilize the built-in image pyramids of WSI. For the concentric image patches cropped at the same location of different resolution levels, we hypothesize the extra input images from lower magnifications will provide context information to enhance the prediction of patch images. We build corresponding ConvNets for feature representation and then combine the extracted features by 1) late fusion: concatenation or averaging the feature vectors before performing classification, 2) early fusion: merge the ConvNet feature maps. We have applied the multi-scale networks to a benchmark breast cancer WSI dataset. Extensive experiments have demonstrated that our multi-scale networks utilizing the WSI image pyramids can achieve higher accuracy for the classification of breast cancer. The late fusion method by taking the average of feature vectors reaches the highest accuracy (81.50%), which is promising for the application of multi-scale analysis of WSI.",Whole-Slide Imaging;Breast Cancer;Image Pyramid;Multi-Scale Convolutional Neural Network,"696, 703",,IEEE Conferences,2019 IEEE 43rd Annual Computer Software and Applications Conference (COMPSAC),IEEE
589,,Multi-scale Discriminative Location-Aware Network for Few-Shot Semantic Segmentation,Z. Dong; R. Zhang; X. Shao; H. Zhou,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8754235,10.1109/COMPSAC.2019.10181,"Few-shot semantic segmentation methods aim for predicting the regions of different object categories with only a few labeled samples. It is difficult to produce segmentation results with high accuracy when a new category appears. In this paper, we propose a Multi-scale Discriminative Location-aware (MDL) network to tackle the few-shot semantic segmentation problem. In order to use information from different levels, we first keep the last three convolutional layers of FCN, and then use the VGG-16 network to extract features from the support image-label pair, which adjusts the weight of the query image segmentation branch. Discriminative location-aware architecture can improve the efficiency of few-shot segmentation, and therefore the global average pooling layer is added to produce location feature information. Finally, we evaluate our MDL model on the Pascal VOC 2012 challenge, and show that it achieves competitive mIoU score compared to methods in recent years.",few-shot semantic segmentation;multi-scale;discriminative location-aware;support image;query image,"42, 47",,IEEE Conferences,2019 IEEE 43rd Annual Computer Software and Applications Conference (COMPSAC),IEEE
590,,Automatic Multi-class Non-Functional Software Requirements Classification Using Neural Networks,C. Baker; L. Deng; S. Chakraborty; J. Dehlinger,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8754214,10.1109/COMPSAC.2019.10275,"Advances in machine learning (ML) algorithms, graphics processing units, and readily available ML libraries have enabled the application of ML to open software engineering challenges. Yet, the use of ML to enable decision-making during the software engineering lifecycle is not well understood as there are various ML models requiring parameter tuning. In this paper, we leverage ML techniques to develop an effective approach to classify software requirements. Specifically, we investigate the design and application of two types of neural network models, an artificial neural network (ANN) and a convolutional neural network (CNN), to classify non-functional requirements (NFRs) into the following five categories: maintainability, operability, performance, security and usability. We illustrate and experimentally evaluate this work through two widely used datasets consisting of nearly 1,000 NFRs. Our results indicate that our CNN model can effectively classify NFRs by achieving precision ranging between 82% and 94%, recall ranging between 76% and 97% with an F-score ranging between 82% and 92%.",non-functional requirements;requirements engineering;machine learning,"610, 615",,IEEE Conferences,2019 IEEE 43rd Annual Computer Software and Applications Conference (COMPSAC),IEEE
591,,Modern Architecture for Deep Learning-Based Automatic Optical Inspection,J. Richter; D. Streitferdt,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8754376,10.1109/COMPSAC.2019.10197,"The advanced optical inspection of manually placed components on through-hole printed circuit boards demands robust and fast classifiers. To train such classifiers, one needs vast amounts of previously labeled sample images. Datasets like this are currently not available and thus hinder the deployment of deep-learning algorithms in environments like electronics manufacturing. This paper proposes a new architecture, which uses a superposition of active and unsupervised learning to build a problem specific, fully annotated dataset while training a suitable classifier. The system validates human-made annotation by selectively re-asking for a different opinion, to reduce the risk of human error. Our experiments show a simplification of inspection programming in contrast to the existing approaches.",THT;automatic optical inspection;electronics manufacturing;component placement;quality control;deep learning;active learning;classification,"141, 145",,IEEE Conferences,2019 IEEE 43rd Annual Computer Software and Applications Conference (COMPSAC),IEEE
592,,Feature Boosting in Natural Image Classification,P. Saxena; D. Saxena; X. Nie; A. Helmers; N. Ramachandran; N. Sakib; S. Ahamed,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8754242,10.1109/COMPSAC.2019.10184,"Computer Vision has become the poster child for Deep Learning. The image classification accuracy of convolutional neural nets on benchmark data sets has increased every year since their inception. This has been aided with advances in feature fusion. The increase in the availability of imagetext occurrence has lead to text augmented feature spaces that have lead to higher accuracy in in image classification tasks. However, these works are limited to instances where text is readily available. This study presents an approach to featurize text within natural images with the goal of augmenting image features for image classification tasks. Text extraction and featurization in natural images is a challenging task due to challenges in reliable text localization and OCR results, both being impeded by the variability in image text and errors in OCR. We overcome these challenges by implementing a novel bounding box concatenation algorithm and a novel feature boosting algorithm. The result is a pipeline that encodes an image into a text feature space. Classifiers trained on the text based feature space have comparable accuracy to the state of the art Convolutional Neural Nets (CNN's) while being significantly inexpensive computationally. Moreover, the augmentation of text features to image features generates a hybrid feature space with a higher information content for a classification problem when compared to a feature space comprised exclusively of image features. Thus, we see a rise in classification accuracy across all state of the art machine learning algorithms.",deep learning;transfer learning;computer vision;feature space augmentation;NLP;feature fusion;hybrid features;hybrid feature space,"61, 67",,IEEE Conferences,2019 IEEE 43rd Annual Computer Software and Applications Conference (COMPSAC),IEEE
593,,"TALS: A Framework for Text Analysis, Fine-Grained Annotation, Localisation and Semantic Segmentation",S. Jaradat; N. Dokoohaki; U. Wara; M. Goswami; K. Hammar; M. Matskin,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8754470,10.1109/COMPSAC.2019.10207,"With around 2.77 billion users using online social media platforms nowadays, it is becoming more attractive for business retailers to reach and to connect to more potential clients through social media. However, providing more effective recommendations to grab clients' attention requires a deep understanding of users' interests. Given the enormous amounts of text and images that users share in social media, deep learning approaches play a major role in performing semantic analysis of text and images. Moreover, object localisation and pixel-bypixel semantic segmentation image analysis neural architectures provide an enhanced level of information. However, to train such architectures in an end-to-end manner, detailed datasets with specific meta-data are required. In our paper, we present a complete framework that can be used to tag images in a hierarchical fashion, and to perform object localisation and semantic segmentation. In addition to this, we show the value of using neural word embeddings in providing additional semantic details to annotators to guide them in annotating images in the system. Our framework is designed to be a fully functional solution capable of providing fine-grained annotations, essential localisation and segmentation services while keeping the core architecture simple and extensible. We also provide a fine-grained labelled fashion dataset that can be a rich source for research purposes.",deep learning;word embeddings;natural language processing;annotations;fine-grained;localisation;semantic segmentation;dataset,"201, 206",,IEEE Conferences,2019 IEEE 43rd Annual Computer Software and Applications Conference (COMPSAC),IEEE
594,,A Scalable Framework for Multilevel Streaming Data Analytics using Deep Learning,S. Ge; H. Isah; F. Zulkernine; S. Khan,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8754149,10.1109/COMPSAC.2019.10205,"The rapid growth of data in velocity, volume, value, variety, and veracity has enabled exciting new opportunities and presented big challenges for businesses of all types. Recently, there has been considerable interest in developing systems for processing continuous data streams with the increasing need for real-time analytics for decision support in the business, healthcare, manufacturing, and security. The analytics of streaming data usually relies on the output of offline analytics on static or archived data. However, businesses and organizations like our industry partner Gnowit, strive to provide their customers with real time market information and continuously look for a unified analytics framework that can integrate both streaming and offline analytics in a seamless fashion to extract knowledge from large volumes of hybrid streaming data. We present our study on designing a multilevel streaming text data analytics framework by comparing leading edge scalable open-source, distributed, and in-memory technologies. We demonstrate the functionality of the framework for a use case of multilevel text analytics using deep learning for language understanding and sentiment analysis including data indexing and query processing. Our framework combines Spark streaming for real time text processing, the Long Short Term Memory (LSTM) deep learning model for higher level sentiment analysis, and other tools for SQL-based analytical processing to provide a scalable solution for multilevel streaming text analytics.",deep learning;natural language processing;news media;sentiment analysis;unstructured data,"189, 194",,IEEE Conferences,2019 IEEE 43rd Annual Computer Software and Applications Conference (COMPSAC),IEEE
595,,An Empirical Comparison of Mutant Selection Assessment Metrics,J. M. Zhang; L. Zhang; D. Hao; L. Zhang; M. Harman,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8728935,10.1109/ICSTW.2019.00037,"Mutation testing is expensive due to the large number of mutants, a problem typically tackled using selective techniques, thereby raising the fundamental question of how to evaluate the selection process. Existing mutant selection approaches rely on one of two types of metrics (or assessment criteria), one based on adequate test sets and the other based on inadequate test sets. This raises the question as to whether these two metrics are correlated, complementary or substitutable for one another. The tester's faith in mutant selection as well as the validity of previous research work using only one metric rely on the answer to this question, yet it currently remains unanswered. To answer it, we perform qualitative and quantitative comparisons with 104 different projects, consisting of over 600,000 lines of code. Our results indicate a strong connection between the two types of metrics (R<sup>2</sup>=0.8622 on average). The strategy for dealing with equivalent mutants and test density is observed to have a negligible impact for mutant selection.",mutation testing;mutant selection;assessment metrics,"90, 101",,IEEE Conferences,"2019 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)",IEEE
596,,Machine Learning to Guide Performance Testing: An Autonomous Test Framework,M. Helali Moghadam; M. Saadatmand; M. Borg; M. Bohlin; B. Lisper,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8728899,10.1109/ICSTW.2019.00046,"Satisfying performance requirements is of great importance for performance-critical software systems. Performance analysis to provide an estimation of performance indices and ascertain whether the requirements are met is essential for achieving this target. Model-based analysis as a common approach might provide useful information but inferring a precise performance model is challenging, especially for complex systems. Performance testing is considered as a dynamic approach for doing performance analysis. In this work-in-progress paper, we propose a self-adaptive learning-based test framework which learns how to apply stress testing as one aspect of performance testing on various software systems to find the performance breaking point. It learns the optimal policy of generating stress test cases for different types of software systems, then replays the learned policy to generate the test cases with less required effort. Our study indicates that the proposed learning-based framework could be applied to different types of software systems and guides towards autonomous performance testing.",performance requirements;performance testing;test case generation;reinforcement learning;autonomous testing,"164, 167",,IEEE Conferences,"2019 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)",IEEE
597,,SeqFuzzer: An Industrial Protocol Fuzzing Framework from a Deep Learning Perspective,H. Zhao; Z. Li; H. Wei; J. Shi; Y. Huang,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8730177,10.1109/ICST.2019.00016,"Industrial networks are the cornerstone of modern industrial control systems. Performing security checks of industrial communication processes helps detect unknown risks and vulnerabilities. Fuzz testing is a widely used method for performing security checks that takes advantage of automation. However, there is a big challenge to carry out security checks on industrial network due to the increasing variety and complexity of industrial communication protocols. In this case, existing approaches usually take a long time to model the protocol for generating test cases, which is labor-intensive and time-consuming. This becomes even worse when the target protocol is stateful. To help in addressing this problem, we employed a deep learning model to learn the structures of protocol frames and deal with the temporal features of stateful protocols. We propose a fuzzing framework named SeqFuzzer which automatically learns the protocol frame structures from communication traffic and generates fake but plausible messages as test cases. For proving the usability of our approach, we applied SeqFuzzer to widely-used Ethernet for Control Automation Technology (EtherCAT) devices and successfully detected several security vulnerabilities.","industrial safety, deep learning, vulnerability mining, self learning, fuzzing, EtherCAT","59, 67",,IEEE Conferences,"2019 12th IEEE Conference on Software Testing, Validation and Verification (ICST)",IEEE
598,,Directing a Search Towards Execution Properties with a Learned Fitness Function,L. Joffe; D. Clark,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8730160,10.1109/ICST.2019.00029,"Search based software testing is a popular and successful approach both in academia and industry. SBST methods typically aim to increase coverage whereas searching for executions with specific properties is largely unresearched. Fitness functions for execution properties often possess search landscapes that are difficult or intractable. We demonstrate how machine learning techniques can convert a property that is not searchable, in this case crashes, into one that is. Through experimentation on 6000 C programs drawn from the Codeflaws repository, we demonstrate a strong, program independent correlation between crashing executions and library function call patterns within those executions as discovered by a neural net. We then exploit the correlation to produce a searchable fitness landscape to modify American Fuzzy Lop, a widely used fuzz testing tool. On a test set of previously unseen programs drawn from Codeflaws, a search strategy based on a crash targeting fitness function outperformed a baseline in 80.1% of cases. The experiments were then repeated on three real world programs: the VLC media player, and the libjpeg and mpg321 libraries. The correlation between library call traces and crashes generalises as indicated by ROC AUC scores of 0.91, 0.88 and 0.61. The produced search landscape however is not convenient due to plateaus. This is likely because these programs do not use standard C libraries as often as do those in Codeflaws. This limitation can be overcome by considering a more powerful observation domain and a broader training corpus in future work. Despite limited generalisability of the experimental setup, this research opens new possibilities in the intersection of machine learning, fitness functions, and search based testing in general.",Search Based Software Engineering;Search Based Software Testing;Fuzzing;Machine Learning,"206, 216",,IEEE Conferences,"2019 12th IEEE Conference on Software Testing, Validation and Verification (ICST)",IEEE
599,,PySE: Automatic Worst-Case Test Generation by Reinforcement Learning,J. Koo; C. Saumya; M. Kulkarni; S. Bagchi,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8730198,10.1109/ICST.2019.00023,"Stress testing is an important task in software testing, which examines the behavior of a program under a heavy load. Symbolic execution is a useful tool to find out the worst-case input values for the stress testing. However, symbolic execution does not scale to a large program, since the number of paths to search grows exponentially with an input size. So far, such a scalability issue has been mostly managed by pruning out unpromising paths in the middle of searching based on heuristics, but this kind of work easily eliminates the true worst case as well, providing sub-optimal one only. Another way to achieve scalability is to learn a branching policy of worst-case complexity from small scale tests and apply it to a large scale. However, use cases of such a method are restricted to programs whose worst-case branching policy has a simple pattern. To address such limitations, we propose PySE that uses symbolic execution to collect the behaviors of a given branching policy, and updates the policy using a reinforcement learning approach through multiple executions. PySE's branching policy keeps evolving in a way that the length of an execution path increases in the long term, and ultimately reaches the worst-case complexity. PySE can also learn the worst-case branching policy of a complex or irregular pattern, using an artificial neural network in a fully automatic way. Experiment results demonstrate that PySE can effectively find a path of worst-case complexity for various Python benchmark programs and scales.",Machine learning;Q-learning;Symbolic execution;Worst-case complexity;Stress testing,"136, 147",,IEEE Conferences,"2019 12th IEEE Conference on Software Testing, Validation and Verification (ICST)",IEEE
600,,An Empirical Assessment of Machine Learning Approaches for Triaging Reports of a Java Static Analysis Tool,U. Koc; S. Wei; J. S. Foster; M. Carpuat; A. A. Porter,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8730210,10.1109/ICST.2019.00036,"Despite their ability to detect critical bugs in software, developers consider high false positive rates to be a key barrier to using static analysis tools in practice. To improve the usability of these tools, researchers have recently begun to apply machine learning techniques to classify and filter false positive analysis reports. Although initial results have been promising, the long-term potential and best practices for this line of research are unclear due to the lack of detailed, large-scale empirical evaluation. To partially address this knowledge gap, we present a comparative empirical study of four machine learning techniques, namely hand-engineered features, bag of words, recurrent neural networks, and graph neural networks, for classifying false positives, using multiple ground-truth program sets. We also introduce and evaluate new data preparation routines for recurrent neural networks and node representations for graph neural networks, and show that these routines can have a substantial positive impact on classification accuracy. Overall, our results suggest that recurrent neural networks (which learn over a program's source code) outperform the other subject techniques, although interesting tradeoffs are present among all techniques. Our observations provide insight into the future research needed to speed the adoption of machine learning approaches in practice.",Static analysis;false positive classification;machine learning,"288, 299",,IEEE Conferences,"2019 12th IEEE Conference on Software Testing, Validation and Verification (ICST)",IEEE
601,,An Extensive Study on Cross-Project Predictive Mutation Testing,D. Mao; L. Chen; L. Zhang,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8730151,10.1109/ICST.2019.00025,"Mutation testing is a powerful technique for evaluating the quality of test suite which plays a key role in ensuring software quality. The concept of mutation testing has also been widely used in other software engineering studies, e.g., test generation, fault localization, and program repair. During the process of mutation testing, large number of mutants may be generated and then executed against the test suite to examine whether they can be killed, making the process extremely computational expensive. Several techniques have been proposed to speed up this process, including selective, weakened, and predictive mutation testing. Among those techniques, Predictive Mutation Testing (PMT) tries to build a classification model based on an amount of mutant execution records to predict whether coming new mutants would be killed or alive without mutant execution, and can achieve significant mutation cost reduction. In PMT, each mutant is represented as a list of features related to the mutant itself and the test suite, transforming the mutation testing problem to a binary classification problem. In this paper, we perform an extensive study on the effectiveness and efficiency of the promising PMT technique under the cross-project setting using a total 654 real world projects with more than 4 Million mutants. Our work also complements the original PMT work by considering more features and the powerful deep learning models. The experimental results show an average of over 0.85 prediction accuracy on 654 projects using cross validation, demonstrating the effectiveness of PMT. Meanwhile, a clear speed up is also observed with an average of 28.7× compared to traditional mutation testing with 5 threads. In addition, we analyze the importance of different groups of features in classification model, which provides important implications for the future research.",software quality;software testing;mutation testing;machine learning;deep learning,"160, 171",,IEEE Conferences,"2019 12th IEEE Conference on Software Testing, Validation and Verification (ICST)",IEEE
602,,Extracting Quality Attributes from User Stories for Early Architecture Decision Making,F. Gilson; M. Galster; F. Georis,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8712367,10.1109/ICSA-C.2019.00031,"Software quality attributes (e.g., security, performance) influence software architecture design decisions, e.g., when choosing technologies, patterns or tactics. As software developers are moving from big upfront design to an evolutionary or emerging design, the architecture of a system evolves as more functionality is added. In agile software development, functional user requirements are often expressed as user stories. Quality attributes might be implicitly referenced in user stories. To support a more systematic analysis and reasoning about quality attributes in agile development projects, this paper explores how to automatically identify quality attributes from user stories. This could help better understand relevant quality attributes (and potential architectural key drivers) before analysing product backlogs and domains in detail and provides the “bigger picture” of potential architectural drivers for early architecture decision making. The goal of this paper is to present our vision and preliminary work towards understanding whether user stories do include information about quality attributes at all, and if so, how we can identify such information in an automated manner.",agile software development;software architecture;decision making;machine learning;natural language processing,"129, 136",,IEEE Conferences,2019 IEEE International Conference on Software Architecture Companion (ICSA-C),IEEE
603,,Tango: A Deep Neural Network Benchmark Suite for Various Accelerators,A. Karki; C. Palangotu Keshava; S. Mysore Shivakumar; J. Skow; G. Madhukeshwar Hegde; H. Jeon,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8695662,10.1109/ISPASS.2019.00021,"Deep neural networks (DNNs) have been proving the effectiveness in various computing fields. To provide more efficient computing platforms for DNN applications, it is essential to have evaluation environments that include assorted benchmark workloads. Though a few DNN benchmark suites have been recently released, most of them require to install proprietary DNN libraries or resource-intensive DNN frameworks, which are hard to run on resource-limited mobile platforms or architecture simulators. To provide a more scalable evaluation environment, we propose a new DNN benchmark suite that can run on any platform that supports CUDA and OpenCL. The proposed benchmark suite includes the most widely used five convolution neural networks and two recurrent neural networks. We provide architectural statistics of these networks while running them on an architecture simulator, a server- and a mobile-GPU, and a mobile FPGA.",Deep neural network;Benchmark Suite,"137, 138",,IEEE Conferences,2019 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS),IEEE
604,,Characterizing Sources of Ineffectual Computations in Deep Learning Networks,M. Nikolić; M. Mahmoud; A. Moshovos; Y. Zhao; R. Mullins,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8695654,10.1109/ISPASS.2019.00030,"Hardware accelerators for inference with neural networks can take advantage of the properties of data they process. Performance gains and reduced memory bandwidth during inference have been demonstrated by using narrower data types [1] [2] and by exploiting the ability to skip and compress values that are zero [3]-[6]. Similarly useful properties have been identified at a lower-level such as varying precision requirements [7] and bit-level sparsity [8] [9]. To date, the analysis of these potential sources of superfluous computation and communication has been constrained to a small number of older Convolutional Neural Networks (CNNs) used for image classification. It is an open question as to whether they exist more broadly. This paper aims to determine whether these properties persist in: (1) more recent and thus more accurate and better performing image classification networks, (2) models for image applications other than classification such as image segmentation and low-level computational imaging, (3) Long-Short-Term-Memory (LSTM) models for non-image applications such as those for natural language processing, and (4) quantized image classification models. We demonstrate that such properties persist and discuss the implications and opportunities for future accelerator designs.",Deep Learning Acceleration;Ineffectual Work;Precision;Sparsity,"165, 176",,IEEE Conferences,2019 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS),IEEE
605,,A Critical Review of Object Detection using Convolution Neural Network,S. U. Nisa; M. Imran,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8681010,10.1109/C-CODE.2019.8681010,"To recognize an object, for a human, is an easy task but for machines, to perform the same task with same efficiency is a complex task. For computer systems, images are sets of numeric values that have no meaning in itself. To make these numbers useful, diverse techniques have been proposed. Comparative to others, deep learning approaches achieved state-of-the-art performance in many computer vision applications, such as object detection, image classification, image retrieval, human pose estimation. To detect object of interest, Convolutional Neural Network (CNN) has been observed widely successful method. Few factors are there to get better accuracy and performance for instance efficient model, larger datasets and hardware support. This study aims to review CNN methods for object detection by highlighting the contribution and challenges from few recent research papers. Also how well to use these CNN techniques in combination to other methods for best suited results with other. Better performance such as increased accuracy, fast processing reduce error rates also introduced few new concerns and issues in parallel regarding the discussed methods such as time consumption, anonymous behavior of Neural Network. To address these issues a conceptual model is presented using CNN and Lease Square Support Vector Machine (LS-SVM).",computer vision;Convolutional Neural Network;detection;image;Lease Square Support Vector Machine;object,"154, 159",,IEEE Conferences,"2019 2nd International Conference on Communication, Computing and Digital systems (C-CODE)",IEEE
606,,Person Detection by Fusion of Visible and Thermal Images Using Convolutional Neural Network,B. Khalid; A. M. Khan; M. U. Akram; S. Batool,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8680991,10.1109/C-CODE.2019.8680991,"In the last couple of decades, Convolution Neural Network (CNN) emerged as the most active field of research. There are a number of applications of CNN, and its architectures are used for the improvement of accuracy and efficiency in various fields. In this paper, we aim to use CNN in order to generate fusion of visible and thermal camera images to detect persons present in those images for a reliable surveillance application. There are various kinds of image fusion methods to achieve multi-sensor, multi-modal, multi-focus and multi-view image fusion. Our proposed methodology includes Encoder-Decoder architecture for fusion of visible and thermal images and ResNet-152 architecture for classification of images to detect if there is a person present in the image or not. Korea Advanced Institute of Science and Technology (KAIST) multispectral dataset consisting of 95,000 visible and thermal images is used for training of CNNs. During experimentation, it is observed that fused architecture outperforms individual visible and thermal based architectures, where fused architecture gives 99.2% accuracy while visible gives 97% and thermal gives 97.6% accuracy.",Fusion;CNN;ResNet;Ecncoder-Decoder;Visible;Thermal,"143, 148",,IEEE Conferences,"2019 2nd International Conference on Communication, Computing and Digital systems (C-CODE)",IEEE
607,,Assessing Performance of Convolutional Features for Terrain Classification Using Remote Sensing Data,J. Ahmed; H. Ahmed,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8680969,10.1109/C-CODE.2019.8680969,"Advancements in satellite imagery and image processing techniques has enabled computerized solutions for various geographical and ecological monitoring and analysis problems. This has assisted geographical experts in topographic mapping, disaster monitoring and analysis of urban sprawl. Automated terrain segmentation and classification is a significant preliminary step in any Geographic Information System (GIS) based application. Nevertheless it is the most challenging task as well. In this paper, we present a complete schematic for assessing the effectiveness of various features and classifiers that are popularly employed in the literature for landform classification. A subset of images from DeepSat SAT-6 dataset is reconstructed using the RGB spectral band information and evaluations are performed using all six terrain classes. A number of color and texture features are then extracted from samples of each class and fed to a number of classifiers. In addition to conventional features, the effectiveness of convolutional features for terrain classification is also assessed in this study. A light convolutional neural network is proposed and trained on the employed dataset. The study also highlights the effect of color, texture and convolutional features on classification of each type of terrain under consideration. Experimental results show that convolutional features outperform both texture and color features by achieveing an overall accuracy of 93%.",Terrain classification;RGB spectral band;textural features;convolutional neural networks;feature extraction,"178, 183",,IEEE Conferences,"2019 2nd International Conference on Communication, Computing and Digital systems (C-CODE)",IEEE
608,,Bilateral Dependency Neural Networks for Cross-Language Algorithm Classification,N. D. Q. Bui; Y. Yu; L. Jiang,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8667995,10.1109/SANER.2019.8667995,"Algorithm classification is to automatically identify the classes of a program based on the algorithm(s) and/or data structure(s) implemented in the program. It can be useful for various tasks, such as code reuse, code theft detection, and malware detection. Code similarity metrics, on the basis of features extracted from syntax and semantics, have been used to classify programs. Such features, however, often need manual selection effort and are specific to individual programming languages, limiting the classifiers to programs in the same language.To recognize the similarities and differences among algorithms implemented in different languages, this paper describes a framework of Bilateral Neural Networks (Bi-NN) that builds a neural network on top of two underlying sub-networks, each of which encodes syntax and semantics of code in one language. A whole Bi-NN can be trained with bilateral programs that implement the same algorithms and/or data structures in different languages and then be applied to recognize algorithm classes across languages.We have instantiated the framework with several kinds of token-, tree- and graph-based neural networks that encode and learn various kinds of information in code. We have applied the instances of the framework to a code corpus collected from GitHub containing thousands of Java and C++ programs implementing 50 different algorithms and data structures. Our evaluation results show that the use of Bi-NN indeed produces promising algorithm classification results both within one language and across languages, and the encoding of dependencies from code into the underlying neural networks helps improve algorithm classification accuracy further. In particular, our custom-built dependency trees with tree-based convolutional neural networks achieve the highest classification accuracy among the different instances of the framework that we have evaluated. Our study points to a possible future research direction to tailor bilateral and multilateral neural networks that encode more relevant semantics for code learning, mining and analysis tasks.",cross-language mapping;program classification;algorithm classification;code embedding;code dependency;neural network;bilateral neural network,"422, 433",,IEEE Conferences,"2019 IEEE 26th International Conference on Software Analysis, Evolution and Reengineering (SANER)",IEEE
609,,DeepLink: A Code Knowledge Graph Based Deep Learning Approach for Issue-Commit Link Recovery,R. Xie; L. Chen; W. Ye; Z. Li; T. Hu; D. Du; S. Zhang,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8667969,10.1109/SANER.2019.8667969,"Links between issue reports and corresponding code commits to fix them can greatly reduce the maintenance costs of a software project. More often than not, however, these links are missing and thus cannot be fully utilized by developers. Current practices in issue-commit link recovery extract text features and code features in terms of textual similarity from issue reports and commit logs to train their models. These approaches are limited since semantic information could be lost. Furthermore, few of them consider the effect of source code files related to a commit on issue-commit link recovery, let alone the semantics of code context. To tackle these problems, we propose to construct code knowledge graph of a code repository and generate embeddings of source code files to capture the semantics of code context. We also use embeddings to capture the semantics of issue- or commit-related text. Then we use these embeddings to calculate semantic similarity and code similarity using a deep learning approach before training a SVM binary classification model with additional features. Evaluations on real-world projects show that our approach DeepLink can outperform the state-of-the-art method.",Issue-Commit Link;Semantic Similarity;Code Knowledge Graph;Code Context;Code Embeddings;Deep Learning,"434, 444",,IEEE Conferences,"2019 IEEE 26th International Conference on Software Analysis, Evolution and Reengineering (SANER)",IEEE
610,,Deep Review Sharing,C. Guo; D. Huang; N. Dong; Q. Ye; J. Xu; Y. Fan; H. Yang; Y. Xu,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8668037,10.1109/SANER.2019.8668037,"Review-Based Software Improvement (RBSI for short) has drawn increasing research attentions in recent years. Relevant efforts focus on how to leverage the underlying information within reviews to obtain a better guidance for further updating. However, few efforts consider the Projects Without sufficient Reviews (PWR for short). Actually, PWR dominates the software projects, and the lack of PWR-based RBSI research severely blocks the improvement of certain software. In this paper, we make the first attempt to pave the road. Our goal is to establish a generic framework for sharing suitable and informative reviews to arbitrary PWR. To achieve this goal, we exploit techniques of code clone detection and review ranking. In order to improve the sharing precision, we introduce Convolutional Neural Network (CNN) into our clone detection, and design a novel CNN based clone searching module for our sharing system. Meanwhile, we adopt a heuristic filtering strategy to reduce the sharing time cost. We implement a prototype review sharing system RSharer and collect 72,440 code-review pairs as our ground knowledge. Empirical experiments on hundreds of real code fragments verify the effectiveness of RSharer. RSharer also achieves positive response and evaluation by expert developers.",code clone;software review;deep learning;CNN,"61, 72",,IEEE Conferences,"2019 IEEE 26th International Conference on Software Analysis, Evolution and Reengineering (SANER)",IEEE
611,,DeepCT: Tomographic Combinatorial Testing for Deep Learning Systems,L. Ma; F. Juefei-Xu; M. Xue; B. Li; L. Li; Y. Liu; J. Zhao,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8668044,10.1109/SANER.2019.8668044,"Deep learning (DL) has achieved remarkable progress over the past decade and has been widely applied to many industry domains. However, the robustness of DL systems recently becomes great concerns, where minor perturbation on the input might cause the DL malfunction. These robustness issues could potentially result in severe consequences when a DL system is deployed to safety-critical applications and hinder the real-world deployment of DL systems. Testing techniques enable the robustness evaluation and vulnerable issue detection of a DL system at an early stage. The main challenge of testing a DL system attributes to the high dimensionality of its inputs and large internal latent feature space, which makes testing each state almost impossible. For traditional software, combinatorial testing (CT) is an effective testing technique to balance the testing exploration effort and defect detection capabilities. In this paper, we perform an exploratory study of CT on DL systems. We propose a set of combinatorial testing criteria specialized for DL systems, as well as a CT coverage guided test generation technique. Our evaluation demonstrates that CT provides a promising avenue for testing DL systems.",Deep learning;combinatorial testing;robustness,"614, 618",,IEEE Conferences,"2019 IEEE 26th International Conference on Software Analysis, Evolution and Reengineering (SANER)",IEEE
612,,Learning-Based Recursive Aggregation of Abstract Syntax Trees for Code Clone Detection,L. Büch; A. Andrzejak,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8668039,10.1109/SANER.2019.8668039,"Code clone detection remains a crucial challenge in maintaining software projects. Many classic approaches rely on handcrafted aggregation schemes, while recent work uses supervised or unsupervised learning. In this work, we study several aspects of aggregation schemes for code clone detection based on supervised learning. To this aim, we implement an AST-based Recursive Neural Network. Firstly, our ablation study shows the influence of model choices and hyperparameters. We introduce error scaling as a way to effectively and efficiently address the class imbalance problem arising in code clone detection. Secondly, we study the influence of pretrained embeddings representing nodes in ASTs. We show that simply averaging all node vectors of a given AST yields strong baseline aggregation scheme. Further, learned AST aggregation schemes greatly benefit from pretrained node embeddings. Finally, we show the importance of carefully separating training and test data by clone clusters, to reliably measure generalization of models learned with supervision.",Code Clone Detection;Abstract Syntax Trees;Embeddings;Recursive Neural Network;Siamese Network,"95, 104",,IEEE Conferences,"2019 IEEE 26th International Conference on Software Analysis, Evolution and Reengineering (SANER)",IEEE
613,,Systematic Comprehension for Developer Reply in Mobile System Forum,C. Guo; W. Wang; Y. Wu; N. Dong; Q. Ye; J. Xu; S. Zhang,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8668016,10.1109/SANER.2019.8668016,"Review-based software development has become increasingly prevalent in recent years. Existing efforts aiming at either informative evaluation or sentiment analysis are mainly from the perspective of the reviewers, while neglecting the attitude and behavior of the developers. Such efforts inevitably suffer from recommendation bias in practice, and thus benefit little for the improvement of user reviews.In this paper, we attempt to bridge the gap between user review and developer reply, and conduct a systematic study for review reply in development forums, especially in Chinese mobile system forums. To this end, we concentrate on three research questions: 1) should a targeted review be replied; 2) how long time it should be replied; 3) does traditional review analysis help to pursue a reply for certain review? To answer such questions, given certain review datasets, we perform a systematical study including the following three stages: 1) a binary classification for reply behavior prediction, 2) a regression for prediction of reply time, 3) a systematic factor study for the relationship between traditional review analysis and reply performance. To enhance the accuracy of prediction and analysis, we proposed a CNN-based weak-supervision analysis framework, which exploits manifold techniques from NLP and deep learning. We validate our approach via extensive comparison experiments. The results show that our analysis framework is effective. More importantly, we have uncovered several interesting findings, which provide valuable guidance for further review improvement and recommendation.",review understanding;deep learning;classification;regression developer perspective;weakly supervised learning,"242, 252",,IEEE Conferences,"2019 IEEE 26th International Conference on Software Analysis, Evolution and Reengineering (SANER)",IEEE
614,,A Neural Model for Method Name Generation from Functional Description,S. Gao; C. Chen; Z. Xing; Y. Ma; W. Song; S. -W. Lin,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8667994,10.1109/SANER.2019.8667994,"The names of software artifacts, e.g., method names, are important for software understanding and maintenance, as good names can help developers easily understand others' code. However, the existing naming guidelines are difficult for developers, especially novices, to come up with meaningful, concise and compact names for the variables, methods, classes and files. With the popularity of open source, an enormous amount of project source code can be accessed, and the exhaustiveness and instability of manually naming methods could now be relieved by automatically learning a naming model from a large code repository. Nevertheless, building a comprehensive naming system is still challenging, due to the gap between natural language functional descriptions and method names. Specifically, there are three challenges: how to model the relationship between the functional descriptions and formal method names, how to handle the explosion of vocabulary when dealing with large repositories, and how to leverage the knowledge learned from large repositories to a specific project. To answer these questions, we propose a neural network to directly generate readable method names from natural language description. The proposed method is built upon the encoder-decoder framework with the attention and copying mechanisms. Our experiments show that our method can generate meaningful and accurate method names and achieve significant improvement over the state-of-the-art baseline models. We also address the cold-start problem using a training trick to utilize big data in Github for specific projects.",Naming Convention;Encoder-Decoder Model;Transfer Learning,"414, 421",,IEEE Conferences,"2019 IEEE 26th International Conference on Software Analysis, Evolution and Reengineering (SANER)",IEEE
615,,Model-Based Adaptation for Robotics Software,J. Aldrich; D. Garlan; C. Kaestner; C. Le Goues; A. Mohseni-Kabir; I. Ruchkin; S. Samuel; B. Schmerl; C. S. Timperley; M. Veloso; I. Voysey; J. Biswas; A. Guha; J. Holtz; J. Camara; P. Jamshidi,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8648279,10.1109/MS.2018.2885058,"We developed model-based adaptation, an approach that leverages models of software and its environment to enable automated adaptation. The goal of our approach is to build long-lasting software systems that can effectively adapt to changes in their environment.",,"83, 90",,IEEE Magazines,IEEE Software,IEEE
616,,Approximate Oracles and Synergy in Software Energy Search Spaces,B. R. Bruce; J. Petke; M. Harman; E. T. Barr,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8338061,10.1109/TSE.2018.2827066,"Reducing the energy consumption of software systems through optimisation techniques such as genetic improvement is gaining interest. However, efficient and effective improvement of software systems requires a better understanding of the code-change search space. One important choice practitioners have is whether to preserve the system's original output or permit approximation, with each scenario having its own search space characteristics. When output preservation is a hard constraint, we report that the maximum energy reduction achievable by the modification operators is 2.69 percent (0.76 percent on average). By contrast, this figure increases dramatically to 95.60 percent (33.90 percent on average) when approximation is permitted, indicating the critical importance of approximate output quality assessment for code optimisation. We investigate synergy, a phenomenon that occurs when simultaneously applied source code modifications produce an effect greater than their individual sum. Our results reveal that 12.0 percent of all joint code modifications produced such a synergistic effect, though 38.5 percent produce an antagonistic interaction in which simultaneously applied modifications are less effective than when applied individually. This highlights the need for more advanced search-based techniques.",Search-based software engineering;search space;energy consumption;genetic improvement;synergy;antagonism;oracle;approximation,"1150, 1169",,IEEE Journals,IEEE Transactions on Software Engineering,IEEE
617,,Bellwethers: A Baseline Method for Transfer Learning,R. Krishna; T. Menzies,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8329264,10.1109/TSE.2018.2821670,"Software analytics builds quality prediction models for software projects. Experience shows that (a) the more projects studied, the more varied are the conclusions; and (b) project managers lose faith in the results of software analytics if those results keep changing. To reduce this conclusion instability, we propose the use of “bellwethers”: given N projects from a community the bellwether is the project whose data yields the best predictions on all others. The bellwethers offer a way to mitigate conclusion instability because conclusions about a community are stable as long as this bellwether continues as the best oracle. Bellwethers are also simple to discover (just wrap a for-loop around standard data miners). When compared to other transfer learning methods (TCA+, transfer Naive Bayes, value cognitive boosting), using just the bellwether data to construct a simple transfer learner yields comparable predictions. Further, bellwethers appear in many SE tasks such as defect prediction, effort estimation, and bad smell detection. We hence recommend using bellwethers as a baseline method for transfer learning against which future work should be compared.",Transfer learning;defect prediction;bad smells;issue close time;effort estimation;prediction,"1081, 1105",,IEEE Journals,IEEE Transactions on Software Engineering,IEEE
618,,A Deep Learning Model for Estimating Story Points,M. Choetkiertikul; H. K. Dam; T. Tran; T. Pham; A. Ghose; T. Menzies,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8255666,10.1109/TSE.2018.2792473,"Although there has been substantial research in software analytics for effort estimation in traditional software projects, little work has been done for estimation in agile projects, especially estimating the effort required for completing user stories or issues. Story points are the most common unit of measure used for estimating the effort involved in completing a user story or resolving an issue. In this paper, we propose a prediction model for estimating story points based on a novel combination of two powerful deep learning architectures: long short-term memory and recurrent highway network. Our prediction system is end-to-end trainable from raw input data to prediction outcomes without any manual feature engineering. We offer a comprehensive dataset for story points-based estimation that contains 23,313 issues from 16 open source projects. An empirical evaluation demonstrates that our approach consistently outperforms three common baselines (Random Guessing, Mean, and Median methods) and six alternatives (e.g., using Doc2Vec and Random Forests) in Mean Absolute Error, Median Absolute Error, and the Standardized Accuracy.",Software analytics;effort estimation;story point estimation;deep learning,"637, 656",,IEEE Journals,IEEE Transactions on Software Engineering,IEEE
619,,ConPredictor: Concurrency Defect Prediction in Real-World Applications,T. Yu; W. Wen; X. Han; J. H. Hayes,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8252721,10.1109/TSE.2018.2791521,"Concurrent programs are difficult to test due to their inherent non-determinism. To address this problem, testing often requires the exploration of thread schedules of a program; this can be time-consuming when applied to real-world programs. Software defect prediction has been used to help developers find faults and prioritize their testing efforts. Prior studies have used machine learning to build such predicting models based on designed features that encode the characteristics of programs. However, research has focused on sequential programs; to date, no work has considered defect prediction for concurrent programs, with program characteristics distinguished from sequential programs. In this paper, we present ConPredictor, an approach to predict defects specific to concurrent programs by combining both static and dynamic program metrics. Specifically, we propose a set of novel static code metrics based on the unique properties of concurrent programs. We also leverage additional guidance from dynamic metrics constructed based on mutation analysis. Our evaluation on four large open source projects shows that ConPredictor improved both within-project defect prediction and cross-project defect prediction compared to traditional features.",Concurrency;defect prediction;software quality;software metrics,"558, 575",,IEEE Journals,IEEE Transactions on Software Engineering,IEEE
620,,Text Filtering and Ranking for Security Bug Report Prediction,F. Peters; T. T. Tun; Y. Yu; B. Nuseibeh,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8240740,10.1109/TSE.2017.2787653,"Security bug reports can describe security critical vulnerabilities in software products. Bug tracking systems may contain thousands of bug reports, where relatively few of them are security related. Therefore finding unlabelled security bugs among them can be challenging. To help security engineers identify these reports quickly and accurately, text-based prediction models have been proposed. These can often mislabel security bug reports due to a number of reasons such as class imbalance, where the ratio of non-security to security bug reports is very high. More critically, we have observed that the presence of security related keywords in both security and non-security bug reports can lead to the mislabelling of security bug reports. This paper proposes FARSEC, a framework for filtering and ranking bug reports for reducing the presence of security related keywords. Before building prediction models, our framework identifies and removes non-security bug reports with security related keywords. We demonstrate that FARSEC improves the performance of text-based prediction models for security bug reports in 90 percent of cases. Specifically, we evaluate it with 45,940 bug reports from Chromium and four Apache projects. With our framework, we mitigate the class imbalance issue and reduce the number of mislabelled security bug reports by 38 percent.",Security cross words;security related keywords;security bug reports;text filtering;ranking;prediction models;transfer learning,"615, 631",,IEEE Journals,IEEE Transactions on Software Engineering,IEEE
621,,On the Multiple Sources and Privacy Preservation Issues for Heterogeneous Defect Prediction,Z. Li; X. -Y. Jing; X. Zhu; H. Zhang; B. Xu; S. Ying,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8168387,10.1109/TSE.2017.2780222,"Heterogeneous defect prediction (HDP) refers to predicting defect-proneness of software modules in a target project using heterogeneous metric data from other projects. Existing HDP methods mainly focus on predicting target instances with single source. In practice, there exist plenty of external projects. Multiple sources can generally provide more information than a single project. Therefore, it is meaningful to investigate whether the HDP performance can be improved by employing multiple sources. However, a precondition of conducting HDP is that the external sources are available. Due to privacy concerns, most companies are not willing to share their data. To facilitate data sharing, it is essential to study how to protect the privacy of data owners before they release their data. In this paper, we study the above two issues in HDP. Specifically, to utilize multiple sources effectively, we propose a multi-source selection based manifold discriminant alignment (MSMDA) approach. To protect the privacy of data owners, a sparse representation based double obfuscation algorithm is designed and applied to HDP. Through a case study of 28 projects, our results show that MSMDA can achieve better performance than a range of baseline methods. The improvement is 3.4-15.3 percent in g-measure and 3.0-19.1 percent in AUG.",Heterogeneous defect prediction;multiple sources;privacy preservation;utility;source selection;manifold discriminant alignment,"391, 411",,IEEE Journals,IEEE Transactions on Software Engineering,IEEE
622,,A Systematic Literature Review and Meta-Analysis on Cross Project Defect Prediction,S. Hosseini; B. Turhan; D. Gunarathna,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8097045,10.1109/TSE.2017.2770124,"Background: Cross project defect prediction (CPDP) recently gained considerable attention, yet there are no systematic efforts to analyse existing empirical evidence. Objective: To synthesise literature to understand the state-of-the-art in CPDP with respect to metrics, models, data approaches, datasets and associated performances. Further, we aim to assess the performance of CPDP versus within project DP models. Method: We conducted a systematic literature review. Results from primary studies are synthesised (thematic, meta-analysis) to answer research questions. Results: We identified 30 primary studies passing quality assessment. Performance measures, except precision, vary with the choice of metrics. Recall, precision, f-measure, and AUC are the most common measures. Models based on Nearest-Neighbour and Decision Tree tend to perform well in CPDP, whereas the popular naïve Bayes yields average performance. Performance of ensembles varies greatly across f-measure and AUC. Data approaches address CPDP challenges using row/column processing, which improve CPDP in terms of recall at the cost of precision. This is observed in multiple occasions including the meta-analysis of CPDP versus WPDP. NASA and Jureczko datasets seem to favour CPDP over WPDP more frequently. Conclusion: CPDP is still a challenge and requires more research before trustworthy applications can take place. We provide guidelines for further research.",Defect prediction;fault prediction;cross project;systematic literature review;meta-analysis;within project,"111, 147",,IEEE Journals,IEEE Transactions on Software Engineering,IEEE
623,,Discrimination of Facial Image Generated via GAN (Work-in-Progress),H. Choi; E. Choi,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9092321,10.1109/ICSSA45270.2018.00027,"Generated fake facial images remain a serious problem for corporations, governments, developers and individuals, as the voice of anxiety about the side effects of artificial intelligence grows. However, today the AI is still done mainly as a way to keep up with a real facial image rather than researching how to discriminate the generated image. As the world that is no longer able to distinguish between real and fake facial images is coming, the need for radical AI technology to detect generated images arises. In this paper, we introduce an approach that addresses these issues, describing in feasible detail the discriminative models based on various machine learning algorithms. Specifically, we show that the model with the highest accuracy in supervised learning achieved a 92.5% detection rate at 7.5% false positive rate (FPR), out of 400 images. And we have also achieved positive results in unsupervised learning. Our results demonstrate that the fake facial images generated by the GAN can be discriminated by the machine learning algorithms. Since GAN models tend to improve rapidly, we foresee new neural network discrimination models gaining in importance as part of a generated image detection strategy in coming years.",AI;GAN;CNN;Clustering;Discrimination,"77, 80",,IEEE Conferences,2018 International Conference on Software Security and Assurance (ICSSA),IEEE
624,,PerfLearner: Learning from Bug Reports to Understand and Generate Performance Test Frames,X. Han; T. Yu; D. Lo,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9000010,10.1145/3238147.3238204,"Software performance is important for ensuring the quality of software products. Performance bugs, defined as programming errors that cause significant performance degradation, can lead to slow systems and poor user experience. While there has been some research on automated performance testing such as test case generation, the main idea is to select workload values to increase the program execution times. These techniques often assume the initial test cases have the right combination of input parameters and focus on evolving values of certain input parameters. However, such an assumption may not hold for highly configurable real-word applications, in which the combinations of input parameters can be very large. In this paper, we manually analyze 300 bug reports from three large open source projects - Apache HTTP Server, MySQL, and Mozilla Firefox. We found that 1) exposing performance bugs often requires combinations of multiple input parameters, and 2) certain input parameters are frequently involved in exposing performance bugs. Guided by these findings, we designed and evaluated an automated approach, PerfLearner, to extract execution commands and input parameters from descriptions of performance bug reports and use them to generate test frames for guiding actual performance test case generation.",Software Testing;Performance Bugs;Software Mining,"17, 28",,IEEE Conferences,2018 33rd IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
625,,AutoConfig: Automatic Configuration Tuning for Distributed Message Systems,L. Bao; X. Liu; Z. Xu; B. Fang,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9000079,10.1145/3238147.3238175,"Distributed message systems (DMSs) serve as the communication backbone for many real-time streaming data processing applications. To support the vast diversity of such applications, DMSs provide a large number of parameters to configure. However, It overwhelms for most users to configure these parameters well for better performance. Although many automatic configuration approaches have been proposed to address this issue, critical challenges still remain: 1) to train a better and robust performance prediction model using a limited number of samples, and 2) to search for a high-dimensional parameter space efficiently within a time constraint. In this paper, we propose AutoConfig - an automatic configuration system that can optimize producer-side throughput on DMSs. AutoConfig constructs a novel comparison-based model (CBM) that is more robust that the prediction-based model (PBM) used by previous learning-based approaches. Furthermore, AutoConfig uses a weighted Latin hypercube sampling (wLHS) approach to select a set of samples that can provide a better coverage over the high-dimensional parameter space. wLHS allows AutoConfig to search for more promising configurations using the trained CBM. We have implemented AutoConfig on the Kafka platform, and evaluated it using eight different testing scenarios deployed on a public cloud. Experimental results show that our CBM can obtain better results than that of PBM under the same random forests based model. Furthermore, AutoConfig outperforms default configurations by 215.40% on average, and five state-of-the-art configuration algorithms by 7.21%-64.56%.",distributed message system;automatic configuration tuning;comparison-based model;weighted Latin hypercube sampling,"29, 40",,IEEE Conferences,2018 33rd IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
626,,DeepRoad: GAN-Based Metamorphic Testing and Input Validation Framework for Autonomous Driving Systems,M. Zhang; Y. Zhang; L. Zhang; C. Liu; S. Khurshid,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9000040,10.1145/3238147.3238187,"While Deep Neural Networks (DNNs) have established the fundamentals of image-based autonomous driving systems, they may exhibit erroneous behaviors and cause fatal accidents. To address the safety issues in autonomous driving systems, a recent set of testing techniques have been designed to automatically generate artificial driving scenes to enrich test suite, e.g., generating new input images transformed from the original ones. However, these techniques are insufficient due to two limitations: first, many such synthetic images often lack diversity of driving scenes, and hence compromise the resulting efficacy and reliability. Second, for machine-learning-based systems, a mismatch between training and application domain can dramatically degrade system accuracy, such that it is necessary to validate inputs for improving system robustness. In this paper, we propose DeepRoad, an unsupervised DNN-based framework for automatically testing the consistency of DNN-based autonomous driving systems and online validation. First, DeepRoad automatically synthesizes large amounts of diverse driving scenes without using image transformation rules (e.g. scale, shear and rotation). In particular, DeepRoad is able to produce driving scenes with various weather conditions (including those with rather extreme conditions) by applying Generative Adversarial Networks (GANs) along with the corresponding real-world weather scenes. Second, DeepRoad utilizes metamorphic testing techniques to check the consistency of such systems using synthetic images. Third, DeepRoad validates input images for DNN-based systems by measuring the distance of the input and training images using their VGGNet features. We implement DeepRoad to test three well-recognized DNN-based autonomous driving systems in Udacity self-driving car challenge. The experimental results demonstrate that DeepRoad can detect thousands of inconsistent behaviors for these systems, and effectively validate input images to potentially enhance the system robustness as well.",Software testing;Test generation;Input validation;Deep Neural Networks,"132, 142",,IEEE Conferences,2018 33rd IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
627,,DeepGauge: Multi-Granularity Testing Criteria for Deep Learning Systems,L. Ma; F. Juefei-Xu; F. Zhang; J. Sun; M. Xue; B. Li; C. Chen; T. Su; L. Li; Y. Liu; J. Zhao; Y. Wang,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9000033,10.1145/3238147.3238202,"Deep learning (DL) defines a new data-driven programming paradigm that constructs the internal system logic of a crafted neuron network through a set of training data. We have seen wide adoption of DL in many safety-critical scenarios. However, a plethora of studies have shown that the state-of-the-art DL systems suffer from various vulnerabilities which can lead to severe consequences when applied to real-world applications. Currently, the testing adequacy of a DL system is usually measured by the accuracy of test data. Considering the limitation of accessible high quality test data, good accuracy performance on test data can hardly provide confidence to the testing adequacy and generality of DL systems. Unlike traditional software systems that have clear and controllable logic and functionality, the lack of interpretability in a DL system makes system analysis and defect detection difficult, which could potentially hinder its real-world deployment. In this paper, we propose DeepGauge, a set of multi-granularity testing criteria for DL systems, which aims at rendering a multi-faceted portrayal of the testbed. The in-depth evaluation of our proposed testing criteria is demonstrated on two well-known datasets, five DL systems, and with four state-of-the-art adversarial attack techniques against DL. The potential usefulness of DeepGauge sheds light on the construction of more generic and robust DL systems.",Deep learning;Software testing;Deep neural networks;Testing criteria,"120, 131",,IEEE Conferences,2018 33rd IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
628,,Expandable Group Identification in Spreadsheets,W. Dou; S. Han; L. Xu; D. Zhang; J. Wei,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9000019,10.1145/3238147.3238222,"Spreadsheets are widely used in various business tasks. Spreadsheet users may put similar data and computations by repeating a block of cells (a unit) in their spreadsheets. We name the unit and all its expanding ones as an expandable group. All units in an expandable group share the same or similar formats and semantics. As a data storage and management tool, expandable groups represent the fundamental structure in spreadsheets. However, existing spreadsheet systems do not recognize any expandable groups. Therefore, other spreadsheet analysis tools, e.g., data integration and fault detection, cannot utilize this structure of expandable groups to perform precise analysis. In this paper, we propose ExpCheck to automatically extract expandable groups in spreadsheets. We observe that continuous units that share the similar formats and semantics are likely to be an expandable group. Inspired by this, we inspect the format of each cell and its corresponding semantics, and further classify them into expandable groups according to their similarity. We evaluate ExpCheck on 120 spreadsheets randomly sampled from the EUSES and VEnron corpora. The experimental results show that ExpCheck is effective. ExpCheck successfully detect expandable groups with F1-measure of 73.1%, significantly outperforming the state-of-the-art techniques (F1-measure of 13.3%).",Spreadsheet;expandable group,"498, 508",,IEEE Conferences,2018 33rd IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
629,,Improving Automatic Source Code Summarization via Deep Reinforcement Learning,Y. Wan; Z. Zhao; M. Yang; G. Xu; H. Ying; J. Wu; P. S. Yu,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9000003,10.1145/3238147.3238206,"Code summarization provides a high level natural language description of the function performed by code, as it can benefit the software maintenance, code categorization and retrieval. To the best of our knowledge, most state-of-the-art approaches follow an encoder-decoder framework which encodes the code into a hidden space and then decode it into natural language space, suffering from two major drawbacks: a) Their encoders only consider the sequential content of code, ignoring the tree structure which is also critical for the task of code summarization; b) Their decoders are typically trained to predict the next word by maximizing the likelihood of next ground-truth word with previous ground-truth word given. However, it is expected to generate the entire sequence from scratch at test time. This discrepancy can cause an exposure bias issue, making the learnt decoder suboptimal. In this paper, we incorporate an abstract syntax tree structure as well as sequential content of code snippets into a deep reinforcement learning framework (i.e., actor-critic network). The actor network provides the confidence of predicting the next word according to current state. On the other hand, the critic network evaluates the reward value of all possible extensions of the current state and can provide global guidance for explorations. We employ an advantage reward composed of BLEU metric to train both networks. Comprehensive experiments on a real-world dataset show the effectiveness of our proposed model when compared with some state-of-the-art methods.",Code summarization;comment generation;deep learning;reinforcement learning,"397, 407",,IEEE Conferences,2018 33rd IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
630,,API Method Recommendation without Worrying about the Task-API Knowledge Gap,Q. Huang; X. Xia; Z. Xing; D. Lo; X. Wang,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9000025,10.1145/3238147.3238191,"Developers often need to search for appropriate APIs for their programming tasks. Although most libraries have API reference documentation, it is not easy to find appropriate APIs due to the lexical gap and knowledge gap between the natural language description of the programming task and the API description in API documentation. Here, the lexical gap refers to the fact that the same semantic meaning can be expressed by different words, and the knowledge gap refers to the fact that API documentation mainly describes API functionality and structure but lacks other types of information like concepts and purposes, which are usually the key information in the task description. In this paper, we propose an API recommendation approach named BIKER (Bi-Information source based KnowledgE Recommendation) to tackle these two gaps. To bridge the lexical gap, BIKER uses word embedding technique to calculate the similarity score between two text descriptions. Inspired by our survey findings that developers incorporate Stack Overflow posts and API documentation for bridging the knowledge gap, BIKER leverages Stack Overflow posts to extract candidate APIs for a program task, and ranks candidate APIs by considering the query's similarity with both Stack Overflow posts and API documentation. It also summarizes supplementary information (e.g., API description, code examples in Stack Overflow posts) for each API to help developers select the APIs that are most relevant to their tasks. Our evaluation with 413 API-related questions confirms the effectiveness of BIKER for both class- and method-level API recommendation, compared with state-of-the-art baselines. Our user study with 28 Java developers further demonstrates the practicality of BIKER for API search.",API Recommendation;API Documentation;Stack Overflow;Word Embedding,"293, 304",,IEEE Conferences,2018 33rd IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
631,,Concolic Testing for Deep Neural Networks,Y. Sun; M. Wu; W. Ruan; X. Huang; M. Kwiatkowska; D. Kroening,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9000035,10.1145/3238147.3238172,"Concolic testing combines program execution and symbolic analysis to explore the execution paths of a software program. This paper presents the first concolic testing approach for Deep Neural Networks (DNNs). More specifically, we formalise coverage criteria for DNNs that have been studied in the literature, and then develop a coherent method for performing concolic testing to increase test coverage. Our experimental results show the effectiveness of the concolic testing approach in both achieving high coverage and finding adversarial examples.",neural networks;symbolic execution;concolic testing,"109, 119",,IEEE Conferences,2018 33rd IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
632,,Automatic Tag Recommendation for Software Development Video Tutorials,E. Parra; J. Escobar-Avila; S. Haiduc,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8973025,,"Software development video tutorials are emerging as a new resource for developers to support their information needs. However, when trying to find the right video to watch for a task at hand, developers have little information at their disposal to quickly decide if they found the right video or not. This can lead to missing the best tutorials or wasting time watching irrelevant ones. Other external sources of information for developers, such as StackOverflow, have benefited from the existence of informative tags, which help developers to quickly gauge the relevance of posts and find related ones. We argue that the same is valid also for videos and propose the first set of approaches to automatically generate tags describing the contents of software development video tutorials. We investigate seven tagging approaches for this purpose, some using information retrieval techniques and leveraging only the information in the videos, others relying on external sources of information, such as StackOverflow, as well as two out-of-the-box commercial video tagging approaches. We evaluated 19 different configurations of these tagging approaches and the results of a user study showed that some of the information retrieval-based approaches performed the best and were able to recommend tags that developers consider relevant for describing programming videos.",Video tutorials;automatic tagging;software engineering;information retrieval,"222, 22210",,IEEE Conferences,2018 IEEE/ACM 26th International Conference on Program Comprehension (ICPC),IEEE
633,,On Artificial Intelligent Malware Tolerant Networking for IoT,M. Zolotukhin; T. Hämäläinen,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8725767,10.1109/NFV-SDN.2018.8725767,"With the recent progress in development of low-budget sensors and machine-to-machine communication, the Internet of Things (IoT) has attracted considerable attention. Unfortunately, many of today's IoT devices are rushed to market with little consideration for basic security and privacy protection making them easy targets for various attacks. As a result, number of malware and their variants designed for IoT devices has been constantly increasing. Traditional intrusion detection approaches are unsuitable for IoT networks due to limited computational capacity of smart devices and diversity in their technology. In this paper, we propose a defense system for IoT networks based on software-defined networking and network function virtualization. The defense system core component is a reinforcement machine learning agent that evaluates risks of potential attack and takes the most optimal action in order to mitigate it.",Network security;intrusion detection;IoT;SDN;NFV;machine learning;reinforcement learning,"1, 6",,IEEE Conferences,2018 IEEE Conference on Network Function Virtualization and Software Defined Networks (NFV-SDN),IEEE
634,,SLAMPA: Recommending Code Snippets with Statistical Language Model,S. Zhou; H. Zhong; B. Shen,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8719437,10.1109/APSEC.2018.00022,"Programming is typically a difficult and repetitive task. Programmers will encounter endless problems during programming, and they often need to write similar code over and over again. Over the years, many tools have been proposed to support programming. However, to the best of our knowledge, these approaches require high-quality queries or programming contexts, which are often difficult to be built or even unavailable. To address this challenge, we propose SLAMPA, a novel tool which takes advantage of statistical language model and clone detection techniques to recommend code snippets during programming. Given a piece of incomplete code, SLAMPA first infers its intention using a neural language model. Then it retrieves code snippets from codebase with the support of an efficient clone detection technology Hybrid-CD we proposed. Finally, it recommends the most similar code snippets to programmers. Our evaluation results demonstrate that Hybrid-CD precisely detects similar code snippets and it outperforms previous techniques. Our results also show that the snippets recommended by SLAMPA catch the intention of programmers and SLAMPA is capable of finding potential code reuse opportunities during programming.",code snippets recommendation;code reuse;statistical language model;clone detection,"79, 88",,IEEE Conferences,2018 25th Asia-Pacific Software Engineering Conference (APSEC),IEEE
635,,A Top-k Learning to Rank Approach to Cross-Project Software Defect Prediction,F. Wang; J. Huang; Y. Ma,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8719520,10.1109/APSEC.2018.00048,"Cross-project defect prediction (CPDP) has recently attracted increasing attention in the field of Software Engineering. Most of the previous studies, which treated it as a binary classification problem or a regression problem, are not practical for software testing activities. To provide developers with a more valuable ranking of the most severe entities (e.g., classes and modules), in this paper, we propose a top-k learning to rank (LTR) approach in the scenario of CPDP. In particular, we first convert the number of defects into graded relevance to a specific query according to the three-sigma rule; then, we put forward a new data resampling method called SMOTE-PENN to tackle the imbalanced data problem. An empirical study on the PROMISE dataset shows that SMOTE-PENN outperforms the other six competitive resampling algorithms and RankNet performs the best for the proposed approach framework. Thus, our work could lay a foundation for efficient search engines for top-ranked defective entities in real software testing activities without local historical data for a target project.","top-k ranking, relevance, resampling, Wilcoxon signed-rank test, transfer learning","335, 344",,IEEE Conferences,2018 25th Asia-Pacific Software Engineering Conference (APSEC),IEEE
636,,MatGap: A Systematic Approach to Perform Match and Gap Analysis among SBVR-Based Domain Specific Business Rules,S. Mitra; C. Prakash; S. Chakraborty; P. K. Chittimalli,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8719543,10.1109/APSEC.2018.00070,"In the modern age, the need for automation has led to Business Organizations representing their functionality as structured Business Rules. SBVR has come up as an universally popular format for representation of Business Rules. The presence of different Business Organizations working in a particular real life domain results in generation of different rules for each of the organization. Due to the varying business practices, like mergers & acquisitions, upgrades, incorporation of a new application, etc., it becomes necessary to compare a set of Business Rules of a particular organization with the rules of a reference model, to get a measure of similarity among the business functionality of the two. Presently, this comparison is carried out manually by business experts or by executing the rules of one organization with the data of another and checking if they are compliant. Both the approaches are extremely tedious and expensive as modern organizations have huge rule sets and data sets.We present MatGap, a tool which performs a systematic Match and Gap Analysis between two sets of SBVR-based Business Rules applicable to a specific domain, using Global Vectors(GloVe) model and SMT-LIBv2. The analysis report gives a measure of Match among the rules and entities, thus providing the best alignment and aids to identify the representational Gaps(if any) among the rules and entities. The tool also checks whether the embedded logic in the reference Business Rule set is covered by the other Rule set, thus highlighting the business functionality gap that is present in the latter.","Natural Language Processing, Gap Analysis, Matching, Verification, SMT, Business Rules, SBVR, Clustering","551, 560",,IEEE Conferences,2018 25th Asia-Pacific Software Engineering Conference (APSEC),IEEE
637,,Detecting Duplicate Bug Reports with Convolutional Neural Networks,Q. Xie; Z. Wen; J. Zhu; C. Gao; Z. Zheng,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8719497,10.1109/APSEC.2018.00056,"Bug tracking systems are widely used to track bugs from users during the lifecycle of software systems for reliability maintenance. When software systems have a large user base, which is common in practice, different users may encounter a same bug and then generate many duplicate bug reports. In a large project, each bug report is usually assigned to a different developer or team to parallelize the bug debugging and fixing activities. The presence of duplicate bug reports thus leads to many unnecessary efforts of developers spending on debugging a same issue. To speed up the bug fixing process and save the cost of developers, there is a high demand for automated detection of duplicate bug reports. In this paper, we explore the use of powerful deep learning techniques, including word embedding and Convolution Neural Networks, to calculate the similarity between a pair of bug reports and thus identify possible duplicates. In contrast to previous work that consider only common words between bug descriptions for lexical similarity computation, our approach is able to better capture semantic similarity between words. We further improve traditional CNN models by combining some domain-specific features extracted from bug reports. Evaluation results on the bug reports from four popular open-source projects show that DBR-CNN has made a significant improvement on duplicate detection accuracy over traditional approaches.","Software reliability, bug reports, duplicate detection, deep learning, CNN","416, 425",,IEEE Conferences,2018 25th Asia-Pacific Software Engineering Conference (APSEC),IEEE
638,,Face Attributes Prediction Based on Deep Learning,F. Ying; W. Xianliang; W. Yannan; Z. Zhaoxing; S. Yilin,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8663729,10.1109/ICSESS.2018.8663729,"Predicting face attributes is of great value in business user management and video surveillance as well as other fields. The face attributes mentioned in the topic include the factors such as gender, age, glasses, ethnic and expression. This paper proposes a kind of `end-to-end' machine learning method to predict these five attributes. The convolution neural network MobileNet is adopted, and different loss functions are designed according to the characteristics of each attribute as well. At the same time, during the process of training, the five attributes are trained by sharing parameters. At last, in terms of the tests of 10,000 samples, the attribute prediction has achieved high performance which means that the accuracy rate of gender reached 97.8 % the average age error was 3.2, the accuracy rate of glasses was 99.3 % and the accuracy rate of nationality was 96.3 % as well as the accuracy rate of expression was 68.9 %.",convolutional neural network;face attribute;multitask learning,"522, 526",,IEEE Conferences,2018 IEEE 9th International Conference on Software Engineering and Service Science (ICSESS),IEEE
639,,Rotor Winding Image Detection Method Based on Model-Based Transfer Learning,Y. Jia; X. Zhang; G. Chen,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8663891,10.1109/ICSESS.2018.8663891,"Rotor is a core component of the electric motor. The qualification of rotor winding is one of the core factors for the proper functioning of the rotor, which is still detected by manual operation. Hence, it is important to achieve automatic detection of the rotor windings and enhance the detection accuracy. Recently, convolutional neural network (CNN) has been successfully applied to image recognition., but it requires a large number of labeled samples and there is almost no dataset bias between the target dataset and the source dataset. The challenges of using CNN to recognize rotor winding are that the winding image dataset of different types of rotor exist large dataset bias and the labeled examples are limited. We proposed a new model-based transfer learning method to deal with the challenges. To solve the dataset bias problem., we proposed a new image binarization method to get binary rotor winding images. Using the binary images to train and test model can significantly reduce the interference of dataset bias. Meanwhile., we proposed a method to build model-based transfer learning model which is based on the pre-trained Inception-V3 model trained with the ImageNet dataset., the method is used to solve the problem of limited labeled samples. The comparing experiments show that the model-based transfer learning model trained and tested with binary images significantly outperform existing other models., and can achieve stable and accurate detection of the rotor images.",qualification detection;rotor winding;binary image;Inception-V3;model-based transfer learning,"1, 5",,IEEE Conferences,2018 IEEE 9th International Conference on Software Engineering and Service Science (ICSESS),IEEE
640,,Transfer Learning on Convolutional Neural Networks for Dog Identification,X. Tu; K. Lai; S. Yanushkevich,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8663718,10.1109/ICSESS.2018.8663718,"This paper considers application of machine learning in the context of animal identity management for veterinary practice. In this application, electronic medical records of animals would include digital photographs that are used to identify them using image processing and recognition technologies. We investigated how combination of the “soft” biometrics such as breed, as well as face biometrics of dogs can improve identification of dogs. We apply transfer learning on GoogLeNet to perform the breed classification on the proposed BreedNet, and then to identify individual dogs within the classified breeds, on the proposed DogNet.",machine learning;animal biometrics;transfer leaning;image processing;convolutional neural networks;identification,"357, 360",,IEEE Conferences,2018 IEEE 9th International Conference on Software Engineering and Service Science (ICSESS),IEEE
641,,KGCNN: A New CNN with Keywords Group for Crime Classification Over Legal Articles,C. Xing; L. Xu; P. Wang,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8663826,10.1109/ICSESS.2018.8663826,"In juridical field, some keywords in evidence, such as murder, robbery, are quite valuable for judges to classify the evidences. Feeding these keywords in evidences with other words into a united model may weaken the significance of these keywords. In order to overcome this weakness, we propose a simple modification to the Convolutional Neural Networks (CNNs) architecture to better use both of these keywords and other words. In this paper, we present a new convolutional neural networks(CNNs) with keywords group for crime classification over legal articles. Unlike previous methods, our approach emphasizes those important keywords and integrates them into a CNN model. Our models achieve better performance on our legal articles dataset.",KGCNN;CNN;word embeddings;classification,"788, 792",,IEEE Conferences,2018 IEEE 9th International Conference on Software Engineering and Service Science (ICSESS),IEEE
642,,Pedestrian Detection Based on YOLO-D Network,Z. Hong; L. Zhang; P. Wang,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8663790,10.1109/ICSESS.2018.8663790,"In view of the poor robustness of pedestrian detector based on artificial extraction feature, a novel pedestrian detection method is proposed by referring to the research results of state-of-the-art object detection. Based on the YOLOv2 network and inspired by the DenseNet, we pass the low-level feature maps of YOLOv2 to the higher layers in turn. By combining the feature maps of different convolutional layers, we propose a novel network architecture, namely YOLO-D network, to make detector performance better. In order to solve the occlusion problem, we introduced a Head-Shoulders model and also reduced computer calculations. The experimental results show that compared with the original YOLOv2 network pedestrian detection method, this paper reduces the missed rate and false rate, improves the localization precision, and the detection speed meets the real-time requirements.",Pedestrian Detection;YOLOv2;YOLO-D Network;Head-Shoulders Model;Object Detection,"802, 806",,IEEE Conferences,2018 IEEE 9th International Conference on Software Engineering and Service Science (ICSESS),IEEE
643,,FAMOUS: Fake News Detection Model Based on Unified Key Sentence Information,N. Kim; D. Seo; C. Jeong,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8663864,10.1109/ICSESS.2018.8663864,"Fake news detection causes a challenging problem due to the great influence of communication media over the public. In this paper, we shall present a new fake news detection model using unified key sentence information which can efficiently perform sentence matching between question and article by using key sentence retrieval based on bilateral multi perspective matching model. Our model makes use of one unified word vector for the key sentences of article by extracting them to the question from article and then merging the word vector for each key sentence. It can efficiently perform the sentence matching by executing matching operations between the contextual information obtained from the word vectors of question and key sentences through bidirectional long short term memory. Our model shows the competitive performance for fake news detection on the Korean article dataset over the previous result.",fake news detectiont;key sentence retrieval;sentence matching;natural language processing,"617, 620",,IEEE Conferences,2018 IEEE 9th International Conference on Software Engineering and Service Science (ICSESS),IEEE
644,,Convolutional Neural Network Architecture for Semaphore Recognition,W. Li; Y. Yang; M. Wang; L. Zhang; M. Zhu,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8663885,10.1109/ICSESS.2018.8663885,"This paper proposes convolutional neural networks SRNet and Tiny-SRNet for human semaphore action recognition. SRNet is composed of 5 layers of convolution and 3 layers of fully connected layers. In addition to the first convolution layer., a batch normalization layer is added before each convolution layer. In order to enable deep learning algorithms to be applied to both mobile and embedded platforms., Tiny-SRNet removes the full connected layers in SRNet and replaces them with a convolutional layer and a global average pooling layer. The experimental results show that compared with the mainstream classification models AlexNet., GoogleNet and VGGI6., SRNet achieves the highest recognition rate of 98.9% on the semaphore dataset., and Tiny-SRNet compresses its model size to 1/24 of SRNet with a reduction of 1.7% accuracy.",semaphore;BN;SRNet;Tiny-SRNet,"559, 562",,IEEE Conferences,2018 IEEE 9th International Conference on Software Engineering and Service Science (ICSESS),IEEE
645,,Enhancing Model Performance of Person Re-Indentification on Unknown Target Domain,R. Xu; Y. Fu; T. Liu; S. Xiang,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8663745,10.1109/ICSESS.2018.8663745,"Person re-identification(ReID) is the task that aims at retrieving the same person from the images taken across different cameras. Benefiting from the improvement of deep learning algorithms and the appearance of large datasets, the performance of ReID models has been greatly improved. However, most ReID models focus on a single dataset and their performance will drop dramatically when the train-set and test-set are from different datasets. To improve the generalization ability of the ReID model, this paper proposes a method that takes the advantage of triplet loss and multi-dataset training. And the experiment results show that this method can enhance the model performace in cross dataset usage.",Triplet loss;Person re-identification;Convolutional Neural Network;Multi-dataset,"1, 4",,IEEE Conferences,2018 IEEE 9th International Conference on Software Engineering and Service Science (ICSESS),IEEE
646,,Detecting Short Near-Duplicates with Semantic Relations,Y. He; J. Gao,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8663731,10.1109/ICSESS.2018.8663731,"Detecting short near-duplicates is significant in information retrieval system. In this paper, we propose a novel semantics-encoded method to improve word semantic representation learning and apply it on an improved hash method to detect short near-duplicates. We evaluate our model in SICK dataset and result shows our methodology outperforms all previous baseline model. The result indicates the correctness and good efficiency of our methodology.",near-duplicate;semantic relation;Word2vec;simhash;similarity,"122, 125",,IEEE Conferences,2018 IEEE 9th International Conference on Software Engineering and Service Science (ICSESS),IEEE
647,,A Complaint Text Classification Model Based on Character-Level Convolutional Network,X. Tong; B. Wu; S. Wang; J. Lv,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8663873,10.1109/ICSESS.2018.8663873,"With the increase of demand for service quality, a growing number of people are expressing their complaints on the Web for services from different businesses. The correct classification of complaint reasons can substantially improve the quality of business service. Existing methods for text classification used on various datasets are mushrooming, however analysis on complaint texts is rare in current literature. There are still great challenges to classify complaint texts. On the one hand, complaint texts contain obvious negative sentiments which are useless for complaint text classification. On the other hand, complaint texts have more semantic and grammatical errors caused by negative emotions especially in Chinese, which enhances the difficulty of modeling. In response to the challenge, we propose a novel complaint text classification model based on character-level convolutional network. First, we employ a Negative Elements Removal (NER)module to denoise complaint texts. Second, in order to reduce effects of semantic and grammatical errors, a character-based convolutional network for complaint texts is proposed. Experiments demonstrate that our model can achieve state-of-the-art results on Chinese and English complaint texts compared with traditional methods and deep learning methods.",text classification;complaint text;convolutional neural network,"507, 511",,IEEE Conferences,2018 IEEE 9th International Conference on Software Engineering and Service Science (ICSESS),IEEE
648,,Improved Deep Belief Network to Feature Extraction in Chinese Text Classification,J. Gao; J. Yi; W. Jia; X. Zhao,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8663827,10.1109/ICSESS.2018.8663827,"In the field of text classification, the classical text categorization method uses bag-of-words (BOW)model as a classification feature. The BOW model only focuses on the word frequency, but ignores the connection between words, so the BOW model has defects of insufficient expression of the text. The word embedding model maps each individual word to the new vector space, which makes up for the defects of isolated vocabulary and insufficient text expression. As a result, using the word embedding can improve the accuracy of the text classification. In order to avoid the dimension explosion, this paper uses keyword-based word embedding to represent Chinese text. In the feature extraction process, we use the combination of deep belief network and deep Boltzmann machine to extract word vector features of the text. This improved deep belief network feature extraction method further improves the accuracy of text classification.",text classification;deep learning;deep belief network;word embedding model;feature extraction,"283, 287",,IEEE Conferences,2018 IEEE 9th International Conference on Software Engineering and Service Science (ICSESS),IEEE
649,,A Compare-Aggregate Model with Embedding Selector for Answer Selection,S. Zheng; J. Yang,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8663905,10.1109/ICSESS.2018.8663905,"Answer selection is a challenging task in natural language processing that requires both natural language understanding and word knowledge. At present, most of recent methods draw on insights from attention mechanism to learn the complex semantic relations between questions and answers. Previous remarkable approaches mainly apply general Compare-Aggregate framework. In this paper, we propose a novel Compare-Aggregate framework with embedding selector to solve answer selection task. Unlike previous Compare-Aggregate methods which just use one type of Attention mechanism and lack the use of word vectors at different level, we employ two types of Attention mechanism in a model and add a selector layer to choose a best input for aggregation layer. We evaluate the model on the two answer selection tasks: WikiQA and TrecQA. On the two different datasets, our approach outperforms several strong baselines and achieves state-of-the-art performance.",NLP;Question Answering;Text Matching,"752, 756",,IEEE Conferences,2018 IEEE 9th International Conference on Software Engineering and Service Science (ICSESS),IEEE
650,,F-GCNN: A Power Defect Texts Classification Model,M. YuWen; B. Wang; B. Wu,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8663846,10.1109/ICSESS.2018.8663846,"In the daily power inspection, a large number of text records on equipment defects are accumulated. To automatically and efficiently classify the defect levels of equipment records, we propose a text classification model based on fasttext and a recurrent convolutional neural network (F-GCNN). Firstly, considering the text characters (especially a good many specialized compound vocabulary)in this field, the F-GCNN model uses fasttext to train the field word vectors. Fasttext can better reflect the semantic relationship of compound vocabulary, because the internal morphological characteristics of words are taken into account. Secondly, a bidirectional Gated Recurrent Unit network is utilized in the classification model to better obtain the contextual information and reduce the information loss. Also, we introduce max-pooling to select the key features and reduce the impact of noise. Finally, we conduct experiments and prove that F-GCNN model outperforms other models in the classification task of power defect texts.",power text processing;defect text classification;fasttext;Bi-GRU;CNN,"512, 517",,IEEE Conferences,2018 IEEE 9th International Conference on Software Engineering and Service Science (ICSESS),IEEE
651,,Chinese Named Entity Recognition Based on CNN-BiLSTM-CRF,Y. Jia; X. Xu,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8663820,10.1109/ICSESS.2018.8663820,"Named Entity Recognition (NER) is an important basic task in natural language processing (NLP). In recent years, the method of word representations enhancement by character embedding has significantly enhanced the effect of entity recognition. However, this kind of character embedding method only works on alphabetic spelling words such as English, and the same method is not suitable for Chinese. Aiming at the inherent characteristics of Chinese as morpheme writing, we propose a novel neural network model based on CNN-BiLSTM-CRF in this paper. Convolution neural network (CNN) extracts the glyph embeddings with morphological features from each Chinese character, which are concatenated with the character embeddings with semantic feature information and fed to the BiLSTM-CRF network. We evaluate our model on the third SIGHAN Bakeoff MSRA dataset for simplified Chinese NER task. The experimental results show that our model reaches 91.09% in F-scores which does not rely on the hand-designed features and domain knowledge.",Chinese NER;glyph embedding;BiLSTM;CNN,"1, 4",,IEEE Conferences,2018 IEEE 9th International Conference on Software Engineering and Service Science (ICSESS),IEEE
652,,Research on Object Information Retrieval and Recommendation Based on GS1 Standard,L. Duan; Z. Wang; Y. Duan; Q. Du,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8663943,10.1109/ICSESS.2018.8663943,"In the supply chain system, using the GS1 standard as key technology in applications have been widely popularized. Through the Internet of Things technology, very large physical object information is stored in the database of the supply chain system. However, two main problems still exist in current systems. First, the data can almost be seen internally by the supplier, the consumer cannot query the object information they need, including the introduction of the object and related logistics information. Second, almost no one studies the object recommendation based on the GS1 standard. To solve the problems, we propose two solutions. 1) Build an open system that can help consumers to query object information. 2) Define extended data model based on EPCIS for object in order to have key content to be used for the recommendation. So we mainly study the object information retrieval and recommendation based on GS1 standard to build a system that can help consumers to query object information, and recommend similar objects according to the query record or the products user may feel interested in.",GS1;IoT;query;EPCIS;recommend;supply chain,"1, 4",,IEEE Conferences,2018 IEEE 9th International Conference on Software Engineering and Service Science (ICSESS),IEEE
653,,A Knowledge Based Method for Pinyin Typo Correction,X. Ji; X. Huang,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8663931,10.1109/ICSESS.2018.8663931,"In the process of using pinyin input engine, users inevitably produce abnormal input. Thus, Pinyin typo correction has become an increasingly important task. In this paper, we propose CBPTC(a CNN based pinyin typo correction engine), which is a model based on the cloud computing input technology by using the neural network structure of CNN. In addition to this, The model is able to integrate click position information into the pinyin typo correction task. In contrast to previous works, CBPTC learns to correct a variety of pinyin input correction without the guidance of manually selected rules. We apply CBPTC to real-life user Pinyin spell correction task, and achieve 8.51% increment in correction rate on the state-of-the-art model.",Spell correction;CNN model;Pinyin input method,"346, 350",,IEEE Conferences,2018 IEEE 9th International Conference on Software Engineering and Service Science (ICSESS),IEEE
654,,Chinese Text Sentiment Analysis Based on Improved Convolutional Neural Networks,X. Lin; C. Han,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8663888,10.1109/ICSESS.2018.8663888,"Aiming at the shortcomings of the existing depth learning methods in text sentiment classification task, a Chinese text emotion classification algorithm with three-channel convolution neural network is proposed. The textual Pinyin is proposed as the corpus of feature extraction for the first time. The PCCNN (Phoneticize Channel Convolutional Neural Network)model is trained by extracting Pinyin char features and traditional word and char features, which improves the model's ability to extract features and improves the classification performance of the model. Experiments show that the performance of the algorithm is not only easier to implement than the traditional algorithm, but also the classification performance is more accurate. At the same time, the sentiment of each category is strictly superior to the single-channel convolutional neural network algorithm.",text sentiment classification;Convolutional Neural Network;Pinyin char channel;chinese text,"296, 300",,IEEE Conferences,2018 IEEE 9th International Conference on Software Engineering and Service Science (ICSESS),IEEE
655,,Attentive Neural Network Recommendation for E-Commerce Based on Various Implicit Feedback,J. Zhu; X. Guo; S. Ma,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8663806,10.1109/ICSESS.2018.8663806,"With the Internet applications and information exploding exponentially, recommendation system has become an effective measure to solve information overloading. The traditional recommendation algorithm for users' implicit feedback is mainly based on collaborative filtering and Bayesian ranking methods, which are not fully utilize implicit feedback features such as click, collecting, adding to shopping cart and purchase. In order to make full use of various implicit feedback in the e-commence scenario and capture the dynamic temporal features of user-item interaction, this paper proposes a recommendation model which integrates use-embedding vector representation and self-attention mechanism. We conduct comparative experiments on public datasets from e-commerce websites. The evaluation results show that compared to the traditional recommendation models and other neural network models recently proposed, our model has better performance and generalization ability.",Recommendation System;Neural Network;Attention Mechanism;Implicit Feedback;Deep Learning,"889, 892",,IEEE Conferences,2018 IEEE 9th International Conference on Software Engineering and Service Science (ICSESS),IEEE
656,,A Multi-Sentiment Classifier Based on GRU and Attention Mechanism,X. Liang; Z. Liu; C. Ouyang,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8663799,10.1109/ICSESS.2018.8663799,"Previous sentiment analysis studies have focused on monolingual texts, and are basically multi-category tasks (ie, a sentence belongs to only one category). However, in practice, a sentence often expresses multiple sentiments, and the text often contains multiple languages. This paper proposes a multi-label sentiment classifier based on GRU and attention mechanism, which has achieved good results in the data set provided by NLP&CC share task 1.",component;GRU;Attention Mechanism;Multi-label;sentiment analysis,"527, 530",,IEEE Conferences,2018 IEEE 9th International Conference on Software Engineering and Service Science (ICSESS),IEEE
657,,A Transfer Learning Method Based on Residual Block,Y. Chenhui; C. Chunling,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8663835,10.1109/ICSESS.2018.8663835,"In order to obtain high image representations in limited amount of datasets, a transfer learning method based on residual block is proposed. In this method, we follow a transfer learning approach by increasing the number of layers of the network to extract the higher order statistical features of the image. The main idea is to conduct feature transfer by means of ResNet (Deep Residual Network) model with setting ImageNet dataset as source domain. Firstly, all image data are pre-processed with data enhancement. Then, on the basis of modifying the source model's fully-connected classification layer, the adjustment module-residual block is added to the end of the network. Finally, after training the adjustment module, the deep model is achieved. Through transfer learning and deep feature extraction, the capability of feature recognition that impacted by content differences between source domain and target domain will be improved. Experiments show that our method achieves 97.98% accuracy on MNIST dataset and 90.45% accuracy on CIFAR-lO dataset, respectively. The experimental results demonstrate that the performance of our proposed method is significantly better than the existing transfer learning methods.",transfer learning;residual block;ResNet;deep feature extraction;feature recognition,"807, 810",,IEEE Conferences,2018 IEEE 9th International Conference on Software Engineering and Service Science (ICSESS),IEEE
658,,Image and Text Correlation Judgement Based on Deep Learning,Y. Liu; X. Xu; F. Li,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8663711,10.1109/ICSESS.2018.8663711,"Deep learning have achieved great success both in image and natural language processing. When to search similar images, there are some data occur which image and image title are not related. To deal with this problem which involves the process of both image and natural language, we propose a convolutional neural network model. The model both uses the feature of images and texts to judge the similarity. In the model, the two type of feature extracted respectively and then give the probability of the relationship between images and titles. This probability is added to the search strategy as a score to improve search quality.",deep learning;image processing;natural language processing;classification,"844, 847",,IEEE Conferences,2018 IEEE 9th International Conference on Software Engineering and Service Science (ICSESS),IEEE
659,,Unsupervised Representation Learning Method for UAV's Scene Perception,Y. Kaidi; M. Zhaowei; L. Jinhong; S. Sibo; Z. Yulin,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8663930,10.1109/ICSESS.2018.8663930,"In view of the application requirements for UAV in forest rescue and border patrol, this paper takes a single small UAV equipped with visual sensors as the research object. Combined the encoder and decoder structure in the field of deep learning, a deep coding algorithm for scene perception is put forward. Based on the autoencoder, the algorithm achieves the dimension reduction and scene reconstruction by increasing the number of layers in the coding network. Based on the ROS environment, this paper sets up a simulation experiment data acquisition system and verifies the effectiveness of the dimensionality reduction algorithm.",scene representation;deep autoencoder;UAV perception,"323, 327",,IEEE Conferences,2018 IEEE 9th International Conference on Software Engineering and Service Science (ICSESS),IEEE
660,,Orthogonal Policy Gradient and Autonomous Driving Application,M. Luo; Y. Tong; J. Liu,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8663794,10.1109/ICSESS.2018.8663794,"One less addressed issue of deep reinforcement learning is the lack of generalization capability based on new state and new target, for complex tasks, it is necessary to give the correct strategy and evaluate all possible actions for current state. Fortunately, deep reinforcement learning has enabled enormous progress in both subproblems: Giving the correct strategy and evaluating all actions based on the state. In this paper we present an approach called orthogonal policy gradient descent(OPGD) that can make agent learn the policy gradient based on the current state and the actions set, by which the agent can learn a policy network with generalization capability. we evaluate the proposed method on the 3D autonomous driving enviroment TORCS compared with the baseline model, detailed analyses of experimental results and proofs are also given.",deep reinforcement learning;orthogonal policy gradient;generalization capability;autonomous driving,"1, 4",,IEEE Conferences,2018 IEEE 9th International Conference on Software Engineering and Service Science (ICSESS),IEEE
661,,Behavioral Malware Classification using Convolutional Recurrent Neural Networks,B. Alsulami; S. Mancoridis,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8659358,10.1109/MALWARE.2018.8659358,"Behavioral malware detection aims to improve on the performance of static signature-based techniques used by anti-virus systems, which are less effective against modern polymorphic and metamorphic malware. Behavioral malware classification aims to go beyond the detection of malware by also identifying a malware's family according to a naming scheme such as the ones used by anti-virus vendors. Behavioral malware classification techniques use run-time features, such as file system or network activities, to capture the behavioral characteristic of running processes. The increasing volume of malware samples, diversity of malware families, and the variety of naming schemes given to malware samples by anti-virus vendors present challenges to behavioral malware classifiers. We describe a behavioral classifier that uses a Convolutional Recurrent Neural Network and data from Microsoft Windows Prefetch files. We demonstrate the model's improvement on the state-of-the-art using a large dataset of malware families and four major anti-virus vendor naming schemes. The model is effective in classifying malware samples that belong to common and rare malware families and can incrementally accommodate the introduction of new malware samples and families.",,"103, 111",,IEEE Conferences,2018 13th International Conference on Malicious and Unwanted Software (MALWARE),IEEE
662,,Resilience of Pruned Neural Network Against Poisoning Attack,B. Zhao; Y. Lao,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8659362,10.1109/MALWARE.2018.8659362,"In the past several years, machine learning, especially deep learning, has achieved remarkable success in various fields. However, it has been shown recently that machine learning algorithms are vulnerable to well-crafted attacks. For instance, poisoning attack is effective in manipulating the results of a predictive model by deliberately contaminating the training data. In this paper, we investigate the implication of network pruning on the resilience against poisoning attacks. Our experimental results show that pruning can effectively increase the difficulty of poisoning attack, possibly due to the reduced degrees of freedom in the pruned network. For example, in order to degrade the test accuracy below 60% for the MNIST-1-7 dataset, only less than 10 retraining epochs with poisoning data are needed for the original network, while about 16 and 40 epochs are required for the 90% and 99% pruned networks, respectively.",,"78, 83",,IEEE Conferences,2018 13th International Conference on Malicious and Unwanted Software (MALWARE),IEEE
663,,Multi-Class Skin Diseases Classification Using Deep Convolutional Neural Network and Support Vector Machine,N. Hameed; A. M. Shabut; M. A. Hossain,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8631525,10.1109/SKIMA.2018.8631525,"Globally, skin diseases are the fourth leading cause of non-fatal disease burden. Both high and low-income countries suffer from this burden; indicates the prevention of skin diseases should be prioritised. In this research work, an intelligent diagnosis scheme is proposed for multi-class skin lesion classification. The proposed scheme is implemented using a hybrid approach i.e. using deep convolution neural network and error-correcting output codes (ECOC) support vector machine (SVM). The proposed scheme is designed, implemented and tested to classify skin lesion image into one of five categories, i.e. healthy, acne, eczema, benign, or malignant melanoma. Experiments were performed on 9,144 images obtained from different sources. AlexNET, a pre-trained CNN model was used to extract the features. For classification, the ECOC SVM classifier was used. Using ECOC SVM, the overall accuracy achieved is 86.21%. 10-fold cross validation technique was used to avoid overfitting. The results indicate that features obtained from the convolutional neural network are capable of enhancing the classification performance of multiple skin lesions.",skin lesion classification;convolutional neural network;skin lesion detection;melanoma classification;eczema classification;acne classification;support vector machine;error-correcting output codes model,"1, 7",,IEEE Conferences,"2018 12th International Conference on Software, Knowledge, Information Management & Applications (SKIMA)",IEEE
664,,Software Engineering Process for Developing a Person Re-identification Framework,J. F. Bustos; M. Á. de la Torre Gómora; S. C. Álvarez,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8625627,10.1109/CIMPS.2018.8625627,"In video surveillance, different security spots are considered to be placed in unconstrained locations. Cameras located on corridors, gateways and roads, have raised the need for new video surveillance systems to support activities like monitoring and look and find lost individuals. Several tools from image and video processing, as well as machine learning research areas have been conceptualized, and tested on limited data. However, implementing academic knowledge into full applications following state of the art software engineering techniques is complex. Similarly, small changes theorized in academia may represent big changes in software development. In this paper, a combination of the COMET (Collaborative Object Modeling and Architectural Design Method) and OCEP (Open Community Engagement Model) methods is proposed to develop a person re-identification framework named Perception. The three operational modes implemented in the framework (evaluation, registration and operation) provide flexibility for researchers and developers, as evidenced by the implementation of two application-specific prototypes.",software engineering;video surveillance;person re-identification;COMET;OCEP,"69, 77",,IEEE Conferences,2018 7th International Conference On Software Process Improvement (CIMPS),IEEE
665,,A Learning Approach to Enhance Assurances for Real-Time Self-Adaptive Systems,A. Rodrigues; R. Diniz Caldas; G. Nunes Rodrigues; T. Vogel; P. Pelliccione,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8595401,,"The assurance of real-time properties is prone to context variability. Providing such assurance at design time would require to check all the possible context and system variations or to predict which one will be actually used. Both cases are not viable in practice since there are too many possibilities to foresee. Moreover, the knowledge required to fully provide the assurance for self-adaptive systems is only available at runtime and therefore difficult to predict at early development stages. Despite all the efforts on assurances for self-adaptive systems at design or runtime, there is still a gap on verifying and validating real-time constraints accounting for context variability. To fill this gap, we propose a method to provide assurance of self-adaptive systems, at design-and runtime, with special focus on real-time constraints. We combine off-line requirements elicitation and model checking with on-line data collection and data mining to guarantee the system's goals, both functional and non-functional, with fine tuning of the adaptation policies towards the optimization of quality attributes. We experimentally evaluate our method on a simulated prototype of a Body Sensor Network system (BSN) implemented in OpenDaVINCI. The results of the validation are promising and show that our method is effective in providing evidence that support the provision of assurance.",Self adaptive systems;assurance evidence;goal oriented;real time systems;data mining;learning approach,"206, 216",,IEEE Conferences,2018 IEEE/ACM 13th International Symposium on Software Engineering for Adaptive and Self-Managing Systems (SEAMS),IEEE
666,,Deep Learning Similarities from Different Representations of Source Code,M. Tufano; C. Watson; G. Bavota; M. Di Penta; M. White; D. Poshyvanyk,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8595238,,"Assessing the similarity between code components plays a pivotal role in a number of Software Engineering (SE) tasks, such as clone detection, impact analysis, refactoring, etc. Code similarity is generally measured by relying on manually defined or hand-crafted features, e.g., by analyzing the overlap among identifiers or comparing the Abstract Syntax Trees of two code components. These features represent a best guess at what SE researchers can utilize to exploit and reliably assess code similarity for a given task. Recent work has shown, when using a stream of identifiers to represent the code, that Deep Learning (DL) can effectively replace manual feature engineering for the task of clone detection. However, source code can be represented at different levels of abstraction: identifiers, Abstract Syntax Trees, Control Flow Graphs, and Bytecode. We conjecture that each code representation can provide a different, yet orthogonal view of the same code fragment, thus, enabling a more reliable detection of similarities in code. In this paper, we demonstrate how SE tasks can benefit from a DL-based approach, which can automatically learn code similarities from different representations.",deep learning;code similarities;neural networks,"542, 553",,IEEE Conferences,2018 IEEE/ACM 15th International Conference on Mining Software Repositories (MSR),IEEE
667,,Bayesian Hierarchical Modelling for Tailoring Metric Thresholds,N. A. Ernst,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8595242,,"Software is highly contextual. While there are cross-cutting 'global' lessons, individual software projects exhibit many 'local' properties. This data heterogeneity makes drawing local conclusions from global data dangerous. A key research challenge is to construct locally accurate prediction models that are informed by global characteristics and data volumes. Previous work has tackled this problem using clustering and transfer learning approaches, which identify locally similar characteristics. This paper applies a simpler approach known as Bayesian hierarchical modeling. We show that hierarchical modeling supports cross-project comparisons, while preserving local context. To demonstrate the approach, we conduct a conceptual replication of an existing study on setting software metrics thresholds. Our emerging results show our hierarchical model reduces model prediction error compared to a global approach by up to 50%.",hierarchical models;metrics thresholds;probabilistic programming,"587, 591",,IEEE Conferences,2018 IEEE/ACM 15th International Conference on Mining Software Repositories (MSR),IEEE
668,,Data-Driven Search-Based Software Engineering,V. Nair; A. Agrawal; J. Chen; W. Fu; G. Mathew; T. Menzies; L. Minku; M. Wagner; Z. Yu,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8595218,,"This paper introduces Data-Driven Search-based Software Engineering (DSE), which combines insights from Mining Software Repositories (MSR) and Search-based Software Engineering (SBSE). While MSR formulates software engineering problems as data mining problems, SBSE reformulates Software Engineering (SE) problems as optimization problems and use meta-heuristic algorithms to solve them. Both MSR and SBSE share the common goal of providing insights to improve software engineering. The algorithms used in these two areas also have intrinsic relationships. We, therefore, argue that combining these two fields is useful for situations (a)~which require learning from a large data source or (b)~when optimizers need to know the lay of the land to find better solutions, faster. This paper aims to answer the following three questions: (1) What are the various topics addressed by DSE?, (2) What types of data are used by the researchers in this area?, and (3) What research approaches do researchers use? The paper briefly sets out to act as a practical guide to develop new DSE techniques and also to serve as a teaching resource. This paper also presents a resource (tiny.cc/data-se) for exploring DSE. The resource contains 89 artifacts which are related to DSE, divided into 13 groups such as requirements engineering, software product lines, software processes. All the materials in this repository have been used in recent software engineering papers; i.e., for all this material, there exist baseline results against which researchers can comparatively assess their new ideas.",Software Analytics;SBSE,"341, 352",,IEEE Conferences,2018 IEEE/ACM 15th International Conference on Mining Software Repositories (MSR),IEEE
669,,Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow,P. Yin; B. Deng; E. Chen; B. Vasilescu; G. Neubig,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8595231,,"For tasks like code synthesis from natural language, code retrieval, and code summarization, data-driven models have shown great promise. However, creating these models require parallel data between natural language (NL) and code with fine-grained alignments. StackOverflow (SO) is a promising source to create such a data set: the questions are diverse and most of them have corresponding answers with high-quality code snippets. However, existing heuristic methods (e.g. pairing the title of a post with the code in the accepted answer) are limited both in their coverage and the correctness of the NL-code pairs obtained. In this paper, we propose a novel method to mine high-quality aligned data from SO using two sets of features: hand-crafted features considering the structure of the extracted snippets, and correspondence features obtained by training a probabilistic model to capture the correlation between NL and code using neural networks. These features are fed into a classifier that determines the quality of mined NL-code pairs. Experiments using Python and Java as test beds show that the proposed method greatly expands coverage and accuracy over existing mining methods, even when using only a small number of labeled examples. Further, we find that reasonable results are achieved even when training the classifier on one language and testing on another, showing promise for scaling NL-code mining to a wide variety of programming languages beyond those for which we are able to annotate data.",Code Mining;Stack Overflow;Neural Networks,"476, 486",,IEEE Conferences,2018 IEEE/ACM 15th International Conference on Mining Software Repositories (MSR),IEEE
670,,Word Embeddings for the Software Engineering Domain,V. Efstathiou; C. Chatzilenas; D. Spinellis,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8595174,,"The software development process produces vast amounts of textual data expressed in natural language. Outcomes from the natural language processing community have been adapted in software engineering research for leveraging this rich textual information; these include methods and readily available tools, often furnished with pretrained models. State of the art pretrained models however, capture general, common sense knowledge, with limited value when it comes to handling data specific to a specialized domain. There is currently a lack of domain-specific pretrained models that would further enhance the processing of natural language artefacts related to software engineering. To this end, we release a word2vec model trained over 15GB of textual data from Stack Overflow posts. We illustrate how the model disambiguates polysemous words by interpreting them within their software engineering context. In addition, we present examples of fine-grained semantics captured by the model, that imply transferability of these results to diverse, targeted information retrieval tasks in software engineering and motivate for further reuse of the model.",Natural Language Processing;Skip gram;word2vec;Stack Overflow,"38, 41",,IEEE Conferences,2018 IEEE/ACM 15th International Conference on Mining Software Repositories (MSR),IEEE
671,,LiaaS: Lawful Interception as a Service,M. Monshizadeh; V. Khatri; M. Varfan; R. Kantola,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8555753,10.23919/SOFTCOM.2018.8555753,"Machine learning techniques are the key to success for big data analytics in forthcoming 5G and cloud networks. Internet Service Providers (ISPs) and mobile networks are still relying on traditional Lawful Interception (LI) mechanisms that use error prone meta data and are vulnerable to cyber-attacks. While new identity methods are used to monitor suspected end users, the major challenge is the amount of data that needs to be monitored to find the traffic of interest related to the specific targets. On the other hand, for a conversation (audio or video) between two or multiple attendees, such as a conference call or interview, extracting, briefing and classifying important information can be prone to errors and exhaustion of resources if it is done by humans. This paper proposes an intelligent, secure, fast and reliable platform called Lawful interception as a Service (LiaaS) to detect, analyze and intercept content from different media such as voice and video call. The proposed platform also extracts the minutes of conversation and the most important information from the media (audio or video) so any desired detail can be searched from it.",Lawful Interception;Machine Learning;Automated Minutes;Automated Audio Analysis;Automated Video Analysis,"1, 6",,IEEE Conferences,"2018 26th International Conference on Software, Telecommunications and Computer Networks (SoftCOM)",IEEE
672,,Development and Evaluation of Word Embeddings for Morphologically Rich Languages,D. Vasić; E. Brajković,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8555822,10.23919/SOFTCOM.2018.8555822,"Recent advancements in natural language processing (NLP) improved many systems that were relying on natural language to achieve better communication with user using this system. One of the main problem in application of NLP in multiple languages is lack of tools that can be used to develop such systems. Croatian language is highly inflected language from Slavic language family and traditional models used that give great results for English language behave poorly for morphology rich languages. In this article we present model for creating word embeddings for morphologically rich languages such as Croatian. We evaluate the generated word embeddings on newly created word similarity corpus, that is based on English similarity corpus. In the evaluation of word embeddings we compare with two of the best word representation models for English language. We also evaluate our approach with multi-language models such as FastText. The word embeddings created in this article will be used for developing component in training neural models for semantic understanding of sentences written in Croatian language. These language tools can be utilized in many systems where natural language understanding (NLU) and natural language generation (NLG) is needed. In the introduction we give global insight about word embeddings, what are the models for creating such representations and where these representations could be used. In the second section we mention some of the best models for creating word embeddings. In the third section we give a frame-work for development and evaluation of word embeddings for Croatian language. In the conclusion we emphasis the importance of developing tools in Croatian language and announcement of future research.",Natural language processing;Neural network models;Morphologically rich languages;Word embeddings;Intelligent tutoring systems,"1, 5",,IEEE Conferences,"2018 26th International Conference on Software, Telecommunications and Computer Networks (SoftCOM)",IEEE
673,,Object Classification for Child Behavior Observation in the Context of Autism Diagnostics Using a Deep Learning-based Approach,M. Presecan; F. Petric; Z. Kovacic,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8555752,10.23919/SOFTCOM.2018.8555752,"In this paper we demonstrate the effectiveness of a deep learning approach for object detection and classification using a mono-vision feedback of a NAO humanoid robot for assessing the child's behavior during a free play with standardized toys. The free play is one of the tasks contained in the standard ADOS-2 autism spectrum disorder diagnostic protocol used by clinicians. In order to make an accurate, robust and fast object detector, a new data set for learning and testing has been created to enable a reliable assessment of the child's behavior while playing with the toys. This has also led to the development of algorithms and mechanism to assess child's attention based on the toys that the child is playing with. This paper concludes with the discussion about the challenges encountered and their solutions, as well as about the prospective development goals focused on achieving more robust and accurate child attention analyzer.",deep learning;object detection;object classification;Faster R-CNN;ADORE;ADOS;ADOSet;free play;autism,"1, 6",,IEEE Conferences,"2018 26th International Conference on Software, Telecommunications and Computer Networks (SoftCOM)",IEEE
674,,[Research Paper] CroLSim: Cross Language Software Similarity Detector Using API Documentation,K. W. Nafi; B. Roy; C. K. Roy; K. A. Schneider,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8530727,10.1109/SCAM.2018.00023,"In today's open source era, developers look forsimilar software applications in source code repositories for anumber of reasons, including, exploring alternative implementations, reusing source code, or looking for a better application. However, while there are a great many studies for finding similarapplications written in the same programming language, there isa marked lack of studies for finding similar software applicationswritten in different languages. In this paper, we fill the gapby proposing a novel modelCroLSimwhich is able to detectsimilar software applications across different programming lan-guages. In our approach, we use the API documentation tofind relationships among the API calls used by the differentprogramming languages. We adopt a deep learning based word-vector learning method to identify semantic relationships amongthe API documentation which we then use to detect cross-language similar software applications. For evaluating CroLSim, we formed a repository consisting of 8,956 Java, 7,658 C#, and 10,232 Python applications collected from GitHub. Weobserved thatCroLSimcan successfully detect similar softwareapplications across different programming languages with a meanaverage precision rate of 0.65, an average confidence rate of3.6 (out of 5) with 75% high rated successful queries, whichoutperforms all related existing approaches with a significantperformance improvement.",API Calls;Paragraph2Vec;Cross-Language Software Similarity Detection;Singular Value Decomposition,"139, 148",,IEEE Conferences,2018 IEEE 18th International Working Conference on Source Code Analysis and Manipulation (SCAM),IEEE
675,,On the Value of Bug Reports for Retrieval-Based Bug Localization,D. Lawrie; D. Binkley,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8530058,10.1109/ICSME.2018.00048,"Software engineering researchers have been applying tools and techniques from information retrieval (IR) to problems such as bug localization to lower the manual effort required to perform maintenance tasks. The central challenge when using an IR-based tool is the formation of a high-quality query. When performing bug localization, one easily accessible source of query words is the bug report. A recent paper investigated the sufficiency of this source by using a genetic algorithm (GA) to build high quality queries. Unfortunately, the GA in essence ""cheats"" as it makes use of query performance when evolving a good query. This raises the question, is it feasible to attain similar results without ""cheating?"" One approach to providing cheat-free queries is to employ automatic summarization. The performance of the resulting summaries calls into question the sufficiency of the bug reports as a source of query words. To better understand the situation, Information Need Analysis (INA) is applied to quantify both how well the GA is performing and, perhaps more importantly, how well a bug report captures the vocabulary needed to perform IR-based bug localization. The results find that summarization shows potential to produce high-quality queries, but it requires more training data. Furthermore, while bug reports provide a useful source of query words, they are rather limited and thus query expansion techniques, perhaps in combination with summarization, will likely produce higher-quality queries.",bug localization;information retrieval;query quality;information need,"524, 528",,IEEE Conferences,2018 IEEE International Conference on Software Maintenance and Evolution (ICSME),IEEE
676,,Adapting Neural Text Classification for Improved Software Categorization,A. LeClair; Z. Eberhart; C. McMillan,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8530052,10.1109/ICSME.2018.00056,"Software Categorization is the task of organizing software into groups that broadly describe the behavior of the software, such as ""editors"" or ""science."" Categorization plays an important role in several maintenance tasks, such as repository navigation and feature elicitation. Current approaches attempt to cast the problem as text classification, to make use of the rich body of literature from the NLP domain. However, as we will this paper, algorithms are generally not applicable off-the-shelf to source code; we found that they work well when high-level project descriptions are available, but suffer very large performance penalties when classifying sourcecode and comments only. We propose a set of adaptations to a state-of-the-art neural classification algorithm and perform two evaluations: one with reference data from Debian end-user programs, and one with a set of C/C++ libraries that we hired professional programmers to annotate. We show that our proposed approach achieves performance exceeding that of previous software classification techniques as well as a state-of-the-art neural text classification technique.",source code categorization;neural classification;deep learning in software engineering,"461, 472",,IEEE Conferences,2018 IEEE International Conference on Software Maintenance and Evolution (ICSME),IEEE
677,,Automatic Traceability Maintenance via Machine Learning Classification,C. Mills; J. Escobar-Avila; S. Haiduc,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8530044,10.1109/ICSME.2018.00045,"Previous studies have shown that software traceability, the ability to link together related artifacts from different sources within a project (e.g., source code, use cases, documentation, etc.), improves project outcomes by assisting developers and other stakeholders with common tasks such as impact analysis, concept location, etc. Establishing traceability links in a software system is an important and costly task, but only half the struggle. As the project undergoes maintenance and evolution, new artifacts are added and existing ones are changed, resulting in outdated traceability information. Therefore, specific steps need to be taken to make sure that traceability links are maintained in tandem with the rest of the project. In this paper we address this problem and propose a novel approach called TRAIL for maintaining traceability information in a system. The novelty of TRAIL stands in the fact that it leverages previously captured knowledge about project traceability to train a machine learning classifier which can then be used to derive new traceability links and update existing ones. We evaluated TRAIL on 11 commonly used traceability datasets from six software systems and compared it to seven popular Information Retrieval (IR) techniques including the most common approaches used in previous work. The results indicate that TRAIL outperforms all IR approaches in terms of precision, recall, and F-score.",traceability link recovery;classification;machine learning;information retrieval;traceability maintenance;query quality,"369, 380",,IEEE Conferences,2018 IEEE International Conference on Software Maintenance and Evolution (ICSME),IEEE
678,,Software Engineering Challenges of Deep Learning,A. Arpteg; B. Brinne; L. Crnkovic-Friis; J. Bosch,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8498185,10.1109/SEAA.2018.00018,"Surprisingly promising results have been achieved by deep learning (DL) systems in recent years. Many of these achievements have been reached in academic settings, or by large technology companies with highly skilled research groups and advanced supporting infrastructure. For companies without large research groups or advanced infrastructure, building high-quality production-ready systems with DL components has proven challenging. There is a clear lack of well-functioning tools and best practices for building DL systems. It is the goal of this research to identify what the main challenges are, by applying an interpretive research approach in close collaboration with companies of varying size and type. A set of seven projects have been selected to describe the potential with this new technology and to identify associated main challenges. A set of 12 main challenges has been identified and categorized into the three areas of development, production, and organizational challenges. Furthermore, a mapping between the challenges and the projects is defined, together with selected motivating descriptions of how and why the challenges apply to specific projects. Compared to other areas such as software engineering or database technologies, it is clear that DL is still rather immature and in need of further work to facilitate development of high-quality systems. The challenges identified in this paper can be used to guide future research by the software engineering and DL communities. Together, we could enable a large number of companies to start taking advantage of the high potential of the DL technology.",deep learning;machine learning;artificial intelligence;software engineering challenges,"50, 59",,IEEE Conferences,2018 44th Euromicro Conference on Software Engineering and Advanced Applications (SEAA),IEEE
679,,The Effects of Vectorization Methods on Non-Functional Requirements Classification,S. Amasaki; P. Leelaprute,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8498203,10.1109/SEAA.2018.00036,CONTEXT: Architecture and design of systems are sensitive to non-functional requirements (NFRs). Identifying NFRs and their categories at early phase is an essential task for project success. Automatic classification methods for that purpose have been studied for supporting requirement analysis. The past studies used simple vectorization methods and might miss semantics and interactions among words in requirements. OBJECTIVE: To examine whether different vectorization methods lead to differences in the classification performance of NFRs and their categories. METHOD: Comparative experiments were conducted with open data. Five vectorization methods including document embedding methods and four supervised classification methods were supplied. RESULTS: Some advanced methods could achieve better performance than traditional ones. The preference was dependent on classification methods. CONCLUSIONS: It is beneficial to consider using advanced methods for classifying non-functional requirements categories.,requirements classification;comparative study;vectorization methods,"175, 182",,IEEE Conferences,2018 44th Euromicro Conference on Software Engineering and Advanced Applications (SEAA),IEEE
680,,Enhanced Feature Selection Using Word Embeddings for Self-Admitted Technical Debt Identification,J. Flisar; V. Podgorelec,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8498212,10.1109/SEAA.2018.00045,"Technical debt (TD) is a term used to describe a trade off between code quality and timely software release. Since technical debt has negative impact on software development, identification of such debt is an important task in the software engineering domain. Sometimes, technical debt is annotated in source code comments. This kind of debt is referred to as self-admitted technical debt (SATD). Recently, some studies have focused on automated detection and classification of SATD using natural language processing methods. However, these methods have only used manually annotated data to train their classifiers. In this paper, we present the results of a performed exploratory study for using large corpus of unlabeled code comments, extracted from open source projects on git-hub, to train word embeddings, in order to improve detection of SATD. Our approach aims to enhance the feature selection method by taking advantage of the pre-trained word embeddings to detect similar features in source code comments. The experimental results show a significant improvement in SATD classification. With achieved 82% of correct predictions of SATD, the method seems to be a good candidate to be adopted in practice.","text classification, feature selection, self-admitted technical debt, word embeddings","230, 233",,IEEE Conferences,2018 44th Euromicro Conference on Software Engineering and Advanced Applications (SEAA),IEEE
681,,A Research and Strategy of Objection Detection on Remote Sensing Image,Y. Fu; F. Wu; J. Zhao,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8477209,10.1109/SERA.2018.8477209,"Data acquisition from satellite is a challenging task due to the limitation of ground station resource and data transmission capacity. Considering that most of the raw data downloaded to the ground are useless, it is worthy to directly get the results by automatic detection on orbit and only transfer the images that include the target objects, which can filter the useless data efficiently. On orbit automatic detection, satellite computing resources need to be considered, so a smaller and faster model needs to be built. Though enormous object detection methods have been proposed and several application have emerged, a detailed survey on different models about detection accuracy and detection speed as well as memory cost is still lacking. This paper aims to provide a survey on the recent object detection researches and make a strategy to detect on orbit. To further compare the performance among different methods, we conduct an experiment in the same real dataset and compare them from accuracy, speed and memory cost. Following the experiment result, a feasible strategy of object detection for the TZ-1 satellite on-orbit which has a low memory dependency, fast speed and comparable accuracy adapt to its computing resources is proposed.",Filter data;Computing Resources;Remote Sensing Image;Object detection on-orbit,"42, 47",,IEEE Conferences,"2018 IEEE 16th International Conference on Software Engineering Research, Management and Applications (SERA)",IEEE
682,,Motion Deblurring via Using Generative Adversarial Networks for Space-Based Imaging,Y. Chen; F. Wu; J. Zhao,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8477191,10.1109/SERA.2018.8477191,"In some missions of NanoSats, we find images captured are disturbed by motion blur which caused under the situation that NanoSats work in low-earth orbit at high speeds. In this paper, we address the problem of deblurring images degraded due to space-based imaging system shaking or movements of observing targets. We propose a motion deblurring strategy via using Generative Adversarial Networks(GAN) to realize an end-to-end image processing without kernel estimation in orbit. We combine Wasserstein GAN(WGAN) and loss function based on adversarial loss and perceptual loss to optimize the result of deblurred image. The experimental results on the two different datasets prove the feasibility and effectiveness of the proposed strategy which outperforms the state-of-the-art blind deblurring algorithms using for remote sensing images both quantitatively and qualitatively.",NanoSats;Space-Based Imaging;Motion De-blurring;Generative Adversarial Networks,"37, 41",,IEEE Conferences,"2018 IEEE 16th International Conference on Software Engineering Research, Management and Applications (SERA)",IEEE
683,,Transfer Learning with Ensemble of Multiple Feature Representations,H. Zhao; Q. Liu; Y. Yang,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8477189,10.1109/SERA.2018.8477189,"Supervised learning algorithms are to discover the hidden patterns of the statistics, assuming that the training data and the test data are from the same distribution. There are two challenges in the traditional supervised machine learning. One is that the test data distribution always differs largely from the training data distribution in the real world, while another is that there is usually very few labeled data to train a machine learning model. In such cases, transfer learning, which emphasizes the transfer of the previous knowledge from different but related domains and tasks, is recommended to deal with these problems. Traditional transfer learning methods care more about the data itself rather than the task. In fact, there is no one universal feature representation can perfectly benefit the model training work. But different feature representations can discover some independent latent knowledge from the original data. In this paper, we propose an instance-based transfer learning method, which is a weighted ensemble transfer learning framework with multiple feature representations. In our work, mutual information is applied as the smart weighting schema to measure the weight of each feature representation. Extensive experiments have been conducted on three facial expression recognition data sets: JAFFE, KDEF and FERG-DB. The experimental results demonstrate that our approach achieves better performance than the traditional transfer learning method and the non-transfer learning method.",transfer learning;ensemble learning;multiple feature representation;mutual information,"54, 61",,IEEE Conferences,"2018 IEEE 16th International Conference on Software Engineering Research, Management and Applications (SERA)",IEEE
684,,A Deep Learning Methodology for Automatic Assessment of Portrait Image Aesthetic Quality,P. Wettayakorn; S. Traivijitkhun; P. Phetchai; S. Tuarob,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8457381,10.1109/JCSSE.2018.8457381,"Generally, a traditional methodology to assess the aesthetics (appreciating beauty) of a photograph involves a number of professional photographers rating the photo based on given criteria and providing ensemble feedback minimize bias. Such a traditional photo assessment method, however, is not applicable to massive users, especially in real-time. To mitigate such an issue, recent studies have devoted on developing algorithms to automatically provide feedback to photo takers. Most of such algorithms train variants of neural networks using ground-truth photos assessed by professional photographers. Regardless, most existing photo assessment algorithms provide the aesthetic score as a single number. From our observation, users typically use multiple criteria to justify the beautifulness of a photo, and hence a single rating score may not be informative. In this paper, we propose a novel Fine-tuned Inception with Fully Connected and Regression Layers model which gives five attribute scores: vivid colour, colour harmony, lighting, balance of elements, and depth of field. T his s olution i ncorporates t he pre-trained inception model which is the state-of-the-art model for processing images. Our proposed algorithm enhances the existing state-of-the-art by fine-tuning the parameters, introducing fully connected layers, and attaching the regression layers to compute the numeric score for each focus attribute. The experimental results show that our model helps to decrease the mean absolute error (MAE) to 0.211, benchmarking on the aesthetics and attributes datasets provided in the previous studies.",Deep Convolutional Neural Networks;Machine Learning;Aesthetic Assessment;Photography,"1, 6",,IEEE Conferences,2018 15th International Joint Conference on Computer Science and Software Engineering (JCSSE),IEEE
685,,Transfer Learning for Leaf Classification with Convolutional Neural Networks,H. Esmaeili; T. Phoka,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8457364,10.1109/JCSSE.2018.8457364,"Convolutional Neural Network (CNN) is taking a big role in image classification. B ut f ully t raining i mages by using CNN takes a plenty of time and uses a very large data set. This paper will focus on transfer learning, a technique that takes a pre-trained model e.g., Inception, Resnet or MobileNets models then retrains the model from the existing weights for a new classification p roblem. T he r etrain t echnique drastically decreases time spending in the training process and many fewer number of image data is required to yield high accuracy trained networks. This paper considers the problem of leaf image classification t hat t he e xisting a pproaches t ake m uch e ffort to choose various types of imagefeatures for classification. This also reflects p utting b iases b y c hoosing s ome f eatures a nd ignoring the other information in images. This paper will conduct the experiments in accuracy comparison between traditional leaf image classification using image processing techniques and CNN with transfer learning. The result will show that without much knowledge in image processing, the leaf image classification can be achieved with high accuracy using the transfer learning technique.",,"1, 6",,IEEE Conferences,2018 15th International Joint Conference on Computer Science and Software Engineering (JCSSE),IEEE
686,,Facial Expression Classification using Deep Extreme Inception Networks,T. Raksarikorn; T. Kangkachit,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8457396,10.1109/JCSSE.2018.8457396,"Facial expression classification p lays crucial role in human-computer interaction. A large number of automated methods have been proposed since the past decades. Recently, deep learning is broadly applied in computer vision field as well as facial expression classification. The reasons are to avoid complex feature extraction process and obtained satisfied classification performance. In this work, we propose a deep convolutional neural networks (CNNs) model, inspired from XCEPTION, to classify seven groups of facial expressions. To efficiently use of model parameters, the model architecture has only 2.2 million parameters which is about 10 times less than XCEPTION. The experimental results on FER-2013 dataset show that our model offers comparable accuracy (0.7169) to the state-of-the-art methods and the upper-bound level of human accuracy ( 0.65 ± 5). In addition, our model uses less number of parameters than the state-of-the-art models and without using extra features and data augmentation.",emotions;facial expression classification;computer vision;convolutional neural networks;XCEPTION,"1, 5",,IEEE Conferences,2018 15th International Joint Conference on Computer Science and Software Engineering (JCSSE),IEEE
687,,Integration of Gesture Control with Large Display Environments Using SAGE2,T. boonnak; V. Visoottiviseth; J. Haga; D. Kobayashi; J. Leigh,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8457323,10.1109/JCSSE.2018.8457323,"With the ever-increasing amount of information, data scientists continue to explore new technologies that will help them access and interact with data in a variety of domains and situations. One technology of particular interest is Scalable Resolution Shared Displays (SRSD) that use a web-based collaboration middleware called SAGE2. These display systems are ideal for exploring large data sets in data intensive applications; however, interacting with content on these largewalls intuitively and rapidly remains a challenge to be addressed. This work introduces a prototype user interface basedon a simple, hand gesture-based approach to control content in a SAGE2 workspace using the Leap Motion controller. Our implementation and preliminary testing of the interface demonstrates its potential as a more natural interaction modalitywhen exploring big data sets.",human computer interface;SAGE2;gestural interface,"1, 5",,IEEE Conferences,2018 15th International Joint Conference on Computer Science and Software Engineering (JCSSE),IEEE
688,,Log-End Cut-Area Detection in Images Taken from Rear End of Eucalyptus Timber Trucks,N. Samdangdech; S. Phiphobmongkol,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8457388,10.1109/JCSSE.2018.8457388,"The visual estimation of log volume and size distribution of eucalyptus logs on a truck is a challenging task. In Thailand, inspectors at paper mills typically perform this task. The information is used to determine whether the logs pass the criteria for the mill and to find the appropriate price. This method is far from accurate and not efficient. This paper presents a new approach to automatically detects eucalyptus logend cut area from rear-end images of eucalyptus timber trucks. The method used machine learning and image processing techniques. It consists of three parts: timber truck detection, log segmentation, and log counting. The proposed system was tested with 300 images of timber truck dataset and achieved an average accuracy of 94.45% in log segmentation and 2.71% of false negative.",log detection;log counting;eucalyptus log;image processing;image segmentation;machine vision;machine learning,"1, 6",,IEEE Conferences,2018 15th International Joint Conference on Computer Science and Software Engineering (JCSSE),IEEE
689,,Enhance Machine Reading Comprehension on Multiple Sentence Questions with Gated and Dense Coreference Information,N. Tretasayuth; P. Vateekul; P. Boonkwan,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8457331,10.1109/JCSSE.2018.8457331,"Machine reading comprehension (MC) is one of the most important problems in natural language processing. Most of the previous works rely heavily on features engineering and handcrafting techniques. Since the release of SQuAD, a large-scale MC dataset, many deep learning models have been proposed. However, these models are limited by the soft attention mechanism only relied on keywords that appears in a question. Therefore, the performance is always poor in a question that needs to infer an answer from multiple sentences, which cannot depend on keywords in a question. In this paper, we propose a deep learning model that incorporates coreference information to improve the prediction performance especially on multiple sentence question. We also propose the bi-directional answering technique that can help the model avoid a local maxima of the single directional answering method in a traditional model. The results have shown that our approach outperforms the baseline in terms of F1 and Exact Match (EM).",,"1, 6",,IEEE Conferences,2018 15th International Joint Conference on Computer Science and Software Engineering (JCSSE),IEEE
690,,Classification of Dhamma Esan Characters By Transfer Learning of a Deep Neural Network,N. Hnoohom; S. Yuenyong,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8457361,10.1109/JCSSE.2018.8457361,"We present an image classification of Dhamma Esan characters by fine-tuning the Inception V3 deep neural network trained on the ImageNet dataset. Dhamma Esan is a traditional alphabet used in the north-eastern region of Thailand, primarily written on Corypha leaves for the purpose of recording Buddhist scriptures. Preservation of these historical documents calls for the ability to classify the characters of the alphabet in order to facilitate digital indexing and searching, as well as assist anyone trying to read them. Our dataset consists of over 70,000 Dhamma Esan character images, much larger than any previous work. The result of ten-fold cross-validation showed that our model had 100% accuracy for four folds, and 99.99% for the other six folds. The previous best accuracy reported was 97.77%. We also developed a Dhamma Esan character classification web service where users can upload images of characters and get immediate classification results as well as mapping to the modern Thai alphabet.",deep neural network;Inception-V3;transfer learning;image classification;Dhamma Esan,"1, 5",,IEEE Conferences,2018 15th International Joint Conference on Computer Science and Software Engineering (JCSSE),IEEE
691,,Efficient Rate-distortion Approximation and Transform Type Selection using Laplacian Operators,K. -S. Lu; A. Ortega; D. Mukherjee; Y. Chen,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8456313,10.1109/PCS.2018.8456313,"Rate-distortion (RD) optimization is an important tool in many video compression standards and can be used for transform selection. However, this is typically very computationally demanding because a full RD search involves the computation of transform co-efficients for each candidate transform. In this paper, we propose an approach that uses sparse Laplacian operators to estimate the RD cost by computing a weighted squared sum of transform coefficients, without having to compute the actual transform coefficients. We demonstrate experimentally how our method can be applied for transform selection. Implemented in the AV1 encoder, our approach yields a significant speed-up in encoding time with a small increase in bitrate.",Transform type selection;graph Laplacian;rate-distortion optimization;rate-distortion approximation,"76, 80",,IEEE Conferences,2018 Picture Coding Symposium (PCS),IEEE
692,,Deep Learning Based HEVC In-Loop Filtering for Decoder Quality Enhancement,S. Kuanar; C. Conly; K. R. Rao,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8456278,10.1109/PCS.2018.8456278,"High Efficiency Video Coding (HEVC), which is the latest video coding standard currently, achieves up to 50% bit rate reduction compared to previous H.264/AVC standard. While performing the block based video coding, these lossy compression techniques produce various artifacts like blurring, distortion, ringing, and contouring effects on output frames, especially at low bit rates. To reduce those compression artifacts HEVC adopted two post processing filtering technique namely de-blocking filter (DBF) and sample adaptive offset (SAO) on the decoder side. While DBF applies to samples located at block boundaries, SAO nonlinear operation applies adaptively to samples satisfying the gradient based conditions through a lookup table. Again SAO filter corrects the quantization errors by sending edge offset values to decoders. This operation consumes extra signaling bit and becomes an overhead to network. In this paper, we proposed a Convolutional Neural Network (CNN) based architecture for SAO in-loop filtering operation without modifying anything on encoding process. Our experimental results show that our proposed model outperformed previous state-of-the-art models in terms of BD-PSNR (0.408 dB) and BD-BR (3.44%), measured on a widely available standard video sequences.",De-blocking;SAO;CNN;De-conv;Quality,"164, 168",,IEEE Conferences,2018 Picture Coding Symposium (PCS),IEEE
693,,Generative Compression,S. Santurkar; D. Budden; N. Shavit,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8456298,10.1109/PCS.2018.8456298,"Traditional image and video compression algorithms rely on hand-crafted encoder/decoder pairs (codecs) that lack adaptability and are agnostic to the data being compressed. We describe the concept of generative compression, the compression of data using generative models, and suggest that it is a direction worth pursuing to produce more accurate and visually pleasing reconstructions at deeper compression levels for both image and video data. We also show that generative compression is orders- of-magnitude more robust to bit errors (e.g., from noisy channels) than traditional variable-length coding schemes.",,"258, 262",,IEEE Conferences,2018 Picture Coding Symposium (PCS),IEEE
694,,Complexity-Constrained Video Encoding and Delivery using Configuration Transfer Matrix,S. Blasi; A. S. Dias; M. Mrak; S. Huang; E. Izquierdo,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8456269,10.1109/PCS.2018.8456269,"Many applications require video content to be encoded and uploaded under specific complexity constraints. While many speed-ups are available in practical video encoder implementations, it is difficult to predict the impact of such techniques on the actual content being encoded and therefore select the best configuration to meet the given constraints. A method is proposed in this paper to automatically select the encoder configuration in order to meet complexity constraints in terms of encoding and uploading time, using a pre-trained encoder configuration transfer matrix. The algorithm ensures that the content is processed within the specified targets, as presented in the experimental evaluation, where it is shown that the encoder can accurately meet specific constraints under a variety of conditions.",,"121, 125",,IEEE Conferences,2018 Picture Coding Symposium (PCS),IEEE
695,,Deep Convolutional AutoEncoder-based Lossy Image Compression,Z. Cheng; H. Sun; M. Takeuchi; J. Katto,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8456308,10.1109/PCS.2018.8456308,"Image compression has been investigated as a fundamental research topic for many decades. Recently, deep learning has achieved great success in many computer vision tasks, and is gradually being used in image compression. In this paper, we present a lossy image compression architecture, which utilizes the advantages of convolutional autoencoder (CAE) to achieve a high coding efficiency. First, we design a novel CAE architecture to replace the conventional transforms and train this CAE using a rate-distortion loss function. Second, to generate a more energy-compact representation, we utilize the principal components analysis (PCA) to rotate the feature maps produced by the CAE, and then apply the quantization and entropy coder to generate the codes. Experimental results demonstrate that our method outperforms traditional image coding algorithms, by achieving a 13.7% BD-rate decrement on the Kodak database images compared to JPEG2000. Besides, our method maintains a moderate complexity similar to JPEG2000.",Convolutional autoencoder;image compression;deep learning;principal component analysis,"253, 257",,IEEE Conferences,2018 Picture Coding Symposium (PCS),IEEE
696,,Natural Engineering: Applying a Genetic Computing Model to Engineering Self-Aware Software,A. Sharma; G. Morana,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8452785,,"Current approaches to software engineering use a Turing machine implementation to intelligently monitor and adjust the internal environment of an algorithm in real-time. These same approaches however, fail to account for fluctuations in the external environment of the computation, leading to a gross underutilization of system resources or requiring a full restart with costly supervision and manual intervention. In this paper, we describe how we can provide the same intelligence for non-functional requirements as there exist for functional requirements in software applications by using the Distributed Intelligence Computing Element (DIME) computing model. By discussing this model in comparison to similar systems in nature, namely in the context of genetics, we develop the concept of services engineering with self-managed software. As a particularly salient example of this model in practice, we explore the potential for such an approach to improve the performance of machine and deep learning algorithms as a function of intelligent computing environments.",intelligent network;DIME;DIME network architecture;artificial intelligence;recursion hierarchy;connectivity;modularity;Turing Machine;Turing O-Machine;cloud computing,"25, 28",,IEEE Conferences,2018 IEEE/ACM 1st International Workshop on Software Engineering for Cognitive Services (SE4COG),IEEE
697,,Distributed Deep Reinforcement Learning on the Cloud for Autonomous Driving,M. Spryn; A. Sharma; D. Parkar; M. Shrimal,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8452725,,"This paper proposes an architecture for leveraging cloud computing technology to reduce training time for deep reinforcement learning models for autonomous driving by distributing the training process across a pool of virtual machines. By parallelizing the training process, careful design of the reward function and use of techniques like transfer learning, we demonstrate a decrease in training time for our example autonomous driving problem from 140 hours to less than 1 hour. We go over our network architecture, job distribution paradigm, reward function design and report results from experiments on small sized cluster (1-6 training nodes) of machines. We also discuss the limitations of our approach when trying to scale up to massive clusters.",Autonomous Driving;Deep Reinforcement Learning;Distributed Machine Learning;Cloud Computing;Simulation,"16, 22",,IEEE Conferences,2018 IEEE/ACM 1st International Workshop on Software Engineering for AI in Autonomous Systems (SEFAIAS),IEEE
698,,Deep Learning for Self-Driving Cars: Chances and Challenges,Q. Rao; J. Frtunikj,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8452728,,"Artificial Intelligence (AI) is revolutionizing the modern society. In the automotive industry, researchers and developers are actively pushing deep learning based approaches for autonomous driving. However, before a neural network finds its way into series production cars, it has to first undergo strict assessment concerning functional safety. The chances and challenges of incorporating deep learning for self-driving cars are presented in this paper.",Deep Learning;Functional Safety;Automotive,"35, 38",,IEEE Conferences,2018 IEEE/ACM 1st International Workshop on Software Engineering for AI in Autonomous Systems (SEFAIAS),IEEE
699,,How Machine Perception Relates to Human Perception: Visual Saliency and Distance in a Frame-by-Frame Semantic Segmentation Task for Highly/Fully Automated Driving,N. Herbig; F. Wiehr; A. Poibrenski; J. Sprenger; C. Müller,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8452723,,"In this paper, we investigate the link between machine perception and human perception for highly/fully automated driving. We compare the classification results of a camera-based frame-by-frame semantic segmentation model Machine with a well-established visual saliency model Human on the Cityscapes dataset. The results show that Machine classifies foreground objects better if they are more salient, indicating a similarity with the human visual system. For background objects, the accuracy drops when the saliency increases, giving evidence for the assumption that Machine has an implicit concept of saliency.",Semantic Segmentation;Saliency;Automated Driving,"6, 10",,IEEE Conferences,2018 IEEE/ACM 1st International Workshop on Software Engineering for AI in Autonomous Systems (SEFAIAS),IEEE
700,,Toward a Methodology for Training with Synthetic Data on the Example of Pedestrian Detection in a Frame-by-Frame Semantic Segmentation Task,A. Poibrenski; J. Sprenger; C. Müller,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8452727,,"In order to make highly/fully automated driving safe, synthetic training and validation data will be required, because critical road situations are too divers and too rare. A few studies on using synthetic data have been published, reporting a general increase in accuracy. In this paper, we propose a novel method to gain more in-depth insights in the quality, performance, and influence of synthetic data during training phase in a bounded setting. We demonstrate this method for the example of pedestrian detection in a frame-by-frame semantic segmentation class.",Semantic Segmentation;Synthetic Data;Automated Driving,"31, 34",,IEEE Conferences,2018 IEEE/ACM 1st International Workshop on Software Engineering for AI in Autonomous Systems (SEFAIAS),IEEE
701,,[Journal First] A Comparative Study to Benchmark Cross-Project Defect Prediction Approaches,S. Herbold; A. Trautsch; J. Grabowski,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453185,10.1145/3180155.3182542,"Cross-Project Defect Prediction (CPDP) as a means to focus quality assurance of software projects was under heavy investigation in recent years. However, within the current state-of-the-art it is unclear which of the many proposals performs best due to a lack of replication of results and diverse experiment setups that utilize different performance metrics and are based on different underlying data. Within this article, we provide a benchmark for CPDP. We replicate 24 approaches proposed by researchers between 2008 and 2015 and evaluate their performance on software products from five different data sets. Based on our benchmark, we determined that an approach proposed by Camargo Cruz and Ochimizu (2009) based on data standardization performs best and is always ranked among the statistically significant best results for all metrics and data sets. Approaches proposed by Turhan et al. (2009), Menzies et al. (2011), and Watanabe et al. (2008) are also nearly always among the best results. Moreover, we determined that predictions only seldom achieve a high performance of 0.75 recall, precision, and accuracy. Thus, CPDP still has not reached a point where the performance of the results is sufficient for the application in practice.",cross project defect prediction;benchmark;comparison;replication,"1063, 1063",,IEEE Conferences,2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE),IEEE
702,,DeepTest: Automated Testing of Deep-Neural-Network-Driven Autonomous Cars,Y. Tian; K. Pei; S. Jana; B. Ray,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453089,10.1145/3180155.3180220,"Recent advances in Deep Neural Networks (DNNs) have led to the development of DNN-driven autonomous cars that, using sensors like camera, LiDAR, etc., can drive without any human intervention. Most major manufacturers including Tesla, GM, Ford, BMW, and Waymo/Google are working on building and testing different types of autonomous vehicles. The lawmakers of several US states including California, Texas, and New York have passed new legislation to fast-track the process of testing and deployment of autonomous vehicles on their roads. However, despite their spectacular progress, DNNs, just like traditional software, often demonstrate incorrect or unexpected corner-case behaviors that can lead to potentially fatal collisions. Several such real-world accidents involving autonomous cars have already happened including one which resulted in a fatality. Most existing testing techniques for DNN-driven vehicles are heavily dependent on the manual collection of test data under different driving conditions which become prohibitively expensive as the number of test conditions increases. In this paper, we design, implement, and evaluate DeepTest, a systematic testing tool for automatically detecting erroneous behaviors of DNN-driven vehicles that can potentially lead to fatal crashes. First, our tool is designed to automatically generated test cases leveraging real-world changes in driving conditions like rain, fog, lighting conditions, etc. DeepTest systematically explore different parts of the DNN logic by generating test inputs that maximize the numbers of activated neurons. DeepTest found thousands of erroneous behaviors under different realistic driving conditions (e.g., blurring, rain, fog, etc.) many of which lead to potentially fatal crashes in three top performing DNNs in the Udacity self-driving car challenge.",deep learning;testing;self-driving cars;deep neural networks;autonomous vehicle;neuron coverage,"303, 314",,IEEE Conferences,2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE),IEEE
703,,SATD Detector: A Text-Mining-Based Self-Admitted Technical Debt Detection Tool,Z. Liu; Q. Huang; X. Xia; E. Shihab; D. Lo; S. Li,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8449432,,"In software projects, technical debt metaphor is used to describe the situation where developers and managers have to accept compromises in long-term software quality to achieve short-term goals. There are many types of technical debt, and self-admitted technical debt (SATD) was proposed recently to consider debt that is introduced intentionally (e.g., through temporary fix) and admitted by developers themselves. Previous work has shown that SATD can be successfully detected using source code comments. However, most current state-of-the-art approaches identify SATD comments through pattern matching, which achieve high precision but very low recall. That means they may miss many SATD comments and are not practical enough. In this paper, we propose SATD Detector, a tool that is able to (i) automatically detect SATD comments using text mining and (ii) highlight, list and manage detected comments in an integrated development environment (IDE). This tool consists of a Java library and an Eclipse plug-in. The Java library is the back-end, which provides command-line interfaces and Java APIs to re-train the text mining model using users' data and automatically detect SATD comments using either the build-in model or a user-specified model. The Eclipse plug-in, which is the front-end, first leverages our pre-trained composite classifier to detect SATD comments, and then highlights and marks these detected comments in the source code editor of Eclipse. In addition, the Eclipse plug-in provides a view in IDE which collects all detected comments for management.",Self-admitted technical debt;SATD detection;Eclipse plug-in,"9, 12",,IEEE Conferences,2018 IEEE/ACM 40th International Conference on Software Engineering: Companion (ICSE-Companion),IEEE
704,,Poster: Automated User Reviews Analyser,A. Ciurumelea; S. Panichella; H. C. Gall,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8449557,,"We present a novel tool, AUREA, that automatically classifies mobile app reviews, filters and facilitates their analysis using fine grained mobile specific topics. We aim to help developers analyse the direct and valuable feedback that users provide through their reviews, in order to better plan maintenance and evolution activities for their apps. Reviews are often difficult to analyse because of their unstructured textual nature and their frequency, moreover only a third of them are informative. We believe that by using our tool, developers can reduce the amount of time required to analyse and understand the issues users encounter and plan appropriate change tasks.",Mobile Applications;User Reviews;Text Classification,"317, 318",,IEEE Conferences,2018 IEEE/ACM 40th International Conference on Software Engineering: Companion (ICSE-Companion),IEEE
705,,Natural Language Requirements Processing: From Research to Practice,A. Ferrari,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8449650,,"Automated manipulation of natural language requirements, for classification, tracing, defect detection, information extraction, and other tasks, has been pursued by requirements engineering (RE) researchers for more than two decades. Recent technological advancements in natural language processing (NLP) have made it possible to apply this research more widely within industrial settings. This technical briefing targets researchers and practitioners, and aims to give an overview of what NLP can do today for RE problems, and what could do if specific research challenges, also emerging from practical experiences, are addressed. The talk will: survey current research on applications of NLP to RE problems; present representative industrially-ready techniques, with a focus on defect detection and information extraction problems; present enabling technologies in NLP that can play a role in RE research, including distributional semantics representations; discuss criteria for evaluation of NLP techniques in the RE context; outline the main challenges for a systematic application of the techniques in industry. The crosscutting topics that will permeate the talk are the need for domain adaptation, and the essential role of the human-in-the-loop.",requirements engineering;natural language processing,"536, 537",,IEEE Conferences,2018 IEEE/ACM 40th International Conference on Software Engineering: Companion (ICSE-Companion),IEEE
706,,Resolving Ambiguities in Regulations: Towards Achieving the Kohlbergian Stage of Principled Morality,S. Ghaisas; A. Sainani; P. R. Anish,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8445159,,"According to Kohlberg, the final stage of morality is characterized by viewing laws as a means to an end by upholding values such as human dignity and fairness as guiding principles for complying with the essence of the law. Given that purpose of compliance is indeed wellbeing of citizens, software systems should, by design, incorporate these values so that laws are followed in spirit. How can we build software systems that incorporate these values? We present our work on disambiguating Health Insurance Portability and Accountability Act (HIPAA) so as to reduce the potential incidents of breach, thereby upholding of the aforesaid guiding principles of morality. We have employed deep learning based approaches to emulate the human process of disambiguation by integrating information from multiple sources, summarizing it, and augmenting the regulatory text with the additional information. This augmented regulatory text can be used by policy makers and software engineers to achieve compliance in spirit.",Regulations;Ambiguity Resolution;Principled Morality;Deep Learning;Compliance,"57, 60",,IEEE Conferences,2018 IEEE/ACM 40th International Conference on Software Engineering: Software Engineering in Society (ICSE-SEIS),IEEE
707,,Tracking by Recognition Using Neural Network,Z. Zeng; Y. K. Yu; K. H. Wong,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8441158,10.1109/SNPD.2018.8441158,"Vision-based object tracking is a challenging problem. In the tracking process, the object is usually first recognized in a given image. Then a bounding box is used to describe the position of the target object. Normally, a vector [x, y, width, height] is adopted to represent the bounding box. Under this viewpoint, the tracking problem can be treated as a regression problem if we handle the image sequence frame by frame. Due to the recent advancement in machine learning, many researchers apply neural networks to solve the visual tracking problem. This greatly improves the accuracy of bounding box prediction. Actually, the neural network based approaches are more suitable for end-to-end systems. In this paper, we propose to train and use a single neural network to tackle the tracking task. With the cropped candidate image patch as the input to the network, the output is the bounding box that indicates the target position. In our network, we first have a mask map to identify the target. It is a binary image and is divided into two classes. The positive class denotes the foreground while the negative class denotes the background. The mask map is then used for the estimation of the bounding box vector. The task now becomes an image mapping problem. We have achieved a good balance between accuracy and computational efficiency. Our tracker can reach an average speed of 178 frames per second(fps) and a maximum of 334 fps in the OTB benchmark.",,"297, 301",,IEEE Conferences,"2018 19th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)",IEEE
708,,Improving Short Text Classification Using Fast Semantic Expansion on Multichannel Convolutional Neural Network,N. Sotthisopha; P. Vateekul,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8441072,10.1109/SNPD.2018.8441072,"Nowadays, text classification is recognized as one of crucial tools for business users to gain more insights from customers. However, textual data from customers such as comments are usually short. Thus, there are two main issues in short text categorization: (1) insufficient contextual information and (2) noisy data due to misspellings. Recently, there is a prior attempt to propose a deep learning approach for short text categorization by expanding semantic using word embeddings clustering via an algorithm based on density peaks searching. Since the number of words in word embeddings is usually large, the clustering algorithm does not scale with the size of data set, thus demanding unacceptable computational cost. In this paper, we aim to propose a fast short-text categorization framework. Rather than using a CNN with k-max pooling layer, we propose to use a faster version of Convolutional Neural Network (CNN) called “multichannel CNN.” To speed up the semantic expansion process, we propose to employ mini batch K-Means++, which is considerably faster and scales well with the size of data set. Furthermore, we also introduce an additional preprocessing step to increase vocabulary coverage rate on word embeddings. We conducted experiments on four public data sets: Google Snippets, TREC, MR, and Subj. The results showed that the proposed framework does not only improve an accuracy on all data sets, but also reduces computational costs from several days to a few hours.",Short Texts;Natural Language Processing;Text Classification;Clustering;Convolutional Neural Network;Word Embeddings;Deep Learning,"182, 187",,IEEE Conferences,"2018 19th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)",IEEE
709,,Multi-label Class-imbalanced Action Recognition in Hockey Videos via 3D Convolutional Neural Networks,K. Sozykin; S. Protasov; A. Khan; R. Hussain; J. Lee,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8441034,10.1109/SNPD.2018.8441034,"Automatic analysis of the video is one of most complex problems in the fields of computer vision and machine learning. A significant part of this research deals with (human) activity recognition (HAR) since humans, and the activities that they perform, generate most of the video semantics. Video-based HAR has applications in various domains, but one of the most important and challenging is HAR in sports videos. Some of the major issues include high inter- and intra-class variations, large class imbalance, the presence of both group actions and single player actions, and recognizing simultaneous actions, i.e., the multi-label learning problem. Keeping in mind these challenges and the recent success of CNNs in solving various computer vision problems, in this work, we implement a 3D CNN based multi-label deep HAR system for multi-label class-imbalanced action recognition in hockey videos. We test our system for two different scenarios: an ensemble of k binary networks vs. a single k-output network, on a publicly available dataset. We also compare our results with the system that was originally designed for the chosen dataset. Experimental results show that the proposed approach performs better than the existing solution.",Action Recognition;Deep Learning;Convolutional Neural Networks,"146, 151",,IEEE Conferences,"2018 19th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)",IEEE
710,,Cross-Entropy: A New Metric for Software Defect Prediction,X. Zhang; K. Ben; J. Zeng,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8424963,10.1109/QRS.2018.00025,"Defect prediction is an active topic in software quality assurance, which can help developers find potential bugs and make better use of resources. To improve prediction performance, this paper introduces cross-entropy, one common measure for natural language, as a new code metric into defect prediction tasks and proposes a framework called DefectLearner for this process. We first build a recurrent neural network language model to learn regularities in source code from software repository. Based on the trained model, the cross-entropy of each component can be calculated. To evaluate the discrimination for defect-proneness, cross-entropy is compared with 20 widely used metrics on 12 open-source projects. The experimental results show that cross-entropy metric is more discriminative than 50% of the traditional metrics. Besides, we combine cross-entropy with traditional metric suites together for accurate defect prediction. With cross-entropy added, the performance of prediction models is improved by an average of 2.8% in F1-score.","software defect prediction, natural language processing, language model, code naturalness, deep learning","111, 122",,IEEE Conferences,"2018 IEEE International Conference on Software Quality, Reliability and Security (QRS)",IEEE
711,,Transfer Learning Method for Very Deep CNN for Text Classification and Methods for its Evaluation,S. Moriya; C. Shibata,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8377848,10.1109/COMPSAC.2018.10220,"In recent years, it has become possible to perform text classification with high accuracy by using convolutional neural networks (CNNs). Zhang et al. decomposed words into characters and classified texts using a CNN with relatively deep layers to obtain excellent classification results. However, it is often difficult to prepare a sufficient number of labeled samples for solving real-world text-classification problems. One method for handling this problem is transfer learning, which uses a network tuned for an arbitrary task as the initial network for a target task. While transfer learning is known to be effective for image recognition, for tasks in natural language processing, such as document classification, it has not yet been shown for what types of data and to what extent transfer learning is effective. In this paper, we first introduce a character-level CNN adopting the structure of a residual network to construct a network with deeper layers for Japanese text classification. We then demonstrate that we can improve classification accuracy by performing transfer learning between two particular datasets. Additionally, we propose an approach to evaluate the effectiveness of transfer learning and use it to evaluate our model.",transfer learning;text classification;CNN;residual network,"153, 158",,IEEE Conferences,2018 IEEE 42nd Annual Computer Software and Applications Conference (COMPSAC),IEEE
712,,Gastric Pathology Image Recognition Based on Deep Residual Networks,B. Liu; K. Yao; M. Huang; J. Zhang; Y. Li; R. Li,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8377895,10.1109/COMPSAC.2018.10267,"Gastric cancer is a malignant neoplasm with a high mortality rate in the world. Nearly one million new cases occur each year. The most important measure to diagnose gastric cancer is the detection and treatment of diseases early. Gastric cancer detection is currently performed by pathologists reviewing large expanses of biological tissues, but this process is labor intensive and error-prone. In this paper, a framework for automatically detection of tumors in gastric pathology image (slide) has been proposed based on deep learning. A deep residual network with 50 layers is built by identity mapping on a dataset of pathology images. The proposed method makes the training of models easier and improves the generalization performance. Finally, the experimental results show that the F-score of our method achieves 96%. The research in auto-classification of gastric pathology images has great value for gastric cancer detection in clinical medicine.",gastric cancer;image recognition;deep residual network;classification,"408, 412",,IEEE Conferences,2018 IEEE 42nd Annual Computer Software and Applications Conference (COMPSAC),IEEE
713,,Near Cloud: Low-cost Low-Power Cloud Implementation for Rural Area Connectivity and Data Processing,J. P. Talusan; Y. Nakamura; T. Mizumoto; K. Yasumoto,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8377935,10.1109/COMPSAC.2018.10307,"Information and communication technologies (ICTs) has enabled growth in developed countries and urban cities through improvements in communication systems, devices and applications. In rural areas, especially in developing countries, ICT penetration is not as high, often due to lack of available infrastructure and funding. With the increasing availability of Internet-of-Things (IoT) devices, low-cost large-scale deployments have become possible even in rural areas. We design, develop and implement, Near Cloud, a cloud-less platform that allows users and IoT devices to communicate and share information. This is built on top of a wireless mesh network (WMN) of low-cost, low-power IoT devices and deployed in areas where there is little to no Internet connectivity. To inject ICT and help bridge the digital divide in rural areas, Near Cloud provides functionalities such as web servers on nodes, accessibility to all users via Wi-Fi, and various data processing including image processing and machine learning. We will show applicability of Near Cloud in improving rural education, health care facilities, disaster response and agriculture.",cloud less platform;wireless mesh network;rural connectivity,"622, 627",,IEEE Conferences,2018 IEEE 42nd Annual Computer Software and Applications Conference (COMPSAC),IEEE
714,,Machine Learning Assisted High-Definition Map Creation,J. Jiao,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8377682,10.1109/COMPSAC.2018.00058,"In recent years, autonomous driving technologies have attracted broad and enormous interests from both academia and industry and are under rapid development. High-Definition (HD) Maps are widely used as an indispensable component of an autonomous vehicle system by researchers and practitioners. HD Maps are digital maps that contain highly precise, fresh and comprehensive geometric information as well as semantics of the road network and surrounding environment. They provide critical inputs to almost all other components of autonomous vehicle systems, including localization, perception, prediction, motion planning, vehicle control etc. Traditionally, it is very laborious and costly to build HD Maps, requiring a significant amount of manual annotation work. In this paper, we first introduce the characteristics and layers of HD Maps; then we provide a formal summary of the workflow of HD Map creation; and most importantly, we present the machine learning techniques being used by the industry to minimize the amount of manual work in the process of HD Map creation.","High-Definition Map, HD Map Creation, Autonomous Vehicle System","367, 373",,IEEE Conferences,2018 IEEE 42nd Annual Computer Software and Applications Conference (COMPSAC),IEEE
715,,Cross-version defect prediction via hybrid active learning with kernel principal component analysis,Z. Xu; J. Liu; X. Luo; T. Zhang,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8330210,10.1109/SANER.2018.8330210,"As defects in software modules may cause product failure and financial loss, it is critical to utilize defect prediction methods to effectively identify the potentially defective modules for a thorough inspection, especially in the early stage of software development lifecycle. For an upcoming version of a software project, it is practical to employ the historical labeled defect data of the prior versions within the same project to conduct defect prediction on the current version, i.e., Cross-Version Defect Prediction (CVDP). However, software development is a dynamic evolution process that may cause the data distribution (such as defect characteristics) to vary across versions. Furthermore, the raw features usually may not well reveal the intrinsic structure information behind the data. Therefore, it is challenging to perform effective CVDP. In this paper, we propose a two-phase CVDP framework that combines Hybrid Active Learning and Kernel PCA (HALKP) to address these two issues. In the first stage, HALKP uses a hybrid active learning method to select some informative and representative unlabeled modules from the current version for querying their labels, then merges them into the labeled modules of the prior version to form an enhanced training set. In the second stage, HALKP employs a non-linear mapping method, kernel PCA, to extract representative features by embedding the original data of two versions into a high-dimension space. We evaluate the HALKP framework on 31 versions of 10 projects with three prevalent performance indicators. The experimental results indicate that HALKP achieves encouraging results with average F-measure, g-mean and Balance of 0.480, 0.592 and 0.580, respectively and significantly outperforms nearly all baseline methods.",,"209, 220",,IEEE Conferences,"2018 IEEE 25th International Conference on Software Analysis, Evolution and Reengineering (SANER)",IEEE
716,,Extracting features from requirements: Achieving accuracy and automation with neural networks,Y. Li; S. Schulze; G. Saake,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8330243,10.1109/SANER.2018.8330243,"Analyzing and extracting features and variability from different artifacts is an indispensable activity to support systematic integration of single software systems and Software Product Line (SPL). Beyond manually extracting variability, a variety of approaches, such as feature location in source code and feature extraction in requirements, has been proposed for automating the identification of features and their variation points. While requirements contain more complete variability information and provide traceability links to other artifacts, current techniques exhibit a lack of accuracy as well as a limited degree of automation. In this paper, we propose an unsupervised learning structure to overcome the abovementioned limitations. In particular, our technique consists of two steps: First, we apply Laplacian Eigenmaps, an unsupervised dimensionality reduction technique, to embed text requirements into compact binary codes. Second, requirements are transformed into a matrix representation by looking up a pre-trained word embedding. Then, the matrix is fed into CNN to learn linguistic characteristics of the requirements. Furthermore, we train CNN by matching the output of CNN with the pre-trained binary codes. Initial results show that accuracy is still limited, but that our approach allows to automate the entire process.",,"477, 481",,IEEE Conferences,"2018 IEEE 25th International Conference on Software Analysis, Evolution and Reengineering (SANER)",IEEE
717,,Enhancing the Description-to-Behavior Fidelity in Android Apps with Privacy Policy,L. Yu; X. Luo; C. Qian; S. Wang; H. K. N. Leung,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7987793,10.1109/TSE.2017.2730198,"Since more than 96 percent of mobile malware targets the Android platform, various techniques based on static code analysis or dynamic behavior analysis have been proposed to detect malicious apps. As malware is becoming more complicated and stealthy, recent research proposed a promising detection approach that looks for the inconsistency between an app's permissions and its description. In this paper, we first revisit this approach and reveal that using description and permission will lead to many false positives because descriptions often fail to declare all sensitive operations. Then, we propose exploiting an app's privacy policy and its bytecode to enhance the malware detection based on description and permissions. It is non-trivial to automatically analyze privacy policy and perform the cross-verification among these four kinds of software artifacts including, privacy policy, bytecode, description, and permissions. To address these challenging issues, we first propose a novel data flow model for analyzing privacy policy, and then develop a new system, named TAPVerifier, for carrying out investigation of individual software artifacts and conducting the cross-verification. The experimental results show that TAPVerifier can analyze privacy policy with a high accuracy and recall rate. More importantly, integrating privacy policy and bytecode level information can remove up to 59.4 percent false alerts of the state-of-the-art systems, such as AutoCog, CHABADA, etc.",Mobile applications;privacy policy,"834, 854",,IEEE Journals,IEEE Transactions on Software Engineering,IEEE
718,,Heterogeneous Defect Prediction,J. Nam; W. Fu; S. Kim; T. Menzies; L. Tan,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7959597,10.1109/TSE.2017.2720603,"Many recent studies have documented the success of cross-project defect prediction (CPDP) to predict defects for new projects lacking in defect data by using prediction models built by other projects. However, most studies share the same limitations: it requires homogeneous data; i.e., different projects must describe themselves using the same metrics. This paper presents methods for heterogeneous defect prediction (HDP) that matches up different metrics in different projects. Metric matching for HDP requires a “large enough” sample of distributions in the source and target projects-which raises the question on how large is “large enough” for effective heterogeneous defect prediction. This paper shows that empirically and theoretically, “large enough” may be very small indeed. For example, using a mathematical model of defect prediction, we identify categories of data sets were as few as 50 instances are enough to build a defect prediction model. Our conclusion for this work is that, even when projects use different metric sets, it is possible to quickly transfer lessons learned about defect prediction.",Defect prediction;quality assurance;heterogeneous metrics;transfer learning,"874, 896",,IEEE Journals,IEEE Transactions on Software Engineering,IEEE
719,,Are Fix-Inducing Changes a Moving Target? A Longitudinal Case Study of Just-In-Time Defect Prediction,S. McIntosh; Y. Kamei,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7898457,10.1109/TSE.2017.2693980,"Just-In-Time (JIT) models identify fix-inducing code changes. JIT models are trained using techniques that assume that past fix-inducing changes are similar to future ones. However, this assumption may not hold, e.g., as system complexity tends to accrue, expertise may become more important as systems age. In this paper, we study JIT models as systems evolve. Through a longitudinal case study of 37,524 changes from the rapidly evolving Qt and OpenStack systems, we find that fluctuations in the properties of fix-inducing changes can impact the performance and interpretation of JIT models. More specifically: (a) the discriminatory power (AUC) and calibration (Brier) scores of JIT models drop considerably one year after being trained; (b) the role that code change properties (e.g., Size, Experience) play within JIT models fluctuates over time; and (c) those fluctuations yield over- and underestimates of the future impact of code change properties on the likelihood of inducing fixes. To avoid erroneous or misleading predictions, JIT models should be retrained using recently recorded data (within three months). Moreover, quality improvement plans should be informed by JIT models that are trained using six months (or more) of historical data, since they are more resilient to period-specific fluctuations in the importance of code change properties.",Just-In-Time prediction;defect prediction;mining software repositories,"412, 428",,IEEE Journals,IEEE Transactions on Software Engineering,IEEE
720,,Intrusion detection for engineering vehicles under the transmission line based on deep learning,C. Yan; C. Wang; J. Du; H. Fang; Y. Wang; X. Xiang; X. Guo,2017,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8342861,10.1109/ICSESS.2017.8342861,"A two-step method based on deep learning is proposed for the intrusion detection of engineering vehicles working under high power transmission lines. In the first step, intrusion detection algorithm is used to identify the potential target area. Then the results are supplied to a trained deep convolution neural network classifier. This way combining intrusion detection method with CNN, the invasion of the engineering vehicles under high power transmission lines can efficiently be detected up to an accuracy of 97.2 %.",Intrusion Detection;PBAS;Deep Learning;CNN;VGGNet-16,"46, 49",,IEEE Conferences,2017 8th IEEE International Conference on Software Engineering and Service Science (ICSESS),IEEE
721,,Incorporating depth into both CNN and CRF for indoor semantic segmentation,J. Jiang; Z. Zhang; Y. Huang; L. Zheng,2017,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8342970,10.1109/ICSESS.2017.8342970,"To improve segmentation performance, a novel neural network architecture (termed DFCN-DCRF) is proposed, which combines an RGB-D fully convolutional neural network (DFCN) with a depth-sensitive fully-connected conditional random field (DCRF). First, a DFCN architecture which fuses depth information into the early layers and applies dilated convolution for later contextual reasoning is designed. Then, a depth-sensitive fully-connected conditional random field (DCRF) is proposed and combined with the previous DFCN to refine the preliminary result. Comparative experiments show that the proposed DFCN-DCRF achieves competitive performance compared with state-of-the-art methods.",convolutional neural networks;conditional random fields;RGB-D;semantic segmentation;transfer learning,"525, 530",,IEEE Conferences,2017 8th IEEE International Conference on Software Engineering and Service Science (ICSESS),IEEE
722,,Lightweight behavioral malware detection for windows platforms,B. Alsulami; A. Srinivasan; H. Dong; S. Mancoridis,2017,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8323959,10.1109/MALWARE.2017.8323959,"We describe a lightweight behavioral malware detection technique that leverages Microsoft Windows prefetch files. We demonstrate that our malware detection achieves a high detection rate with a low false-positive rate of 1 × 10<sup>-3</sup>, and scales linearly for training samples. We demonstrate the generalization of our malware detection on two different Windows platforms with a different set of applications. We study the loss in performance of our malware detection in case of concept drift and its ability to adapt. Finally, we measure our malware detection against evasive malware and present an effective auxiliary defensive technique against such attacks.",,"75, 81",,IEEE Conferences,2017 12th International Conference on Malicious and Unwanted Software (MALWARE),IEEE
723,,Transfer Learning for Cross-Platform Software Crowdsourcing Recommendation,S. Yan; B. Shen; W. Mo; N. Li,2017,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8305949,10.1109/APSEC.2017.33,"Recently, with the development of software crowdsourcing industry, an increasing number of users joined the software crowdsourcing platforms to publish software project tasks or to seek proper work opportunities. One of competitive functions of these platforms is to recommend proficient projects to developers. However, in such recommender system, there exists a serious platform cold-start problem, especially for new software crowdsourcing platforms, as they usually have too little cumulative data to support accurate model training and prediction. This paper focuses on solving the platform cold-start problem in software crowdsourcing recommendation system by transfer learning technologies. We proposed a novel cross-platform recommendation method for new software crowdsourcing platforms, whose idea is trying to transfer data and knowledge from other mature software crowdsourcing platforms (source domains) to solve the insufficient recommendation model training problem in a new platform (target domain). The proposed method maps different kinds of features both in the source domain and the target domain after a certain transformation and combination to a latent space by learning the correspondences between features. Specifically, our method is an instance of content-based recommendation, which uses tags and keywords extracted from project description in crowdsourcing platforms as features, and then set weights for each feature to reflect its importance. Then, Weight-SCL is proposed to merge and distinguish tag features and keyword features before doing feature mapping and data migration to implement knowledge transformation. Finally, we use the data from two famous software crowdsourcing platform as dataset, and a series of experiments are conducted to evaluate the performance of the multi-source recommendation system in comparison with the baseline methods, and get 1.2X performance promotion.",Tranfer Learning;Software Crowdsourcing;Recommender Systems;Cold-start Problem,"269, 278",,IEEE Conferences,2017 24th Asia-Pacific Software Engineering Conference (APSEC),IEEE
724,,Exploring Metadata in Bug Reports for Bug Localization,X. Zhang; Y. Yao; Y. Wang; F. Xu; J. Lu,2017,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8305955,10.1109/APSEC.2017.39,"Information retrieval methods have been proposed to help developers locate related buggy source files for a given bug report. The basic assumption of these methods is that the bug description in a bug report should be textually similar to its buggy source files. However, the metadata (such as the component and version information) in bug reports is largely ignored by these methods. In this paper, we propose to explore the metadata for the bug localization task. In particular, we first apply a generative model to locate buggy source files based on the bug descriptions, and then propose to add the available metadata in bug reports into the localization process. Experimental evaluations on several software projects indicate that the metadata is useful to improve the localization accuracy and that the proposed bug localization method outperforms several existing methods.",Bug localization;bug reports;bug metadata;supervised topic modeling,"328, 337",,IEEE Conferences,2017 24th Asia-Pacific Software Engineering Conference (APSEC),IEEE
725,,Improving Bug Localization with an Enhanced Convolutional Neural Network,Y. Xiao; J. Keung; Q. Mi; K. E. Bennin,2017,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8305956,10.1109/APSEC.2017.40,"Background: Localizing buggy files automatically speeds up the process of bug fixing so as to improve the efficiency and productivity of software quality teams. There are other useful semantic information available in bug reports and source code, but are mostly underutilized by existing bug localization approaches. Aims: We propose DeepLocator, a novel deep learning based model to improve the performance of bug localization by making full use of semantic information. Method: DeepLocator is composed of an enhanced CNN (Convolutional Neural Network) proposed in this study considering bug-fixing experience, together with a new rTF-IDuF method and pretrained word2vec technique. DeepLocator is then evaluated on over 18,500 bug reports extracted from AspectJ, Eclipse, JDT, SWT and Tomcat projects. Results: The experimental results show that DeepLocator achieves 9.77% to 26.65% higher Fmeasure than the conventional CNN and 3.8% higher MAP than a state-of-the-art method HyLoc using less computation time. Conclusion: DeepLocator is capable of automatically connecting bug reports to the corresponding buggy files and successfully achieves better performance based on a deep understanding of semantics in bug reports and source code.",bug localization;convolutional neural network;word2vec;TF-IDF;deep learning;semantic information,"338, 347",,IEEE Conferences,2017 24th Asia-Pacific Software Engineering Conference (APSEC),IEEE
726,,Learning platform for visually impaired children through artificial intelligence and computer vision,B. K. Balasuriya; N. P. Lokuhettiarachchi; A. R. M. D. N. Ranasinghe; K. D. C. Shiwantha; C. Jayawardena,2017,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8294106,10.1109/SKIMA.2017.8294106,"The topic Visual Disabilities and Computer Vision are the most researched topics of recent years. Researchers have been trying to combine two topics to create most usable systems to the visually disabled to aid them in their day to day tasks. In this research, we are trying to create an application which is targeting children between the age of 6-14 who suffers from visual disabilities to aid them in their primary learning task of learning to identify objects without a supervision of a third-party. We are trying to achieve this task by combining latest advancements of Computer Vision and Artificial Intelligence technologies by using Deep Region Based Convolutional Networks (R-CNN), Recurrent Neural Networks (RNN) and Speech models to provide an interactive learning experience to such individuals. The paper discusses.",R-CNN;RNN;Speech Models;Computer Vision;Artificial Intelligence;ROI,"1, 7",,IEEE Conferences,"2017 11th International Conference on Software, Knowledge, Information Management and Applications (SKIMA)",IEEE
727,,Cooperative VLC systems for data transmission and environment perception,H. Huang; J. -Y. Wang; X. Zhou; T. Xiang; Y. Zhang; H. Wu; Y. Wang,2017,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8230187,10.1109/ICCSN.2017.8230187,"In this paper, we propose a novel visible light communications (VLC) system that can both achieves good performance in data transmission and environment perception. To be specific, based on the traditional VLC system, we develop a cooperative VLC scheme derived from the idea of clouding computing and machine learning. Afterwards, measurement for data transmission and algorithms for environment perception are well investigated. Then, numerical and simulation results show that the proposed VLC system achieve good performance in terms of data transmission aspect, which is shown that its bit error rate (BER) is only 10<sup>-5</sup> when the signal to noise ratio (SNR) is set as 10 dB. In addition, experiments result also demonstrates the proposed VLC system can make high-accuracy environment detection.",visible light communications;non-orthogonal multiple access;symbol error rate,"624, 629",,IEEE Conferences,2017 IEEE 9th International Conference on Communication Software and Networks (ICCSN),IEEE
728,,The signal processing and recognition of street view images by CNNs and softmax,L. Jian-Min; Y. Min-Hua,2017,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8230237,10.1109/ICCSN.2017.8230237,"In this paper, we focused on signal processing and proposed a recognition model of street view images based on convolutional neural networks (CNNs) and softmax. On the one hand, this paper processed signal and visualized signal processing activation values of each layer of with CNNs on street view images detection, on the second hand, this paper analyzed and compared them layer by layer. For independent common scene of city street scene image, the kappa coefficient of this method was better than some existing methods.",signal processing;recognition;street view;CNNs,"875, 879",,IEEE Conferences,2017 IEEE 9th International Conference on Communication Software and Networks (ICCSN),IEEE
729,,Low-resolution face recognition via convolutional neural network,C. Ding; T. Bao; S. Karmoshi; M. Zhu,2017,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8230292,10.1109/ICCSN.2017.8230292,"Face images captured by real-world video surveillance applications usually have low resolution. This leads to poor performance or even failure of most face recognition algorithms. As a consequence, identifying the face of the query in low resolution, based on the high-resolution image gallery, proves to be a huge challenge. To address this problem, a novel multi-resolution convolutional neural network (MRCNN) model is proposed in order to study the consistent feature representation from high-resolution and low-resolution face images. First, the corresponding labeled multi-resolution face images are utilized to train the MRCNN model. After this process, the trained model is used as the feature extractor in order to obtain features for the targets in the gallery and query images, respectively. Finally, the nearest neighbor method is applied as the classifier for the purpose of final identification. The experimental results from the two publicly available databases demonstrate the superiority of the proposed MRCNN.",face recognition;low resolution;convolutional neural network;feature representation,"1157, 1161",,IEEE Conferences,2017 IEEE 9th International Conference on Communication Software and Networks (ICCSN),IEEE
730,,Tree and word embedding based sentence similarity for evaluation of good answers in intelligent tutoring system,E. Brajković; D. Vasić,2017,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115592,10.23919/SOFTCOM.2017.8115592,"This article presents an approach to examining the similarity of the sentences. In our approach, Euler algorithm was used to generate a series of words based on tree and Sorensen-Dice coefficient was applied to determine the similarity between compared trees. The emphasis is on defining the similarity between the correct and incorrect answers from the Yahoo Question and Answer of the Non-Factual Data Set. Proposed algorithm was used on two types of trees. First is the constituency tree generated by Stanford CoreNLP, and second is custom-made algorithm that produces second type of tree, called knowledge tree which is derived from parse tree. In our comparison, Zhuang-Sasha algorithm was also used. Second approach that was used for sentence comparison uses Word2Vec model for finding word embedding's and calculating sentence average vector, after that cosine distance was applied to determine similarity between two sentences. Results generated with this method were compared with our method in finding sentence similarity based on knowledge tree. Approach described in this paper can be used in evaluation of correct answers which will be used in our implementation of Intelligent Tutoring System.",Intelligent Tutoring Systems;ITS;Sentence Similarity;Word2 Vec;Tree Edit Distance;Fuzzy analytic hierarchy process,"1, 5",,IEEE Conferences,"2017 25th International Conference on Software, Telecommunications and Computer Networks (SoftCOM)",IEEE
731,,Learning effective changes for software projects,R. Krishna,2017,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115719,10.1109/ASE.2017.8115719,"The primary motivation of much of software analytics is decision making. How to make these decisions? Should one make decisions based on lessons that arise from within a particular project? Or should one generate these decisions from across multiple projects? This work is an attempt to answer these questions. Our work was motivated by a realization that much of the current generation software analytics tools focus primarily on prediction. Indeed prediction is a useful task, but it is usually followed by ""planning"" about what actions need to be taken. This research seeks to address the planning task by seeking methods that support actionable analytics by offering clear guidance on what to do. Specifically, we propose XTREE and BELLTREE algorithms for generating a set of actionable plans within and across projects. Each of these plans, if followed will improve the quality of the software project.",Planning;bellwethers;defect prediction,"1002, 1005",,IEEE Conferences,2017 32nd IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
732,,Transfer learning for performance modeling of configurable systems: An exploratory analysis,P. Jamshidi; N. Siegmund; M. Velez; C. Kästner; A. Patel; Y. Agarwal,2017,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115661,10.1109/ASE.2017.8115661,"Modern software systems provide many configuration options which significantly influence their non-functional properties. To understand and predict the effect of configuration options, several sampling and learning strategies have been proposed, albeit often with significant cost to cover the highly dimensional configuration space. Recently, transfer learning has been applied to reduce the effort of constructing performance models by transferring knowledge about performance behavior across environments. While this line of research is promising to learn more accurate models at a lower cost, it is unclear why and when transfer learning works for performance modeling. To shed light on when it is beneficial to apply transfer learning, we conducted an empirical study on four popular software systems, varying software configurations and environmental conditions, such as hardware, workload, and software versions, to identify the key knowledge pieces that can be exploited for transfer learning. Our results show that in small environmental changes (e.g., homogeneous workload change), by applying a linear transformation to the performance model, we can understand the performance behavior of the target environment, while for severe environmental changes (e.g., drastic workload change) we can transfer only knowledge that makes sampling more efficient, e.g., by reducing the dimensionality of the configuration space.",Performance analysis;transfer learning,"497, 508",,IEEE Conferences,2017 32nd IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
733,,Learning Feature Representations from Change Dependency Graphs for Defect Prediction,P. Loyola; Y. Matsuo,2017,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8109101,10.1109/ISSRE.2017.30,"Given the heterogeneity of the data that can be extracted from the software development process, defect prediction techniques have focused on associating different sources of data with the introduction of faulty code, usually relying on handcrafted features. While these efforts have generated considerable progress over the years, little attention has been given to the fact that the performance of any predictive model depends heavily on the representation of the data used, and that different representations can lead to different results. We consider this a relevant problem, as it could be affecting directly the efforts towards generating safer software systems. Therefore, we propose to study the impact of the representation of the data in defect prediction models. To this end, we focus on the use of developer activity data, from which we structure dependency graphs. Then, instead of manually generating features, such as network metrics, we propose two models inspired by recent advances in representation learning which are able to automatically generate feature representations from graph data. These new representations are compared against manually crafted features for defect prediction in real world software projects. Our results show that automatically learned features are competitive, reaching increments in prediction performance up to 13%.",representation learning;defect prediction;machine learning,"361, 372",,IEEE Conferences,2017 IEEE 28th International Symposium on Software Reliability Engineering (ISSRE),IEEE
734,,Work-in-progress: driving behavior modeling and estimation for battery optimization in electric vehicles,K. Vatanpavar; S. Faezi; I. Burago; M. Levorato; M. A. Al Faruque,2017,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8101289,10.1145/3125502.3125542,"Battery and energy management methodologies such as automotive climate controls have been proposed to address the design challenges of driving range and battery lifetime in Electric Vehicles (EV). However, driving behavior estimation is a major factor neglected in these methodologies. In this paper, we propose a novel contextaware methodology for estimating the driving behavior in terms of future vehicle speeds that will be integrated into the EV battery optimization. We implement a driving behavior model using a variation of Artificial Neural Networks (ANN) called Nonlinear AutoRegressive model with eXogenous inputs (NARX). We train our novel context-aware NARX model based on historical behavior of real drivers, their recent driving reactions, and the route average speed retrieved from Google Maps in order to enable driver-specific and self-adaptive driving behavior modeling and long-term estimation. Our methodology shows only 12% error for up to 30-second speed prediction which is improved by 27% compared to the state-of-theart. Hence, it can achieve up to 82% of the maximum energy saving and battery lifetime improvement possible by the ideal methodology where the future vehicle speed is known.",CPS;Electric Vehicle;Battery;HVAC;Statistical Modeling;Neural Network;Model Predictive Control;Power Optimization,"1, 2",,IEEE Conferences,2017 International Conference on Hardware/Software Codesign and System Synthesis (CODES+ISSS),IEEE
735,,Keynote: small neural nets are beautiful: enabling embedded systems with small deep-neural- network architectures,F. Iandola; K. Keutzer,2017,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8101283,10.1145/3125502.3125606,"Over the last five years Deep Neural Nets have offered more accurate solutions to many problems in speech recognition, and computer vision, and these solutions have surpassed a threshold of acceptability for many applications. As a result, Deep Neural Networks have supplanted other approaches to solving problems in these areas, and enabled many new applications. While the design of Deep Neural Nets is still something of an art form, in our work we have found basic principles of design space exploration used to develop embedded microprocessor architectures to be highly applicable to the design of Deep Neural Net architectures. In particular, we have used these design principles to create a novel Deep Neural Net called SqueezeNet that requires only 480KB of storage for its model parameters. We have further integrated all these experiences to develop something of a playbook for creating small Deep Neural Nets for embedded systems.",Embedded computer vision;Deep Learning;Deep Neural Nets;Convolutional Neural Nets,"1, 10",,IEEE Conferences,2017 International Conference on Hardware/Software Codesign and System Synthesis (CODES+ISSS),IEEE
736,,Work-in-progress: snapshot-based offloading for machine learning web app,I. Jeong; H. Jeong; S. Moon,2017,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8094375,10.1145/3125503.3125625,"We propose a new approach to running machine learning (ML) web app on resource-constrained embedded devices by offloading ML computations to servers. We can dynamically offload computations depending on the problem size and network status. The execution state is saved in the form of another web app called snapshot which simplifies the state migration. Some issues related to ML such as how to handle the Canvas object, the ML model, and the privacy of user data are addressed. The proposed offloading works for real web apps with a performance comparable to running the app entirely on the server.",Computation offloading;machine learning;web application,"1, 2",,IEEE Conferences,2017 International Conference on Embedded Software (EMSOFT),IEEE
737,,Heterogeneous Defect Prediction Through Multiple Kernel Learning and Ensemble Learning,Z. Li; X. -Y. Jing; X. Zhu; H. Zhang,2017,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8094412,10.1109/ICSME.2017.19,"Heterogeneous defect prediction (HDP) aims to predict defect-prone software modules in one project using heterogeneous data collected from other projects. Recently, several HDP methods have been proposed. However, these methods do not sufficiently incorporate the two characteristics of the defect prediction data: (1) data could be linearly inseparable, and (2) data could be highly imbalanced. These two data characteristics make it challenging to build an effective HDP model. In this paper, we propose a novel Ensemble Multiple Kernel Correlation Alignment (EMKCA) based approach to HDP, which takes into consideration the two characteristics of the defect prediction data. Specifically, we first map the source and target project data into high dimensional kernel space through multiple kernel leaning, where the defective and non-defective modules can be better separated. Then, we design a kernel correlation alignment method to make the data distribution of the source and target projects similar in the kernel space. Finally, we integrate multiple kernel classifiers with ensemble learning to relieve the influence caused by class imbalance problem, which can improve the accuracy of the defect prediction model. Consequently, EMKCA owns the advantages of both multiple kernel learning and ensemble learning. Extensive experiments on 30 public projects show that EMKCA outperforms the related competing methods.",heterogeneous defect prediction;kernel correlation alignment;multiple kernel learning;ensemble learning;linearly inseparable;class imbalance,"91, 102",,IEEE Conferences,2017 IEEE International Conference on Software Maintenance and Evolution (ICSME),IEEE
738,,Learning to Predict Severity of Software Vulnerability Using Only Vulnerability Description,Z. Han; X. Li; Z. Xing; H. Liu; Z. Feng,2017,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8094415,10.1109/ICSME.2017.52,"Software vulnerabilities pose significant security risks to the host computing system. Faced with continuous disclosure of software vulnerabilities, system administrators must prioritize their efforts, triaging the most critical vulnerabilities to address first. Many vulnerability scoring systems have been proposed, but they all require expert knowledge to determine intricate vulnerability metrics. In this paper, we propose a deep learning approach to predict multi-class severity level of software vulnerability using only vulnerability description. Compared with intricate vulnerability metrics, vulnerability description is the ""surface level"" information about how a vulnerability works. To exploit vulnerability description for predicting vulnerability severity, discriminative features of vulnerability description have to be defined. This is a challenging task due to the diversity of software vulnerabilities and the richness of vulnerability descriptions. Instead of relying on manual feature engineering, our approach uses word embeddings and a one-layer shallow Convolutional Neural Network (CNN) to automatically capture discriminative word and sentence features of vulnerability descriptions for predicting vulnerability severity. We exploit large amounts of vulnerability data from the Common Vulnerabilities and Exposures (CVE) database to train and test our approach.",vulnerability severity prediction;multi-class classification;deep learning;mining software repositories,"125, 136",,IEEE Conferences,2017 IEEE International Conference on Software Maintenance and Evolution (ICSME),IEEE
739,,Towards Accurate Duplicate Bug Retrieval Using Deep Learning Techniques,J. Deshmukh; K. M. Annervaz; S. Podder; S. Sengupta; N. Dubash,2017,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8094414,10.1109/ICSME.2017.69,"Duplicate Bug Detection is the problem of identifying whether a newly reported bug is a duplicate of an existing bug in the system and retrieving the original or similar bugs from the past. This is required to avoid costly rediscovery and redundant work. In typical software projects, the number of duplicate bugs reported may run into the order of thousands, making it expensive in terms of cost and time for manual intervention. This makes the problem of duplicate or similar bug detection an important one in Software Engineering domain. However, an automated solution for the same is not quite accurate yet in practice, in spite of many reported approaches using various machine learning techniques. In this work, we propose a retrieval and classification model using Siamese Convolutional Neural Networks (CNN) and Long Short Term Memory (LSTM) for accurate detection and retrieval of duplicate and similar bugs. We report an accuracy close to 90% and recall rate close to 80%, which makes possible the practical use of such a system. We describe our model in detail along with related discussions from the Deep Learning domain. By presenting the detailed experimental results, we illustrate the effectiveness of the model in practical systems, including for repositories for which supervised training data is not available.",Information Retrieval;Duplicate Bug Detection;Deep Learning;Natural Language Processing;Word Embeddings;Siamese Networks;Convolutional Neural Networks;Long Short Term Memory,"115, 124",,IEEE Conferences,2017 IEEE International Conference on Software Maintenance and Evolution (ICSME),IEEE
740,,Wrist Player: A Smartwatch Gesture Controller for Smart TVs,M. M. Luna; T. P. Carvalho; F. A. A. M. N. Soares; H. A. D. Nascimento; R. M. Costa,2017,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8029952,10.1109/COMPSAC.2017.266,"Emerging technology on mobile and wearable market, smartwatches have embedded movement sensors whose potential is yet to be fully explored. This paper proposes an interaction method with smart TVs via gestures performed by person's wrist using a smartwatch. Detailed architecture and implementation for a complete prototype, named Wrist Player, is presented. A user study is also conducted, in order to evaluate the prototype performance and the user's interest on the proposal. Results show that the method works very well, with participants reporting having a good experience with the prototype. We present our insights on the concept, challenges faced in our research and ideas for future studies.",Gesture Recognition;Human Computer Interaction;Movement Sensors;smart TV;smartwatch;Internet of Things,"336, 341",,IEEE Conferences,2017 IEEE 41st Annual Computer Software and Applications Conference (COMPSAC),IEEE
741,,FeSCH: A Feature Selection Method using Clusters of Hybrid-data for Cross-Project Defect Prediction,C. Ni; W. Liu; Q. Gu; X. Chen; D. Chen,2017,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8029590,10.1109/COMPSAC.2017.127,"Cross project defect prediction (CPDP) is a challenging task since the predictor built on the source projects can hardly generalize well to the target project. Previous studies have shown that both feature mapping and feature selection can alleviate the differences between the source and target projects. In this paper, we propose a novel method FeSCH (Feature Selection using Clusters of Hybrid-data). In particular it includes two phases. The first is the feature clustering phase, which uses a density-based clustering method DPC to group highly co-related features into clusters. The second is the feature selection phase, which selects beneficial features from each cluster. We design three ranking strategies to choose appropriate features. During the empirical studies, we design experiments based on real-world software projects, and evaluate the prediction performance of FeSCH by analyzing the influence of ranking strategies. The experimental results show that FeSCH can outperform three baseline methods (i.e., WPDP, ALL, and TCA+) in most cases, and its performance is independent of the used classifiers.",Software Defect Prediction;Cross-project Defect Prediction;Feature Clustering;Feature Selection,"51, 56",,IEEE Conferences,2017 IEEE 41st Annual Computer Software and Applications Conference (COMPSAC),IEEE
742,,Source code classification using Neural Networks,S. Gilda,2017,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8025917,10.1109/JCSSE.2017.8025917,"Programming languages are the primary tools of the software development industry. As of today, the programming language of the vast majority of the published source code is manually specified or programmatically assigned based solely on the respective file extension. This work shows that the identification of the programming language can be done automatically by utilizing an artificial neural network based on supervised learning and intelligent feature extraction from the source code files. We employ a multi-layer neural network - word embedding layers along with a Convolutional Neural Network - to achieve this goal. Our criteria for an automatic source code identification solution include high accuracy, fast performance, and large programming language coverage. The model achieves a 97% accuracy rate while classifying 60 programming languages.",Artificial neural network;Multi-layer neural network;Supervised learning;Feature extraction,"1, 6",,IEEE Conferences,2017 14th International Joint Conference on Computer Science and Software Engineering (JCSSE),IEEE
743,,"Comments on ScottKnottESD in response to ""An empirical comparison of model validation techniques for defect prediction models""",S. Herbold,2017,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8024011,10.1109/TSE.2017.2748129,"In this article, we discuss the ScottKnottESD test, which was proposed in a recent paper “An Empirical Comparison of Model Validation Techniques for Defect Prediction Models” that was published in this journal. We discuss the implications and the empirical impact of the proposed normality correction of ScottKnottESD and come to the conclusion that this correction does not necessarily lead to the fulfillment of the assumptions of the original Scott-Knott test and may cause problems with the statistical analysis.","Scott-knott test, log transformation, statistics","1091, 1094",,IEEE Journals,IEEE Transactions on Software Engineering,IEEE
744,,Alleviating adversarial attacks via convolutional autoencoder,W. Bai; C. Quan; Z. Luo,2017,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8022700,10.1109/SNPD.2017.8022700,"In order to defend adversarial attacks in computer vision models, the conventional approach arises on actively incorporate such samples into the training datasets. Nonetheless, the manual production of adversarial samples is painful and labor intensive. Here we propose a novel generative model: Convolutional Autoencoder Model to add unsupervised adversarial training, i.e., the production of adversarial images from the encoded feature representation, on conventional supervised convolutional neural network training. To accomplish such objective, we first provide a novel statistical understanding of convolutional neural network to translate convolution and pooling computations equivalently as a hierarchy of encoders, and sampling tricks, respectively. Then, we derive our proposed Convolutional Autoencoder Model with the `adversarial decoders' to automate the generation of adversarial samples. We validated our proposed Convolutional Autoencoder Model on MNIST dataset, and achieved the clear-cut performance improvement over the normal Convolutional Neural Network.",Convolutional Neural Network;Adversarial Attacks;Encoder and Decoder;Generative Models,"53, 58",,IEEE Conferences,"2017 18th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)",IEEE
745,,Environmental sound classification based on time-frequency representation,K. Z. Thwe; N. War,2017,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8022729,10.1109/SNPD.2017.8022729,"This paper proposes a feature extraction method for environmental sound event classification based on time-frequency representation such as spectrogram. There are three portions to perform environmental classification. Firstly, the input signal is converted into spectrogram image with time-frequency representation using short time Fourier transforms. Secondly, this spectrogram is used to extract features with local binary pattern of three different radius and neighborhood sizes. The three distinct features resulted from local binary pattern based on spectrogram are concatenated and used as one feature vector. Finally, multi support vector machine is used for classification of environmental sound event. Evaluation is tested on ESC-10 dataset.",local binary pattern;spectrogram;environment sound;time-frequency representation;sound event classification,"251, 255",,IEEE Conferences,"2017 18th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)",IEEE
746,,SURF: Summarizer of User Reviews Feedback,A. Di Sorbo; S. Panichella; C. V. Alexandru; C. A. Visaggio; G. Canfora,2017,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7965256,10.1109/ICSE-C.2017.5,"Continuous Delivery (CD) enables mobile developers to release small, high quality chunks of working software in a rapid manner. However, faster delivery and a higher software quality do neither guarantee user satisfaction nor positive business outcomes. Previous work demonstrates that app reviews may contain crucial information that can guide developer's software maintenance efforts to obtain higher customer satisfaction. However, previous work also proves the difficulties encountered by developers in manually analyzing this rich source of data, namely (i) the huge amount of reviews an app may receive on a daily basis and (ii) the unstructured nature of their content. In this paper, we propose SURF (Summarizer of User Reviews Feedback), a tool able to (i) analyze and classify the information contained in app reviews and (ii) distill actionable change tasks for improving mobile applications. Specifically, SURF performs a systematic summarization of thousands of user reviews through the generation of an interactive, structured and condensed agenda of recommended software changes. An end-to-end evaluation of SURF, involving 2622 reviews related to 12 different mobile applications, demonstrates the high accuracy of SURF in summarizing user reviews content. In evaluating our approach we also involve the original developers of some apps, who confirm the practical usefulness of the software change recommendations made by SURF. Demo URL: https://youtu.be/Yf-U5ylJXvo Demo webpage: http://www.ifi.uzh.ch/en/seal/people/panichella/tools/SURFTool.html.",Natural Language Processing;Mobile Applications;Summarization;Software Maintenance,"55, 58",,IEEE Conferences,2017 IEEE/ACM 39th International Conference on Software Engineering Companion (ICSE-C),IEEE
747,,Cross-Project Defect Prediction Using a Credibility Theory Based Naive Bayes Classifier,W. N. Poon; K. E. Bennin; J. Huang; P. Phannachitta; J. W. Keung,2017,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8009947,10.1109/QRS.2017.53,"Several defect prediction models proposed are effective when historical datasets are available. Defect prediction becomes difficult when no historical data exist. Cross-project defect prediction (CPDP), which uses projects from other sources/companies to predict the defects in the target projects proposed in recent studies has shown promising results. However, the performance of most CPDP approaches are still beyond satisfactory mainly due to distribution mismatch between the source and target projects. In this study, a credibility theory based Naïve Bayes (CNB) classifier is proposed to establish a novel reweighting mechanism between the source projects and target projects so that the source data could simultaneously adapt to the target data distribution and retain its own pattern. Our experimental results show that the feasibility of the novel algorithm design and demonstrate the significant improvement in terms of the performance metrics considered achieved by CNB over other CPDP approaches.",cross-project defect prediction;credibility theory;Nai¨ve Bayes classifier;transfer learning;software engineering;quality assurance,"434, 441",,IEEE Conferences,"2017 IEEE International Conference on Software Quality, Reliability and Security (QRS)",IEEE
748,,Investigating the Significance of Bellwether Effect to Improve Software Effort Estimation,S. Mensah; J. Keung; S. G. MacDonell; M. F. Bosu; K. E. Bennin,2017,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8009938,10.1109/QRS.2017.44,"Bellwether effect refers to the existence of exemplary projects (called the Bellwether) within a historical dataset to be used for improved prediction performance. Recent studies have shown an implicit assumption of using recently completed projects (referred to as moving window) for improved prediction accuracy. In this paper, we investigate the Bellwether effect on software effort estimation accuracy using moving windows. The existence of the Bellwether was empirically proven based on six postulations. We apply statistical stratification and Markov chain methodology to select the Bellwether moving window. The resulting Bellwether moving window is used to predict the software effort of a new project. Empirical results show that Bellwether effect exist in chronological datasets with a set of exemplary and recently completed projects representing the Bellwether moving window. Result from this study has shown that the use of Bellwether moving window with the Gaussian weighting function significantly improve the prediction accuracy.",Bellwether Effect;Bellwether moving window;Markov chains;Chronological dataset,"340, 351",,IEEE Conferences,"2017 IEEE International Conference on Software Quality, Reliability and Security (QRS)",IEEE
749,,Transferring Context-Dependent Test Inputs,A. Reichstaller; A. Knapp,2017,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8009909,10.1109/QRS.2017.16,"We consider the question of how to treat existing, context-based test inputs when contextual conditions change. Simply ignoring the voided inputs reduces confidence in the correctness of the system under test (SuT). Instead, we suggest to adjust the parameters of those inputs to the new conditions in a way that retains their original intention. This often comprises behavioral assumptions, e.g., because of coverage or risk considerations. Transferred test inputs should consequently trigger similar behavior of the SuT within the new environment as the original ones did in the old. We formalize this claim by a distance function on test inputs which compares the expected reactions of the SuT. The more similar the responses, the closer the test inputs. The proposed metric can thus be used for guiding test input transfer. In addition to a recursive definition, we present an algorithm that utilizes neural models to estimate the metric by simply observing a given simulation which sketches the intended behavior of the SuT. As this approach seems to specifically match the prerequisites when testing proactive systems, motivation and first experiments consider a simplified instance of those: an exemplary smart vacuum system.",Software Testing;Machine Learning;Test Transfer;Distance Metric;Proactive Systems;Autonomous Systems,"65, 72",,IEEE Conferences,"2017 IEEE International Conference on Software Quality, Reliability and Security (QRS)",IEEE
750,,Software Defect Prediction via Convolutional Neural Network,J. Li; P. He; J. Zhu; M. R. Lyu,2017,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8009936,10.1109/QRS.2017.42,"To improve software reliability, software defect prediction is utilized to assist developers in finding potential bugs and allocating their testing efforts. Traditional defect prediction studies mainly focus on designing hand-crafted features, which are input into machine learning classifiers to identify defective code. However, these hand-crafted features often fail to capture the semantic and structural information of programs. Such information is important in modeling program functionality and can lead to more accurate defect prediction. In this paper, we propose a framework called Defect Prediction via Convolutional Neural Network (DP-CNN), which leverages deep learning for effective feature generation. Specifically, based on the programs' Abstract Syntax Trees (ASTs), we first extract token vectors, which are then encoded as numerical vectors via mapping and word embedding. We feed the numerical vectors into Convolutional Neural Network to automatically learn semantic and structural features of programs. After that, we combine the learned features with traditional hand-crafted features, for accurate software defect prediction. We evaluate our method on seven open source projects in terms of F-measure in defect prediction. The experimental results show that in average, DP-CNN improves the state-of-the-art method by 12%.",software reliability;software defect prediction;deep learning;CNN,"318, 328",,IEEE Conferences,"2017 IEEE International Conference on Software Quality, Reliability and Security (QRS)",IEEE
751,,Transfer Learning for Improving Model Predictions in Highly Configurable Software,P. Jamshidi; M. Velez; C. Kästner; N. Siegmund; P. Kawthekar,2017,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7968130,10.1109/SEAMS.2017.11,"Modern software systems are built to be used in dynamic environments using configuration capabilities to adapt to changes and external uncertainties. In a self-adaptation context, we are often interested in reasoning about the performance of the systems under different configurations. Usually, we learn a black-box model based on real measurements to predict the performance of the system given a specific configuration. However, as modern systems become more complex, there are many configuration parameters that may interact and we end up learning an exponentially large configuration space. Naturally, this does not scale when relying on real measurements in the actual changing environment. We propose a different solution: Instead of taking the measurements from the real system, we learn the model using samples from other sources, such as simulators that approximate performance of the real system at low cost. We define a cost model that transform the traditional view of model learning into a multi-objective problem that not only takes into account model accuracy but also measurements effort as well. We evaluate our cost-aware transfer learning solution using real-world configurable software including (i) a robotic system, (ii) 3 different stream processing applications, and (iii) a NoSQL database system. The experimental results demonstrate that our approach can achieve (a) a high prediction accuracy, as well as (b) a high model reliability.",Highly configurable software;machine learning;transfer learning;self-adaptive system;self-optimization,"31, 41",,IEEE Conferences,2017 IEEE/ACM 12th International Symposium on Software Engineering for Adaptive and Self-Managing Systems (SEAMS),IEEE
752,,"DARVIZ: Deep Abstract Representation, Visualization, and Verification of Deep Learning Models",A. Sankaran; R. Aralikatte; S. Mani; S. Khare; N. Panwar; N. Gantayat,2017,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7966878,10.1109/ICSE-NIER.2017.13,"Traditional software engineering programming paradigms are mostly object or procedure oriented, driven by deterministic algorithms. With the advent of deep learning and cognitive sciences there is an emerging trend for data-driven programming, creating a shift in the programming paradigm among the software engineering communities. Visualizing and interpreting the execution of a current large scale data-driven software development is challenging. Further, for deep learning development there are many libraries in multiple programming languages such as TensorFlow (Python), CAFFE (C++), Theano (Python), Torch (Lua), and Deeplearning4j (Java), driving a huge need for interoperability across libraries. We propose a model driven development based solution framework, that facilitates intuitive designing of deep learning models in a platform agnostic fashion. This framework could potentially generate library specific code, perform program translation across languages, and debug the training process of a deep learning model from a fault localization and repair perspective. Further we identify open research problems in this emerging domain, and discuss some new software tooling requirements to serve this new age data-driven programming paradigm.",deep learning;software tools;model driven development;model validation;visualization,"47, 50",,IEEE Conferences,2017 IEEE/ACM 39th International Conference on Software Engineering: New Ideas and Emerging Technologies Results Track (ICSE-NIER),IEEE
753,,Building Usage Profiles Using Deep Neural Nets,D. Curro; K. G. Derpanis; A. V. Miranskyy,2017,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7966877,10.1109/ICSE-NIER.2017.12,"To improve software quality, one needs to build test scenarios resembling the usage of a software product in the field. This task is rendered challenging when a product's customer base is large and diverse. In this scenario, existing profiling approaches, such as operational profiling, are difficult to apply. In this work, we considerpublicly available video tutorials of a product to profile usage. Our goal is to construct an automatic approach to extract information about user actions from instructional videos. To achieve this goal, we use a Deep Convolutional Neural Network (DCNN) to recognize user actions. Our pilot study shows that a DCNN trained to recognize user actions in video can classify five different actions in a collection of 236 publicly available Microsoft Word tutorial videos (published on YouTube). In our empirical evaluation we report a mean average precision of 94.42% across all actions. This study demonstrates the efficacy of DCNN-based methods for extracting software usage information from videos. Moreover, this approach may aid in other software engineering activities that require information about customer usage of a product.",Deep Neural Net;Usage Profile;Video,"43, 46",,IEEE Conferences,2017 IEEE/ACM 39th International Conference on Software Engineering: New Ideas and Emerging Technologies Results Track (ICSE-NIER),IEEE
754,,Data-Driven Application Maintenance: Experience from the Trenches,J. Misra; S. Sengupta; D. Rawat; M. Savagaonkar; S. Podder,2017,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7964365,10.1109/SER-IP.2017..8,"In this paper we present our experience during design, development, and pilot deployments of a data-driven machine learning based application maintenance solution. We implemented a proof of concept to address a spectrum of interrelated problems encountered in application maintenance projects including duplicate incident ticket identification, assignee recommendation, theme mining, and mapping of incidents to business processes. In the context of IT services, these problems are frequently encountered, yet there is a gap in bringing automation and optimization. Despite long-standing research around mining and analysis of software repositories, such research outputs are not adopted well in practice due to the constraints these solutions impose on the users. We discuss need for designing pragmatic solutions with low barriers to adoption and addressing right level of complexity of problems with respect to underlying business constraints and nature of data.",application maintenance;incident management;duplicate bug identification;assignee recommendation;theme mining;business process mapping;text analysis;machine learning,"48, 54",,IEEE Conferences,2017 IEEE/ACM 4th International Workshop on Software Engineering Research and Industrial Practice (SER&IP),IEEE
755,,Bug Localization with Combination of Deep Learning and Information Retrieval,A. N. Lam; A. T. Nguyen; H. A. Nguyen; T. N. Nguyen,2017,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7961519,10.1109/ICPC.2017.24,"The automated task of locating the potential buggy files in a software project given a bug report is called bug localization. Bug localization helps developers focus on crucial files. However, the existing automated bug localization approaches face a key challenge, called lexical mismatch. Specifically, the terms used in bug reports to describe a bug are different from the terms and code tokens used in source files. To address that, we present a novel approach that uses deep neural network (DNN) in combination with rVSM, an information retrieval (IR) technique. rVSM collects the feature on the textual similarity between bug reports and source files. DNN is used to learn to relate the terms in bug reports to potentially different code tokens and terms in source files. Our empirical evaluation on real-world bug reports in the open-source projects shows that DNN and IR complement well to each other to achieve higher bug localization accuracy than individual models. Importantly, our new model, DNNLOC, with a combination of the features built from DNN, rVSM, and project's bug-fixing history, achieves higher accuracy than the state-of-the-art IR and machine learning techniques. In half of the cases, it iscorrect with just a single suggested file. In 66% of the time, acorrect buggy file is in the list of three suggested files. With 5 suggested files, it is correct in almost 70% of the cases.",Bug Localization;Deep Learning;Code Retrieval;Information Retrieval,"218, 229",,IEEE Conferences,2017 IEEE/ACM 25th International Conference on Program Comprehension (ICPC),IEEE
756,,An Improved SDA Based Defect Prediction Framework for Both Within-Project and Cross-Project Class-Imbalance Problems,X. Jing; F. Wu; X. Dong; B. Xu,2017,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7530877,10.1109/TSE.2016.2597849,"Background. Solving the class-imbalance problem of within-project software defect prediction (SDP) is an important research topic. Although some class-imbalance learning methods have been presented, there exists room for improvement. For cross-project SDP, we found that the class-imbalanced source usually leads to misclassification of defective instances. However, only one work has paid attention to this cross-project class-imbalance problem. Objective. We aim to provide effective solutions for both within-project and cross-project class-imbalance problems. Method. Subclass discriminant analysis (SDA), an effective feature learning method, is introduced to solve the problems. It can learn features with more powerful classification ability from original metrics. For within-project prediction, we improve SDA for achieving balanced subclasses and propose the improved SDA (ISDA) approach. For cross-project prediction, we employ the semi-supervised transfer component analysis (SSTCA) method to make the distributions of source and target data consistent, and propose the SSTCA+ISDA prediction approach. Results. Extensive experiments on four widely used datasets indicate that: 1) ISDA-based solution performs better than other state-of-the-art methods for within-project class-imbalance problem; 2) SSTCA+ISDA proposed for cross-project class-imbalance problem significantly outperforms related methods. Conclusion. Within-project and cross-project class-imbalance problems greatly affect prediction performance, and we provide a unified and effective prediction framework for both problems.",Software defect prediction (SDP);within-project class-imbalance;cross-project class-imbalance;improved subclass discriminant analysis (ISDA);ISDA based defect prediction framework,"321, 339",,IEEE Journals,IEEE Transactions on Software Engineering,IEEE
757,,Lossless compression of curated erythrocyte images using deep autoencoders for malaria infection diagnosis,H. Shen; W. David Pan; Y. Dong; M. Alim,2016,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7906393,10.1109/PCS.2016.7906393,"While autoencoders have been used as an unsupervised machine learning technique for classification and dimensionality reduction of the input data, they are lossy in nature when used alone in data compression. In this work, we proposed an image coding scheme by using stacked autoencoders, where the reconstruction residuals were entropy-coded to achieve lossless compression. As a case study, we compressed labeled red blood cell images from a database curated by pathologists for malaria infection diagnosis. Specifically, we trained two separate stacked autoencoders to automatically learn the discriminative features from input images of infected and non-infected cells. Subsequently, the residuals of these two classes of images were coded by two independent Golomb-Rice encoders. Testing results showed that this deep learning approach provided remarkably higher compression on average than several other lossless coding methods including JPEG-LS, JPEG 2000 lossless mode, and CALIC.",,"1, 5",,IEEE Conferences,2016 Picture Coding Symposium (PCS),IEEE
758,,Testing Android Apps via Guided Gesture Event Generation,X. Wu; Y. Jiang; C. Xu; C. Cao; X. Ma; J. Lu,2016,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7890589,10.1109/APSEC.2016.037,"Mobile applications (apps) are mostly driven by touch gestures whose interactions are natural to human beings. However, generating gesture events for effective and efficient testing of such apps remains to be a challenge. Existing event generation techniques either feed the apps under test with random gestures or exhaustively enumerate all possible gestures. While the former strategy leads to incomplete test coverage, the latter suffers from efficiency issues. In this paper, we study the particular problem of gesture event generation for Android apps. We present a static analysis technique to obtain the gesture information: each UI component's potentially relevant gestures, so as to reduce the amount of gesture events to be delivered in the automated testing. We implemented our technique as a prototype tool GAT and evaluated it with real-world Android apps. The experimental results show that GAT is both effective and efficient in covering more code as well as detecting gesturerelated bugs.",Android app;gesture;static analysis;testing,"201, 208",,IEEE Conferences,2016 23rd Asia-Pacific Software Engineering Conference (APSEC),IEEE
759,,Heterogeneous Cross-Company Effort Estimation through Transfer Learning,S. Tong; Q. He; Y. Chen; Y. Yang; B. Shen,2016,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7890585,10.1109/APSEC.2016.033,"Software effort estimation is vital but challenging activity during software development. In many small or medium-sized companies, such challenges are stemmed from historical data shortage. The problem can be solved by leveraging cross-company data for effort estimation. While in practice, cross-company effort estimation may not be easy to take because the cross-company data for effort estimation can be heterogenous. In this paper, we propose a novel approach named Mixture of Canonical Correlation Analysis and Restricted Boltzmann Machines (MCR) to address data heterogeneity issue in cross-company effort estimation. The essential ideas in MCR are (1) to present a unified metric representing heterogenous effort estimation data; and (2) to combine Canonical Correlation Analysis and Restricted Boltzmann Machines method to estimate effort in heterogenous cross-company effort estimation. The MCR approach is evaluated on 5 public datasets in PROMISE repository. The evaluation results show that: (1) for estimations with partially different metrics, the MCR approach outperforms within-company effort estimator KNN with a decrease in MMRE by 0.60, an increase in PRED(25) by 0.16, and a decrease in MdMRE by 0.19; (2) for estimations with totally different metrics, the MCR approach outperforms within-company effort estimator KNN with a decrease in MMRE by 0.49, an increase in PRED(25) by 0.08, and a decrease in MdMRE by 0.10.",Software Effort Estimation;Transfer Learning;Heterogenous Data;Canonical Correlation Analysis;Restricted Boltzmann Machines,"169, 176",,IEEE Conferences,2016 23rd Asia-Pacific Software Engineering Conference (APSEC),IEEE
760,,A deep belief networks adaptive Kalman filtering algorithm,Rui Wang; Ming-Shan Liu; Yuan Zhou; Yan-Qin Xun; Wen-Bo Zhang,2016,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7883043,10.1109/ICSESS.2016.7883043,"Deep belief networks (DBN) which is an important method in deep learning has a good learning ability to get the characteristics from data. By using DBN to learn the consistency of the innovation sequence in Kalman filtering (KF), the accuracy and robustness of KF can be improved. Through the simulation of the positioning system in Automated Guided Vehicle (AGV), the new algorithm that combines KF with DBN has good adaptability to the statistical properties of noise system, which can improve the positioning accuracy and prevent filter divergence.",deep belief networks;Kalman filtering;Automated Guided Vehicle;positioning,"178, 181",,IEEE Conferences,2016 7th IEEE International Conference on Software Engineering and Service Science (ICSESS),IEEE
761,,Image retrieval using two-dimensional inverted index and semantic attributes,Wang Lei; Guoqiang Xiao,2016,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7883159,10.1109/ICSESS.2016.7883159,"Most of the current image retrieval systems for large scale database rely on the Bag-of-Words (BoW) representation and inverted index. We analyze these systems and find that the retrieval performance is largely determined by the discriminative ability of their inverted indexes. This motivates us to combine SIFT and local color features into a two-dimensional inverted index (TD-II). Each dimension of TD-II corresponds to one kind of features, so the precision of visual match is enhanced. After constructing the TD-II of local features, we introduce a semantic-aware co-indexing algorithm which utilizes 1000 semantic attributes to insert similar images to the initial set of TD-II. Embedding semantic attributes into TD-II is totally off-line and effectively enhances the retrieval performance of TD-II. Experimental results demonstrate the competitive performance of our method, comparing with recent retrieval methods on two benchmark datasets, i.e., Ukbench and Holidays.",image retrieval system;two-dimensional inverted index;semantic attributes;bag-of-words,"679, 682",,IEEE Conferences,2016 7th IEEE International Conference on Software Engineering and Service Science (ICSESS),IEEE
762,,Road vehicle detection and classification based on Deep Neural Network,Zhaojin Zhang; Cunlu Xu; Wei Feng,2016,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7883158,10.1109/ICSESS.2016.7883158,"The deep learning is a growing multi-layer neural network learning algorithm in the field of machine learning in recent years. Firstly, this paper analyzes the superiority of the deep learning at the aspect of feature extraction. Aimed at the lack of feature expression capacity and curse of dimensionality results from excessive feature dimensions of shallow learning, this paper proposes that using deep learning can extract high-lever features from low-lever features though its given layer structure. Secondly, the deep learning algorithm is applied in the case of road vehicle detection. Based on the traditional method, such as neural network the deep learning structure is further studied to increase the performance of feature extraction and classification recognition. Also, some tests are run in the Matlab software. The tests results show that with the increasing the amount of the data, the mean error and misclassification rate gradually decrease, so this algorithm based on the neural network has good superiority and adaptability of the deep learning. Finally, this paper proposes some suggestions for the improvement of the algorithm and prospects the development direction of the deep learning in the field of machine learning and artificial intelligence.",Mode recognition;Road vehicle detection;Deep neural network;Classification,"675, 678",,IEEE Conferences,2016 7th IEEE International Conference on Software Engineering and Service Science (ICSESS),IEEE
763,,Deep learning with stock indicators and two-dimensional principal component analysis for closing price prediction system,Tingwei Gao; Xiu Li; Yueting Chai; Youhua Tang,2016,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7883040,10.1109/ICSESS.2016.7883040,"The stock market is an important component in the current economic market. And stock price prediction has recently garnered significant interest among investment brokers, individual investors and researchers. In general, stock market is very complex nonlinear dynamic system. Accordingly, accurate prediction of stock market is a very challenging task, owing to the inherent noisy environment and high volatility related to outside factors. In this paper, we focus on deep learning method to achieve high precision in stock market forecast. And a deep belief networks(DBNs), which is a kind of deep learning algorithm model, coupled with stock technical indicators(STIs) and two-dimensional principal component analysis((2D)<sup>2</sup>PCA) is introduced as a novel approach to predict the closing price of stock market. A comparison experiment is also performed to evaluate this model.",Stock Prediction;Deep Learning;Technical Indicators;2D Principal Component Analysis,"166, 169",,IEEE Conferences,2016 7th IEEE International Conference on Software Engineering and Service Science (ICSESS),IEEE
764,,User-based AutoenCoder for QoS prediction,Le Van Thinh; Nguyen Xuan Hau; Truong Dinh Tu,2016,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7883073,10.1109/ICSESS.2016.7883073,"With the rapidly growing in number of service providers and service users, as a result, there are three important issues that we now have to deal: Firstly, we face challenges with increasing large data sets. Secondly, many Web service providers have similar functionality but differ in quality properties and services. Finally, there will have many number of missing QoS values in history records. Therefore, predicting the missing QoS values in the large data sets is becoming more attractive. In this paper, we proposed a new model to predict the missing QoS of Web service, it is called User-based AutoenCoder (U-AuCo). The backpropagation algorithm and Stochastic Gradient Descent are used to train the model. We have used both Response time and Throughput datasets in WS-DREAM for experiments. The experimental results demonstrated that the proposed model has performance better than other approaches.",Web service;Autoencoder;QoS prediction,"308, 311",,IEEE Conferences,2016 7th IEEE International Conference on Software Engineering and Service Science (ICSESS),IEEE
765,,Vehicle make and model recognition based on convolutional neural networks,Yongguo Ren; Shanzhen Lan,2016,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7883162,10.1109/ICSESS.2016.7883162,"Vehicle analysis is an important task in many intelligent applications, which involves vehicle-type classification (VTC), license-plate recognition (LPR) and vehicle make and model recognition (MMR). Among these tasks, MMR plays an important complementary role with respect to LPR. In this paper, we propose a novel framework to detect moving vehicle and MMR using convolutional neural networks. The frontal view of vehicle images first extracted and fed into convolutional neural networks for training and testing. The experimental results show that our proposed framework achieves favorable recognition accuracy 98.7% in terms of our vehicle MMR.",moving vehicle detection;vehicle make and model recognition;deep learning;pattern recognition,"692, 695",,IEEE Conferences,2016 7th IEEE International Conference on Software Engineering and Service Science (ICSESS),IEEE
766,,TASSAL: Autofolding for Source Code Summarization,J. Fowkes; P. Chanthirasegaran; R. Ranca; M. Allamanis; M. Lapata; C. Sutton,2016,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7883362,,"We present a novel tool, TASSAL, that automatically creates a summary of each source file in a project by folding its least salient code regions. The intended use-case for our tool is the first-look problem: to help developers who are unfamiliar with a new codebase and are attempting to understand it. TASSAL is intended to aid developers in this task by folding away less informative regions of code and allowing them to focus their efforts on the most informative ones. While modern code editors do provide \emph{code folding} to selectively hide blocks of code, it is impractical to use as folding decisions must be made manually or based on simple rules. We find through a case study that TASSAL is strongly preferred by experienced developers over simple folding baselines, demonstrating its usefulness. In short, we strongly believe TASSAL can aid program comprehension by turning code folding into a usable and valuable tool. A video highlighting the main features of TASSAL can be found at https://youtu.be/_yu7JZgiBA4.",code summarization;code folding;program comprehension;topic modelling,"649, 652",,IEEE Conferences,2016 IEEE/ACM 38th International Conference on Software Engineering Companion (ICSE-C),IEEE
767,,Integrating Mobile and Cloud for PPG Signal Selection to Monitor Heart Rate during Intensive Physical Exercise,V. Jindal,2016,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832965,10.1109/MobileSoft.2016.027,"Heart rate monitoring has become increasingly popular in the industry through mobile phones and wearable devices. However current determination of heart rate through mobile applications suffer from high corruption of signals during intensive physical exercise. In this paper, we present a novel technique for accurately determining heart rate during intensive motion by classifying PPG signals obtained from smartphones or wearable devices combined with motion data obtained from accelerometer sensors. Our approach utilizes the Internet of Things (IoT) cloud connectivity of smartphones for PPG signals selection using deep learning. The technique is validated using the TROIKA dataset and is accurately able to predict heart rate with a 10-fold cross validation error margin of 4.88%.",deep learning;deep belief network;Internet of Things (IoT);PPG signals;heart rate monitoring,"36, 37",,IEEE Conferences,2016 IEEE/ACM International Conference on Mobile Software Engineering and Systems (MOBILESoft),IEEE
768,,Which Is More Important for Cross-Project Defect Prediction: Instance or Feature?,Q. Yu; S. Jiang; J. Qian,2016,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780200,10.1109/SATE.2016.22,"Software defect prediction plays an important role in software testing. We can build the prediction model based on historical data. However, for a new project, we cannot be able to build a good prediction model due to lack of historical data. Therefore, researchers have proposed the cross-project defect prediction (CPDP) methods to share the historical data among different projects. In practice, there may be the problems of instance distribution differences and feature redundancy in cross-project datasets. To investigate which is more important for CPDP, instance or feature, we take instance filter and feature selection as examples to show their efficiency for CPDP. Our experiments are conducted on NASA and PROMISE datasets, and the results indicate that feature selection performs better than instance filter in improving the performance of CPDP. We can conclude that feature could be more important than instance for CPDP.",software testing;cross-project defect prediction;instance filter;feature selection,"90, 95",,IEEE Conferences,"2016 International Conference on Software Analysis, Testing and Evolution (SATE)",IEEE
769,,Applying Assemble Clustering Algorithm and Fault Prediction to Test Case Prioritization,L. Xiao; H. Miao; W. Zhuang; S. Chen,2016,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780203,10.1109/SATE.2016.25,"Cluster application is proposed as an efficient approach to improve test case prioritization, Test case in a same cluster are considered to have similar behaviors. In the process of cluster test case, the selection of test case feature and the clusters number have great influence on the clustering results. but to date almost clustering algorithm to improve test case prioritization are selected random clusters number and clustering result are based on one or a few of the code features, the paper propose a new prioritization techniques that not only consider the best clusters number but also produce the best clustering result based on test case multidimensional feature. After clustering, considering the inter-cluster prioritization and intra-cluster prioritization,in order to improve the effectiveness of our approach, the fault prediction value of code corresponding to the test case is used as one of a prioritization weight. Finally,we implemented an empirical studies using an industrial software to illustrate the effectiveness of the test case prioritization techniques.",Assemble clustering algorithm;fault prediction;the best clusters number;test case prioritization,"108, 116",,IEEE Conferences,"2016 International Conference on Software Analysis, Testing and Evolution (SATE)",IEEE
770,,Efficient design space exploration by knowledge transfer,D. Li; S. Wang; S. Yao; Y. -H. Liu; Y. Cheng; X. -H. Sun,2016,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7750966,,"Due to the exponentially increasing size of design space of microprocessors and time-consuming simulations, predictive models have been widely employed in design space exploration (DSE). Traditional approaches mostly build a program-specific predictor that needs a large number of program-specific samples. Thus considerable simulation cost is required for each program. In this paper, we study the novel problem of transferring knowledge from the labeled samples of previous programs to help predict the responses of the new target program whose labeled samples are very sparse. Inspired by the recent advances of transfer learning, we propose a transfer learning based DSE framework TrDSE to build a more efficient and effective predictive model for the target program with only a few simulations by borrowing knowledge from previous programs. Specifically, TrDSE includes two phases: 1) clustering the programs based on the proposed orthogonal array sampling and the distribution related features, and 2) with the guidance of clustering results, predicting the responses of configurations in design space of the target program by a transfer learning based regression algorithm. We evaluate the proposed TrDSE on the benchmarks of SPEC CPU 2006 suite. The results demonstrate that the proposed framework is more efficient and effective than state-of-art DSE techniques.",Design space exploration;Processor design;Knowledge transfer,"1, 10",,IEEE Conferences,2016 International Conference on Hardware/Software Codesign and System Synthesis (CODES+ISSS),IEEE
771,,A study of sentiment analysis using deep learning techniques on Thai Twitter data,P. Vateekul; T. Koomsubha,2016,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7748849,10.1109/JCSSE.2016.7748849,"Sentiment analysis is very important for social listening, especially, when there are millions of Twitter users in Thailand nowadays. Almost all prior works are based on classical classification techniques, e.g., SVM, Naïve Bayes, etc. Recently, the deep learning techniques have shown promising accuracy in this domain on English tweet corpus. In this paper, we propose the first study that applies deep learning techniques to classify sentiment of Thai Twitter data. There are two deep learning techniques included in our study: Long Short Term Memory (LSTM) and Dynamic Convolutional Neural Network (DCNN). A proper data preprocessing has been conducted. Moreover, we also investigate an effect of word orders in Thai tweets. The results show that the deep learning techniques significantly outperform many classical techniques: Naïve Bayes and SVM, except Maximum Entropy.",Sentiment Analysis;Thai Twitter Data;Deep Learning;Long Short Term Memory;Dynamic Convolutional Neural Network,"1, 6",,IEEE Conferences,2016 13th International Joint Conference on Computer Science and Software Engineering (JCSSE),IEEE
772,,Comparison deep learning method to traditional methods using for network intrusion detection,B. Dong; X. Wang,2016,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7586590,10.1109/ICCSN.2016.7586590,"Recently, deep learning has gained prominence due to the potential it portends for machine learning. For this reason, deep learning techniques have been applied in many fields, such as recognizing some kinds of patterns or classification. Intrusion detection analyses got data from monitoring security events to get situation assessment of network. Lots of traditional machine learning method has been put forward to intrusion detection, but it is necessary to improvement the detection performance and accuracy. This paper discusses different methods which were used to classify network traffic. We decided to use different methods on open data set and did experiment with these methods to find out a best way to intrusion detection.",deep learning;intrusion detection;network security;classifier machine learning,"581, 585",,IEEE Conferences,2016 8th IEEE International Conference on Communication Software and Networks (ICCSN),IEEE
773,,Research of 3D face recognition algorithm based on deep learning stacked denoising autoencoder theory,J. Zhang; Z. Hou; Z. Wu; Y. Chen; W. Li,2016,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7586606,10.1109/ICCSN.2016.7586606,"This electronic Due to the fact that the 3D face depth data have more information, the 3D face recognition is attracting more and more attention in the machine learning area. Firstly, this paper selects 30 feature points from the 113 feature points of Candide-3 face model to characterize face, which improves the efficiency of recognition algorithm obviously without affecting the recognition accuracy. With the significant advantage of the characterization of essential features by learning a deep nonlinear network, this paper presents a stacked denoising autoencoder algorithm model based on deep learning which improves neural networks model. This algorithm conducts the unsupervised preliminary training of face depth data and the supervised training to fine-tuning the network which is better than neural network's random initialization. The experiment indicates that compared with real face data, the reconstruction face model has a small matching error by using SDAE algorithm and it achieves an excellent face recognition effect.",3D face depth data;deep learning;neural networks;stacked denoising autoencoder;unsupervised preliminary training,"663, 667",,IEEE Conferences,2016 8th IEEE International Conference on Communication Software and Networks (ICCSN),IEEE
774,,A pre-training strategy for convolutional neural network applied to Chinese digital gesture recognition,Y. Li; Y. Yang; Y. Chen; M. Zhu,2016,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7586597,10.1109/ICCSN.2016.7586597,"In this paper, we present an approach to classify Chinese digital gesture based on convolutional neural network (CNN). Principal Component Analysis (PCA) is employed to learn convolution kernels as the pre-training strategy. The learned convolution kernels are used for extracting features instead of the random convolution kernels. The convolutional layers can be directly implemented without any further training, such as Back Propagation (BP). For better understanding, we name the proposed architecture for PCA-based Convolutional Neural Network (PCNN). The dataset is divided into six gesture classes including 14500 gesture images, with 12000 images for training and 2500 images for testing. We examine the robustness of the PCNN against noises and distortions. In addition, the MNIST database of handwritten digits is employed to assess the suitability of the PCNN. Different from the CNN, the PCNN reduces the high computational cost of convolution kernels training. About one-fifth of the training time is shortened. The result shows that our approach classifies six gesture classes with 99.92% accuracy. Multiple experiments manifest the PCNN serving as an efficient approach for image processing and object recognition.",convolutional neural network;Chinese digital gesture recognition;principal component analysis;convolution kernels,"620, 624",,IEEE Conferences,2016 8th IEEE International Conference on Communication Software and Networks (ICCSN),IEEE
775,,Learning a dual-language vector space for domain-specific cross-lingual question retrieval,G. Chen; C. Chen; Z. Xing; B. Xu,2016,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582810,,"The lingual barrier limits the ability of millions of non-English speaking developers to make effective use of the tremendous knowledge in Stack Overflow, which is archived in English. For cross-lingual question retrieval, one may use translation-based methods that first translate the non-English queries into English and then perform monolingual question retrieval in English. However, translation-based methods suffer from semantic deviation due to inappropriate translation, especially for domain-specific terms, and lexical gap between queries and questions that share few words in common. To overcome the above issues, we propose a novel cross-lingual question retrieval based on word embed-dings and convolutional neural network (CNN) which are the state-of-the-art deep learning techniques to capture word- and sentence-level semantics. The CNN model is trained with large amounts of examples from Stack Overflow duplicate questions and their corresponding translation by machine, which guides the CNN to learn to capture informative word and sentence features to recognize and quantify semantic similarity in the presence of semantic deviations and lexical gaps. A uniqueness of our approach is that the trained CNN can map documents in two languages (e.g., Chinese queries and English questions) in a dual-language vector space, and thus reduce the cross-lingual question retrieval problem to a simple k-nearest neighbors search problem in the dual-language vector space, where no query or question translation is required. Our evaluation shows that our approach significantly outperforms the translation-based method, and can be extended to dual-language documents retrieval from different sources.",Cross-lingual question retrieval;Word embeddings;Convolutional Neural Network;Dual-Language Vector Space,"744, 755",,IEEE Conferences,2016 31st IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
776,,Too much automation? The bellwether effect and its implications for transfer learning,R. Krishna; T. Menzies; W. Fu,2016,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582751,,"“Transfer learning”: is the process of translating quality predictors learned in one data set to another. Transfer learning has been the subject of much recent research. In practice, that research means changing models all the time as transfer learners continually exchange new models to the current project. This paper offers a very simple “bellwether” transfer learner. Given N data sets, we find which one produces the best predictions on all the others. This “bellwether” data set is then used for all subsequent predictions (or, until such time as its predictions start failing-at which point it is wise to seek another bellwether). Bellwethers are interesting since they are very simple to find (just wrap a for-loop around standard data miners). Also, they simplify the task of making general policies in SE since as long as one bellwether remains useful, stable conclusions for N data sets can be achieved just by reasoning over that bellwether. From this, we conclude (1) this bellwether method is a useful (and very simple) transfer learning method; (2) “bellwethers” are a baseline method against which future transfer learners should be compared; (3) sometimes, when building increasingly complex automatic methods, researchers should pause and compare their supposedly more sophisticated method against simpler alternatives.",Defect Prediction;Data Mining;Transfer learning,"122, 131",,IEEE Conferences,2016 31st IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
777,,Visual hand gesture recognition with convolution neural network,M. Han; J. Chen; L. Li; Y. Chang,2016,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7515915,10.1109/SNPD.2016.7515915,"Hand gestures are a type of communication that is multifaceted in a number of ways and they provide an attractive alternative to the cumbersome interface devices used for human-computer interaction (HCI). However, there are still limitations regarding its usage in unfavorable live situations where hand gestures variation, illumination change or background complexity are an issue. Therefore, this paper propose a convolution neural network (CNN) method to reduce the difficulty of gestures recognition from a camera image. To achieve the robustness performance, the skin model and background subtraction are applied to obtain the training and testing data for the CNN. Since the light condition seriously affects the skin color, we adopt a simple Gaussian skin color model to robustly filter out non-skin colors of an image. In addition, it also employs elastic distortions to obtain lager database for more effective training and reduce potential overfitting. Experimental evaluation achieves an average correct classification rate of 93.8%, which shows the feasibility and reliability of the method.",hand gesture recognition;convolution neural network;background subtraction;human computer interaction,"287, 291",,IEEE Conferences,"2016 17th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)",IEEE
778,,Ultra High Frequency Polynomial and Cosine Artificial Higher Order Neural Networks,M. Zhang; C. Hu,2016,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7515877,10.1109/SNPD.2016.7515877,"New open box and nonlinear model of Ultra High Frequency Polynomial and Cosine Artificial Higher Order Neural Network (UPC-HONN) is presented in this paper. A new learning algorithm for UPC-HONN is also developed from this study. A time series data simulation and analysis system, UPC-HONN Simulator, is built based on the UPC-HONN models too. Test results show that average error of UPC-HONN models are closing to zero (0.0000%). The average errors of Polynomial Higher Order Neural Network (PHONN), Trigonometric Higher Order Neural Network (THONN), and Sigmoid polynomial Higher Order Neural Network (SPHONN) models are from 2.8546% to 3.4185%. It means that UPC-HONN models are 2.8546% to 3.4185% better than PHONN, THONN, and SPHONN models.",SHONN;PHONN;THONN;SPHONN;UPC-HONN,"49, 56",,IEEE Conferences,"2016 17th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)",IEEE
779,,Revisiting the Description-to-Behavior Fidelity in Android Applications,L. Yu; X. Luo; C. Qian; S. Wang,2016,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7476662,10.1109/SANER.2016.67,"Since more than 96% of mobile malware targets on Android platform, various techniques based on static code analysis or dynamic behavior analysis have been proposed to detect malicious applications. As malware is becoming more complicated and stealthy, recent research proposed a promising detection approach that looks for the inconsistency between an application's permissions and its description. In this paper, we revisit this approach and find that using description and permission will lead to many false positives. Therefore, we propose employing app's privacy policy and its bytecode to enhance description and permission for malware detection. It is non-trivial to automatically analyze privacy policy and perform the cross-verification among these four kinds of software artifacts including, privacy policy, bytecode, description, and permissions. We propose a novel data flow model for analyzing privacy policy, and develop a novel system, named TAPVerifier, for carrying out investigation of individual software artifacts and conducting the cross-verification. The experimental results show that TAPVerifier can analyze privacy policy with a high accuracy and recall rate. More importantly, integrating privacy policy and code level information removes 8.1%-65.5% false positives of existing systems based on description and permission.",Android Applications;Privacy Policy;Malware Detection;Description-to-Behavior Fidelity,"415, 426",,IEEE Conferences,"2016 IEEE 23rd International Conference on Software Analysis, Evolution, and Reengineering (SANER)",IEEE
780,,MICHAC: Defect Prediction via Feature Selection Based on Maximal Information Coefficient with Hierarchical Agglomerative Clustering,Z. Xu; J. Xuan; J. Liu; X. Cui,2016,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7476658,10.1109/SANER.2016.34,"Defect prediction aims to estimate software reliability via learning from historical defect data. A defect prediction method identifies whether a software module is defect-prone or not according to metrics that are mined from software projects. These metric values, also known as features, may involve irrelevance and redundancy, which will hurt the performance of defect prediction methods. Existing work employs feature selection to preprocess defect data to filter out useless features. In this paper, we propose a novel feature selection framework, MICHAC, short for defect prediction via Maximal Information Coefficient with Hierarchical Agglomerative Clustering. MICHAC consists of two major stages. First, MICHAC employs maximal information coefficient to rank candidate features to filter out irrelevant ones, second, MICHAC groups features with hierarchical agglomerative clustering and selects one feature from each resulted group to remove redundant features. We evaluate our proposed method on 11 widelystudied NASA projects and four open-source AEEEM projects using three different classifiers with four performance metrics (precision, recall, F-measure, and AUC). Comparison with five existing methods demonstrates that MICHAC is effective in selecting features in defect prediction.",defect prediction;feature selection;maximal information coefficient,"370, 381",,IEEE Conferences,"2016 IEEE 23rd International Conference on Software Analysis, Evolution, and Reengineering (SANER)",IEEE
781,,HYDRA: Massively Compositional Model for Cross-Project Defect Prediction,X. Xia; D. Lo; S. J. Pan; N. Nagappan; X. Wang,2016,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7435328,10.1109/TSE.2016.2543218,"Most software defect prediction approaches are trained and applied on data from the same project. However, often a new project does not have enough training data. Cross-project defect prediction, which uses data from other projects to predict defects in a particular project, provides a new perspective to defect prediction. In this work, we propose a HYbrid moDel Reconstruction Approach (HYDRA) for cross-project defect prediction, which includes two phases: genetic algorithm (GA) phase and ensemble learning (EL) phase. These two phases create a massive composition of classifiers. To examine the benefits of HYDRA, we perform experiments on 29 datasets from the PROMISE repository which contains a total of 11,196 instances (i.e., Java classes) labeled as defective or clean. We experiment with logistic regression as the underlying classification algorithm of HYDRA. We compare our approach with the most recently proposed cross-project defect prediction approaches: TCA+ by Nam et al., Peters filter by Peters et al., GP by Liu et al., MO by Canfora et al., and CODEP by Panichella et al. Our results show that HYDRA achieves an average F1-score of 0.544. On average, across the 29 datasets, these results correspond to an improvement in the F1-scores of 26.22 , 34.99, 47.43, 28.61, and 30.14 percent over TCA+, Peters filter, GP, MO, and CODEP, respectively. In addition, HYDRA on average can discover 33 percent of all bugs if developers inspect the top 20 percent lines of code, which improves the best baseline approach (TCA+) by 44.41 percent. We also find that HYDRA improves the F1-score of Zero-R which predict all the instances to be defective by 5.42 percent, but improves Zero-R by 58.65 percent when inspecting the top 20 percent lines of code. In practice, Zero-R can be hard to use since it simply predicts all of the instances to be defective, and thus developers have to inspect all of the instances to find the defective ones. Moreover, we notice the improvement of HYDRA over other baseline approaches in terms of F1-score and when inspecting the top 20 percent lines of code are substantial, and in most cases the improvements are significant and have large effect sizes across the 29 datasets.",Cross-project defect prediction;transfer learning;genetic algorithm;ensemble learning,"977, 998",,IEEE Journals,IEEE Transactions on Software Engineering,IEEE
782,,Data Mining Methods and Cost Estimation Models: Why is it So Hard to Infuse New Ideas?,J. Hihn; T. Menzies,2015,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7426628,10.1109/ASEW.2015.27,"Infusing new technologies and methods is hard and can often be described as ""banging ones head on a brick wall"". The is especially true when trying to get project managers, systems, engineers and cost analysts to add a radically new tool to their tool box. In this paper we suggest that the underlying causes are rooted in the fact that the different players have fundamental differences in mental models, vocabulary and objectives. We based this work on lessons learned from ten years of working on the infusion of software costing models into NASA. The good news is that, lately, a crack has begun to appear in what was previously a brick wall.",software;cost estimation;effort estimation;data mining;technology infusion,"5, 9",,IEEE Conferences,2015 30th IEEE/ACM International Conference on Automated Software Engineering Workshop (ASEW),IEEE
783,,CrossPare: A Tool for Benchmarking Cross-Project Defect Predictions,S. Herbold,2015,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7426644,10.1109/ASEW.2015.8,"During the last decade, many papers on defect prediction were published. One still for the most part unresolved issue are cross-project defect predictions. Here, the aim is to predict the defects of a project, with data from other projects. Many approaches were suggested and evaluated in recent years. However, due to the usage of different implementations and data sets, the comparison between the work is a hard task. Within this paper, we present the tool CrossPare. CrossPare is designed to facilitate benchmarks for cross-project defect predictions. The tool already implements many techniques proposed within the current state of the art of cross-project defect predictions. Moreover, the tool is able to load different data sets that are commonly used for the evaluation of techniques and supports all major performance metrics. Through the usage of CrossPare other reseachers can improve the comparability of their results and possibly also reduce their implementation efforts for new cross-project defect prediction techniques by reusing features already offered by CrossPare.",defect prediction;cross-project;benchmark,"90, 96",,IEEE Conferences,2015 30th IEEE/ACM International Conference on Automated Software Engineering Workshop (ASEW),IEEE
784,,Deep neural network based malware detection using two dimensional binary program features,J. Saxe; K. Berlin,2015,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7413680,10.1109/MALWARE.2015.7413680,"In this paper we introduce a deep neural network based malware detection system that Invincea has developed, which achieves a usable detection rate at an extremely low false positive rate and scales to real world training example volumes on commodity hardware. We show that our system achieves a 95% detection rate at 0.1% false positive rate (FPR), based on more than 400,000 software binaries sourced directly from our customers and internal malware databases. In addition, we describe a non-parametric method for adjusting the classifier's scores to better represent expected precision in the deployment environment. Our results demonstrate that it is now feasible to quickly train and deploy a low resource, highly accurate machine learning classification model, with false positive rates that approach traditional labor intensive expert rule based malware detection, while also detecting previously unseen malware missed by these traditional approaches. Since machine learning models tend to improve with larger datasizes, we foresee deep neural network classification models gaining in importance as part of a layered network defense strategy in coming years.",,"11, 20",,IEEE Conferences,2015 10th International Conference on Malicious and Unwanted Software (MALWARE),IEEE
785,,Deep learning based large scale handwritten Devanagari character recognition,S. Acharya; A. K. Pant; P. K. Gyawali,2015,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7400041,10.1109/SKIMA.2015.7400041,"In this paper, we introduce a new public image dataset for Devanagari script: Devanagari Handwritten Character Dataset (DHCD). Our dataset consists of 92 thousand images of 46 different classes of characters of Devanagari script segmented from handwritten documents. We also explore the challenges in recognition of Devanagari characters. Along with the dataset, we also propose a deep learning architecture for recognition of those characters. Deep Convolutional Neural Network (CNN) have shown superior results to traditional shallow networks in many recognition tasks. Keeping distance with the regular approach of character recognition by Deep CNN, we focus the use of Dropout and dataset increment approach to improve test accuracy. By implementing these techniques in Deep CNN, we were able to increase test accuracy by nearly 1 percent. The proposed architecture scored highest test accuracy of 98.47% on our dataset.",Devanagari Handwritten Character Dataset;Image processing;Computer Vision;Deep learning;Deep Convolutional Neural Network;Optical Character Recognition;Dropout,"1, 6",,IEEE Conferences,"2015 9th International Conference on Software, Knowledge, Information Management and Applications (SKIMA)",IEEE
786,,CLAMI: Defect Prediction on Unlabeled Datasets (T),J. Nam; S. Kim,2015,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372033,10.1109/ASE.2015.56,"Defect prediction on new projects or projects with limited historical data is an interesting problem in software engineering. This is largely because it is difficult to collect defect information to label a dataset for training a prediction model. Cross-project defect prediction (CPDP) has tried to address this problem by reusing prediction models built by other projects that have enough historical data. However, CPDP does not always build a strong prediction model because of the different distributions among datasets. Approaches for defect prediction on unlabeled datasets have also tried to address the problem by adopting unsupervised learning but it has one major limitation, the necessity for manual effort. In this study, we propose novel approaches, CLA and CLAMI, that show the potential for defect prediction on unlabeled datasets in an automated manner without need for manual effort. The key idea of the CLA and CLAMI approaches is to label an unlabeled dataset by using the magnitude of metric values. In our empirical study on seven open-source projects, the CLAMI approach led to the promising prediction performances, 0.636 and 0.723 in average f-measure and AUC, that are comparable to those of defect prediction based on supervised learning.",,"452, 463",,IEEE Conferences,2015 30th IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
787,,Pseudogen: A Tool to Automatically Generate Pseudo-Code from Source Code,H. Fudaba; Y. Oda; K. Akabe; G. Neubig; H. Hata; S. Sakti; T. Toda; S. Nakamura,2015,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372074,10.1109/ASE.2015.107,"Understanding the behavior of source code written in an unfamiliar programming language is difficult. One way to aid understanding of difficult code is to add corresponding pseudo-code, which describes in detail the workings of the code in a natural language such as English. In spite of its usefulness, most source code does not have corresponding pseudo-code because it is tedious to create. This paper demonstrates a tool Pseudogen that makes it possible to automatically generate pseudo-code from source code using statistical machine translation (SMT). Pseudogen currently supports generation of English or Japanese pseudo-code from Python source code, and the SMT framework makes it easy for users to create new generators for their preferred source code/pseudo-code pairs.",machine translation;programming language;natural language,"824, 829",,IEEE Conferences,2015 30th IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
788,,A Knowledge Resources Based Neural Network for Learning Word and Relation Representations,S. Yuan; Y. Xiang; M. Li,2015,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7336421,10.1109/HPCC-CSS-ICESS.2015.107,"Using neural networks to train high quality distributed representations of words and multi-relational data has attracted a great attention in recent years. Mapping the words and their relations to low-dimensional continues vector spaces has proved to be useful in natural language processing and information extraction tasks. In this paper, we present a neural network based model that can train word embeddings and relation embeddings taking into account unlabeled text data and knowledge resources jointly. In particular, we use both contexts and definitions of words as neural network inputs to train word embeddings. Based on the word embeddings, we train relation embeddings by defining a proper projecting operation between words. Experiments on various tasks like word similarity and link prediction show that the proposed method can achieve high quality on word and relation representations.",Word embeddings;relation embeddings;neural network;knowledge resources,"1731, 1736",,IEEE Conferences,"2015 IEEE 17th International Conference on High Performance Computing and Communications, 2015 IEEE 7th International Symposium on Cyberspace Safety and Security, and 2015 IEEE 12th International Conference on Embedded Software and Systems",IEEE
789,,Privacy Preserving Back-Propagation Based on BGV on Cloud,F. Bu; Y. Ma; Z. Chen; H. Xu,2015,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7336431,10.1109/HPCC-CSS-ICESS.2015.323,"Back-propagation is the most effective algorithm for training deep learning models that are proved to have a great ability for big data feature learning. However, back-propagation is of high time complexity, leading to a low efficiency of big data learning. Aiming at this problem, the paper proposes a privacy preserving back-propagation algorithm based on the BGV encryption scheme on cloud. The proposed algorithm improved the efficiency of back-propagation learning by offloading the expensive operations on the cloud. Furthermore, the BGV encryption scheme is used to protect the private data during the learning process using the power of the cloud computing. Experiments show that our proposed scheme is secure and efficient.",Back-propagation; big data learning; cloud computing; the BGV encryption scheme,"1791, 1795",,IEEE Conferences,"2015 IEEE 17th International Conference on High Performance Computing and Communications, 2015 IEEE 7th International Symposium on Cyberspace Safety and Security, and 2015 IEEE 12th International Conference on Embedded Software and Systems",IEEE
790,,Big/little deep neural network for ultra low power inference,E. Park; D. Kim; S. Kim; Y. -D. Kim; G. Kim; S. Yoon; S. Yoo,2015,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7331375,10.1109/CODESISSS.2015.7331375,"Deep neural networks (DNNs) have recently proved their effectiveness in complex data analyses such as object/speech recognition. As their applications are being expanded to mobile devices, their energy efficiencies are becoming critical. In this paper, we propose a novel concept called big/LITTLE DNN (BL-DNN) which significantly reduces energy consumption required for DNN execution at a negligible loss of inference accuracy. The BL-DNN consists of a little DNN (consuming low energy) and a full-fledged big DNN. In order to reduce energy consumption, the BL-DNN aims at avoiding the execution of the big DNN whenever possible. The key idea for this goal is to execute the little DNN first for inference (without big DNN execution) and simply use its result as the final inference result as long as the result is estimated to be accurate. On the other hand, if the result from the little DNN is not considered to be accurate, the big DNN is executed to give the final inference result. This approach reduces the total energy consumption by obtaining the inference result only with the little, energy-efficient DNN in most cases, while maintaining the similar level of inference accuracy through selectively utilizing the big DNN execution. We present design-time and runtime methods to control the execution of big DNN under a trade-off between energy consumption and inference accuracy. Experiments with state-of-the-art DNNs for ImageNet and MNIST show that our proposed BL-DNN can offer up to 53.7% (ImageNet) and 94.1% (MNIST) reductions in energy consumption at a loss of 0.90% (ImageNet) and 0.12% (MNIST) in inference accuracy, respectively.",Deep neural network;low power,"124, 132",,IEEE Conferences,2015 International Conference on Hardware/Software Codesign and System Synthesis (CODES+ISSS),IEEE
791,,Improving Cross-Project Defect Prediction Methods with Data Simplification,S. Amasaki; K. Kawata; T. Yokogawa,2015,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7302438,10.1109/SEAA.2015.25,"Context: Cross-project defect prediction (CPDP) research has been popular and many CPDP methods were proposed. While these methods used cross-project data as is for their inputs, useless or noisy information in the cross-project data can cause the degradation of predictive and computation performance. Removing such information makes the cross-project data simple and it will affect the performance of CPDP methods. Objective: To identify and quantify the effects of the data simplification for CPDP methods. Method: We conducted experiments that compared the predictive performance between CPDP with and without the data simplification. We adopted a data simplification method based on an active learning method proposed for software effort estimation. The experiments adopted 44 versions of OSS projects, four prediction models, and two CPDP methods, namely, Burak-filter and cross-project selection. Results: The data simplification achieved significant improvement in predictive performance for the cross-project selection. It did not improve Burak-filter. Conclusion: The data simplification can be helpful for the cross-project selection in terms of predictive performance and size reduction of cross-project data.",cross-project defect prediction;data simplification,"96, 103",,IEEE Conferences,2015 41st Euromicro Conference on Software Engineering and Advanced Applications,IEEE
792,,EvoAE -- A New Evolutionary Method for Training Autoencoders for Deep Learning Networks,S. Lander; Y. Shang,2015,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7273701,10.1109/COMPSAC.2015.63,"Although deep learning has achieved outstanding performances on several difficult machine learning applications, there are multiple issues that make its application on new problems difficult: speed of training, local minima, and manual selection of hyper-parameters. To overcome these problems, this paper proposes a new evolutionary method, EvoAE, to train auto encoders for deep learning networks. By evolving a population of auto encoders, EvoAE learns multiple features in each auto encoder in the form of hidden nodes, evaluates the auto encoders based on their reconstruction quality, and generates new auto encoders using crossover and mutation with chromosomes made up of hidden nodes and associated connections and weights. EvoAE optimizes network weights and structures of auto encoders simultaneously and employs a mini-batch variant, called Evo-batch, to speed up auto encoder search on large datasets. Furthermore, EvoAE supports different training methods in data partitioning and selection, requires little manual intervention, and reduces overall training time drastically over traditional methods on large datasets.",autoencoder;neural networks;deep learning;evolutionary algorithm,"790, 795",,IEEE Conferences,2015 IEEE 39th Annual Computer Software and Applications Conference,IEEE
793,,Cross-Project Aging Related Bug Prediction,F. Qin; Z. Zheng; C. Bai; Y. Qiao; Z. Zhang; C. Chen,2015,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7272913,10.1109/QRS.2015.17,"In a long running system, software tends to encounter performance degradation and increasing failure rate during execution, which is called software aging. The bugs contributing to the phenomenon of software aging are defined as Aging Related Bugs (ARBs). Lots of manpower and economic costs will be saved if ARBs can be found in the testing phase. However, due to the low presence probability and reproducing difficulty of ARBs, it is usually hard to predict ARBs within a project. In this paper, we study whether and how ARBs can be located through cross-project prediction. We propose a transfer learning based aging related bug prediction approach (TLAP), which takes advantage of transfer learning to reduce the distribution difference between training sets and testing sets while preserving their data variance. Furthermore, in order to mitigate the severe class imbalance, class imbalance learning is conducted on the transferred latent space. Finally, we employ machine learning methods to handle the bug prediction tasks. The effectiveness of our approach is validated and evaluated by experiments on two real software systems. It indicates that after the processing of TLAP, the performance of ARB bug prediction can be dramatically improved.",aging related bug;ross-project;bug prediction;software aging;transfer learning,"43, 48",,IEEE Conferences,"2015 IEEE International Conference on Software Quality, Reliability and Security",IEEE
794,,Deep Learning for Just-in-Time Defect Prediction,X. Yang; D. Lo; X. Xia; Y. Zhang; J. Sun,2015,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7272910,10.1109/QRS.2015.14,"Defect prediction is a very meaningful topic, particularly at change-level. Change-level defect prediction, which is also referred as just-in-time defect prediction, could not only ensure software quality in the development process, but also make the developers check and fix the defects in time. Nowadays, deep learning is a hot topic in the machine learning literature. Whether deep learning can be used to improve the performance of just-in-time defect prediction is still uninvestigated. In this paper, to bridge this research gap, we propose an approach Deeper which leverages deep learning techniques to predict defect-prone changes. We first build a set of expressive features from a set of initial change features by leveraging a deep belief network algorithm. Next, a machine learning classifier is built on the selected features. To evaluate the performance of our approach, we use datasets from six large open source projects, i.e., Bugzilla, Columba, JDT, Platform, Mozilla, and PostgreSQL, containing a total of 137,417 changes. We compare our approach with the approach proposed by Kamei et al. The experimental results show that on average across the 6 projects, Deeper could discover 32.22% more bugs than Kamei et al's approach (51.04% versus 18.82% on average). In addition, Deeper can achieve F1-scores of 0.22-0.63, which are statistically significantly higher than those of Kamei et al.'s approach on 4 out of the 6 projects.",Deep Learning;Just-In-Time Defect Prediction;Deep Belief Network;Cost Effectiveness,"17, 26",,IEEE Conferences,"2015 IEEE International Conference on Software Quality, Reliability and Security",IEEE
795,,Learning to Log: Helping Developers Make Informed Logging Decisions,J. Zhu; P. He; Q. Fu; H. Zhang; M. R. Lyu; D. Zhang,2015,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194593,10.1109/ICSE.2015.60,"Logging is a common programming practice of practical importance to collect system runtime information for postmortem analysis. Strategic logging placement is desired to cover necessary runtime information without incurring unintended consequences (e.g., Performance overhead, trivial logs). However, in current practice, there is a lack of rigorous specifications for developers to govern their logging behaviours. Logging has become an important yet tough decision which mostly depends on the domain knowledge of developers. To reduce the effort on making logging decisions, in this paper, we propose a ""learning to log"" framework, which aims to provide informative guidance on logging during development. As a proof of concept, we provide the design and implementation of a logging suggestion tool, Log Advisor, which automatically learns the common logging practices on where to log from existing logging instances and further leverages them for actionable suggestions to developers. Specifically, we identify the important factors for determining where to log and extract them as structural features, textual features, and syntactic features. Then, by applying machine learning techniques (e.g., Feature selection and classifier learning) and noise handling techniques, we achieve high accuracy of logging suggestions. We evaluate Log Advisor on two industrial software systems from Microsoft and two open-source software systems from Git Hub (totally 19.1M LOC and 100.6K logging statements). The encouraging experimental results, as well as a user study, demonstrate the feasibility and effectiveness of our logging suggestion tool. We believe our work can serve as an important first step towards the goal of ""learning to log"".",,"415, 425",,IEEE Conferences,2015 IEEE/ACM 37th IEEE International Conference on Software Engineering,IEEE
796,,Facilitating Coordination between Software Developers: A Study and Techniques for Timely and Efficient Recommendations,K. Blincoe; G. Valetto; D. Damian,2015,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7105409,10.1109/TSE.2015.2431680,"When software developers fail to coordinate, build failures, duplication of work, schedule slips and software defects can result. However, developers are often unaware of when they need to coordinate, and existing methods and tools that help make developers aware of their coordination needs do not provide timely or efficient recommendations. We describe our techniques to identify timely and efficient coordination recommendations, which we developed and evaluated in a study of coordination needs in the Mylyn software project. We describe how data obtained from tools that capture developer actions within their Integrated Development Environment (IDE) as they occur can be used to timely identify coordination needs; we also describe how properties of tasks coupled with machine learning can focus coordination recommendations to those that are more critical to the developers to reduce information overload and provide more efficient recommendations. We motivate our techniques through developer interviews and report on our quantitative analysis of coordination needs in the Mylyn project. Our results suggest that by leveraging IDE logging facilities, properties of tasks and machine learning techniques awareness tools could make developers aware of critical coordination needs in a timely way. We conclude by discussing implications for software engineering research and tool design.",Human Factors in Software Design;Management;Metrics/Measurement;Productivity;Programming Teams;Computer-supported cooperative work;human factors in software design;management;metrics/measurement;productivity;programming teams,"969, 985",,IEEE Journals,IEEE Transactions on Software Engineering,IEEE
797,,Cross-project build co-change prediction,X. Xia; D. Lo; S. McIntosh; E. Shihab; A. E. Hassan,2015,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7081841,10.1109/SANER.2015.7081841,"Build systems orchestrate how human-readable source code is translated into executable programs. In a software project, source code changes can induce changes in the build system (aka. build co-changes). It is difficult for developers to identify when build co-changes are necessary due to the complexity of build systems. Prediction of build co-changes works well if there is a sufficient amount of training data to build a model. However, in practice, for new projects, there exists a limited number of changes. Using training data from other projects to predict the build co-changes in a new project can help improve the performance of the build co-change prediction. We refer to this problem as cross-project build co-change prediction. In this paper, we propose CroBuild, a novel cross-project build co-change prediction approach that iteratively learns new classifiers. CroBuild constructs an ensemble of classifiers by iteratively building classifiers and assigning them weights according to its prediction error rate. Given that only a small proportion of code changes are build co-changing, we also propose an imbalance-aware approach that learns a threshold boundary between those code changes that are build co-changing and those that are not in order to construct classifiers in each iteration. To examine the benefits of CroBuild, we perform experiments on 4 large datasets including Mozilla, Eclipse-core, Lucene, and Jazz, comprising a total of 50,884 changes. On average, across the 4 datasets, CroBuild achieves a F1-score of up to 0.408. We also compare CroBuild with other approaches such as a basic model, AdaBoost proposed by Freund et al., and TrAdaBoost proposed by Dai et al.. On average, across the 4 datasets, the CroBuild approach yields an improvement in F1-scores of 41.54%, 36.63%, and 36.97% over the basic model, AdaBoost, and TrAdaBoost, respectively.",Cross-project;Build Co-change Prediction;Transfer Learning;Imbalance Data,"311, 320",,IEEE Conferences,"2015 IEEE 22nd International Conference on Software Analysis, Evolution, and Reengineering (SANER)",IEEE
798,,Untangling fine-grained code changes,M. Dias; A. Bacchelli; G. Gousios; D. Cassou; S. Ducasse,2015,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7081844,10.1109/SANER.2015.7081844,"After working for some time, developers commit their code changes to a version control system. When doing so, they often bundle unrelated changes (e.g., bug fix and refactoring) in a single commit, thus creating a so-called tangled commit. Sharing tangled commits is problematic because it makes review, reversion, and integration of these commits harder and historical analyses of the project less reliable. Researchers have worked at untangling existing commits, i.e., finding which part of a commit relates to which task. In this paper, we contribute to this line of work in two ways: (1) A publicly available dataset of untangled code changes, created with the help of two developers who accurately split their code changes into self contained tasks over a period of four months; (2) a novel approach, EpiceaUntangler, to help developers share untangled commits (aka. atomic commits) by using fine-grained code change information. EpiceaUntangler is based and tested on the publicly available dataset, and further evaluated by deploying it to 7 developers, who used it for 2 weeks. We recorded a median success rate of 91% and average one of 75%, in automatically creating clusters of untangled fine-grained code changes.",,"341, 350",,IEEE Conferences,"2015 IEEE 22nd International Conference on Software Analysis, Evolution, and Reengineering (SANER)",IEEE
799,,Embedded Face Detection Application Based on Local Binary Patterns,L. Acasandrei; A. Barriga,2014,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7056811,10.1109/HPCC.2014.121,"In computer vision during the recent years a new paradigm for object detection has stimulated researchers and designers interest. The foundation of this new paradigm is the Local Binary Pattern (LBP) which is a nonparametric operator that efficiently extracts the features of local structures in images. This communication describes a software embedded implementation of LBP based algorithm for object detection, in particular targeting frontal face detection.",Face detection;LBP;Embedded Software,"641, 644",,IEEE Conferences,"2014 IEEE Intl Conf on High Performance Computing and Communications, 2014 IEEE 6th Intl Symp on Cyberspace Safety and Security, 2014 IEEE 11th Intl Conf on Embedded Software and Syst (HPCC,CSS,ICESS)",IEEE
800,,Learning to Combine Multiple Ranking Metrics for Fault Localization,J. Xuan; M. Monperrus,2014,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6976085,10.1109/ICSME.2014.41,"Fault localization is an inevitable step in software debugging. Spectrum-based fault localization consists in computing a ranking metric on execution traces to identify faulty source code. Existing empirical studies on fault localization show that there is no optimal ranking metric for all faults in practice. In this paper, we propose Multric, a learning-based approach to combining multiple ranking metrics for effective fault localization. In Multric, a suspiciousness score of a program entity is a combination of existing ranking metrics. Multric consists two major phases: learning and ranking. Based on training faults, Multric builds a ranking model by learning from pairs of faulty and non-faulty source code elements. When a new fault appears, Multric computes the final ranking with the learned model. Experiments are conducted on 5386 seeded faults in ten open-source Java programs. We empirically compare Multric against four widely-studied metrics and three recently-proposed one. Our experimental results show that Multric localizes faults more effectively than state-of-art metrics, such as Tarantula, Ochiai, and Ample.",Fault localization;learning to rank;multiple ranking metrics,"191, 200",,IEEE Conferences,2014 IEEE International Conference on Software Maintenance and Evolution,IEEE
801,,A novel deep model for image recognition,M. Zhu; Y. Wu,2014,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6933585,10.1109/ICSESS.2014.6933585,"In this paper we propose a hybrid deep network for image recognition. First we use the sparse autoencoder(SAE) which is a method to extract high-level feature representations of data in an unsupervised way, without any manual feature engineering, and then we perform the classification using the deep belief networks(DBNs), which consist of restricted Boltzmann machine(RBM). Finally, we implement some comparative experiments on image datasets, and the results show that our methods achieved better performance when compared with neural network and other deep learning techniques such as DBNs.",deep belief network;sparse autoencoder;image recognition,"373, 376",,IEEE Conferences,2014 IEEE 5th International Conference on Software Engineering and Service Science,IEEE
802,,Metadata for Energy Disaggregation,J. Kelly; W. Knottenbelt,2014,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6903193,10.1109/COMPSACW.2014.97,"Energy disaggregation is the process of estimating the energy consumed by individual electrical appliances given only a time series of the whole-home power demand. Energy disaggregation researchers require datasets of the power demand from individual appliances and the whole-home power demand. Multiple such datasets have been released over the last few years but provide metadata in a disparate array of formats including CSV files and plain-text README files. At best, the lack of a standard metadata schema makes it unnecessarily time-consuming to write software to process multiple datasets and, at worse, the lack of a standard means that crucial information is simply absent from some datasets. We propose a metadata schema for representing appliances, meters, buildings, datasets, prior knowledge about appliances and appliance models. The schema is relational and provides a simple but powerful inheritance mechanism.",energy;disaggregation;NILM;NIALM;metadata;YAML,"578, 583",,IEEE Conferences,2014 IEEE 38th International Computer Software and Applications Conference Workshops,IEEE
803,,Specific Touch Gesture on Mobile Devices to Find Attractive Phrases in News Browsing,S. Ito; T. Yoshida; F. Harada; H. Shimakawa,2014,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6899256,10.1109/COMPSAC.2014.74,"When smart phone users browse web news articles, they encounter attractive phrases by chance. At that time, they try to obtain information on the smart phones. In order to search the information of the attractive phrases on web pages, they have to manipulate the smart phones to search on the small screen. It causes stresses because of situations such as manipulation errors. Such stresses could be eliminated if the attractive phrases can be identified and be input automatically in order to recommend the web pages with the information of the attractive phrases. Because of the small screens of smart phones, users move the news article area displayed on the screens with touch gestures such as swipe. The history of touch gestures during browsing an article implies the position and the timing users have focused on in the article. This paper proposes a method to identify the areas where attractive phrases have appeared on news articles, in order to enable automatic identification of attractive phrases. We have achieved real-time identification by utilizing the history of touch gestures during a web news browsing. The proposed method shows the history of touch gestures by a gesture trail. It is a graph showing the time series of the vertical coordinate of the displayed article area. When users encounter attractive phrases, they take certain patterns of touch gestures to confirm or read carefully the neighborhood of the attractive phrases. Thus, in the proposed method, the news article area including an attractive phrase is detected by matching the gesture trail in a sliding time window with a pattern obtained by pre-training. We use slow-down and resting patterns approximated by quadratic functions with the parameters defined for individual users. Experiments to identify time windows of attractive phrases on gesture trails has revealed that the highest and the lowest precision ratios are 0.579 and 0.278, respectively.",mobile device;recommendation;smartphone;touch gesture;web news,"519, 528",,IEEE Conferences,2014 IEEE 38th Annual Computer Software and Applications Conference,IEEE
804,,Learning from Open-Source Projects: An Empirical Study on Defect Prediction,Z. He; F. Peters; T. Menzies; Y. Yang,2013,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6681337,10.1109/ESEM.2013.20,"The fundamental issue in cross project defect prediction is selecting the most appropriate training data for creating quality defect predictors. Another concern is whether historical data of open-source projects can be used to create quality predictors for proprietary projects from a practical point-of-view. Current studies have proposed statistical approaches to finding these training data, however, thus far no apparent effort has been made to study their success on proprietary data. Also these methods apply brute force techniques which are computationally expensive. In this work we introduce a novel data selection procedure which takes into account the similarities between the distribution of the test and potential training data. Additionally we use feature subset selection to increase the similarity between the test and training sets. Our procedure provides a comparable and scalable means of solving the cross project defect prediction problem for creating quality defect predictors. To evaluate our procedure we conducted empirical studies with comparisons to the within company defect prediction and a relevancy filtering method. We found that our proposed method performs relatively better than the filtering method in terms of both computation cost and prediction performance.",software defect prediction;cross-project;instance selection;feature subset selection;data similarity,"45, 54",,IEEE Conferences,2013 ACM / IEEE International Symposium on Empirical Software Engineering and Measurement,IEEE
805,,An Analysis of Machine Learning Algorithms for Condensing Reverse Engineered Class Diagrams,M. H. Osman; M. R. V. Chaudron; P. v. d. Putten,2013,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6676885,10.1109/ICSM.2013.25,"There is a range of techniques available to reverse engineer software designs from source code. However, these approaches generate highly detailed representations. The condensing of reverse engineered representations into more high-level design information would enhance the understandability of reverse engineered diagrams. This paper describes an automated approach for condensing reverse engineered diagrams into diagrams that look as if they are constructed as forward designed UML models. To this end, we propose a machine learning approach. The training set of this approach consists of a set of forward designed UML class diagrams and reverse engineered class diagrams (for the same system). Based on this training set, the method 'learns' to select the key classes for inclusion in the class diagrams. In this paper, we study a set of nine classification algorithms from the machine learning community and evaluate which algorithms perform best for predicting the key classes in a class diagram.",Software Engineering;UML;Reverse Engineering;Machine Learning;Program Comprehension,"140, 149",,IEEE Conferences,2013 IEEE International Conference on Software Maintenance,IEEE
806,,Better cross company defect prediction,F. Peters; T. Menzies; A. Marcus,2013,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6624057,10.1109/MSR.2013.6624057,"How can we find data for quality prediction? Early in the life cycle, projects may lack the data needed to build such predictors. Prior work assumed that relevant training data was found nearest to the local project. But is this the best approach? This paper introduces the Peters filter which is based on the following conjecture: When local data is scarce, more information exists in other projects. Accordingly, this filter selects training data via the structure of other projects. To assess the performance of the Peters filter, we compare it with two other approaches for quality prediction. Within-company learning and cross-company learning with the Burak filter (the state-of-the-art relevancy filter). This paper finds that: 1) within-company predictors are weak for small data-sets; 2) the Peters filter+cross-company builds better predictors than both within-company and the Burak filter+cross-company; and 3) the Peters filter builds 64% more useful predictors than both within-company and the Burak filter+cross-company approaches. Hence, we recommend the Peters filter for cross-company learning.",Cross company;defect prediction;data mining,"409, 418",,IEEE Conferences,2013 10th Working Conference on Mining Software Repositories (MSR),IEEE
807,,Which work-item updates need your response?,D. Mukherjee; M. Garg,2013,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6623998,10.1109/MSR.2013.6623998,"Work-item notifications alert the team collaborating on a work-item about any update to the work-item (e.g., addition of comments, change in status). However, as software professionals get involved with multiple tasks in project(s), they are inundated by too many notifications from the work-item tool. Users are upset that they often miss the notifications that solicit their response in the crowd of mostly useless ones. We investigate the severity of this problem by studying the work-item repositories of two large collaborative projects and conducting a user study with one of the project teams. We find that, on an average, only 1 out of every 5 notifications that are received by the users require a response from them. We propose TWINY - a machine learning based approach to predict whether a notification will prompt any action from its recipient. Such a prediction can help to suitably mark up notifications and to decide whether a notification needs to be sent out immediately or be bundled in a message digest. We conduct empirical studies to evaluate the efficacy of different classification techniques in this setting. We find that incremental learning algorithms are ideally suited, and ensemble methods appear to give the best results in terms of prediction accuracy.",,"12, 21",,IEEE Conferences,2013 10th Working Conference on Mining Software Repositories (MSR),IEEE
808,,Predicting more from less: Synergies of learning,E. Kocaguneli; B. Cukic; H. Lu,2013,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6615203,10.1109/RAISE.2013.6615203,"Thanks to the ever increasing importance of project data, its collection has been one of the primary focuses of software organizations. Data collection activities have resulted in the availability of massive amounts of data through software data repositories. This is great news for the predictive modeling research in software engineering. However, widely used supervised methods for predictive modeling require labeled data that is relevant to the local context of a project. This requirement cannot be met by many of the available data sets, introducing new challenges for software engineering research. How to transfer data between different contexts? How to handle insufficient number of labeled instances? In this position paper, we investigate synergies between different learning methods (transfer, semi-supervised and active learning) which may overcome these challenges.",,"42, 48",,IEEE Conferences,2013 2nd International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering (RAISE),IEEE
809,,Image fusion with double sparse representation in wavelet domain,Wang Jun; Peng Jinye; Wu Jun; Yan Kun,2013,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6615476,10.1109/ICSESS.2013.6615476,"Aiming at the problem of image fusion method based on sparse representation being easy to lose image details, a fusion method based on double sparse representation in wavelet domain is presented. Firstly, training images are transformed into the wavelet domain and learning dictionary for each sub-band respectively. And the double sparse representation coefficients for source images can be acquired by the learned dictionary and the coefficients being combined with the choose-max fusion rule. Finally, the fusion image is reconstructed by the inverse wavelet transform. The computer simulation results show that the proposed method performs very well in fusion both noiseless and noisy situations, and outperform conventional methods in terms of visual effect and quantitative fusion evaluation indexes.",image fusion;adaptive systems;computer simulation;double sparse representation;wavelet,"1006, 1009",,IEEE Conferences,2013 IEEE 4th International Conference on Software Engineering and Service Science,IEEE
810,,Transfer defect learning,J. Nam; S. J. Pan; S. Kim,2013,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606584,10.1109/ICSE.2013.6606584,"Many software defect prediction approaches have been proposed and most are effective in within-project prediction settings. However, for new projects or projects with limited training data, it is desirable to learn a prediction model by using sufficient training data from existing source projects and then apply the model to some target projects (cross-project defect prediction). Unfortunately, the performance of cross-project defect prediction is generally poor, largely because of feature distribution differences between the source and target projects. In this paper, we apply a state-of-the-art transfer learning approach, TCA, to make feature distributions in source and target projects similar. In addition, we propose a novel transfer defect learning approach, TCA+, by extending TCA. Our experimental results for eight open-source projects show that TCA+ significantly improves cross-project prediction performance.",cross-project defect prediction;transfer learning;empirical software engineering,"382, 391",,IEEE Conferences,2013 35th International Conference on Software Engineering (ICSE),IEEE
811,,Outsourcing Software Testing: A Case Study in the Oulu Area,I. Tervonen; A. Haapalahti; L. Harjumaa; J. Similä,2013,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6605910,10.1109/QSIC.2013.53,"Global software engineering increases coordination, communication, and control challenges in software development. The testing phase in this context is not a widely researched subject. In this paper, we study the outsourcing of software testing in the Oulu area, research the ways in which it is used, and determine the observable benefits and obstacles. The companies that participated in this study were found to use the outsourcing possibility of software testing with good efficiency and their testing process was considered to be mature. The most common benefits, in addition to the companies' cost savings, included the utilization of time zone differences for around-the-clock productivity, a closer proximity to the market, an improved record of communication and the tools that record the audit materials. The most commonly realized difficulties consisted of teamwork challenges, a disparate tool infrastructure, tool expense, and often-elevated coordination costs. We utilized in our study two matrices that consist in one dimension of the three distances, control, coordination, and communication, and in another dimension of four distances, temporal, geographical, socio-cultural and technical. The technical distance was our extension to the matrix that has been used as the basis for many other studies about global software development and outsourcing efforts. Our observations justify the extension of matrices with respect to the technical distance.",software testing;coding;observations;technical distance;coordination;communication;control,"65, 74",,IEEE Conferences,2013 13th International Conference on Quality Software,IEEE
812,,Beyond Data Mining,T. Menzies,2013,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6504887,10.1109/MS.2013.49,"Last century, it wasn't known if data miners could find structure within software projects. This century, we know better: data mining has been successfully applied to many different artifacts from software projects. So it's time to move on to ""What's next?"" In the author's view, ""discussion mining"" is the next great challenge for the predictive modeling community. These discussion miners know that while predictions and decisions are important, so too are the questions and insights generated on the way to those conclusions.",data mining;software engineering;predictive modeling,"92, 92",,IEEE Magazines,IEEE Software,IEEE
813,,Active learning and effort estimation: Finding the essential content of software effort estimation data,E. Kocaguneli; T. Menzies; J. Keung; D. Cok; R. Madachy,2013,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6392173,10.1109/TSE.2012.88,"Background: Do we always need complex methods for software effort estimation (SEE)? Aim: To characterize the essential content of SEE data, i.e., the least number of features and instances required to capture the information within SEE data. If the essential content is very small, then 1) the contained information must be very brief and 2) the value added of complex learning schemes must be minimal. Method: Our QUICK method computes the euclidean distance between rows (instances) and columns (features) of SEE data, then prunes synonyms (similar features) and outliers (distant instances), then assesses the reduced data by comparing predictions from 1) a simple learner using the reduced data and 2) a state-of-the-art learner (CART) using all data. Performance is measured using hold-out experiments and expressed in terms of mean and median MRE, MAR, PRED(25), MBRE, MIBRE, or MMER. Results: For 18 datasets, QUICK pruned 69 to 96 percent of the training data (median = 89 percent). K = 1 nearest neighbor predictions (in the reduced data) performed as well as CART's predictions (using all data). Conclusion: The essential content of some SEE datasets is very small. Complex estimation methods may be overelaborate for such datasets and can be simplified. We offer QUICK as an example of such a simpler SEE method.",Software cost estimation;active learning;analogy;k-NN,"1040, 1053",,IEEE Journals,IEEE Transactions on Software Engineering,IEEE
814,,Automated Discovery of Valid Test Strings from the Web Using Dynamic Regular Expressions Collation and Natural Language Processing,M. Shahbaz; P. McMinn; M. Stevenson,2012,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6319228,10.1109/QSIC.2012.15,"Classic approaches to test input generation -- such as dynamic symbolic execution and search-based testing -- are commonly driven by a test adequacy criterion such as branch coverage. However, there is no guarantee that these techniques will generate meaningful and realistic inputs, particularly in the case of string test data. Also, these techniques have trouble handling path conditions involving string operations that are inherently complex in nature. This paper presents a novel approach of finding valid values by collating suitable regular expressions dynamically that validate the format of the string values, such as an email address. The regular expressions are found using web searches that are driven by the identifiers appearing in the program, for example a string parameter called email Address. The identifier names are processed through natural language processing techniques to tailor the web queries. Once a regular expression has been found, a secondary web search is performed for strings matching the regular expression. An empirical study is performed on case studies involving String input validation code from 10 open source projects. Compared to other approaches, the precision of generating valid strings is significantly improved by employing regular expressions and natural language processing techniques.",test data generation;string inputs;valid inputs;web queries;regular expressions;natural language processing,"79, 88",,IEEE Conferences,2012 12th International Conference on Quality Software,IEEE
815,,Comparing User Performance on an iPad to a 17-inch BackPad,F. Rajabiyazdi; T. Gedeon,2012,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6245644,10.1109/CISIS.2012.215,"What will a truly large iPad be like? Will it have a touchscreen at the front, or will some other changes be forced by the sheer sizeof the device? We mocked up a working device using a 17-inch Macbook laptop screen. The device size was too large for us to comfortably hold with one hand while using the other hand for touch input, so we placed the touch pad at the back. Hence, wecall our device a BackPad. In the first experiment, we compared user performance with our 17-inch BackPad and a normal iPad in game and typing tasks. The results on the game completion time and score were similar, and users liked our large screen,while time but not spelling errors were different in the BackPad versus the iPad. For the second experiment, we compared the front touchscreen versus the back trackpad user performance on same sized devices. Similar results to the first experiment were found on game completing time and score.",Keywords-component; 17-inch screen;back touchpad;natural,"469, 474",,IEEE Conferences,"2012 Sixth International Conference on Complex, Intelligent, and Software Intensive Systems",IEEE
816,,Application of intelligent control based on neural networks in power system,Shengchun Yang; Lixin Yin,2011,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5982234,10.1109/ICSESS.2011.5982234,"Increasingly nonlinear dynamic loads have been connected into power systems; such as variable speed drives, robotic factories and power electronics loads. This adds to the complexity of load modeling. The increasing complexity of the modern power grid highlights the need for advanced modeling and control techniques for effective control of excitation and turbine systems. The crucial factors affecting the modern power systems today is voltage control and system stabilization during small and large disturbances. Simulation studies and real-time laboratory experimental studies carried out are described and the results show the successful control of the power system excitation and turbine systems with adaptive and optimal neurocontrol approaches.",excitation control;load modeling;reinforcement learning;turbine control;neural networks;stability analysis,"348, 351",,IEEE Conferences,2011 IEEE 2nd International Conference on Software Engineering and Service Science,IEEE
817,,A Simulation Study of Deep Belief Network Combined with the Self-Organizing Mechanism of Adaptive Resonance Theory,Y. Wu; H. J. Cai,2010,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5677265,10.1109/CISE.2010.5677265,"Computer simulation study of brain neuronal networks is an active academic field. Deep Belief Network (DBN) introduces an effective way of training deep neural networks and the Adaptive Resonance Theory (ART) puts forward a two-layer competitive network emulating human cognitive processes. In our study, we implement a DBN with the mechanism of ART which benefits from DBN's multi-layer structure and ART's self-organizing stable learning mechanism. Our preliminary results show that the optimal number of layers is relevant to the data learned. The correct reconstruction rate decreases slowly with respect to the volume of data stored.",,"1, 4",,IEEE Conferences,2010 International Conference on Computational Intelligence and Software Engineering,IEEE
818,,Web wrapper generation using tree alignment and transfer learning,Y. Xia; S. Zhang; H. Yu,2010,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5542885,,"This paper studies the web wrapper generation for web pages of forum, blog and news web sites. While more and more web pages are dynamically generated using a common template populated with data from databases. This paper proposes a novel method that uses tree alignment and transfer learning method to generate the wrapper from this kind of web pages. We present a new tree alignment algorithm to find the best matching structure of the input web pages. A kind of linear regression method is employed to get the weight of different tag-matching. Based on the alignment, we merge the trees into one union tree whose nodes record the statistical information gotten from multiple web pages. We use a transfer learning method to find the most likely content block and use the alignment algorithm to detect the repeat patterns on the union tree. After that, we generate a wrapper to extract data from web pages. Experimental results show that the method can achieve high extraction accuracy and has steady performance.",wrapper;tree;alignment (key words),"410, 415",,IEEE Conferences,The 2nd International Conference on Software Engineering and Data Mining,IEEE
819,,"A Novel Tracking and Recognition Algorithm Using ""Continuous Autoencoder"" Network",Y. Zhao; Z. Hu,2009,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5366549,10.1109/CISE.2009.5366549,"Motion trajectory is one of the most important cues for tracking and behavior recognition and can be widely applied to numerous fields such as visual surveillance and guidance. However, it is a difficult problem to directly model the spatiotemporal variations of trajectories due to their high dimensionality and nonlinearity. In this paper, a novel tracking and trajectory recognition algorithm is proposed by combining a bi-directional deep neural network called ""Continuous Autoencoder"" into a probabilistic framework. First, the ""Continuous Autoencoder"" network embeds high-dimensional trajectories in a two-dimensional plane based on a peculiar training rule and learns a trajectory generative model by its inverse mapping. Then a set of plausible trajectories are generated by the trajectory generative model. In the tracking process, the target state at each time step is estimated by combining above plausible trajectory set with particle filter. The trajectory identity is inferred by evaluating the improved Hausdorff distance between the estimated trajectory up to now and the truncated reference trajectories. Furthermore, the trajectory recognition results can provide valuable information for the next tracking. The experiments on tracking and recognizing handwritten digits show that the proposed algorithm can not only robustly track and exactly recognize in background clutter and occlusion, but also realize the track before identification.",,"1, 5",,IEEE Conferences,2009 International Conference on Computational Intelligence and Software Engineering,IEEE
820,,Fault Diagnosis of Gas Blower Based on Genetic Fuzzy Neural Network,H. Fangxia; L. Jie; C. Xinglong,2009,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5319705,10.1109/WCSE.2009.217,"In order to make full use of the capability of GA's global searching and BP network's local searching, a genetic fuzzy neural network model is proposed. And the way of fault characteristic parameters' fuzzy processing and optimizing the weights and thresholds of ANN by GA are studied. As a result, the convergence speed and convergence precision are greatly increased. Application to the fault diagnosis of a gas blower system shows that the new model overcomes the low learning rate and local minimum of BP algorithm and the fault diagnosis precision is effectively improved.",gas blower;fuzzy processing;neural network;genetic algorithm;fault diagnosis,"77, 81",,IEEE Conferences,2009 WRI World Congress on Software Engineering,IEEE
821,,An Automatic Compiler Optimizations Selection Framework for Embedded Applications,S. Hung; C. Tu; H. Lin; C. Chen,2009,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5066672,10.1109/ICESS.2009.86,"Optimizing compilers provide users with compiler options to maximize program performance. The selection of compiler options is important as the resulted performance can vary significantly. The best combination of compiler options is not only dependent on the program itself, but it also is highly related to the configuration of the system and the architecture of the processor that the program runs on. The determination of the best combination of compiler options is very complicated, as its complexity grows exponentially with the number of the optimization options the compiler offers. Many previous work attempts to shorten the search time by reducing the complexity of the problem. However, most of them focus on computational intensive applications, which run with little or no invocation of kernel functions and device input/output activities, which often dominate system performance in specific embedded environment, such as network appliance. This paper aims at system-wide compiler optimizations selection for embedded applications. We proposed an automated framework to judiciously select the compiler options not only for the control software in the user space but also for the associated kernel functions which perform the I/O operations for an embedded application. For this framework, we implemented compiler optimization selection algorithms and evaluated its efficiencies with and without performance monitoring hardware support. We argue that our framework is a platform-independent and system-level compiler options selection framework. Our experience in optimizing the performance of the embedded application on a production storage appliance show that an I/O-intensive application composed by various kernel modules device drivers under Linux can be optimized effectively and systematically.",Performance optimization;compiler options selection;embedded application,"381, 387",,IEEE Conferences,2009 International Conference on Embedded Software and Systems,IEEE
822,,Pair programming as a teaching tool: a student review of empirical studies,P. Brereton; M. Turner; R. Kaur,2009,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4812704,10.1109/CSEET.2009.11,"The aims of this research were to investigate the applicability of the systematic literature review (SLR) process within the constraints of a 13-week masterpsilas level project and to aggregate evidence about the effectiveness of pair programming for teaching introductory programming. It was found that, with certain modifications to the process, it was possible to undertake an SLR within a limited time period and to produce valid results. Based on pre-defined inclusion and exclusion criteria, the student found 28 publications reporting empirical studies of pair programming, of which nine publications were used for data extraction and analysis. Results of the review indicates that whilst pair programming has little effect on the marks obtained for examinations and assignments, it can significantly improve the pass and retention rates and the studentspsila confidence and enjoyment of programming. Following the student study, experienced reviewers re-applied the inclusion and exclusion criteria to the 28 publications and carried out data extraction and synthesis using the resulting papers. A comparison of the studentpsilas results and those of the experienced reviewers is presented.",Systematic literature review;Pair programming;Student project,"240, 247",,IEEE Conferences,2009 22nd Conference on Software Engineering Education and Training,IEEE
823,,Information Integration Architecture of Risk Management for RoHS,D. Gong; W. Hsiao; P. Ho,2008,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4722104,10.1109/CSSE.2008.1375,"The idea of green product overthrows the traditional product deign and production concepts and procedures. Moreover, there many directives and regulations are announced to strengthen the development trend of green product. To help enterprise to deal with the upcoming threats from these regulations and directives, this research proposes architecture of RoHS risk management to automate information gathering, system integration, and the risk evaluation. With evaluated risk value and following improvement suggestions, enterprise can have better view of RoHS risk and can better control the risk to avoid violating RoHS directives so as to gain more competitive edges.",Risk Management;Information Architecture;RoHS,"512, 515",,IEEE Conferences,2008 International Conference on Computer Science and Software Engineering,IEEE
824,,Bayesian Task-Level Transfer Learning for Non-linear Regression,P. Yang; Q. Tan; Y. Ding,2008,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4721692,10.1109/CSSE.2008.1612,"Multi-task learning utilizes labeled data from other ldquosimilarrdquo tasks and can achieve efficient knowledge-sharing between tasks. Previous research mainly focused on multi-task learning for linear regression. In this paper, a novel Bayesian multi-task learning model for non-linear regression, i.e. HiRBF, is proposed. HiRBF is constructed under a hierarchical Bayesian framework. In the model all tasks are combined in a single RBF network. The input-to-hidden weights are shared between tasks, and the hidden-to-output weights are assumed to be sampled randomly from a certain prior distribution. The HiRBF algorithm is compared with two transfer-unaware approaches. The experiments demonstrate that HiRBF significantly outperforms the others.",transfer learning;Bayesian hierarchical model;regression;RBF network,"62, 65",,IEEE Conferences,2008 International Conference on Computer Science and Software Engineering,IEEE
825,,Hand Gesture Detection and Segmentation Based on Difference Background Image with Complex Background,Q. Zhang; F. Chen; X. Liu,2008,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4595579,10.1109/ICESS.2008.23,"The video streams of backgrounds are frequently influenced by the background changes (e.g. illumination changes and changes due to adding or removing parts of the background). Further more, the quality of the foreground and the segmented image of hand gesture severely drops. We propose a novel method, which is based on difference background image between consecutive video frames, of using the '3<sup>sigma</sup> -principle' of normal distribution for hand gesture detection to cope with the problem. The adaptive method of automatic threshold selection based on the method of maximal between-class variance is proposed for hand gesture segmentation to select optimal threshold. Experimentations show that the better images are obtained with complex background, no matter if the proportion of the hand gesture is high or low. Several experimental images are presented to support the validity of the method.",Complex backgrounds;Difference background image;Hand gesture segmentation;Maximal between-class variance,"338, 343",,IEEE Conferences,2008 International Conference on Embedded Software and Systems,IEEE
826,,Perceived Effects of Pair Programming in an Industrial Context,J. Vanhanen; C. L. Lassenius,2007,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4301082,10.1109/EUROMICRO.2007.47,"We studied the perceived effects of pair programming (PP) compared to solo programming in a large scale, industrial software development context. We surveyed developers (N=28) regarding effects of PP on learning, quality, effort, schedule, and human factors. Our findings support earlier results from studies done with students, or professionals doing small tasks. The positive effects of PP were largest for learning, schedule adherence of tasks, getting to know other developers, and team spirit. A small but clearly positive effect was perceived for various quality aspects, discipline in following work practices, and enjoyment of work. The improvement of estimation accuracy was almost negligible. The amount of refactoring did not change. On the negative side, the development effort for individual features was higher. In the beginning of the adoption, the exhaustiveness of work was perceived higher, but over time it decreased to the level of solo programming.",,"211, 218",,IEEE Conferences,33rd EUROMICRO Conference on Software Engineering and Advanced Applications (EUROMICRO 2007),IEEE
827,,Issues and Tactics when Adopting Pair Programming: A Longitudinal Case Study,J. Vanhanen; C. Lassenius; M. V. Mantyla,2007,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4299950,10.1109/ICSEA.2007.48,"We present experiences from a two-year study of adopting pair programming (PP) in a Finnish software product company. When adopting PP, the company used five tactics: the creation of simple PP guidelines, the use of a PP champion, making the use of PP voluntary, creating a positive atmosphere for PP, and instituting a separate PP room. By the end of the study the feelings of PP considerably surpassed developers' preconceptions of PP, and even the feelings of solo programming. Issues identified in the infrastructure for PP were solved through the adoption of the PP room. In the end of the study, a majority of the developers thought that PP should be utilized more than the reached ca. 10% of development effort. Unresolved issues in resourcing PP probably hindered reaching the desired level for the use of PP.",,"70, 70",,IEEE Conferences,International Conference on Software Engineering Advances (ICSEA 2007),IEEE
828,,Message from the Chairs,,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9319301,10.1145/3417113.3423372,"It is our great pleasure to welcome you to the first “virtual” IEEE/ACM International Conference on Automated Software Engineering! The ASE conference series is the premier research forum for automated software engineering research and practice. Each year it brings together researchers and practitioners from academia and industry to discuss foundations, techniques, and tools for automated analysis, design, implementation, testing, and maintenance of software systems.",,"1, 5",,IEEE Conferences,2020 35th IEEE/ACM International Conference on Automated Software Engineering Workshops (ASEW),IEEE
829,,Message from the Chairs,,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9285666,,Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.,,"xxii, xxii",,IEEE Conferences,2020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
830,,Correction of “A Comparative Study to Benchmark Cross-Project Defect Prediction Approaches”,S. Herbold; A. Trautsch; J. Grabowski,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8248781,10.1109/TSE.2018.2790413,"Unfortunately, the article “A Comparative Study to Benchmark Cross-project Defect Prediction Approaches” has a problem in the statistical analysis which was pointed out almost immediately after the pre-print of the article appeared online. While the problem does not negate the contribution of the the article and all key findings remain the same, it does alter some rankings of approaches used in the study. Within this correction, we will explain the problem, how we resolved it, and present the updated results.",Cross-project defect prediction;benchmark;comparison;replication;correction,"632, 636",,IEEE Journals,IEEE Transactions on Software Engineering,IEEE
831,,Message from the PROMISE 2013 Chairs,B. Turhan; S. Wagner; A. Bener; M. D. Penta; Y. Yang,2013,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6681384,10.1109/ESEM.2013.7,"PROMISE conference is an annual forum for researchers and practitioners to present, discuss and exchange ideas, results, expertise and experiences in construction and/or application of prediction models in software engineering. Such models could be targeted at: planning, design, implementation, testing, maintenance, quality assurance, evaluation, process improvement, management, decision making, and risk assessment in software and systems development. PROMISE is distinguished from similar forums with its public data repository and focus on methodological details, providing a unique interdisciplinary venue for software engineering and machine learning communities, and seeking for verifiable and repeatable prediction models that are useful in practice.",predictive models; defect prediction; effort estimation; software analytics; empirical software engineering,"394, 394",,IEEE Conferences,2013 ACM / IEEE International Symposium on Empirical Software Engineering and Measurement,IEEE
832,,Table of Contents,,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9463592,10.1109/ICCSN52437.2021.9463592,Presents the table of contents/splash page of the proceedings record.,,"iii, viii",,IEEE Conferences,2021 13th International Conference on Communication Software and Networks (ICCSN),IEEE
833,,Table of Contents,,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9426018,10.1109/SANER50967.2021.00004,Presents the table of contents/splash page of the proceedings record.,,"5, 14",,IEEE Conferences,"2021 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)",IEEE
834,,Table of Contents,,2021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9404143,10.1109/ICICSE52190.2021.9404143,Presents the table of contents/splash page of the proceedings record.,,"iii, vii",,IEEE Conferences,2021 IEEE International Conference on Information Communication and Software Engineering (ICICSE),IEEE
835,,Table of contents,,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9307699,10.1109/ISSREW51248.2020.00004,Presents the table of contents/splash page of the proceedings record.,,"4, 13",,IEEE Conferences,2020 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW),IEEE
836,,Table of Contents,,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9286070,,Presents the table of contents/splash page of the proceedings record.,,"v, xxi",,IEEE Conferences,2020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE),IEEE
837,,Table of Contents,,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283970,,Presents the table of contents/splash page of the proceedings record.,,"v, xxiii",,IEEE Conferences,2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE),IEEE
838,,Table of Contents,,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9251088,10.1109/ISSRE5003.2020.00004,Presents the table of contents/splash page of the proceedings record.,,"5, 10",,IEEE Conferences,2020 IEEE 31st International Symposium on Software Reliability Engineering (ISSRE),IEEE
839,,Table of contents,,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9237648,10.1109/ICSESS49938.2020.9237648,Presents the table of contents/splash page of the proceedings record.,,"1, 8",,IEEE Conferences,2020 IEEE 11th International Conference on Software Engineering and Service Science (ICSESS),IEEE
840,,Table of contents,,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9240628,10.1109/ICSME46990.2020.00004,Presents the table of contents/splash page of the proceedings record.,,"5, 16",,IEEE Conferences,2020 IEEE International Conference on Software Maintenance and Evolution (ICSME),IEEE
841,,Table of Contents,,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9202809,10.1109/COMPSAC48688.2020.00004,Presents the table of contents/splash page of the proceedings record.,,"5, 36",,IEEE Conferences,"2020 IEEE 44th Annual Computers, Software, and Applications Conference (COMPSAC)",IEEE
842,,2019 Index IEEE Transactions on Software Engineering Vol. 45,,2020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952733,10.1109/TSE.2019.2960169,"This index covers all technical items - papers, correspondence, reviews, etc. - that appeared in this periodical during the year, and items from previous years that were commented upon or corrected in this year. Departments and other items may also be covered if they have been judged to have archival value. The Author Index contains the primary entry for each item, listed under the first author's name. The primary entry includes the co-authors' names, the title of the paper or other item, and its location, specified by the publication abbreviation, year, month, and inclusive pagination. The Subject Index contains entries describing the item under all appropriate subject headings, plus the first author's name, the publication abbreviation, month, and year, and inclusive pages. Note that the item title is found only under the primary entry in the Author Index.",,"1, 9",,IEEE Journals,IEEE Transactions on Software Engineering,IEEE
843,,UBMYK 2019 Table of Contents,,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8965452,10.1109/UBMYK48245.2019.8965452,Presents the table of contents/splash page of the proceedings record.,,"x, xviii",,IEEE Conferences,2019 1st International Informatics and Software Engineering Conference (UBMYK),IEEE
844,,Table of contents,,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8854696,10.1109/QRS.2019.00004,Presents the table of contents/splash page of the proceedings record.,,"5, 10",,IEEE Conferences,"2019 IEEE 19th International Conference on Software Quality, Reliability and Security (QRS)",IEEE
845,,Table of Contents,,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8754469,10.1109/COMPSAC.2019.00004,Presents the table of contents/splash page of the proceedings record.,,"5, 23",,IEEE Conferences,2019 IEEE 43rd Annual Computer Software and Applications Conference (COMPSAC),IEEE
846,,Table of contents,,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8477197,10.1109/SERA.2018.8477197,Presents the table of contents/splash page of the proceedings record.,,"1, 2",,IEEE Conferences,"2018 IEEE 16th International Conference on Software Engineering Research, Management and Applications (SERA)",IEEE
847,,JCSSE 2018 TOC,,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8457370,10.1109/JCSSE.2018.8457370,,,"i, x",,IEEE Conferences,2018 15th International Joint Conference on Computer Science and Software Engineering (JCSSE),IEEE
848,,Table of Contents - Volume 2,,2018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8377812,10.1109/COMPSAC.2018.10186,Presents the table of contents/splash page of the proceedings record.,,"4, 23",,IEEE Conferences,2018 IEEE 42nd Annual Computer Software and Applications Conference (COMPSAC),IEEE
849,,Table of contents,,2016,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7586575,10.1109/ICCSN.2016.7586575,Presents the table of contents/splash page of the xxxxxxconf proceedings record.,,"iii, xiii",,IEEE Conferences,2016 8th IEEE International Conference on Communication Software and Networks (ICCSN),IEEE
