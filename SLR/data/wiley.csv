,id,title,author,year,publisher,issn,url,doi,abstract,keyword,pages,num_pages,publication_venue
0,@article: https://doi.org/10.1002/stvr.1658,Heterogeneous fault prediction with cost-sensitive domain adaptation,"Li, Zhiqiang and Jing, Xiao-Yuan and Zhu, Xiaoke",201,,,https://onlinelibrary.wiley.com/doi/abs/10.1002/stvr.1658,https://doi.org/10.1002/stvr.1658,"Summary In the early phases of software testing, projects may have only limited historical defect data. Learning prediction model with such insufficient training data will limit the efficacy of learned predictor. In practice, there are usually many publicly available fault prediction datasets. Recently, heterogeneous fault prediction (HFP) has been proposed. However, existing HFP models do not investigate how to use mixed project data to predict target. Furthermore, defect data are often imbalanced. The imbalanced data distribution of source usually leads to serious misclassification of fault-prone instances, which will degrade the predictor's performance. Existing HFP methods do not consider the class imbalance problem in the training stages. In this paper, we propose a novel Cost-sensitive Label and Structure-consistent Unilateral Projection (CLSUP) approach for HFP. CLSUP can not only make better use of the within-project and cross-project data but also alleviate the class imbalance problem by setting different misclassification costs for fault-prone and non–fault-prone instances. Extensive experiments on 30 projects demonstrate the effectiveness of CLSUP.","cost-sensitive learning, class imbalance, heterogeneous domain adaptation, heterogeneous fault prediction, mixed project, software quality assurance",e1658,,"Software Testing, Verification and Reliability"
1,@article: https://doi.org/10.1049/sfw2.12053,A cross-project defect prediction method based on multi-adaptation and nuclear norm,"Huang, Qingan and Ma, Le and Jiang, Siyu and Wu, Guobin and Song, Hengjie and Jiang, Libiao and Zheng, Chunyun",,,,https://onlinelibrary.wiley.com/doi/abs/10.1049/sfw2.12053,https://doi.org/10.1049/sfw2.12053,"Abstract Cross-project defect prediction (CPDP) is an important research direction in software defect prediction. Traditional CPDP methods based on hand-crafted features ignore the semantic information in the source code. Existing CPDP methods based on the deep learning model may not fully consider the differences among projects. Additionally, these methods may not accurately classify the samples near the classification boundary. To solve these problems, the authors propose a model based on multi-adaptation and nuclear norm (MANN) to deal with samples in projects. The feature of samples were embedded into the multi-core Hilbert space for distribution and the multi-kernel maximum mean discrepancy method was utilised to reduce differences among projects. More importantly, the nuclear norm module was constructed, which improved the discriminability and diversity of the target sample by calculating and maximizing the nuclear norm of the target sample in the process of domain adaptation, thus improving the performance of MANN. Finally, extensive experiments were conducted on 11 sizeable open-source projects. The results indicate that the proposed method exceeds the state of the art under the widely used metrics","neural nets, software quality, software reliability, unsupervised learning",,,IET Software
2,@article: https://doi.org/10.1049/sfw2.12052,An empirical study on the effectiveness of data resampling approaches for cross-project software defect prediction,"Bennin, Kwabena Ebo and Tahir, Amjed and MacDonell, Stephen G. and Börstler, Jürgen",,,,https://onlinelibrary.wiley.com/doi/abs/10.1049/sfw2.12052,https://doi.org/10.1049/sfw2.12052,"Abstract Cross-project defect prediction (CPDP), where data from different software projects are used to predict defects, has been proposed as a way to provide data for software projects that lack historical data. Evaluations of CPDP models using the Nearest Neighbour (NN) Filter approach have shown promising results in recent studies. A key challenge with defect-prediction datasets is class imbalance, that is, highly skewed datasets where non-buggy modules dominate the buggy modules. In the past, data resampling approaches have been applied to within-projects defect prediction models to help alleviate the negative effects of class imbalance in the datasets. To address the class imbalance issue in CPDP, the authors assess the impact of data resampling approaches on CPDP models after the NN Filter is applied. The impact on prediction performance of five oversampling approaches (MAHAKIL, SMOTE, Borderline-SMOTE, Random Oversampling and ADASYN) and three undersampling approaches (Random Undersampling, Tomek Links and One-sided selection) is investigated and results are compared to approaches without data resampling. The authors examined six defect prediction models on 34 datasets extracted from the PROMISE repository. The authors' results show that there is a significant positive effect of data resampling on CPDP performance, suggesting that software quality teams and researchers should consider applying data resampling approaches for improved recall (pd) and g-measure prediction performance. However, if the goal is to improve precision and reduce false alarm (pf) then data resampling approaches should be avoided","class imbalance, defect prediction, software metrics, software quality",,,IET Software
3,@article: https://doi.org/10.1049/sfw2.12028,Systematic review of question answering over knowledge bases,"Pereira, Arnaldo and Trifan, Alina and Lopes, Rui Pedro and Oliveira, José Luís",202,,,https://onlinelibrary.wiley.com/doi/abs/10.1049/sfw2.12028,https://doi.org/10.1049/sfw2.12028,"Abstract Over the years, a growing number of semantic data repositories have been made available on the web. However, this has created new challenges in exploiting these resources efficiently. Querying services require knowledge beyond the typical user’s expertise, which is a critical issue in adopting semantic information solutions. Several proposals to overcome this difficulty have suggested using question answering (QA) systems to provide user-friendly interfaces and allow natural language use. Because question answering over knowledge bases (KBQAs) is a very active research topic, a comprehensive view of the field is essential. The purpose of this study was to conduct a systematic review of methods and systems for KBQAs to identify their main advantages and limitations. The inclusion criteria rationale was English full-text articles published since 2015 on methods and systems for KBQAs. Sixty-six articles were reviewed to describe their underlying reference architectures.",,1-13,,IET Software
4,@article: https://doi.org/10.1049/sfw2.12029,Software defect prediction based on stacked sparse denoising autoencoders and enhanced extreme learning machine,"Zhang, Nana and Ying, Shi and Zhu, Kun and Zhu, Dandan",202,,,https://onlinelibrary.wiley.com/doi/abs/10.1049/sfw2.12029,https://doi.org/10.1049/sfw2.12029,"Abstract Software defect prediction is an important software quality assurance technique. Nevertheless, the prediction performance of the constructed model is easily susceptible to irrelevant or redundant features in the software projects and is not predominant enough. To address these two issues, a novel defect prediction model called SSEPG based on Stacked Sparse Denoising AutoEncoders (SSDAE) and Extreme Learning Maching (ELM) optimised by Particle Swarm Optimisation (PSO) and another complementary Gravitational Search Algorithm (GSA) are proposed in this paper, which has two main merits: (1) employ a novel deep neural network – SSDAE to extract new combined features, which can effectively learn the robust deep semantic feature representation. (2) integrate strong exploitation capacity of PSO with strong exploration capability of GSA to optimise the input weights and hidden layer biases of ELM, and utilise the superior discriminability of the enhanced ELM to predict the defective modules. The SSDAE is compared with eleven state-of-the-art feature extraction methods in effect and efficiency, and the SSEPG model is compared with multiple baseline models that contain five classic defect predictors and three variants across 24 software defect projects. The experimental results exhibit the superiority of the SSDAE and the SSEPG on six evaluation metrics.",,29-47,,IET Software
5,@article: https://doi.org/10.1049/sfw2.12017,Improving source code suggestion with code embedding and enhanced convolutional long short-term memory,"Hussain, Yasir and Huang, Zhiqiu and Zhou, Yu",202,,,https://onlinelibrary.wiley.com/doi/abs/10.1049/sfw2.12017,https://doi.org/10.1049/sfw2.12017,"Abstract Source code suggestion is the utmost helpful feature in the integrated development environments that helps to quicken software development by suggesting the next possible source code tokens. The source code contains useful semantic information but is ignored or not utilised to its full potential by existing approaches. To improve the performance of source code suggestion, the authors propose a deep semantic net (DeepSN) that makes use of semantic information of the source code. First, DeepSN uses an enhanced hierarchical convolutional neural network combined with code-embedding to automatically extract the top-notch features of the source code and to learn useful semantic information. Next, the source code's long and short-term context dependencies are captured by using long short-term memory. We extensively evaluated the proposed approach with three baselines on ten real-world projects and the results are suggesting that the proposed approach surpasses state-of-the-art approaches. On average, DeepSN achieves 7.6\% higher accuracy than the best baseline.",,199-213,,IET Software
6,@article: https://doi.org/10.1049/iet-sen.2019.0389,Manifold embedded distribution adaptation for cross-project defect prediction,"Sun, Ying and Jing, Xiao-Yuan and Wu, Fei and Sun, Yanfei",202,,,https://onlinelibrary.wiley.com/doi/abs/10.1049/iet-sen.2019.0389,https://doi.org/10.1049/iet-sen.2019.0389,"Cross-project defect prediction (CPDP) technology refers to the constructing prediction model to predict the instance label of the target project by utilising labelled data from an external project. The challenge of CPDP methods is the distribution difference between the data from different projects. Transfer learning can transfer the knowledge from the source domain to the target domain with the aim to minimise the domain difference between different domains. However, most existing methods reduce the distribution discrepancy in the original feature space, where the features are high-dimensional and non-linear, which makes it hard to reduce the distribution distance between different projects. Moreover, previous works mainly consider marginal distribution or conditional distribution difference. In this study, the authors proposed a manifold embedded distribution adaptation (MDA) approach to narrow the distribution gap in manifold feature subspace. MDA maps source and target project data to manifold subspace and then joint distribution adaptation of conditional and marginal distributions is performed on manifold subspace. To evaluate the effectiveness of MDA, the authors perform extensive experiments on 20 public projects with three indicators. The experiment results show that MDA improves the average performance, but the improvement is not statistically significant in comparison to HYDRA (one of the baselines).","learning (artificial intelligence), program debugging, neural nets, cross-project defect prediction, CPDP methods, source domain, target domain, distribution discrepancy, distribution distance, transfer learning, public projects, joint distribution adaptation, manifold feature subspace, MDA, manifold embedded distribution adaptation approach, conditional distribution difference, marginal distribution difference",825-838,,IET Software
7,@article: https://doi.org/10.1049/iet-sen.2019.0378,Deep learning-based prototyping of android GUI from hand-drawn mockups,"Abdelhamid, Abdelaziz A. and Alotaibi, Sultan R. and Mousa, Abdelaziz",202,,,https://onlinelibrary.wiley.com/doi/abs/10.1049/iet-sen.2019.0378,https://doi.org/10.1049/iet-sen.2019.0378,"Recently, transforming graphical user interface (GUI) mockups into code becomes a common challenging practice for current software developers. However, this transformation usually takes time especially when GUI changes keep pace with evolutionary features. There are many studies admitted this challenge and presented solutions in terms of computer-based GUI mockups. However, there is a research gap in this kind of research as very few of them adopted hand-drawn mockups as an input. In this study, the authors employed YOLOv5 is a fast and accurate deep learning framework to automate the process of converting hand-drawn GUI mockups into Android-based GUI prototype. The process starts with detecting all GUI mockups in an input image and determining their bounding boxes, classifying these mockups into their corresponding GUI objects, then finally aligning these objects together to form the output prototype based on the layout presented in the input image. Experimental results show the effectiveness of the proposed approach in generating a visually appealing Android GUI from hand-drawn mockups with a recognition accuracy of 98.54\% when tested on various hand-drawn GUI structures designed by five developers.","graphical user interfaces, learning (artificial intelligence), Android (operating system), graphical user interface mockups, common challenging practice, current software developers, GUI changes, adopted hand-drawn mockups, fast learning framework, accurate deep learning framework, hand-drawn GUI mockups, input image, corresponding GUI objects, output prototype, hand-drawn GUI structures, deep learning-based prototyping, computer-based GUI mockups, Android-based GUI prototype",816-824,,IET Software
8,@article: https://doi.org/10.1049/sfw2.12012,Correlation feature and instance weights transfer learning for cross project software defect prediction,"Zou, Quanyi and Lu, Lu and Qiu, Shaojian and Gu, Xiaowei and Cai, Ziyi",202,,,https://onlinelibrary.wiley.com/doi/abs/10.1049/sfw2.12012,https://doi.org/10.1049/sfw2.12012,"Abstract Due to the differentiation between training and testing data in the feature space, cross-project defect prediction (CPDP) remains unaddressed within the field of traditional machine learning. Recently, transfer learning has become a research hot-spot for building classifiers in the target domain using the data from the related source domains. To implement better CPDP models, recent studies focus on either feature transferring or instance transferring to weaken the impact of irrelevant cross-project data. Instead, this work proposes a dual weighting mechanism to aid the learning process, considering both feature transferring and instance transferring. In our method, a local data gravitation between source and target domains determines instance weight, while features that are highly correlated with the learning task, uncorrelated with other features and minimizing the difference between the domains are rewarded with a higher feature weight. Experiments on 25 real-world datasets indicate that the proposed approach outperforms the existing CPDP methods in most cases. By assigning weights based on the different contribution of features and instances to the predictor, the proposed approach is able to build a better CPDP model and demonstrates substantial improvements over the state-of-the-art CPDP models.",,55-74,,IET Software
9,@article: https://doi.org/10.1049/iet-sen.2020.0084,Literature survey of deep learning-based vulnerability analysis on source code,"Semasaba, Abubakar Omari Abdallah and Zheng, Wei and Wu, Xiaoxue and Agyemang, Samuel Akwasi",202,,,https://onlinelibrary.wiley.com/doi/abs/10.1049/iet-sen.2020.0084,https://doi.org/10.1049/iet-sen.2020.0084,"Vulnerabilities in software source code are one of the critical issues in the realm of software code auditing. Due to their high impact, several approaches have been studied in the past few years to mitigate the damages from such vulnerabilities. Among the approaches, deep learning has gained popularity throughout the years to address such issues. In this literature survey, the authors provide an extensive review of the many works in the field software vulnerability analysis that utilise deep learning-based techniques. The reviewed works are systemised according to their objectives (i.e. the type of vulnerability analysis aspect), the area of focus (i.e. the focus area of the analysis), what information about source code is used (i.e. the features), and what deep learning techniques they employ (i.e. what algorithm is used to process the input and produce the output). They also study the limitations of the papers and topical trends concerning vulnerability analysis.","security of data, learning (artificial intelligence), neural nets, source code (software), auditing, software engineering, deep learning-based vulnerability analysis, software source code, software code auditing, software vulnerability analysis, vulnerability analysis aspect",654-664,,IET Software
10,@article: https://doi.org/10.1049/iet-sen.2018.5334,Software development effort estimation: a systematic mapping study,"Eduardo Carbonera, Carlos and Farias, Kleinner and Bischoff, Vinicius",202,,,https://onlinelibrary.wiley.com/doi/abs/10.1049/iet-sen.2018.5334,https://doi.org/10.1049/iet-sen.2018.5334,"The field of software-development effort estimation explores ways of defining effort through prediction approaches. Even though this field has a crucial impact on budgeting and project planning in industry, the number of works classifying and examining currently available approaches is still small. This article, therefore, presents a comprehensive overview of these approaches, and pinpoints research gaps, challenges and trends. A systematic mapping of the literature was designed and performed based on well-established practical guidelines. In total, 120 primary studies were selected, analysed and categorised, after applying a careful filtering process from a sample of 3746 candidate studies to answer six research questions. Over 70\% of the selected studies adopted multiple effort estimation approaches; over 45\% adopted evaluation research as a research method; over 90\% of the participants were students, rather than professionals; most studies had their quality assessed as high, and were most commonly published in journals. Our study benefits practitioners and researchers by providing a body of knowledge about the current literature, serving as a starting point for upcoming studies. This article reports challenges worth investigating, regarding the use of cognitive load and team interaction.","software development management, software quality, project management, software cost estimation, planning, software development effort estimation, systematic mapping study, prediction approaches, multiple effort estimation approaches",328-344,,IET Software
11,@article: https://doi.org/10.1049/iet-sen.2019.0045,Image recognition in UAV videos using convolutional neural networks,"Quiñonez, Yadira and Lizarraga, Carmen and Peraza, Juan and Zatarain, Oscar",202,,,https://onlinelibrary.wiley.com/doi/abs/10.1049/iet-sen.2019.0045,https://doi.org/10.1049/iet-sen.2019.0045,"In recent years, unmanned aerial vehicles (UAVs) have been used in different areas of applications such as rescue operations, surveillance, agriculture, aerial mapping, engineering applications and research, among others, in order to perform tasks with greater efficiency. This work focuses on the use of UAVs in the fishing sector in order to optimise the detection process of a shoal of fish. In this sense, the main idea is to perform images recognition using the images acquired through videos captured by UAV in the open sea; to achieve the objective the convolutional neural networks were used, a new dataset with different images captured through UAV videos in the open sea were taken into account, these classes correspond to dolphin, dolphin\_pod, open\_sea, and seabirds. The training tests were by transfer of learning using the following models: Inception V3, MobileNet V2, and NASNet-A (large) trained on TensorFlow platform. The experimental results show the detection performance with high-precision values in reasonable processing time. This study ends with a critical discussion of the experimental results.","learning (artificial intelligence), autonomous aerial vehicles, surveillance, image sensors, image recognition, remotely operated vehicles, convolutional neural nets, robot vision, UAV videos, convolutional neural networks, unmanned aerial vehicles, rescue operations, aerial mapping, engineering applications, fishing sector, detection process, images recognition, open sea, detection performance, Inception V3, MobileNet V2, NASNet-A, TensorFlow platform",176-181,,IET Software
12,@article: https://doi.org/10.1049/iet-sen.2020.0093,Guest Editorial: Software Engineering Applications to Solve Organisations Issues,,202,,,https://onlinelibrary.wiley.com/doi/abs/10.1049/iet-sen.2020.0093,https://doi.org/10.1049/iet-sen.2020.0093,,,73-74,,IET Software
13,@article: https://doi.org/10.1049/iet-sen.2018.5131,Joint distribution matching model for distribution–adaptation-based cross-project defect prediction,"Qiu, Shaojian and Lu, Lu and Jiang, Siyu",201,,,https://onlinelibrary.wiley.com/doi/abs/10.1049/iet-sen.2018.5131,https://doi.org/10.1049/iet-sen.2018.5131,"Using classification methods to predict software defect is receiving a great deal of attention and most of the existing studies primarily conduct prediction under the within-project setting. However, there usually had no or very limited labelled data to train an effective prediction model at an early phase of the software lifecycle. Thus, cross-project defect prediction (CPDP) is proposed as an alternative solution, which is learning a defect predictor for a target project by using labelled data from a source project. Differing from previous CPDP methods that mainly apply instances selection and classifiers adjustment to improve the performance, in this study, the authors put forward a novel distribution–adaptation-based CPDP approach, joint distribution matching (JDM). Specifically, JDM aims to minimise the joint distribution divergence between the source and target project to improve the CPDP performance. By constructing an adaptive weight vector for the instances of the source project, JDM can be effective and robust at reducing marginal distribution discrepancy and conditional distribution discrepancy simultaneously. Extensive experiments verify that JDM can outperform related distribution–adaptation-based methods on 15 open-source projects that are derived from two types of repositories.","pattern classification, learning (artificial intelligence), vectors, joint distribution matching model, classification methods, software defect, within-project setting, labelled data, effective prediction model, software lifecycle, defect predictor, target project, source project, instances selection, classifiers adjustment, JDM, joint distribution divergence, CPDP performance, adaptive weight vector, marginal distribution discrepancy, conditional distribution discrepancy, distribution–adaptation-based methods, open-source projects, cross-project defect prediction",393-402,,IET Software
14,@article: https://doi.org/10.1049/iet-sen.2017.0111,Multiple-components weights model for cross-project software defect prediction,"Qiu, Shaojian and Lu, Lu and Jiang, Siyu",201,,,https://onlinelibrary.wiley.com/doi/abs/10.1049/iet-sen.2017.0111,https://doi.org/10.1049/iet-sen.2017.0111,"Software defect prediction (SDP) technology is receiving widely attention and most of SDP models are trained on data from the same project. However, at an early phase of the software lifecycle, there are little to no within-project training data to learn an available supervised defect-prediction model. Thus, cross-project defect prediction (CPDP), which is learning a defect predictor for a target project by using labelled data from a source project, has shown promising value in SDP. To better perform the CPDP, most current studies focus on filtering instances or selecting features to weaken the impact of irrelevant cross-project data. Instead, the authors propose a novel multiple-components weights (MCWs) learning model to analyse the varying auxiliary power of multiple components in a source project to construct a more precise ensemble classifiers for a target project. By combining the MCW model with kernel mean matching algorithm, their proposed approach adjusts the source-instance weights and source-component weights to jointly alleviate the negative impacts of irrelevant cross-project data. They conducted comprehensive experiments by employing 15 real-world datasets to demonstrate the advantages and effectiveness of their proposed approach.","learning (artificial intelligence), software quality, cross-project defect prediction, CPDP, MCW model, source-instance weights, source-component weights, cross-project software defect prediction, SDP models, software lifecycle, within-project training data, Multiple-components weights model, MCW learning model",345-355,,IET Software
15,@article: https://doi.org/10.1049/iet-sen.2017.0148,Progress on approaches to software defect prediction,"Li, Zhiqiang and Jing, Xiao-Yuan and Zhu, Xiaoke",201,,,https://onlinelibrary.wiley.com/doi/abs/10.1049/iet-sen.2017.0148,https://doi.org/10.1049/iet-sen.2017.0148,"Software defect prediction is one of the most popular research topics in software engineering. It aims to predict defect-prone software modules before defects are discovered, therefore it can be used to better prioritise software quality assurance effort. In recent years, especially for recent 3 years, many new defect prediction studies have been proposed. The goal of this study is to comprehensively review, analyse and discuss the state-of-the-art of defect prediction. The authors survey almost 70 representative defect prediction papers in recent years (January 2014–April 2017), most of which are published in the prominent software engineering journals and top conferences. The selected defect prediction papers are summarised to four aspects: machine learning-based prediction algorithms, manipulating the data, effort-aware prediction and empirical studies. The research community is still facing a number of challenges for building methods and many research opportunities exist. The identified challenges can give some practical guidelines for both software engineering researchers and practitioners in future software defect prediction.","software quality, software reliability, quality assurance, research and development, software defect prediction, software engineering journals, defect-prone software modules, software quality assurance, machine learning-based prediction algorithms, data manipulation, effort-aware prediction, empirical studies",161-175,,IET Software
16,@article: https://doi.org/10.1002/smr.2426,FlexParser—The adaptive log file parser for continuous results in a changing world,"Rücker, Nadine and Maier, Andreas",,,,https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2426,https://doi.org/10.1002/smr.2426,"Abstract Any modern system writes events into files, called log files. Those contain crucial information which are subject to various analyses. Examples range from cybersecurity, intrusion detection over usage analyses to trouble shooting. Before data analysis is possible, desired information needs to be extracted first out of the semi-structured log messages. State-of-the-art event parsing often assumes static log events. However, any modern system is updated consistently and with updates also log file structures can change. We call those changes “mutation” and study parsing performance for different mutation cases. Latest research discovers mutations using anomaly detection post mortem, however, does not cover actual continuous parsing. Thus, we propose a novel and flexible parser, called FlexParser, which can extract desired values despite gradual changes in the log messages. It implies basic text preprocessing followed by a supervised Deep Learning method. We train a stateful LSTM on parsing one event per data set. Statefulness enforces the model to learn log message structures across several examples. Our model was tested on seven different, publicly available log file data sets and various kinds of mutations. Exhibiting an average F1-Score of 0.98, it outperforms other Deep Learning methods as well as state-of-the-art unsupervised parsers","deep learning, flexible parsing, log parser, LSTM, system log",e2426,,Journal of Software: Evolution and Process
17,@article: https://doi.org/10.1002/smr.2422,An empirical evaluation of deep learning-based source code vulnerability detection: Representation versus models,"Semasaba, Abubakar Omari Abdallah and Zheng, Wei and Wu, Xiaoxue and Agyemang, Samuel Akwasi and Liu, Tao and Ge, Yuan",,,,https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2422,https://doi.org/10.1002/smr.2422,"Abstract Vulnerabilities in the source code of the software are critical issues in the realm of software engineering. Coping with vulnerabilities in software source code is becoming more challenging due to several aspects such as complexity and volume. Deep learning has gained popularity throughout the years as a means of addressing such issues. This paper proposes an evaluation of vulnerability detection performance on source code representations and evaluates how machine learning (ML) strategies can improve them. The structure of our experiment consists of three deep neural networks (DNNs) in conjunction with five different source code representations: abstract syntax trees (ASTs), code gadgets (CGs), semantics-based vulnerability candidates (SeVCs), lexed code representations (LCRs), and composite code representations (CCRs). Experimental results show that employing different ML strategies in conjunction with the base model structure influences the performance results to a varying degree. However, ML-based techniques suffer from poor performance on class imbalance handling and dimensionality reduction when used in conjunction with source code representations","deep learning, security, software vulnerability detection",e2422,,Journal of Software: Evolution and Process
18,@article: https://doi.org/10.1002/smr.2414,A graph sequence neural architecture for code completion with semantic structure features,"Yang, Kang and Yu, Huiqun and Fan, Guisheng and Yang, Xingguang and Huang, Zijie",202,,,https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2414,https://doi.org/10.1002/smr.2414,"Abstract Code completion plays an important role in intelligent software development for accelerating coding efficiency. Recently, the prediction models based on deep learning have achieved good performance in code completion task. However, the existing models cannot avoid three drawbacks: (i) In the existing models, the code representation loses the information (parent–child information between nodes) and lacks many effective features (orientation between nodes). (ii) The known code structure information is not fully utilized, which will cause the model to generate completely irrelevant results. (iii) Simple sequence modeling ignores repeated patterns and structural information. Besides, previous works cannot capture the characteristics of correlation and directionality between nodes. In this paper, we propose a Code Completion approach named CC-GGNN, which is graph model based on Gated Graph Neural Networks (GGNNs) to address the problems. We introduce a new architecture to obtain the effective code features from code representation. In order to utilize the known information, we propose Classification Mechanism, which classifies the representation of the node using the known parent node and constructs training graph in the model. The experimental results show that our model outperforms the state-of-the-art methods MRR@5 at most 9.2\% and ACC at most 11.4\% in datasets.","code completion, deep learning, program comprehension",e2414,,Journal of Software: Evolution and Process
19,@article: https://doi.org/10.1002/smr.2404,CrowdAssist: A multidimensional decision support system for crowd workers,"Abhinav, Kumar and Kaur Bhatia, Gurpriya and Dubey, Alpana and Jain, Sakshi and Bhardwaj, Nitish",,,,https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2404,https://doi.org/10.1002/smr.2404,"Abstract Lately, crowdsourcing has emerged as a viable option for getting work done by leveraging the collective intelligence of the crowd. With many tasks posted every day, the size of crowdsourcing platforms is growing exponentially. Hence, workers face an important challenge in selecting the right task. Despite the task filtering criteria available on the platform to select the right task, crowd workers find it difficult to choose the most relevant task and must glean through the filtered tasks to find the relevant tasks. In this paper, we propose a framework for recommending tasks to workers. The proposed framework evaluates the worker's fitment over the tasks based on the worker's preference, past tasks he/she has performed, and tasks done by similar workers. We also proposed an approach to estimate the right price for a crowdsourced task for a specific worker. We evaluated our approach on the datasets collected from popular crowdsourcing platforms. Our experimental results show that the recommendation made by our framework for task and price is significantly better as compared with the baseline approach","crowdsourcing, personalization, price estimation, recommendation, task selection",e2404,,Journal of Software: Evolution and Process
20,@article: https://doi.org/10.1002/smr.2403,MARS: Detecting brain class/method code smell based on metric–attention mechanism and residual network,"Zhang, Yang and Dong, Chunhao",,,,https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2403,https://doi.org/10.1002/smr.2403,"Abstract Code smell is the structural design defect that makes programs difficult to understand, maintain, and evolve. Existing works of code smell detection mainly focus on prevalent code smells, such as feature envy, god class, and long method. Few works have been done on detecting brain class/method. Furthermore, existing deep-learning-based approaches leverage the CNN model to improve accuracy by barely increasing the number of layers, which may cause a problem of gradient degradation. To this end, this paper proposes a novel approach called MARS to detect brain class/method. MARS improves the gradient degradation by employing an improved residual network. It increases the weight value of those important code metrics to label smelly samples by introducing a metric–attention mechanism. To support the training of MARS, a dataset called BrainCode is generated by extracting more than 270,000 samples from 20 real-world applications. MARS is evaluated on BrainCode and compared to other machine-learning-based and deep-learning-based approaches. The experimental results demonstrate that the average accuracy of MARS is 2.01 \% higher than that of the existing approaches, which improves state-of-the-art","brain class/method, code smell, deep learning, metric–attention mechanism, residual network",e2403,,Journal of Software: Evolution and Process
21,@article: https://doi.org/10.1002/smr.2402,A systematic mapping study on the employment of neural networks on software engineering projects: Where to go next?,"dos Santos, Rodrigo Augusto and Vieira, Darli and Bravo, Alencar and Suzuki, Larissa and Qudah, Fadiah",,,,https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2402,https://doi.org/10.1002/smr.2402,"Abstract Deep learning has recently experienced explosive growth in use, largely due to advances in neural networks and the availability of large corpora of domain data. Project management activities generate and handle large volumes of data. Software engineering closely relates to project management, so software engineering projects must be prone to the use of neural networks. We seek to obtain an accurate vision of how neural networks are being used in software engineering projects through a systematic mapping study. We confirm that neural networks have already made their way into these projects; however, we show that their current uses are limited to certain repetitive and legacy tasks. Given uncovered ample room for expansion, we point out a few directions the industry and academy can lean toward to in the next years for taking better advantage of neural networks in software engineering projects and immediately advancing the field. We investigate if, how, and to what extent have neural networks been employed to the advancement of software engineering projects. As such, a systematic mapping study was conducted, which led to the conclusion that even though these algorithms have indeed been employed on several software engineering tasks, this employment so far has been shy, mostly relying on legacy types of neural networks. More modern variants, namely, deep learning algorithms, are slowly gaining momentum and should be the trend going forward","deep learning, machine learning, neural networks, project management, software engineering",e2402,,Journal of Software: Evolution and Process
22,@article: https://doi.org/10.1002/smr.2381,Deployment of a change-level software defect prediction solution into an industrial setting,"Eken, Beyza and Tufan, Selda and Tunaboylu, Alper and Guler, Tevfik and Atar, Rifat and Tosun, Ayse",202,,,https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2381,https://doi.org/10.1002/smr.2381,"Abstract Applying change-level software defect prediction (SDP) in practice has several challenges regarding model validation techniques, data accuracy, and prediction performance consistency. A few studies report on these challenges in an industrial context. We share our experience in integrating an SDP into an industrial context. We investigate whether an “offline” SDP could reflect its “online” (real-life) performance, and other deployment decisions: the model re-training process and update period. We employ an online prediction strategy by considering the actual labels of training commits at the time of prediction and compare its performance against an offline prediction. We empirically assess the online SDP's performance with various lengths of the time gap between the train and test set and model update periods. Our online SDP's performance could successfully reach its offline performance. The time gap between the train and test commits, and model update period significantly impacts the online performance by 37\% and 18\% in terms of probability of detection (pd), respectively. We deploy the best SDP solution (73\% pd) with an 8-month time gap and a 3-day update period. Contextual factors may determine the model performance in practice, its consistency, and trustworthiness. As future work, we plan to investigate the reasons for fluctuations in model performance over time.","change-level defect prediction, deployment, industrial case study, online prediction",e2381,,Journal of Software: Evolution and Process
23,@article: https://doi.org/10.1002/smr.2371,Modeling and verifying NDN-based IoV using CSP,"Chen, Ningning and Zhu, Huibiao and Yin, Jiaqi and Fei, Yuan and Xiao, Lili and Zhu, Minghua",,,,https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2371,https://doi.org/10.1002/smr.2371,"Abstract As a crucial component of intelligent transportation system, Internet of Vehicles (IoV) plays an important role in the smart and intelligent cities. However, current Internet architectures cannot guarantee efficient data delivery and adequate data security for IoV. Therefore, Named Data Networking (NDN), a leading architecture of Information-Centric Networking (ICN), is introduced into IoV. Although problems about data distribution can be resolved effectively, the combination of NDN and IoV causes some new security issues. In this paper, we apply Communicating Sequential Processes (CSP) to formalize NDN-based IoV. We mainly focus on its data access mechanism and model this mechanism in detail. By feeding the formalized model into the model checker Process Analysis Toolkit (PAT), we verify four vital properties, namely, deadlock freedom, data reliability, PIT deletion faking, and CS caching pollution. According to verification results, the model cannot ensure the security of data with the appearance of intruders. To solve these problems, we construct a blockchain-based mechanism by creating a blockchain-based distribution trusted platform on top of NDN-based IoV. Through the analysis of the improved model, the blockchain-based mechanism can truly guarantee the security of NDN-based IoV","Named Data Networking (NDN), Internet of Vehicles (IoV), process algebra CSP, blockchain, modeling and verification",e2371,,Journal of Software: Evolution and Process
24,@article: https://doi.org/10.1002/smr.2369,image2emmet: Automatic code generation from web user interface image,"Xu, Yong and Bo, Lili and Sun, Xiaobing and Li, Bin and Jiang, Jing and Zhou, Wei",202,,,https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2369,https://doi.org/10.1002/smr.2369,"Abstract Web development usually follows with analyzing the functionality, designing the user interface (UI) prototype, implementing the UI by front-end (FE) developers and implementing the REpresentational State Transfer (RESTful) application programming interface (API) by back-end (BE) programmers. Unfortunately, web development is a tedious, cumbersome, and time-consuming task, which makes it a challenge for the FE programmers to work in an efficient way. In this paper, we propose an approach, image2emmet, to assist FE programmers in implementing the UI. First, we collect HyperText Markup Language, Cascading Style Sheets (HTML-CSS) dataset in an automatic and efficient way. The HTML-CSS dataset used for model training consists of HTML-CSS code and its display images. Second, the faster region-based convolutional neural network (CNN) (R-CNN) is utilized to detect the UI component. Finally, we build a model combining CNN and long short-term memory (LSTM) to transform the UI component into the HTML-CSS code. The empirical study demonstrates that image2emmet can achieve a precision of 80\% on the UI component detection and 60\% on the transformation of UI component into HTML-CSS code.","code generation, HTML-CSS code, UI component, web development",e2369,,Journal of Software: Evolution and Process
25,@article: https://doi.org/10.1002/smr.2333,Digital for real: A multicase study on the digital transformation of companies in the embedded systems domain,"Bosch, Jan and Olsson, Helena H.",202,,,https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2333,https://doi.org/10.1002/smr.2333,"Abstract With digitalization and with technologies such as software, data, and artificial intelligence, companies in the embedded systems domain are experiencing a rapid transformation of their conventional businesses. While the physical products and associated product sales provide the core revenue, these are increasingly being complemented with service offerings, new data-driven services, and digital products that allow for continuous value creation and delivery to customers. However, although there is significant research on digitalization and digital transformation, few studies highlight the specific needs of embedded systems companies and what it takes to transform from a traditional towards a digital company within business domains characterized by high complexity, hardware dependencies, and safety-critical system functionality. In this paper, we capture the difference between what constitutes a traditional and a digital company and we detail the typical evolution path embedded systems companies take when transitioning towards becoming digital companies.","business models, continuous value delivery, digital transformation, digitalization, embedded systems",e2333,,Journal of Software: Evolution and Process
26,@article: https://doi.org/10.1002/smr.2330,MPT-embedding: An unsupervised representation learning of code for software defect prediction,"Shi, Ke and Lu, Yang and Liu, Guangliang and Wei, Zhenchun and Chang, Jingfei",202,,,https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2330,https://doi.org/10.1002/smr.2330,"Abstract Software project defect prediction can help developers allocate debugging resources. Existing software defect prediction models are usually based on machine learning methods, especially deep learning. Deep learning-based methods tend to build end-to-end models that directly use source code-based abstract syntax trees (ASTs) as input. They do not pay enough attention to the front-end data representation. In this paper, we propose a new framework to represent source code called multiperspective tree embedding (MPT-embedding), which is an unsupervised representation learning method. MPT-embedding parses the nodes of ASTs from multiple perspectives and encodes the structural information of a tree into a vector sequence. Experiments on both cross-project defect prediction (CPDP) and within-project defect prediction (WPDP) show that, on average, MPT-embedding provides improvements over the state-of-the-art method.","deep learning, defect prediction, representation learning, tree embedding",e2330,,Journal of Software: Evolution and Process
27,@article: https://doi.org/10.1002/smr.2309,"Conceptualising, extracting and analysing requirements arguments in users' forums: The CrowdRE-Arg framework","Ali Khan, Javed and Liu, Lin and Wen, Lijie and Ali, Raian",202,,,https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2309,https://doi.org/10.1002/smr.2309,"Abstract Due to the pervasive use of online forums and social media, users' feedback are more accessible today and can be used within a requirements engineering context. However, such information is often fragmented, with multiple perspectives from multiple parties involved during on-going interactions. In this paper, the authors propose a Crowd-based Requirements Engineering approach by Argumentation (CrowdRE-Arg). The framework is based on the analysis of the textual conversations found in user forums, identification of features, issues and the arguments that are in favour or opposing a given requirements statement. The analysis is to generate an argumentation model of the involved user statements, retrieve the conflicting-viewpoints, reason about the winning-arguments and present that to systems analysts to make informed-requirements decisions. For this purpose, the authors adopted a bipolar argumentation framework and a coalition-based meta-argumentation framework as well as user voting techniques. The CrowdRE-Arg approach and its algorithms are illustrated through two sample conversations threads taken from the Reddit forum. Additionally, the authors devised algorithms that can identify conflict-free features or issues based on their supporting and attacking arguments. The authors tested these machine learning algorithms on a set of 3,051 user comments, preprocessed using the content analysis technique. The results show that the proposed algorithms correctly and efficiently identify conflict-free features and issues along with their winning arguments.","argumentation, machine learning, natural language processing, new features, requirements, user forum",e2309,,Journal of Software: Evolution and Process
28,@article: https://doi.org/10.1002/smr.2234,Do different cross-project defect prediction methods identify the same defective modules?,"Chen, Xiang and Mu, Yanzhou and Qu, Yubin and Ni, Chao and Liu, Meng and He, Tong and Liu, Shangqing",202,,,https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2234,https://doi.org/10.1002/smr.2234,"Abstract Cross-project defect prediction (CPDP) is needed when the target projects are new projects or the projects have less training data, since these projects do not have sufficient historical data to build high-quality prediction models. The researchers have proposed many CPDP methods, and previous studies have conducted extensive comparisons on the performance of different CPDP methods. However, to the best of our knowledge, it remains unclear whether different CPDP methods can identify the same defective modules, and this issue has not been thoroughly explored. In this article, we select 12 state-of-the-art CPDP methods, including eight supervised methods and four unsupervised methods. We first compare the performance of these methods in the same experiment settings on five widely used datasets (ie, NASA, SOFTLAB, PROMISE, AEEEM, and ReLink) and rank these methods via the Scott-Knott test. Final results confirm the competitiveness of unsupervised methods. Then we perform diversity analysis on defective modules for these methods by using the McNemar test. Empirical results verify that different CPDP methods may lead to difference in the modules predicted as defective, especially when the comparison is performed between the supervised methods and unsupervised methods. Finally, we also find there exist a certain number of defective modules, which cannot be correctly identified by any of the CPDP methods or can be correctly identified by only one CPDP method. These findings can be utilized to design more effective methods to further improve the performance of CPDP.","cross-project defect prediction, diversity analysis, empirical study, software defect prediction",e2234,,Journal of Software: Evolution and Process
29,@article: https://doi.org/10.1002/smr.2203,Multitask defect prediction,"Ni, Chao and Chen, Xiang and Xia, Xin and Gu, Qing and Zhao, Yingquan",201,,,https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2203,https://doi.org/10.1002/smr.2203,"Abstract Within-project defect prediction assumes that we have sufficient labeled data from the same project, while cross-project defect prediction assumes that we have plenty of labeled data from source projects. However, in practice, we might only have limited labeled data from both the source and target projects in some scenarios. In this paper, we want to apply multitask learning to investigate such a new scenario. To our best knowledge, this problem (ie, both the source project and the target project have limited labeled data) has not been thoroughly investigated, and we are the first to propose a novel multitask defect prediction approach mask. mask consists of a differential evolution optimization phase and a multitask learning phase. The former phase aims to find optimal weights for shared and nonshared information in related projects (ie, the target project and its related source projects), while the latter phase builds prediction models for each project simultaneously. To verify the effectiveness of mask, we perform experimental studies on 18 real-world software projects and compare our approach with four state-of-the-art baseline approaches: single-task learning (STL), simple combined learning (SCL), Peters filter, and Burak filter. Experimental results show that mask can achieve F1 of 0.397 and AUC of 0.608 on average with a few labeled data (ie, 10\% of data). Across the 18 projects, mask can outperform baseline methods significantly in terms of F1 and AUC. Therefore, by utilizing the relatedness among multiple projects, mask can perform significantly better than the state-of-the-art methods. The results confirm that mask is promising for software defect prediction when the source and target projects both have limited training data.","differential evolution, empirical studies, multitask learning, software defect prediction",e2203,,Journal of Software: Evolution and Process
30,@article: https://doi.org/10.1002/smr.2172,An improved transfer adaptive boosting approach for mixed-project defect prediction,"Gong, Lina and Jiang, Shujuan and Jiang, Li",201,,,https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2172,https://doi.org/10.1002/smr.2172,"Abstract Software defect prediction (SDP) has been a very important research topic in software engineering, since it can provide high-quality results when given sufficient historical data of the project. Unfortunately, there are not abundant data to bulid the defect prediction model at the beginning of a project. For this scenario, one possible solution is to use data from other projects in the same company. However, using these data practically would get poor performance because of different distributional characteristics among projects. Also, software has more non-defective instances than defective instances that may cause a significant bias towards defective instances. Considering these two problems, we propose an improved transfer adaptive boosting (ITrAdaBoost) approach for being given a small number of labeled data in the testing project. In our approach, ITrAdaBoost can not only employ the Matthews correlation coefficient (MCC) as the measure instead of accuracy rate but also use the asymmetric misclassification costs for non-defective and defective instances. Extensive experiments on 18 public projects from four datasets indicate that: (a) our approach significantly outperforms state-of-the-art cross-project defect prediction (CPDP) approaches, and (b) our approach can obtain comparable prediction performances in contrast with within project prediction results. Consequently, the proposed approach can build an effective prediction model with a small number of labeled instances for mixed-project defect prediction (MPDP).","class imbalance, cross-project, mixed-project, software defect prediction, transfer learning",e2172,,Journal of Software: Evolution and Process
31,@article: https://doi.org/10.1002/smr.2176,Investigating the use of duration-based windows and estimation by analogy for COCOMO,"Nguyen, Vu and Huynh, Thuy and Boehm, Barry and Huang, LiGuo and Truong, Thong",201,,,https://onlinelibrary.wiley.com/doi/abs/10.1002/smr.2176,https://doi.org/10.1002/smr.2176,"Abstract In model-based software estimation, using the right training data is a key contributor for making accurate predictions, which is crucial for the success of software projects. This study investigates the use of duration-based windows and estimation by analogy to calibrate COCOMO and assess their estimation performance. We compare these approaches as well as the use of all available historical data using the COCOMO data set of 341 projects and NASA data set of 93 projects. The results show that timing information exists in the data sets affecting estimation accuracy. Given sufficient data for calibration, using recently completed projects within short durations generates more accurate estimates than retaining all historical data or using k-nearest neighbors based on estimation by analogy. More training data spanning a long period of time may not lead to improved estimation accuracy. This study offers evidence to support the use of projects completed within recent years for training estimation models.","COCOMO, duration-based windows, estimation by analogy, k-nearest neighbors, moving windows, software estimation",e2176,,Journal of Software: Evolution and Process
32,@article: https://doi.org/10.1002/spe.2742,A crosswalk pedestrian recognition system by using deep learning and zebra-crossing recognition techniques,"Dow, Chyi-Ren and Ngo, Huu-Huy and Lee, Liang-Hsuan and Lai, Po-Yu and Wang, Kuan-Chieh and Bui, Van-Tung",202,,,https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.2742,https://doi.org/10.1002/spe.2742,"Summary Pedestrian detection is essential for improving pedestrian safety in an intelligent traffic system. The efficiency of the system is affected by real-time processing and the error rate of detection. These concerns have not been completely addressed in previous studies. Therefore, this study proposes a real-time pedestrian recognition system that ensures high accuracy by using a deep learning classifier and zebra-crossing recognition techniques. The proposed system was designed to improve pedestrian safety and reduce accidents at intersections. Environmental feature vectors were first used to detect zebra crossings and to determine crossing areas. An adaptive mapping technique was then used to map the pedestrian waiting area based on the crossing area. A dual camera mechanism was used to maintain detection accuracy and improve system fault tolerance. Finally, the you-only-look-once model was used to recognize pedestrians at intersections. A system prototype was implemented to verify the feasibility of the proposed system. The results revealed that the proposed scheme outperforms the conventional histogram of oriented gradients and Haarcascade schemes.","classifier, deep learning, pedestrian recognition, YOLO, zebra-crossing recognition",630-644,,Software: Practice and Experience
33,@article: https://doi.org/10.1002/spe.2687,Intelligent sentiment analysis approach using edge computing-based deep learning technique,"Sankar, H. and Subramaniyaswamy, V. and Vijayakumar, V. and Arun Kumar, Sangaiah and Logesh, R. and Umamakeswari, A.",202,,,https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.2687,https://doi.org/10.1002/spe.2687,"Summary Sentiment analysis and opinion mining has become a major tool for collecting information from customer reviews on user sentiments and emotions, especially for online video streaming services and social networks. The increasing use of smartphones has popularized subscription to various streaming services that provide streaming media and video-on-demand. These applications offer a gateway to analyze user reviews by introducing sentiment analysis in the mobile environment. Online user reviews can hold a lot of useful information and help predict user interests. Analysis of user reviews can provide substantive information for business processing. Sentiment classification of these reviews is a commonly used analysis technique. Usually, these reviews are given in a text format, with every word in each considered a feature, so selection should focus on optimal features from all available features present in the reviews. This study employs machine learning algorithms to extract the best features from the training review data set. Then, the selected features are fed into the convolutional neural network and other fully connected layers for further processing. The proposed approach is evaluated with the standard evaluation metrics, such as precision, accuracy, recall, and f-measure, using three distinct benchmark data sets: polarity, Rotten Tomatoes, and IMDb. This work has also employed a pretrained sentiment analysis model over an Android application framework to classify reviews on a Smartphone without the need for any cloud or server-side API.","convolutional neural network, deep learning, edge computing, mobile, sentiment analysis, word embeddings",645-657,,Software: Practice and Experience
34,@article: https://doi.org/10.1002/spe.2636,Cloud-based video analytics using convolutional neural networks,"Yaseen, Muhammad Usman and Anjum, Ashiq and Farid, Mohsen and Antonopoulos, Nick",201,,,https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.2636,https://doi.org/10.1002/spe.2636,"Summary Object classification is a vital part of any video analytics system, which could aid in complex applications such as object monitoring and management. Traditional video analytics systems work on shallow networks and are unable to harness the power of distributed processing for training and inference. We propose a cloud-based video analytics system based on an optimally tuned convolutional neural network to classify objects from video streams. The tuning of convolutional neural network is empowered by in-memory distributed computing. The object classification is performed by comparing the target object with the prestored trained patterns, generating a set of matching scores. The matching scores greater than an empirically determined threshold reveal the classification of the target object. The proposed system proved to be robust to classification errors with an accuracy and precision of 97\% and 96\%, respectively, and can be used as a general-purpose video analytics system.","cloud computing, convolutional neural networks, deep learning, hyperparameter tuning, video analytics",565-583,,Software: Practice and Experience
35,@article: https://doi.org/10.1002/spe.2586,Metropolitan intelligent surveillance systems for urban areas by harnessing IoT and edge computing paradigms,"Dautov, Rustem and Distefano, Salvatore and Bruneo, Dario and Longo, Francesco and Merlino, Giovanni and Puliafito, Antonio and Buyya, Rajkumar",201,,,https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.2586,https://doi.org/10.1002/spe.2586,"Summary Recent technological advances led to the rapid and uncontrolled proliferation of intelligent surveillance systems (ISSs), serving to supervise urban areas. Driven by pressing public safety and security requirements, modern cities are being transformed into tangled cyber-physical environments, consisting of numerous heterogeneous ISSs under different administrative domains with low or no capabilities for reuse and interaction. This isolated pattern renders itself unsustainable in city-wide scenarios that typically require to aggregate, manage, and process multiple video streams continuously generated by distributed ISS sources. A coordinated approach is therefore required to enable an interoperable ISS for metropolitan areas, facilitating technological sustainability to prevent network bandwidth saturation. To meet these requirements, this paper combines several approaches and technologies, namely the Internet of Things, cloud computing, edge computing and big data, into a common framework to enable a unified approach to implementing an ISS at an urban scale, thus paving the way for the metropolitan intelligent surveillance system (MISS). The proposed solution aims to push data management and processing tasks as close to data sources as possible, thus increasing performance and security levels that are usually critical to surveillance systems. To demonstrate the feasibility and the effectiveness of this approach, the paper presents a case study based on a distributed ISS scenario in a crowded urban area, implemented on clustered edge devices that are able to off-load tasks in a “horizontal” manner in the context of the developed MISS framework. As demonstrated by the initial experiments, the MISS prototype is able to obtain face recognition results 8 times faster compared with the traditional off-loading pattern, where processing tasks are pushed “vertically” to the cloud.","big data, cloud computing, distributed smart camera, edge computing, intelligent surveillance system, IoT, smart city, Stack4Things, stream processing",1475-1492,,Software: Practice and Experience
36,@article: https://doi.org/10.1002/spe.2426,Resource requests prediction in the cloud computing environment with a deep belief network,"Zhang, Weishan and Duan, Pengcheng and Yang, Laurence T and Xia, Feng and Li, Zhongwei and Lu, Qinghua and Gong, Wenjuan and Yang, Su",201,,,https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.2426,https://doi.org/10.1002/spe.2426,"Summary Accurate resource requests prediction is essential to achieve optimal job scheduling and load balancing for cloud Computing. Existing prediction approaches fall short in providing satisfactory accuracy because of high variances of cloud metrics. We propose a deep belief network (DBN)-based approach to predict cloud resource requests. We design a set of experiments to find the most influential factors for prediction accuracy and the best DBN parameter set to achieve optimal performance. The innovative points of the proposed approach is that it introduces analysis of variance and orthogonal experimental design techniques into the parameter learning of DBN. The proposed approach achieves high accuracy with mean square error of [10−6,10−5], approximately 72\% reduction compared with the traditional autoregressive integrated moving average predictor, and has better prediction accuracy compared with the state-of-art fractal modeling approach. Copyright © 2016 John Wiley \& Sons, Ltd.","deep belief network, prediction, cloud computing, resource request",473-488,,Software: Practice and Experience
37,@article: https://doi.org/10.1002/spe.3035,A computational approach for progressive architecture shrinkage in action recognition,"Tomei, Matteo and Baraldi, Lorenzo and Fiameni, Giuseppe and Bronzin, Simone and Cucchiara, Rita",202,,,https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.3035,https://doi.org/10.1002/spe.3035,"Abstract Efficiency plays a key role in video understanding modeling, and developing more efficient spatiotemporal deep networks is a key ingredient for enabling their usage in production scenarios. In this work, we propose a methodology for reducing the computational complexity of a video understanding backbone while limiting the drop in accuracy caused by architectural changes. Our approach, named, Progressive Architecture Shrinkage, applies a sequence of reduction operators to the hyperparameters of a network to reduce its computational footprint. The choice of the sequence of operations is automatically optimized in a coordinate-descent schema, and the approach transfers knowledge from both the initial network and previous stages of the shrinking process by employing a Knowledge Distillation and an adaptive fine-tuning strategy. As each iteration of the shrinking algorithm requires to train a large-scale video understanding network, we perform experiments on MARCONI 100—a supercomputer equipped with an IBM Power9 architecture and Volta NVIDIA GPUs. Experimental evaluations are conducted using two backbones and three different action recognition benchmarks. We show that, through our approach, high accuracy levels can be maintained while reducing the number of multiply–adds operations by four times with respect to the original architectures. Code will be made available.","architectural optimization, distributed training, video understanding",537-554,,Software: Practice and Experience
38,@article: https://doi.org/10.1002/spe.3033,Proactive content caching in edge computing environment: A review,"Aghazadeh, Rafat and Shahidinejad, Ali and Ghobaei-Arani, Mostafa",,,,https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.3033,https://doi.org/10.1002/spe.3033,"Abstract Edge computing environment provides processing capability and computing at the network edge and close to users. Edge equipment includes small data centers that locally perform process and content delivery. Therefore, edge equipment management has received much attention due to the rapid growth of information and resources limitations. The content caching and proactive caching techniques are management methods of edge equipment resources. We recently witnessed the development of proactive caching mechanisms that have a crucial enabler in improving heavy traffic, energy, and bandwidth. Also, it has huge potential to increase the quick response time to users' requests that today, these services are demanded by many users and applications. This article prepares a systematic literature review content caching approach in the edge computing environment. The purpose of this study is to survey the research done on the proactive caching strategies in the edge computing environment to identify subjects that must be emphasized more in current and future research paths. This research has studied 71 articles divided into three classes: model-based, machine-learning-based, and heuristic-based. Next, we discuss content caching approaches based on critical factors such as performance metrics, case studies, utilized techniques, assessment tools, advantages, and disadvantages. Finally, open issues and challenges are presented, and the survey is concluded","edge caching, edge computing, heuristic-based, machine learning, model-based, proactive caching, systematic review",,,Software: Practice and Experience
39,@article: https://doi.org/10.1002/spe.3015,A method to acquire cross-domain requirements based on Syntax Direct Technique,"Liu, Huaxiao and Zhang, Mengxi and Liu, Lei and Liu, Zhou",202,,,https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.3015,https://doi.org/10.1002/spe.3015,"Abstract With the rapid increase in the number of Apps, the requirement of users has also become extremely complex. Developers have to continuously acquire innovative requirements that provide the guideline for developing more competitive products. However, traditional methods to acquire requirements are not suitable for the App development due to the disadvantage that it cannot interact with users directly. Besides, some methods that use text and data analysis to acquire requirements automatically are hard to expand innovative products because they are often confined to the specific App or the same domain. Therefore, to attract more new users, developers try to find new portable inspiration from other domains for enriching the functions of the App. In this article, we propose a feature extraction method from the descriptions of Apps and use similarity matching to acquire cross-domain requirements. Our experiments have verified that the Precision, the Recall, and the F-measure are all as high as 80\% of our feature extraction method. Besides, the requirements list we recommend also makes a good performance in terms of reusability with the average Reuse Rank of 59.33\% and average Adjusted Functional Points of 7.49, the adaptability gets an average score of 3.3, and the average score of operability is 3.","cross-domain, feature extraction, requirement acquisition, Syntax Direct Technique",236-253,,Software: Practice and Experience
40,@article: https://doi.org/10.1002/spe.2987,Ajalon: Simplifying the authoring of wearable cognitive assistants,"Pham, Truong An and Wang, Junjue and Iyengar, Roger and Xiao, Yu and Pillai, Padmanabhan and Klatzky, Roberta and Satyanarayanan, Mahadev",202,,,https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.2987,https://doi.org/10.1002/spe.2987,"Summary Wearable Cognitive Assistance (WCA) amplifies human cognition in real time through a wearable device and low-latency wireless access to edge computing infrastructure. It is inspired by, and broadens, the metaphor of GPS navigation tools that provide real-time step-by-step guidance, with prompt error detection and correction. WCA applications are likely to be transformative in education, health care, industrial troubleshooting, manufacturing, assisted driving, and sports training. Today, WCA application development is difficult and slow, requiring skills in areas such as machine learning and computer vision that are not widespread among software developers. This paper describes Ajalon, an authoring toolchain for WCA applications that reduces the skill and effort needed at each step of the development pipeline. Our evaluation shows that Ajalon significantly reduces the effort needed to create new WCA applications.","artificial intelligence, augmented reality, cloudlets, computer vision, edge computing, Gabriel, machine learning, mobile computing, software productivity, wearables",1773-1797,,Software: Practice and Experience
41,@article: https://doi.org/10.1002/spe.2965,Cybersecurity protection on in-vehicle networks for distributed automotive cyber-physical systems: State-of-the-art and future challenges,"Xie, Yong and Zhou, Yu and Xu, Jing and Zhou, Jian and Chen, Xiaobai and Xiao, Fu",202,,,https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.2965,https://doi.org/10.1002/spe.2965,"Abstract The ever-evolving trip mode of human being leads the automobiles moving toward connected, autonomous, sharing, and electrified vehicles rapidly. But the connection introduces new cybersecurity problems on in-vehicle networks, which poses great challenges for safety guarantee of distributed automotive cyber-physical systems. This article first analyzes the cybersecurity vulnerabilities and defines the security requirements for in-vehicle networks, and then introduces the architecture evolution of in-vehicle network. Based on the definition on architecture of in-vehicle networks, this article defines a security protection framework for it. And then, it surveys the state-of-the-art works for availability protection, integrity protection, and confidentiality protection of in-vehicle networks, respectively, and detailed analysis and comparisons are given about the proposed cybersecurity protection mechanisms. Finally, it summarizes the future challenges for cybersecurity protection of in-vehicle networks, and proposes possible solutions for these challenges.","controller area network, cybersecurity protection, distributed automotive cyber-physical systems, in-vehicle networks, system design and optimization",2108-2127,,Software: Practice and Experience
42,@article: https://doi.org/10.1002/spe.2937,Deep domain adversarial residual neural network for sustainable wind turbine cyber-physical system fault diagnosis,"Jin, Yanrui and Feng, Qiang and Zhang, Xiping and Lu, Peili and Shen, Jiaqi and Tu, Yihui and Wu, Zhiquan",202,,,https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.2937,https://doi.org/10.1002/spe.2937,"Abstract As a popular renewable energy generation technology, wind turbine system has become a critical enabler for building the sustainable cyber-physical system (CPS). The main shaft bearing is an important part of the wind turbine CPS and often runs under variable working conditions. Thus, the reliable bearing diagnosis method can timely discover the main shaft bearing fault, which reduces the maintenance cost of wind turbines. Inspired by the idea of domain adaptation, we combined domain adversarial neural network and residual network and proposed a novel deep domain adversarial residual neural network (DDA-RNN) for diagnosing bearing fault and improving model performance on the unlabeled dataset. This proposed software and hardware co-design method was evaluated by our bearing dataset, which was collected from two wind turbine CPSs from Sanmenxia in Henan Province. Besides, F1 score and accuracy are served as model metrics, which reflect the diagnosis performance. Compared with other methods, the experimental results show that DDA-RNN can improve model performance. Meanwhile, DDA-RNN extracts diagnosis knowledge from labeled dataset and improves the model performance on the unlabeled dataset under different working condition. Therefore, the proposed method can be potentially used to benefit many practical scenarios in the future.","bearing fault diagnosis, domain adversarial learning, residual block, sustainable wind turbine cyber-physical system",2128-2142,,Software: Practice and Experience
43,@article: https://doi.org/10.1002/spe.2964,An unmanned aerial vehicles navigation system on the basis of pattern recognition applications—Review of implementation options and prospects for development,"Al Said, Nidal and Gorbachev, Yuri and Avdeenko, Aleksei",202,,,https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.2964,https://doi.org/10.1002/spe.2964,"Abstract The popularity of unmanned aerial vehicles (UAVs) kept growing over the past few decades. UAVs are widely used in various fields for monitoring, mapping, aerial photography, rescue operations, and so forth. However, the UAV navigation may be difficult in areas where global positioning systems is not available. This article presents an analytical review of the promising methods of creating a navigation system based on a pattern recognition algorithm. Linear algebra and mathematical morphology to describe the operation of pattern recognition applications in UAV's navigation systems, classification of images through profound research of intelligent navigation systems, analysis and digital image processing to identify options for the implementation of artificial intelligent systems of visual navigation were used to identify existing pattern recognition applications in the process of constructive systematization. It was shown that the UAV navigation requires objects in the image to be recognized and the distance to a possible obstacle to be estimated using deep learning method for image segmentation and depth map estimation. An obligatory initial point in the image recognition algorithm is the image preprocessing procedures based on machine vision techniques. The results of this study may be useful in developing efficient UAV applications for military and civil purposes.","navigation, object model, pattern recognition, unmanned aerial vehicle",1509-1517,,Software: Practice and Experience
44,@article: https://doi.org/10.1002/spe.2856,Issue Information,,202,,,https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.2856,https://doi.org/10.1002/spe.2856,,,485-486,,Software: Practice and Experience
45,@article: https://doi.org/10.1002/spe.2955,Modified deep belief network based human emotion recognition with multiscale features from video sequences,"Sreenivas, Velagapudi and Namdeo, Varsha and Vijay Kumar, Eda",202,,,https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.2955,https://doi.org/10.1002/spe.2955,"Summary Emotion recognition from human faces are recently considered as growing topic for the applications in HCI (human–computer interaction) field. Therefore, a new framework is introduced in this method for emotion recognition from video. Human faces may carry huge features which increase the complexity of recognizing the emotions from the give video. Therefore, to minimize such defect, the wrapper based feature selection technique is introduced which reduce the complexity of proposed recognition framework. Initially, the frames from the input video is preprocessed. Next, the features exhibited by each emotions are extracted with geometric and local binary pattern-based feature extraction methods. Then, the features that reduce the performance of recognition technique is avoided using a feature selection algorithm. It selects the features that provides effective result on recognition process. Finally, the selected features are provided to deep belief network (DBN) for emotion recognition. The weight parameter selection of DBN is improved using an efficient Harris Hawk optimization algorithm. The performance of presented architecture is evaluated using a three different datasets they are FAMED, CK+, and MMI. The overall rate shown by proposed architecture is found better than existing methods. Furthermore, the precision, recall, and specificity are also evaluated for six different emotions (angry, disgust, fear, happy, sad, and surprise) in this proposed method. This entire emotion recognition process is implemented in Python platform.","adaptive butterfly optimization, bilateral filter, deep belief network, emotion recognition, geometric features, local binary pattern",1259-1279,,Software: Practice and Experience
46,@article: https://doi.org/10.1002/spe.2952,"Semantic eSystems: Engineering methods, techniques, and tools","Baker, Thar and Al-Jumeily, Dhiya and Maamar, Zakaria and Tari, Zahir",202,,,https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.2952,https://doi.org/10.1002/spe.2952,,,487-488,,Software: Practice and Experience
47,@article: https://doi.org/10.1002/spe.2933,Leveraging machine learning for software redocumentation—A comprehensive comparison of methods in practice,"Geist, Verena and Moser, Michael and Pichler, Josef and Santos, Rodolfo and Wieser, Volkmar",202,,,https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.2933,https://doi.org/10.1002/spe.2933,"Abstract Source code comments contain key information about the underlying software system. Many redocumentation approaches, however, cannot exploit this valuable source of information. This is mainly due to the fact that not all comments have the same goals and target audience and can therefore only be used selectively for redocumentation. Performing a required classification manually, for example, in the form of heuristics, is usually time-consuming and error-prone and strongly dependent on programming languages and guidelines of concrete software systems. By leveraging machine learning (ML), it should be possible to classify comments and thus transfer valuable information from the source code into documentation with less effort but the same quality. We applied classical ML techniques but also deep learning (DL) approaches to legacy systems by transferring source code comments into meaningful representations using, for example, word embeddings but also novel approaches using quick response codes or a special character-to-image encoding. The results were compared with industry-strength heuristic classification. As a result, we found that ML outperforms the heuristics in number of errors and less effort, that is, we finally achieve an accuracy of more than 95\% for an image-based DL network and even over 96\% for a traditional approach using a random forest classifier.","comment classification, heuristics, legacy systems, machine learning, QR code, software system documentation",798-823,,Software: Practice and Experience
48,@article: https://doi.org/10.1002/spe.2894,Towards efficient federated learning-based scheme in medical cyber-physical systems for distributed data,"Guo, Kehua and Li, Nan and Kang, Jian and Zhang, Jian",202,,,https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.2894,https://doi.org/10.1002/spe.2894,"Summary In recent years, Cyber-Physical Systems (CPS) and Artificial Intelligence (AI) have made good progress in the medical field. The medical CPS (MCPS) based on AI can realize the efficient and reasonable utilization of medical resources and improve the quality of medical process. However, current MCPS are still facing several challenges, and the privacy protection of medical data is one of the most critical challenges. Since medical data is stored in different hospitals, most studies collect data from decentralized hospitals to train a disease diagnosis model, which is not conducive to the privacy protection of patients. And in some existing solutions, it is also difficult for doctors to select the optimal model from multiple models in clinical diagnosis. In this paper, we propose a novel scheme based on federated learning in MCPS for training disease diagnosis models from distributed medical image data. Our scheme is divided into three parts: the model provider, the server, and the consumer, and a detailed working process is designed for each part. This scheme can not only effectively solve the problem of privacy protection, but also solve the problem of model selection for doctors and save storage space. It can ensure that consumers automatically get a steadily improved disease diagnosis model. This scheme is performed on simulated distributed medical image datasets. The experimental results show the effectiveness and superiority of our scheme.","computer-aided diagnosis, distributed data, federated learning, medical cyber-physical systems, model fusion",2274-2289,,Software: Practice and Experience
49,@article: https://doi.org/10.1002/spe.2884,Author classification using transfer learning and predicting stars in co-author networks,"Abbasi, Rashid and Kashif Bashir, Ali and Chen, Jianwen and Mateen, Abdul and Piran, Jalil and Amin, Farhan and Luo, Bin",202,,,https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.2884,https://doi.org/10.1002/spe.2884,"Summary The vast amount of data is key challenge to mine a new scholar that is plausible to be star in the upcoming period. The enormous amount of unstructured data raise every year is infeasible for traditional learning; consequently, we need a high quality of preprocessing technique to expand the performance of traditional learning. We have persuaded a novel approach, Authors classification algorithm using Transfer Learning (ACTL) to learn new task on target area to mine the external knowledge from the source domain. Comprehensive experimental outcomes on real-world networks showed that ACTL, Node-based Influence Predicting Stars, Corresponding Authors Mutual Influence based on Predicting Stars, and Specific Topic Domain-based Predicting Stars enhanced the node classification accuracy as well as predicting rising stars to compared with contemporary baseline methods.","author classification, semantic web, social network, transfer learning",645-669,,Software: Practice and Experience
50,@article: https://doi.org/10.1002/spe.2893,Effective approaches to combining lexical and syntactical information for code summarization,"Zhou, Ziyi and Yu, Huiqun and Fan, Guisheng",202,,,https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.2893,https://doi.org/10.1002/spe.2893,"Summary Natural language summaries of source codes are important during software development and maintenance. Recently, deep learning based models have achieved good performance on the task of automatic code summarization, which encode token sequence or abstract syntax tree (AST) of code with neural networks. However, there has been little work on the efficient combination of lexical and syntactical information of code for better summarization quality. In this paper, we propose two general and effective approaches to leveraging both types of information: a convolutional neural network that aims to better extract vector representation of AST node for downstream models; and a Switch Network that learns an adaptive weight vector to combine different code representations for summary generation. We integrate these approaches into a comprehensive code summarization model, which includes a sequential encoder for token sequence of code and a tree based encoder for its AST. We evaluate our model on a large Java dataset. The experimental results show that our model outperforms several state-of-the-art models on various metrics, and the proposed approaches contribute a lot to the improvements.","code summarization, deep learning, program comprehension",2313-2336,,Software: Practice and Experience
51,@article: https://doi.org/10.1002/spe.2878,R2D2: A scalable deep learning toolkit for medical imaging segmentation,"Guedria, Soulaimane and De Palma, Noël and Renard, Félix and Vuillerme, Nicolas",202,,,https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.2878,https://doi.org/10.1002/spe.2878,"Summary Deep learning has gained a significant popularity in recent years thanks to its tremendous success across a wide range of relevant fields of applications, including medical image analysis domain in particular. Although convolutional neural networks (CNNs) based medical applications have been providing powerful solutions and revolutionizing medicine, efficiently training of CNNs models is a tedious and challenging task. It is a computationally intensive process taking long time and rare system resources, which represents a significant hindrance to scientific research progress. In order to address this challenge, we propose in this article, R2D2, a scalable intuitive deep learning toolkit for medical imaging semantic segmentation. To the best of our knowledge, the present work is the first that aims to tackle this issue by offering a novel distributed versions of two well-known and widely used CNN segmentation architectures [ie, fully convolutional network (FCN) and U-Net]. We introduce the design and the core building blocks of R2D2. We further present and analyze its experimental evaluation results on two different concrete medical imaging segmentation use cases. R2D2 achieves up to 17.5× and 10.4× speedup than single-node based training of U-Net and FCN, respectively, with a negligible, though still unexpected segmentation accuracy loss. R2D2 offers not only an empirical evidence and investigates in-depth the latest published works but also it facilitates and significantly reduces the effort required by researchers to quickly prototype and easily discover cutting-edge CNN configurations and architectures.","deep learning, distributed optimization, distributed systems, high-performance computing, medical imaging, semantic segmentation, software engineering",1966-1985,,Software: Practice and Experience
52,@article: https://doi.org/10.1002/spe.2851,Spatiotemporal-based sentiment analysis on tweets for risk assessment of event using deep learning approach,"Parimala, M. and Swarna Priya, R. M. and Praveen Kumar Reddy, M. and Lal Chowdhary, Chiranji and Kumar Poluru, Ravi and Khan, Suleman",202,,,https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.2851,https://doi.org/10.1002/spe.2851,"Summary Social media plays a vital role in analyzing the actual emotions of people after and during a disaster. Sentiment analysis is a method to detect a pattern from the emotions and feedback of the user. The main objective of the proposed work is to perform sentiment analysis on the tweets on a specific disaster context for a particular location at different intervals of time. LSTM network with word embedding algorithm is used to derive keywords based on the history of tweets and the context of the tweets. The proposed algorithm risk assessment sentiment analysis (RASA) uses the keywords generated from the network to classify the tweets and sentiment score for each location is identified. The model is validated with various state-of-art algorithms, namely, support vector machine, Naive-Bayes, maximum entropy, logistic regression, random forest, XGBoost, stochastic gradient descent, and convolution neural networks in 2-fold scenario: one for binary class and the other multiclass with three target classes. The results infer that the proposed RASA performs better in a binary class scenario with an increase of 1\% when compared with XGBoost and 30\% in multiclass scenario on an average when compared with all the other techniques. The model helps the government to take preventive measures to manage the posteffect of the disaster event in a location.","deep learning, long short-term memory, random forest, risk assessment sentiment analysis, stochastic gradient descent, XGBoost",550-570,,Software: Practice and Experience
53,@article: https://doi.org/10.1002/spe.2816,Deployment of a cloud pipeline for real-time visual inspection using fast streaming high-definition images,"Srivastava, Aishwarya and Aggarwal, Siddhant and Apon, Amy and Duffy, Edward and Kennedy, Ken and Luckow, Andre and Posey, Brandon and Ziolkowski, Marcin",202,,,https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.2816,https://doi.org/10.1002/spe.2816,"Summary We investigate the challenges of building an end-to-end cloud pipeline for real-time intelligent visual inspection system for use in automotive manufacturing. Current methods of visual detection in automotive assembly are highly labor intensive, and thus prone to errors. An automated process is sought that can operate within the real-time constraints of the assembly line and can reduce errors. Components of the cloud pipeline include capture of a large set of high-definition images from a camera setup at the assembly location, transfer and storage of the images as needed, execution of object detection, and notification to a human operator when a fault is detected. The end-to-end execution must complete within a fixed time frame before the next car arrives in the assembly line. In this article, we report the design, development, and experimental evaluation of the tradeoffs of performance, accuracy, and scalability for a cloud system.","cloud, end-to-end pipeline, latency, real-time system, visual inspection",868-898,,Software: Practice and Experience
54,@article: https://doi.org/10.1002/spe.2813,Software tools and techniques for fog and edge computing,"Ranjan, Rajiv and Villari, Massimo and Shen, Haiying and Rana, Omer and Buyya, Rajkumar",202,,,https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.2813,https://doi.org/10.1002/spe.2813,,,473-475,,Software: Practice and Experience
55,@article: https://doi.org/10.1002/spe.2783,Deep locality-sensitive discriminative dictionary learning for semantic video analysis,"Benuwa, Ben-Bright and Ghansah, Benjamin and Ansah, Ernest K.",202,,,https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.2783,https://doi.org/10.1002/spe.2783,"Summary Video semantic analysis (VSA) has received significant attention in the area of Machine Learning for some time now, particularly video surveillance applications with sparse representation and dictionary learning. Studies have shown that the duo has significantly impacted on the classification performance of video detection analysis. In VSA, the locality structure of video semantic data containing more discriminative information is very essential for classification. However, there has been modest feat by the current SR-based approaches to fully utilize the discriminative information for high performance. Furthermore, similar coding outcomes are missing from current video features with the same video category. To handle these issues, we first propose an improved deep learning algorithm—locality deep convolutional neural network algorithm (LDCNN) to better extract salient features and obtain local information from semantic video. Second, we propose a novel DL method, called deep locality-sensitive discriminative dictionary learning (DLSDDL) for VSA. In the proposed DLSDDL, a discriminant loss function for the video category based on sparse coding of sparse coefficients is introduced into the structure of the locality-sensitive dictionary learning (LSDL) method. After solving the optimized dictionary, the sparse coefficients for the testing video feature samples are obtained, and then the classification result for video semantic is realized by reducing the error existing between the original and recreated samples. The experiment results show that the proposed DLSDDL technique considerably increases the efficiency of video semantic detection as against competing methods used in our experiment.","deep learning, dictionary learning, kernel density estimator, locality information, sparse representation, video semantic analysis",388-406,,Software: Practice and Experience
56,@article: https://doi.org/10.1002/spe.2778,A deep recurrent Q network towards self-adapting distributed microservice architecture,"Magableh, Basel and Almiani, Muder",202,,,https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.2778,https://doi.org/10.1002/spe.2778,"Summary One desired aspect of microservice architecture is the ability to self-adapt its own architecture and behavior in response to changes in the operational environment. To achieve the desired high levels of self-adaptability, this research implements distributed microservice architecture model running a swarm cluster, as informed by the Monitor, Analyze, Plan, and Execute over a shared Knowledge (MAPE-K) model. The proposed architecture employs multiadaptation agents supported by a centralized controller, which can observe the environment and execute a suitable adaptation action. The adaptation planning is managed by a deep recurrent Q-learning network (DRQN). It is argued that such integration between DRQN and Markov decision process (MDP) agents in a MAPE-K model offers distributed microservice architecture with self-adaptability and high levels of availability and scalability. Integrating DRQN into the adaptation process improves the effectiveness of the adaptation and reduces any adaptation risks, including resource overprovisioning and thrashing. The performance of DRQN is evaluated against deep Q-learning and policy gradient algorithms, including (1) a deep Q-learning network (DQN), (2) a dueling DQN (DDQN), (3) a policy gradient neural network, and (4) deep deterministic policy gradient. The DRQN implementation in this paper manages to outperform the aforementioned algorithms in terms of total reward, less adaptation time, lower error rates, plus faster convergence and training time. We strongly believe that DRQN is more suitable for driving the adaptation in distributed services-oriented architecture and offers better performance than other dynamic decision-making algorithms.","deep Q-learning networks, multiagent environment, policy approximation, Q-learning algorithms, recurrent Q-learning networks, reinforcement learning, self-adaptive architectures, service-oriented architecture",116-135,,Software: Practice and Experience
