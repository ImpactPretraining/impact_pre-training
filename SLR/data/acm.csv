,id,title,author,year,publisher,issn,url,doi,abstract,keyword,pages,num_pages,publication_venue
0,@inproceedings: 10.1145/3453483.3454056,Fast and Precise Certification of Transformers,"Bonaert, Gregory and Dimitrov, Dimitar I. and Baader, Maximilian and Vechev, Martin",2021,Association for Computing Machinery,,https://doi.org/10.1145/3453483.3454056,10.1145/3453483.3454056,"We present DeepT, a novel method for certifying Transformer networks based on abstract interpretation. The key idea behind DeepT is our new Multi-norm Zonotope abstract domain, an extension of the classical Zonotope designed to handle ℓ1 and ℓ2-norm bound perturbations. We introduce all Multi-norm Zonotope abstract transformers necessary to handle these complex networks, including the challenging softmax function and dot product. Our evaluation shows that DeepT can certify average robustness radii that are 28\texttimes{} larger than the state-of-the-art, while scaling favorably. Further, for the first time, we certify Transformers against synonym attacks on long sequences of words, where each word can be replaced by any synonym. DeepT achieves a high certification success rate on sequences of words where enumeration-based verification would take 2 to 3 orders of magnitude more time.","Abstract Interpretation, Adversarial attacks, Transformer Networks, Deep Learning, Robustness Certification",466–481,16,Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation
1,@inproceedings: 10.1109/ICSE-SEIP52600.2021.00019,Robustness of On-Device Models: Adversarial Attack to Deep Learning Models on Android Apps,"Huang, Yujin and Hu, Han and Chen, Chunyang",2021,IEEE Press,,https://doi.org/10.1109/ICSE-SEIP52600.2021.00019,10.1109/ICSE-SEIP52600.2021.00019,"Deep learning has shown its power in many applications, including object detection in images, natural-language understanding, and speech recognition. To make it more accessible to end users, many deep learning models are now embedded in mobile apps. Compared to offloading deep learning from smartphones to the cloud, performing machine learning on-device can help improve latency, connectivity, and power consumption. However, most deep learning models within Android apps can easily be obtained via mature reverse engineering, while the models' exposure may invite adversarial attacks. In this study, we propose a simple but effective approach to hacking deep learning models using adversarial attacks by identifying highly similar pre-trained models from TensorFlow Hub. All 10 real-world Android apps in the experiment are successfully attacked by our approach. Apart from the feasibility of the model attack, we also carry out an empirical study that investigates the characteristics of deep learning models used by hundreds of Android apps on Google Play. The results show that many of them are similar to each other and widely use fine-tuning techniques to pre-trained models on the Internet.","deep learning, mobile apps, adversarial attack, Android",101–110,10,Proceedings of the 43rd International Conference on Software Engineering: Software Engineering in Practice
2,@inbook: 10.1109/ICSE-Companion52605.2021.00137,"NLP for Requirements Engineering: Tasks, Techniques, Tools, and Technologies","Ferrari, Alessio and Zhao, Liping and Alhoshan, Waad",2021,IEEE Press,,https://doi.org/10.1109/ICSE-Companion52605.2021.00137,,"Requirements engineering (RE) is one of the most natural language-intensive fields within the software engineering area. Therefore, several works have been developed across the years to automate the analysis of natural language artifacts that are relevant for RE, including requirements documents, but also app reviews, privacy policies, and social media content related to software products. Furthermore, the recent diffusion of game-changing natural language processing (NLP) techniques and platforms has also boosted the interest of RE researchers. However, a reference framework to provide a holistic understanding of the field of NLP for RE is currently missing. Based on the results of a recent systematic mapping study, and stemming from a previous ICSE tutorial by one of the authors, this technical briefing gives an overview of NLP for RE tasks, available techniques, supporting tools and NLP technologies. It is oriented to both researchers and practitioners, and will gently guide the audience towards a clearer view of how NLP can empower RE, providing pointers to representative works and specialised tools.",,322–323,,Proceedings of the 43rd International Conference on Software Engineering: Companion Proceedings
3,@inbook: 10.1109/ICSE-Companion52605.2021.00061,ProMal: Precise Window Transition Graphs for Android via Synergy of Program Analysis and Machine Learning,"Liu, Changlin and Xiao, Xusheng",2021,IEEE Press,,https://doi.org/10.1109/ICSE-Companion52605.2021.00061,,"Mobile apps have been an integral part in our daily life. As these apps become more complex, it is critical to provide automated analysis techniques to ensure the correctness, security, and performance of these apps. A key component for these automated analysis techniques is to create a graphical user interface (GUI) model of an app, i.e., a window transition graph (WTG), that models windows and transitions among the windows. While existing work has provided both static and dynamic analysis to build the WTG for an app, the constructed WTG misses many transitions or contains many infeasible transitions due to the coverage issues of dynamic analysis and over-approximation of the static analysis. We propose ProMal, a ""tribrid"" analysis that synergistically combines static analysis, dynamic analysis, and machine learning to construct a precise WTG. Specifically, ProMal first applies static analysis to build a static WTG, and then applies dynamic analysis to verify the transitions in the static WTG. For the unverified transitions, ProMal further provides machine learning techniques that leverage runtime information (i.e., screenshots, UI layouts, and text information) to predict whether they are feasible transitions. Our evaluations on 40 real-world apps demonstrate the superiority of ProMal in building WTGs over static analysis, dynamic analysis, and machine learning techniques when they are applied separately.",,144–146,,Proceedings of the 43rd International Conference on Software Engineering: Companion Proceedings
4,@inbook: 10.1109/ICSE-Companion52605.2021.00115,Scalable Quantitative Verification for Deep Neural Networks,"Baluta, Teodora and Chua, Zheng Leong and Meel, Kuldeep S. and Saxena, Prateek",2021,IEEE Press,,https://doi.org/10.1109/ICSE-Companion52605.2021.00115,,"Despite the functional success of deep neural networks (DNNs), their trustworthiness remains a crucial open challenge. To address this challenge, both testing and verification techniques have been proposed. But these existing techniques provide either scalability to large networks or formal guarantees, not both. In this paper, we propose a scalable quantitative verification framework for deep neural networks, i.e., a test-driven approach that comes with formal guarantees that a desired probabilistic property is satisfied. Our technique performs enough tests until soundness of a formal probabilistic property can be proven. It can be used to certify properties of both deterministic and randomized DNNs. We implement our approach in a tool called PROVERO1 and apply it in the context of certifying adversarial robustness of DNNs. In this context, we first show a new attack-agnostic measure of robustness which offers an alternative to purely attack-based methodology of evaluating robustness being reported today. Second, PROVERO provides certificates of robustness for large DNNs, where existing state-of-the-art verification tools fail to produce conclusive results. Our work paves the way forward for verifying properties of distributions captured by real-world deep neural networks, with provable guarantees, even where testers only have black-box access to the neural network.",,248–249,,Proceedings of the 43rd International Conference on Software Engineering: Companion Proceedings
5,@inproceedings: 10.1109/ICSE-SEIS52602.2021.00019,Image-Based Social Sensing: Combining AI and the Crowd to Mine Policy-Adherence Indicators from Twitter,"Negri, Virginia and Scuratti, Dario and Agresti, Stefano and Rooein, Donya and Scalia, Gabriele and Shankar, Amudha Ravi and Marquez, Jose Luis Fernandez and Carman, Mark James and Pernici, Barbara",2021,IEEE Press,,https://doi.org/10.1109/ICSE-SEIS52602.2021.00019,10.1109/ICSE-SEIS52602.2021.00019,"Social Media provides a trove of information that, if aggregated and analysed appropriately can provide important statistical indicators to policy makers. In some situations these indicators are not available through other mechanisms. For example, given the ongoing COVID-19 outbreak, it is essential for governments to have access to reliable data on policy-adherence with regards to mask wearing, social distancing, and other hard-to-measure quantities. In this paper we investigate whether it is possible to obtain such data by aggregating information from images posted to social media. The paper presents VisualCit, a pipeline for image-based social sensing combining recent advances in image recognition technology with geocoding and crowdsourcing techniques. Our aim is to discover in which countries, and to what extent, people are following COVID-19 related policy directives. We compared the results with the indicators produced within the CovidDataHub behavior tracker initiative. Preliminary results shows that social media images can produce reliable indicators for policy makers.","image classification, social media, citizen science, machine learning, social sensing, crowdsourcing",92–101,10,Proceedings of the 43rd International Conference on Software Engineering: Software Engineering in Society
6,@inbook: 10.1109/ICSE-Companion52605.2021.00117,RPT: Effective and Efficient <b>r</b>Etrieval of <b>p</b>Rogram <b>t</b>Ranslations from Big Code,"Chen, Binger and Abedjan, Ziawasch",2021,IEEE Press,,https://doi.org/10.1109/ICSE-Companion52605.2021.00117,,"Program translation is a growing demand in software engineering. Manual program translation requires programming expertise in source and target language. One way to automate this process is to make use of the big data of programs, i.e., Big Code. In particular, one can search for program translations in Big Code. However, existing code retrieval techniques are not designed for cross-language code retrieval. Other data-driven approaches require human efforts in constructing cross-language parallel datasets to train translation models. In this paper, we present Rpt, a novel code translation retrieval system. We propose a lightweight but informative program representation, which can be generalized to all imperative PLs. Furthermore, we present our index structure and hierarchical filtering mechanism for efficient code retrieval from a Big Code database.",,252–253,,Proceedings of the 43rd International Conference on Software Engineering: Companion Proceedings
7,@inproceedings: 10.1109/ICSE-SEIP52600.2021.00031,Neural Knowledge Extraction from Cloud Service Incidents,"Shetty, Manish and Bansal, Chetan and Kumar, Sumit and Rao, Nikitha and Nagappan, Nachiappan and Zimmermann, Thomas",2021,IEEE Press,,https://doi.org/10.1109/ICSE-SEIP52600.2021.00031,10.1109/ICSE-SEIP52600.2021.00031,"The move from boxed products to services and the widespread adoption of cloud computing has had a huge impact on the software development life cycle and DevOps processes. Particularly, incident management has become critical for developing and operating large-scale services. Prior work on incident management has heavily focused on the challenges with incident triaging and de-duplication. In this work, we address the fundamental problem of structured knowledge extraction from service incidents. We have built SoftNER, a framework for unsupervised knowledge extraction from service incidents. We frame the knowledge extraction problem as a Named-Entity Recognition task for extracting factual information. SoftNER leverages structural patterns like key-value pairs and tables for bootstrapping the training data. Further, we build a novel multitask learning based BiLSTM-CRF model which leverages not just the semantic context but also the data-types for named-entity extraction. We have deployed SoftNER at Microsoft, a major cloud service provider and have evaluated it on more than 2 months of cloud incidents. We show that the unsupervised machine learning pipeline has a high precision of 0.96. Our multi-task learning based deep learning model also outperforms the state of the art NER models. Lastly, using the knowledge extracted by SoftNER we are able to build significantly more accurate models for important downstream tasks like incident triaging.",,218–227,10,Proceedings of the 43rd International Conference on Software Engineering: Software Engineering in Practice
8,@inbook: 10.1109/ICSE-Companion52605.2021.00026,Roosterize: Suggesting Lemma Names for Coq Verification Projects Using Deep Learning,"Nie, Pengyu and Palmskog, Karl and Li, Junyi Jessy and Gligoric, Milos",2021,IEEE Press,,https://doi.org/10.1109/ICSE-Companion52605.2021.00026,,"Naming conventions are an important concern in large verification projects using proof assistants, such as Coq. In particular, lemma names are used by proof engineers to effectively understand and modify Coq code. However, providing accurate and informative lemma names is a complex task, which is currently often carried out manually. Even when lemma naming is automated using rule-based tools, generated names may fail to adhere to important conventions not specified explicitly. We demonstrate a toolchain, dubbed Roosterize, which automatically suggests lemma names in Coq projects. Roosterize leverages a neural network model trained on existing Coq code, thus avoiding manual specification of naming conventions. To allow proof engineers to conveniently access suggestions from Roosterize during Coq project development, we integrated the toolchain into the popular Visual Studio Code editor. Our evaluation shows that Roosterize substantially outperforms strong baselines for suggesting lemma names and is useful in practice. The demo video for Roosterize can be viewed at: https://youtu.be/HZ5ac7Q14rc.",,21–24,,Proceedings of the 43rd International Conference on Software Engineering: Companion Proceedings
9,@inbook: 10.1109/ICSE-SEIP52600.2021.00029,NNStreamer: Efficient and Agile† Development of on-Device AI Systems,"Ham, MyungJoo and Moon, Jijoong and Lim, Geunsik and Jung, Jaeyun and Ahn, Hyoungjoo and Song, Wook and Woo, Sangjung and Kapoor, Parichay and Chae, Dongju and Jang, Gichan and Ahn, Yongjoo and Lee, Jihoon",2021,IEEE Press,,https://doi.org/10.1109/ICSE-SEIP52600.2021.00029,,"We propose NNStreamer, a software system that handles neural networks as filters of stream pipelines, applying the stream processing paradigm to deep neural network applications. A new trend with the wide-spread of deep neural network applications is on-device AI. It is to process neural networks on mobile devices or edge/IoT devices instead of cloud servers. Emerging privacy issues, data transmission costs, and operational costs signify the need for on-device AI, especially if we deploy a massive number of devices. NNStreamer efficiently handles neural networks with complex data stream pipelines on devices, significantly improving the overall performance with minimal effort. Besides, NNStreamer simplifies implementations and allows reusing off-the-shelf media filters directly, which reduces developmental costs significantly. We are already deploying NNStreamer for a wide range of products and platforms, including the Galaxy series and various consumer electronic devices. The experimental results suggest a reduction in developmental costs and enhanced performance of pipeline architectures and NNStreamer. It is an open-source project incubated by Linux Foundation AI &amp; Data, available to the public and applicable to various hardware and software platforms.",,198–207,1,Proceedings of the 43rd International Conference on Software Engineering: Software Engineering in Practice
10,@inbook: 10.1109/ICSE-Companion52605.2021.00114,PyART: Python API Recommendation in Real-Time,"He, Xincheng and Xu, Lei and Zhang, Xiangyu and Hao, Rui and Feng, Yang and Xu, Baowen",2021,IEEE Press,,https://doi.org/10.1109/ICSE-Companion52605.2021.00114,,"This is the research artifact of the paper titled 'PyART: Python API Recommendation in Real-Time'. PyART is a real-time API recommendation tool for Python, which includes two main functions: data-flow analysis and real-time API recommendation for both incomplete and complete Python code context. Compared to classical tools, PyART has two important particularities: it is able to work on real-time recommendation scenario, and it provides data-flow analysis and API recommendation for dynamic language. Classical tools often fail to make static analysis in real-time recommendation scenario, due to the incompletion of syntax. And the dynamic features of Python language also bring challenges to type inference and API recommendation. Different from classical tools, PyART derives optimistic data-flow that is neither sound nor complete but sufficient for API recommendation and cost-effective to collect, and provides real-time API recommendations based on novel candidate collection, context analysis and feature learning techniques. The artifact evaluation experiments of PyART include three main aspects: data-flow analysis, intra-project API recommendation and across-project API recommendation. We assume users of the artifact is able to use Linux Ubuntu Operating System.",,246–247,,Proceedings of the 43rd International Conference on Software Engineering: Companion Proceedings
11,@inproceedings: 10.1109/ICSE-NIER52604.2021.00030,Mining Software Repositories with a Collaborative Heuristic Repository,"Babii, Hlib and Prenner, Julian Aron and Stricker, Laurin and Karmakar, Anjan and Janes, Andrea and Robbes, Romain",2021,IEEE Press,,https://doi.org/10.1109/ICSE-NIER52604.2021.00030,10.1109/ICSE-NIER52604.2021.00030,"Many software engineering studies or tasks rely on categorizing software engineering artifacts. In practice, this is done either by defining simple but often imprecise heuristics, or by manual labelling of the artifacts. Unfortunately, errors in these categorizations impact the tasks that rely on them. To improve the precision of these categorizations, we propose to gather heuristics in a collaborative heuristic repository, to which researchers can contribute a large amount of diverse heuristics for a variety of tasks on a variety of SE artifacts. These heuristics are then leveraged by state-of-the-art weak supervision techniques to train high-quality classifiers, thus improving the categorizations. We present an initial version of the heuristic repository, which we applied to the concrete task of commit classification.",,106–110,5,Proceedings of the 43rd International Conference on Software Engineering: New Ideas and Emerging Results
12,@inbook: 10.1109/ICSE43902.2021.00040,Traceability Transformed: Generating More Accurate Links with Pre-Trained BERT Models,"Lin, Jinfeng and Liu, Yalin and Zeng, Qingkai and Jiang, Meng and Cleland-Huang, Jane",2021,IEEE Press,,https://doi.org/10.1109/ICSE43902.2021.00040,,"Software traceability establishes and leverages associations between diverse development artifacts. Researchers have proposed the use of deep learning trace models to link natural language artifacts, such as requirements and issue descriptions, to source code; however, their effectiveness has been restricted by availability of labeled data and efficiency at runtime. In this study, we propose a novel framework called Trace BERT (T-BERT) to generate trace links between source code and natural language artifacts. To address data sparsity, we leverage a three-step training strategy to enable trace models to transfer knowledge from a closely related Software Engineering challenge, which has a rich dataset, to produce trace links with much higher accuracy than has previously been achieved. We then apply the T-BERT framework to recover links between issues and commits in Open Source Projects. We comparatively evaluated accuracy and efficiency of three BERT architectures. Results show that a Single-BERT architecture generated the most accurate links, while a Siamese-BERT architecture produced comparable results with significantly less execution time. Furthermore, by learning and transferring knowledge, all three models in the framework outperform classical IR trace models. On the three evaluated real-word OSS projects, the best T-BERT stably outperformed the VSM model with average improvements of 60.31% measured using Mean Average Precision (MAP). RNN severely underper-formed on these projects due to insufficient training data, while T-BERT overcame this problem by using pretrained language models and transfer learning.",,324–335,1,Proceedings of the 43rd International Conference on Software Engineering
13,@inbook: 10.1109/ICSE43902.2021.00107,CURE: Code-Aware Neural Machine Translation for Automatic Program Repair,"Jiang, Nan and Lutellier, Thibaud and Tan, Lin",2021,IEEE Press,,https://doi.org/10.1109/ICSE43902.2021.00107,,"Automatic program repair (APR) is crucial to improve software reliability. Recently, neural machine translation (NMT) techniques have been used to fix software bugs automatically. While promising, these approaches have two major limitations. Their search space often does not contain the correct fix, and their search strategy ignores software knowledge such as strict code syntax. Due to these limitations, existing NMT-based techniques underperform the best template-based approaches.We propose CURE, a new NMT-based APR technique with three major novelties. First, CURE pre-trains a programming language (PL) model on a large software codebase to learn developer-like source code before the APR task. Second, CURE designs a new code-aware search strategy that finds more correct fixes by focusing on compilable patches and patches that are close in length to the buggy code. Finally, CURE uses a subword tokenization technique to generate a smaller search space that contains more correct fixes.Our evaluation on two widely-used benchmarks shows that CURE correctly fixes 57 Defects4J bugs and 26 QuixBugs bugs, outperforming all existing APR techniques on both benchmarks.",,1161–1173,1,Proceedings of the 43rd International Conference on Software Engineering
14,@inproceedings: 10.1109/ICSE43902.2021.00088,Identifying Key Features from App User Reviews,"Wu, Huayao and Deng, Wenjun and Niu, Xintao and Nie, Changhai",2021,IEEE Press,,https://doi.org/10.1109/ICSE43902.2021.00088,10.1109/ICSE43902.2021.00088,"Due to the rapid growth and strong competition of mobile application (app) market, app developers should not only offer users with attractive new features, but also carefully maintain and improve existing features based on users' feedbacks. User reviews indicate a rich source of information to plan such feature maintenance activities, and it could be of great benefit for developers to evaluate and magnify the contribution of specific features to the overall success of their apps. In this study, we refer to the features that are highly correlated to app ratings as key features, and we present KEFE, a novel approach that leverages app description and user reviews to identify key features of a given app. The application of KEFE especially relies on natural language processing, deep machine learning classifier, and regression analysis technique, which involves three main steps: 1) extracting feature-describing phrases from app description; 2) matching each app feature with its relevant user reviews; and 3) building a regression model to identify features that have significant relationships with app ratings. To train and evaluate KEFE, we collect 200 app descriptions and 1,108,148 user reviews from Chinese Apple App Store. Experimental results demonstrate the effectiveness of KEFE in feature extraction, where an average F-measure of 78.13% is achieved. The key features identified are also likely to provide hints for successful app releases, as for the releases that receive higher app ratings, 70% of features improvements are related to key features.","user reviews, feature extraction, app store analysis, key features",922–932,11,Proceedings of the 43rd International Conference on Software Engineering
15,@inproceedings: 10.1109/ICSE43902.2021.00145,PyART: Python API Recommendation in Real-Time,"He, Xincheng and Xu, Lei and Zhang, Xiangyu and Hao, Rui and Feng, Yang and Xu, Baowen",2021,IEEE Press,,https://doi.org/10.1109/ICSE43902.2021.00145,10.1109/ICSE43902.2021.00145,"API recommendation in real-time is challenging for dynamic languages like Python. Many existing API recommendation techniques are highly effective, but they mainly support static languages. A few Python IDEs provide API recommendation functionalities based on type inference and training on a large corpus of Python libraries and third-party libraries. As such, they may fail to recommend or make poor recommendations when type information is missing or target APIs are project-specific. In this paper, we propose a novel approach, PyART, to recommending APIs for Python programs in real-time. It features a light-weight analysis to derive so-called optimistic data-flow, which is neither sound nor complete, but simulates the local data-flow information humans can derive. It extracts three kinds of features: data-flow, token similarity, and token co-occurrence, in the context of the program point where a recommendation is solicited. A predictive model is trained on these features using the Random Forest algorithm. Evaluation on 8 popular Python projects demonstrates that PyART can provide effective API recommendations. When historic commits can be leveraged, which is the target scenario of a state-of-the-art tool ARIREC, our average top-1 accuracy is over 50% and average top-10 accuracy over 70%, outperforming APIREC and Intellicode (i.e., the recommendation component in Visual Studio) by 28.48%-39.05% for top-1 accuracy and 24.41%-30.49% for top-10 accuracy. In other applications such as when historic comments are not available and cross-project recommendation, PyART also shows better overall performance. The time to make a recommendation is less than a second on average, satisfying the real-time requirement.","Python, real-time recommendation, API recommendation, context analysis, data flow analysis",1634–1645,12,Proceedings of the 43rd International Conference on Software Engineering
16,@inbook: 10.1109/ICSE43902.2021.00046,Prioritizing Test Inputs for Deep Neural Networks via Mutation Analysis,"Wang, Zan and You, Hanmo and Chen, Junjie and Zhang, Yingyi and Dong, Xuyuan and Zhang, Wenbin",2021,IEEE Press,,https://doi.org/10.1109/ICSE43902.2021.00046,,"Deep Neural Network (DNN) testing is one of the most widely-used ways to guarantee the quality of DNNs. However, labeling test inputs to check the correctness of DNN prediction is very costly, which could largely affect the efficiency of DNN testing, even the whole process of DNN development. To relieve the labeling-cost problem, we propose a novel test input prioritization approach (called PRIMA) for DNNs via intelligent mutation analysis in order to label more bug-revealing test inputs earlier for a limited time, which facilitates to improve the efficiency of DNN testing. PRIMA is based on the key insight: a test input that is able to kill many mutated models and produce different prediction results with many mutated inputs, is more likely to reveal DNN bugs, and thus it should be prioritized higher. After obtaining a number of mutation results from a series of our designed model and input mutation rules for each test input, PRIMA further incorporates learning-to-rank (a kind of supervised machine learning to solve ranking problems) to intelligently combine these mutation results for effective test input prioritization. We conducted an extensive study based on 36 popular subjects by carefully considering their diversity from five dimensions (i.e., different domains of test inputs, different DNN tasks, different network structures, different types of test inputs, and different training scenarios). Our experimental results demonstrate the effectiveness of PRIMA, significantly outperforming the state-of-the-art approaches (with the average improvement of 8.50%~131.01% in terms of prioritization effectiveness). In particular, we have applied PRIMA to the practical autonomous-vehicle testing in a large motor company, and the results on 4 real-world scene-recognition models in autonomous vehicles further confirm the practicability of PRIMA.",,397–409,1,Proceedings of the 43rd International Conference on Software Engineering
17,@inbook: 10.1109/ICSE43902.2021.00131,DeepLV: Suggesting Log Levels Using Ordinal Based Neural Networks,"Li, Zhenhao and Li, Heng and Chen, Tse-Hsun Peter and Shang, Weiyi",2021,IEEE Press,,https://doi.org/10.1109/ICSE43902.2021.00131,,"Developers write logging statements to generate logs that provide valuable runtime information for debugging and maintenance of software systems. Log level is an important component of a logging statement, which enables developers to control the information to be generated at system runtime. However, due to the complexity of software systems and their runtime behaviors, deciding a proper log level for a logging statement is a challenging task. For example, choosing a higher level (e.g., error) for a trivial event may confuse end users and increase system maintenance overhead, while choosing a lower level (e.g., trace) for a critical event may prevent the important execution information to be conveyed opportunely. In this paper, we tackle the challenge by first conducting a preliminary manual study on the characteristics of log levels. We find that the syntactic context of the logging statement and the message to be logged might be related to the decision of log levels, and log levels that are further apart in order (e.g., trace and error) tend to have more differences in their characteristics. Based on this, we then propose a deep-learning based approach that can leverage the ordinal nature of log levels to make suggestions on choosing log levels, by using the syntactic context and message features of the logging statements extracted from the source code. Through an evaluation on nine large-scale open source projects, we find that: 1) our approach outperforms the state-of-the-art baseline approaches; 2) we can further improve the performance of our approach by enlarging the training data obtained from other systems; 3) our approach also achieves promising results on cross-system suggestions that are even better than the baseline approaches on within-system suggestions. Our study highlights the potentials in suggesting log levels to help developers make informed logging decisions.",,1461–1472,1,Proceedings of the 43rd International Conference on Software Engineering
18,@inproceedings: 10.1109/ICSE43902.2021.00090,Prioritize Crowdsourced Test Reports via Deep Screenshot Understanding,"Yu, Shengcheng and Fang, Chunrong and Cao, Zhenfei and Wang, Xu and Li, Tongyu and Chen, Zhenyu",2021,IEEE Press,,https://doi.org/10.1109/ICSE43902.2021.00090,10.1109/ICSE43902.2021.00090,"Crowdsourced testing is increasingly dominant in mobile application (app) testing, but it is a great burden for app developers to inspect the incredible number of test reports. Many researches have been proposed to deal with test reports based only on texts or additionally simple image features. However, in mobile app testing, texts contained in test reports are condensed and the information is inadequate. Many screenshots are included as complements that contain much richer information beyond texts. This trend motivates us to prioritize crowdsourced test reports based on a deep screenshot understanding.In this paper, we present a novel crowdsourced test report prioritization approach, namely DeepPrior. We first represent the crowdsourced test reports with a novelly introduced feature, namely DeepFeature, that includes all the widgets along with their texts, coordinates, types, and even intents based on the deep analysis of the app screenshots, and the textual descriptions in the crowdsourced test reports. DeepFeature includes the Bug Feature, which directly describes the bugs, and the Context Feature, which depicts the thorough context of the bug. The similarity of the DeepFeature is used to represent the test reports' similarity and prioritize the crowdsourced test reports. We formally define the similarity as DeepSimilarity. We also conduct an empirical experiment to evaluate the effectiveness of the proposed technique with a large dataset group. The results show that DeepPrior is promising, and it outperforms the state-of-the-art approach with less than half the overhead.","Deep Screenshot Understanding, Crowdsourced testing, Mobile App Testing",946–956,11,Proceedings of the 43rd International Conference on Software Engineering
19,@inbook: 10.1109/ICSE43902.2021.00115,Automatic Extraction of Opinion-Based Q&amp;A from Online Developer Chats,"Chatterjee, Preetha and Damevski, Kostadin and Pollock, Lori",2021,IEEE Press,,https://doi.org/10.1109/ICSE43902.2021.00115,,"Virtual conversational assistants designed specifically for software engineers could have a huge impact on the time it takes for software engineers to get help. Research efforts are focusing on virtual assistants that support specific software development tasks such as bug repair and pair programming. In this paper, we study the use of online chat platforms as a resource towards collecting developer opinions that could potentially help in building opinion Q&amp;A systems, as a specialized instance of virtual assistants and chatbots for software engineers. Opinion Q&amp;A has a stronger presence in chats than in other developer communications, thus mining them can provide a valuable resource for developers in quickly getting insight about a specific development topic (e.g., What is the best Java library for parsing JSON?). We address the problem of opinion Q&amp;A extraction by developing automatic identification of opinion-asking questions and extraction of participants' answers from public online developer chats. We evaluate our automatic approaches on chats spanning six programming communities and two platforms. Our results show that a heuristic approach to opinion-asking questions works well (.87 precision), and a deep learning approach customized to the software domain outperforms heuristics-based, machine-learning-based and deep learning for answer extraction in community question answering.",,1260–1272,1,Proceedings of the 43rd International Conference on Software Engineering
20,@inbook: 10.1109/ICSE43902.2021.00130,Semi-Supervised Log-Based Anomaly Detection via Probabilistic Label Estimation,"Yang, Lin and Chen, Junjie and Wang, Zan and Wang, Weijing and Jiang, Jiajun and Dong, Xuyuan and Zhang, Wenbin",2021,IEEE Press,,https://doi.org/10.1109/ICSE43902.2021.00130,,"With the growth of software systems, logs have become an important data to aid system maintenance. Log-based anomaly detection is one of the most important methods for such purpose, which aims to automatically detect system anomalies via log analysis. However, existing log-based anomaly detection approaches still suffer from practical issues due to either depending on a large amount of manually labeled training data (supervised approaches) or unsatisfactory performance without learning the knowledge on historical anomalies (unsupervised and semi-supervised approaches).In this paper, we propose a novel practical log-based anomaly detection approach, PLELog, which is semi-supervised to get rid of time-consuming manual labeling and incorporates the knowledge on historical anomalies via probabilistic label estimation to bring supervised approaches' superiority into play. In addition, PLELog is able to stay immune to unstable log data via semantic embedding and detect anomalies efficiently and effectively by designing an attention-based GRU neural network. We evaluated PLELog on two most widely-used public datasets, and the results demonstrate the effectiveness of PLELog, significantly outperforming the compared approaches with an average of 181.6% improvement in terms of F1-score. In particular, PLELog has been applied to two real-world systems from our university and a large corporation, further demonstrating its practicability.",,1448–1460,1,Proceedings of the 43rd International Conference on Software Engineering
21,@inbook: 10.1109/ICSE43902.2021.00035,DeepPayload: Black-Box Backdoor Attack on Deep Learning Models through Neural Payload Injection,"Li, Yuanchun and Hua, Jiayi and Wang, Haoyu and Chen, Chunyang and Liu, Yunxin",2021,IEEE Press,,https://doi.org/10.1109/ICSE43902.2021.00035,,"Deep learning models are increasingly used in mobile applications as critical components. Unlike the program bytecode whose vulnerabilities and threats have been widely-discussed, whether and how the deep learning models deployed in the applications can be compromised are not well-understood since neural networks are usually viewed as a black box. In this paper, we introduce a highly practical backdoor attack achieved with a set of reverse-engineering techniques over compiled deep learning models. The core of the attack is a neural conditional branch constructed with a trigger detector and several operators and injected into the victim model as a malicious payload. The attack is effective as the conditional logic can be flexibly customized by the attacker, and scalable as it does not require any prior knowledge from the original model. We evaluated the attack effectiveness using 5 state-of-the-art deep learning models and real-world samples collected from 30 users. The results demonstrated that the injected backdoor can be triggered with a success rate of 93.5%, while only brought less than 2ms latency overhead and no more than 1.4% accuracy decrease. We further conducted an empirical study on real-world mobile deep learning apps collected from Google Play. We found 54 apps that were vulnerable to our attack, including popular and security-critical ones. The results call for the awareness of deep learning application developers and auditors to enhance the protection of deployed models.",,263–274,1,Proceedings of the 43rd International Conference on Software Engineering
22,@inbook: 10.1109/ICSE43902.2021.00026,Code Prediction by Feeding Trees to Transformers,"Kim, Seohyun and Zhao, Jinman and Tian, Yuchi and Chandra, Satish",2021,IEEE Press,,https://doi.org/10.1109/ICSE43902.2021.00026,,"Code prediction, more specifically autocomplete, has become an essential feature in modern IDEs. Autocomplete is more effective when the desired next token is at (or close to) the top of the list of potential completions offered by the IDE at cursor position. This is where the strength of the underlying machine learning system that produces a ranked order of potential completions comes into play.We advance the state-of-the-art in the accuracy of code prediction (next token prediction) used in autocomplete systems. Our work uses Transformers as the base neural architecture. We show that by making the Transformer architecture aware of the syntactic structure of code, we increase the margin by which a Transformer-based system outperforms previous systems. With this, it outperforms the accuracy of several state-of-the-art next token prediction systems by margins ranging from 14% to 18%.We present in the paper several ways of communicating the code structure to the Transformer, which is fundamentally built for processing sequence data. We provide a comprehensive experimental evaluation of our proposal, along with alternative design choices, on a standard Python dataset, as well as on a company internal Python corpus. Our code and data preparation pipeline will be available in open source.",,150–162,1,Proceedings of the 43rd International Conference on Software Engineering
23,@inproceedings: 10.1109/ICSE43902.2021.00047,Testing Machine Translation via Referential Transparency,"He, Pinjia and Meister, Clara and Su, Zhendong",2021,IEEE Press,,https://doi.org/10.1109/ICSE43902.2021.00047,10.1109/ICSE43902.2021.00047,"Machine translation software has seen rapid progress in recent years due to the advancement of deep neural networks. People routinely use machine translation software in their daily lives for tasks such as ordering food in a foreign restaurant, receiving medical diagnosis and treatment from foreign doctors, and reading international political news online. However, due to the complexity and intractability of the underlying neural networks, modern machine translation software is still far from robust and can produce poor or incorrect translations; this can lead to misunderstanding, financial loss, threats to personal safety and health, and political conflicts. To address this problem, we introduce referentially transparent inputs (RTIs), a simple, widely applicable methodology for validating machine translation software. A referentially transparent input is a piece of text that should have similar translations when used in different contexts. Our practical implementation, Purity, detects when this property is broken by a translation. To evaluate RTI, we use Purity to test Google Translate and Bing Microsoft Translator with 200 unlabeled sentences, which detected 123 and 142 erroneous translations with high precision (79.3% and 78.3%). The translation errors are diverse, including examples of under-translation, over-translation, word/phrase mistranslation, incorrect modification, and unclear logic.","Machine translation, Referential transparency, Metamorphic testing, Testing",410–422,13,Proceedings of the 43rd International Conference on Software Engineering
24,@inbook: 10.1109/ICSE43902.2021.00059,IdBench: Evaluating Semantic Representations of Identifier Names in Source Code,"Wainakh, Yaza and Rauf, Moiz and Pradel, Michael",2021,IEEE Press,,https://doi.org/10.1109/ICSE43902.2021.00059,,"Identifier names convey useful information about the intended semantics of code. Name-based program analyses use this information, e.g., to detect bugs, to predict types, and to improve the readability of code. At the core of name-based analyses are semantic representations of identifiers, e.g., in the form of learned embeddings. The high-level goal of such a representation is to encode whether two identifiers, e.g., len and size, are semantically similar. Unfortunately, it is currently unclear to what extent semantic representations match the semantic relatedness and similarity perceived by developers. This paper presents IdBench, the first benchmark for evaluating semantic representations against a ground truth created from thousands of ratings by 500 software developers. We use IdBench to study state-of-the-art embedding techniques proposed for natural language, an embedding technique specifically designed for source code, and lexical string distance functions. Our results show that the effectiveness of semantic representations varies significantly and that the best available embeddings successfully represent semantic relatedness. On the downside, no existing technique provides a satisfactory representation of semantic similarities, among other reasons because identifiers with opposing meanings are incorrectly considered to be similar, which may lead to fatal mistakes, e.g., in a refactoring tool. Studying the strengths and weaknesses of the different techniques shows that they complement each other. As a first step toward exploiting this complementarity, we present an ensemble model that combines existing techniques and that clearly outperforms the best available semantic representation.",,562–573,1,Proceedings of the 43rd International Conference on Software Engineering
25,@inproceedings: 10.1109/ICSE43902.2021.00117,Automatic Solution Summarization for Crash Bugs,"Wang, Haoye and Xia, Xin and Lo, David and Grundy, John and Wang, Xinyu",2021,IEEE Press,,https://doi.org/10.1109/ICSE43902.2021.00117,10.1109/ICSE43902.2021.00117,"The causes of software crashes can be hidden anywhere in the source code and development environment. When encountering software crashes, recurring bugs that are discussed on Q&amp;A sites could provide developers with solutions to their crashing problems. However, it is difficult for developers to accurately search for relevant content on search engines, and developers have to spend a lot of manual effort to find the right solution from the returned results. In this paper, we present CraSolver, an approach that takes into account both the structural information of crash traces and the knowledge of crash-causing bugs to automatically summarize solutions from crash traces. Given a crash trace, CraSolver retrieves relevant questions from Q&amp;A sites by combining a proposed position dependent similarity - based on the structural information of the crash trace - with an extra knowledge similarity, based on the knowledge from official documentation sites. After obtaining the answers to these questions from the Q&amp;A site, CraSolver summarizes the final solution based on a multi-factor scoring mechanism. To evaluate our approach, we built two repositories of Java and Android exception-related questions from Stack Overflow with size of 69,478 and 33,566 questions respectively. Our user study results using 50 selected Java crash traces and 50 selected Android crash traces show that our approach significantly outperforms four baselines in terms of relevance, usefulness, and diversity. The evaluation also confirms the effectiveness of the relevant question retrieval component in our approach for crash traces.",,1286–1297,12,Proceedings of the 43rd International Conference on Software Engineering
26,@inbook: 10.1109/ICSE43902.2021.00111,TransRegex: Multi-Modal Regular Expression Synthesis by Generate-and-Repair,"Li, Yeting and Li, Shuaimin and Xu, Zhiwu and Cao, Jialun and Chen, Zixuan and Hu, Yun and Chen, Haiming and Cheung, Shing-Chi",2021,IEEE Press,,https://doi.org/10.1109/ICSE43902.2021.00111,,"Since regular expressions (abbrev. regexes) are difficult to understand and compose, automatically generating regexes has been an important research problem. This paper introduces TRANSREGEX, for automatically constructing regexes from both natural language descriptions and examples. To the best of our knowledge, TransRegex is the first to treat the NLP-and-example-based regex synthesis problem as the problem of NLP-based synthesis with regex repair. For this purpose, we present novel algorithms for both NLP-based synthesis and regex repair. We evaluate TransRegex with ten relevant state-of-the-art tools on three publicly available datasets. The evaluation results demonstrate that the accuracy of our TransRegex is 17.4%, 35.8% and 38.9% higher than that of NLP-based approaches on the three datasets, respectively. Furthermore, TransRegex can achieve higher accuracy than the state-of-the-art multi-modal techniques with 10% to 30% higher accuracy on all three datasets. The evaluation results also indicate TransRegex utilizing natural language and examples in a more effective way.",,1210–1222,1,Proceedings of the 43rd International Conference on Software Engineering
27,@inbook: 10.1109/ICSE43902.2021.00034,DeepLocalize: Fault Localization for Deep Neural Networks,"Wardat, Mohammad and Le, Wei and Rajan, Hridesh",2021,IEEE Press,,https://doi.org/10.1109/ICSE43902.2021.00034,," Deep neural networks (DNNs) are becoming an integral part of most software systems. Previous work has shown that DNNs have bugs. Unfortunately, existing debugging techniques don't support localizing DNN bugs because of the lack of understanding of model behaviors. The entire DNN model appears as a black box. To address these problems, we propose an approach and a tool that automatically determines whether the model is buggy or not, and identifies the root causes for DNN errors. Our key insight is that historic trends in values propagated between layers can be analyzed to identify faults, and also localize faults. To that end, we first enable dynamic analysis of deep learning applications: by converting it into an imperative representation and alternatively using a callback mechanism. Both mechanisms allows us to insert probes that enable dynamic analysis over the traces produced by the DNN while it is being trained on the training data. We then conduct dynamic analysis over the traces to identify the faulty layer or hyperparameter that causes the error. We propose an algorithm for identifying root causes by capturing any numerical error and monitoring the model during training and finding the relevance of every layer/parameter on the DNN outcome. We have collected a benchmark containing 40 buggy models and patches that contain real errors in deep learning applications from Stack Overflow and GitHub. Our benchmark can be used to evaluate automated debugging tools and repair techniques. We have evaluated our approach using this DNN bug-and-patch benchmark, and the results showed that our approach is much more effective than the existing debugging approach used in the state-of-the-practice Keras library. For 34/40 cases, our approach was able to detect faults whereas the best debugging approach provided by Keras detected 32/40 faults. Our approach was able to localize 21/40 bugs whereas Keras did not localize any faults.",,251–262,1,Proceedings of the 43rd International Conference on Software Engineering
28,@inbook: 10.1109/ICSE43902.2021.00068,An Empirical Study on Deployment Faults of Deep Learning Based Mobile Applications,"Chen, Zhenpeng and Yao, Huihan and Lou, Yiling and Cao, Yanbin and Liu, Yuanqiang and Wang, Haoyu and Liu, Xuanzhe",2021,IEEE Press,,https://doi.org/10.1109/ICSE43902.2021.00068,,"Deep learning (DL) is moving its step into a growing number of mobile software applications. These software applications, named as DL based mobile applications (abbreviated as mobile DL apps) integrate DL models trained using large-scale data with DL programs. A DL program encodes the structure of a desirable DL model and the process by which the model is trained using training data. Due to the increasing dependency of current mobile apps on DL, software engineering (SE) for mobile DL apps has become important. However, existing efforts in SE research community mainly focus on the development of DL models and extensively analyze faults in DL programs. In contrast, faults related to the deployment of DL models on mobile devices (named as deployment faults of mobile DL apps) have not been well studied. Since mobile DL apps have been used by billions of end users daily for various purposes including for safety-critical scenarios, characterizing their deployment faults is of enormous importance. To fill in the knowledge gap, this paper presents the first comprehensive study to date on the deployment faults of mobile DL apps. We identify 304 real deployment faults from Stack Overflow and GitHub, two commonly used data sources for studying software faults. Based on the identified faults, we construct a fine-granularity taxonomy consisting of 23 categories regarding to fault symptoms and distill common fix strategies for different fault symptoms. Furthermore, we suggest actionable implications and research avenues that can potentially facilitate the deployment of DL models on mobile devices.",,674–685,1,Proceedings of the 43rd International Conference on Software Engineering
29,@inbook: 10.1109/ICSE43902.2021.00094,Representation of Developer Expertise in Open Source Software,"Dey, Tapajit and Karnauch, Andrey and Mockus, Audris",2021,IEEE Press,,https://doi.org/10.1109/ICSE43902.2021.00094,,"Background: Accurate representation of developer expertise has always been an important research problem. While a number of studies proposed novel methods of representing expertise within individual projects, these methods are difficult to apply at an ecosystem level. However, with the focus of software development shifting from monolithic to modular, a method of representing developers' expertise in the context of the entire OSS development becomes necessary when, for example, a project tries to find new maintainers and look for developers with relevant skills. Aim: We aim to address this knowledge gap by proposing and constructing the Skill Space where each API, developer, and project is represented and postulate how the topology of this space should reflect what developers know (and projects need). Method: we use the World of Code infrastructure to extract the complete set of APIs in the files changed by open source developers and, based on that data, employ Doc2Vec embeddings for vector representations of APIs, developers, and projects. We then evaluate if these embeddings reflect the postulated topology of the Skill Space by predicting what new APIs/projects developers use/join, and whether or not their pull requests get accepted. We also check how the developers' representations in the Skill Space align with their self-reported API expertise. Result: Our results suggest that the proposed embeddings in the Skill Space appear to satisfy the postulated topology and we hope that such representations may aid in the construction of signals that increase trust (and efficiency) of open source ecosystems at large and may aid investigations of other phenomena related to developer proficiency and learning.",,995–1007,1,Proceedings of the 43rd International Conference on Software Engineering
30,@inbook: 10.1109/ICSE43902.2021.00074,Guigan: Learning to Generate GUI Designs Using Generative Adversarial Networks,"Zhao, Tianming and Chen, Chunyang and Liu, Yuanning and Zhu, Xiaodong",2021,IEEE Press,,https://doi.org/10.1109/ICSE43902.2021.00074,,"Graphical User Interface (GUI) is ubiquitous in almost all modern desktop software, mobile applications, and online websites. A good GUI design is crucial to the success of the software in the market, but designing a good GUI which requires much innovation and creativity is difficult even to well-trained designers. Besides, the requirement of the rapid development of GUI design also aggravates designers' working load. So, the availability of various automated generated GUIs can help enhance the design personalization and specialization as they can cater to the taste of different designers. To assist designers, we develop a model GUIGAN to automatically generate GUI designs. Different from conventional image generation models based on image pixels, our GUIGAN is to reuse GUI components collected from existing mobile app GUIs for composing a new design that is similar to natural-language generation. Our GUIGAN is based on SeqGAN by modeling the GUI component style compatibility and GUI structure. The evaluation demonstrates that our model significantly outperforms the best of the baseline methods by 30.77% in Fr\'{e}chet Inception distance (FID) and 12.35% in 1-Nearest Neighbor Accuracy (1-NNA). Through a pilot user study, we provide initial evidence of the usefulness of our approach for generating acceptable brand new GUI designs.",,748–760,1,Proceedings of the 43rd International Conference on Software Engineering
31,@inproceedings: 10.1109/ICSE43902.2021.00050,Early Life Cycle Software Defect Prediction: Why? How?,"Shrikanth, N. C. and Majumder, Suvodeep and Menzies, Tim",2021,IEEE Press,,https://doi.org/10.1109/ICSE43902.2021.00050,10.1109/ICSE43902.2021.00050,"Many researchers assume that, for software analytics, ""more data is better."" We write to show that, at least for learning defect predictors, this may not be true.To demonstrate this, we analyzed hundreds of popular GitHub projects. These projects ran for 84 months and contained 3,728 commits (median values). Across these projects, most of the defects occur very early in their life cycle. Hence, defect predictors learned from the first 150 commits and four months perform just as well as anything else. This means that, at least for the projects studied here, after the first few months, we need not continually update our defect prediction models.We hope these results inspire other researchers to adopt a ""simplicity-first"" approach to their work. Some domains require a complex and data-hungry analysis. But before assuming complexity, it is prudent to check the raw data looking for ""short cuts"" that can simplify the analysis.","analytics, sampling, early, defect prediction",448–459,12,Proceedings of the 43rd International Conference on Software Engineering
32,@inbook: 10.1145/3445814.3446741,Training for Multi-Resolution Inference Using Reusable Quantization Terms,"Zhang, Sai Qian and McDanel, Bradley and Kung, H. T. and Dong, Xin",2021,Association for Computing Machinery,,https://doi.org/10.1145/3445814.3446741,,"Low-resolution uniform quantization (e.g., 4-bit bitwidth) for both Deep Neural Network (DNN) weights and data has emerged as an important technique for efficient inference. Departing from conventional quantization, we describe a novel training approach to support inference at multiple resolutions by reusing a single set of quantization terms (the same set of nonzero bits in values). The proposed approach streamlines the training and supports dynamic selection of resolution levels during inference. We evaluate the method on a diverse range of applications including multiple CNNs on ImageNet, an LSTM on Wikitext-2, and YOLO-v5 on COCO. We show that models resulting from our multi-resolution training can support up to 10 resolutions with only a moderate performance reduction (e.g., ≤ 1%) compared to training them individually. Lastly, using an FPGA, we compare our multi-resolution multiplier-accumulator (mMAC) against other conventional MAC designs and evaluate the inference performance. We show that the mMAC design broadens the choices in trading off cost, efficiency, and latency across a range of computational budgets.",,845–860,1,Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems
33,@inproceedings: 10.1145/3445814.3446747,Defensive Approximation: Securing CNNs Using Approximate Computing,"Guesmi, Amira and Alouani, Ihsen and Khasawneh, Khaled N. and Baklouti, Mouna and Frikha, Tarek and Abid, Mohamed and Abu-Ghazaleh, Nael",2021,Association for Computing Machinery,,https://doi.org/10.1145/3445814.3446747,10.1145/3445814.3446747,"In the past few years, an increasing number of machine-learning and deep learning structures, such as Convolutional Neural Networks (CNNs), have been applied to solving a wide range of real-life problems. However, these architectures are vulnerable to adversarial attacks: inputs crafted carefully to force the system output to a wrong label. Since machine-learning is being deployed in safety-critical and security-sensitive domains, such attacks may have catastrophic security and safety consequences. In this paper, we propose for the first time to use hardware-supported approximate computing to improve the robustness of machine learning classifiers. We show that our approximate computing implementation achieves robustness across a wide range of attack scenarios. Specifically, we show that successful adversarial attacks against the exact classifier have poor transferability to the approximate implementation. The transferability is even poorer for the black-box attack scenarios, where adversarial attacks are generated using a proxy model. Surprisingly, the robustness advantages also apply to white-box attacks where the attacker has unrestricted access to the approximate classifier implementation: in this case, we show that substantially higher levels of adversarial noise are needed to produce adversarial examples. Furthermore, our approximate computing model maintains the same level in terms of classification accuracy, does not require retraining, and reduces resource utilization and energy consumption of the CNN. We conducted extensive experiments on a set of strong adversarial attacks; We empirically show that the proposed implementation increases the robustness of a LeNet-5 and an Alexnet CNNs by up to 99% and 87%, respectively for strong transferability-based attacks along with up to 50% saving in energy consumption due to the simpler nature of the approximate logic. We also show that a white-box attack requires a remarkably higher noise budget to fool the approximate classifier, causing an average of 4&nbsp;dB degradation of the PSNR of the input image relative to the images that succeed in fooling the exact classifier.","Deep neural network, approximate computing, adversarial example, security",990–1003,14,Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems
34,@inproceedings: 10.1145/3445814.3446693,Sinan: ML-Based and QoS-Aware Resource Management for Cloud Microservices,"Zhang, Yanqi and Hua, Weizhe and Zhou, Zhuangzhuang and Suh, G. Edward and Delimitrou, Christina",2021,Association for Computing Machinery,,https://doi.org/10.1145/3445814.3446693,10.1145/3445814.3446693,"Cloud applications are increasingly shifting from large monolithic services, to large numbers of loosely-coupled, specialized microservices. Despite their advantages in terms of facilitating development, deployment, modularity, and isolation, microservices complicate resource management, as dependencies between them introduce backpressure effects and cascading QoS violations.  We present Sinan, a data-driven cluster manager for interactive cloud microservices that is online and QoS-aware. Sinan leverages a set of scalable and validated machine learning models to determine the performance impact of dependencies between microservices, and allocate appropriate resources per tier in a way that preserves the end-to-end tail latency target. We evaluate Sinan both on dedicated local clusters and large-scale deployments on Google Compute Engine (GCE) across representative end-to-end applications built with microservices, such as social networks and hotel reservation sites. We show that Sinan always meets QoS, while also maintaining cluster utilization high, in contrast to prior work which leads to unpredictable performance or sacrifices resource efficiency. Furthermore, the techniques in Sinan are explainable, meaning that cloud operators can yield insights from the ML models on how to better deploy and design their applications to reduce unpredictable performance.","Cloud computing, cluster management, machine learning for system, quality of service, resource management, datacenter, mi-croservices, resource efficiency, resourceallocation, tail latency",167–181,15,Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems
35,@inproceedings: 10.1145/3445814.3446753,Neural Architecture Search as Program Transformation Exploration,"Turner, Jack and Crowley, Elliot J. and O'Boyle, Michael F. P.",2021,Association for Computing Machinery,,https://doi.org/10.1145/3445814.3446753,10.1145/3445814.3446753,"Improving the performance of deep neural networks (DNNs) is important to both the compiler and neural architecture search (NAS) communities. Compilers apply program transformations in order to exploit hardware parallelism and memory hierarchy. However, legality concerns mean they fail to exploit the natural robustness of neural networks. In contrast, NAS techniques mutate networks by operations such as the grouping or bottlenecking of convolutions, exploiting the resilience of DNNs. In this work, we express such neural architecture operations as program transformations whose legality depends on a notion of representational capacity. This allows them to be combined with existing transformations into a unified optimization framework. This unification allows us to express existing NAS operations as combinations of simpler transformations. Crucially, it allows us to generate and explore new tensor convolutions. We prototyped the combined framework in TVM and were able to find optimizations across different DNNs, that significantly reduce inference time - over 3\texttimes{} in the majority of cases. Furthermore, our scheme dramatically reduces NAS search time.","Machine learning, compilers",915–927,13,Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems
36,@inproceedings: 10.1145/3445814.3446700,Sage: Practical and Scalable ML-Driven Performance Debugging in Microservices,"Gan, Yu and Liang, Mingyu and Dev, Sundar and Lo, David and Delimitrou, Christina",2021,Association for Computing Machinery,,https://doi.org/10.1145/3445814.3446700,10.1145/3445814.3446700,"Cloud applications are increasingly shifting from large monolithic services to complex graphs of loosely-coupled microservices. Despite the advantages of modularity and elasticity microservices offer, they also complicate cluster management and performance debugging, as dependencies between tiers introduce backpressure and cascading QoS violations. Prior work on performance debugging for cloud services either relies on empirical techniques, or uses supervised learning to diagnose the root causes of performance issues, which requires significant application instrumentation, and is difficult to deploy in practice.  We present Sage, a machine learning-driven root cause analysis system for interactive cloud microservices that focuses on practicality and scalability. Sage leverages unsupervised ML models to circumvent the overhead of trace labeling, captures the impact of dependencies between microservices to determine the root cause of unpredictable performance online, and applies corrective actions to recover a cloud service’s QoS. In experiments on both dedicated local clusters and large clusters on Google Compute Engine we show that Sage consistently achieves over 93% accuracy in correctly identifying the root cause of QoS violations, and improves performance predictability.","performance debugging, variational autoencoder, counterfactual, QoS, microservices, Bayesian network, cloud computing",135–151,17,Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems
37,@inproceedings: 10.1145/3452383.3452393,Using Paragraph Vectors to Improve Our Existing Code Review Assisting Tool-CRUSO,"Kapur, Ritu and Sodhi, Balwinder and Rao, Poojith U and Sharma, Shipra",2021,Association for Computing Machinery,,https://doi.org/10.1145/3452383.3452393,10.1145/3452383.3452393," Code reviews are one of the effective methods to estimate defectiveness in source code. However, the existing methods are dependent on experts or inefficient. In this paper, we improve the performance (in terms of speed and memory usage) of our existing code review assisting tool–CRUSO. The central idea of the approach is to estimate the defectiveness for an input source code by using the defectiveness score of similar code fragments present in various StackOverflow (SO) posts. The significant contributions of our paper are i) SOpostsDB: a dataset containing the PVA vectors and the SO posts information, ii) CRUSO-P: a code review assisting system based on PVA models trained on SOpostsDB. For a given input source code, CRUSO-P labels it as {Likely to be defective, Unlikely to be defective, Unpredictable}. To develop CRUSO-P, we processed &gt;3 million SO posts and 188200+ GitHub source files. CRUSO-P is designed to work with source code written in the popular programming languages {C, C#, Java, JavaScript, and Python}. CRUSO-P outperforms CRUSO with an improvement of 97.82% in response time and a storage reduction of 99.15%. CRUSO-P achieves the highest mean accuracy score of 99.6% when tested with the C programming language, thus achieving an improvement of 5.6% over the existing method.","Automated code review, Software maintenance, StackOverflow, Code quality, Paragraph Vector",,11,14th Innovations in Software Engineering Conference (Formerly Known as India Software Engineering Conference)
38,@inproceedings: 10.1145/3452383.3452396,Open Information Extraction Using Dependency Parser for Business Rule Mining in SBVR Format,"Prakash, Chandan and Chittimalli, Pavan Kumar and Naik, Ravindra",2021,Association for Computing Machinery,,https://doi.org/10.1145/3452383.3452396,10.1145/3452383.3452396," Business Rules exists at the core of any Business Organization. For efficient execution of the business system, all the business rules must be in machine-interpretable format. There is an absence of such a system that can convert the business rule sentences into corresponding structured format automatically. We present BRMiner, a system which automatically converts business rules represented as Natural Language sentences to the corresponding SBVR format which is a structured representation that can be further converted to the machine-interpretable format. BRMiner is based on the idea of Open Information Extraction (OIE). We have shown that existing OIE systems are not suitable for SBVR rule formation that leads to the development of a new OIE system BRMiner, with more accurate prediction and additional capabilities. BRMiner uses the state of the art dependency parser to convert an unstructured business rule to the corresponding structured format. We have used internal as well as publically available datasets for our system evaluation and the results are encouraging which we have shown in the paper.","Dependency Parser, Business Rules, Controlled Natural Language, Open Information Extraction, SBVR",,11,14th Innovations in Software Engineering Conference (Formerly Known as India Software Engineering Conference)
39,@inproceedings: 10.1145/3456126.3456133,Enhanced Neural Architecture Search Using Super Learner and Ensemble Approaches,"Lankford, Seamus and Grimes, Diarmuid",2021,Association for Computing Machinery,,https://doi.org/10.1145/3456126.3456133,10.1145/3456126.3456133,"Neural networks, and in particular Convolutional Neural Networks (CNNs), are often optimized using default parameters. Neural Architecture Search (NAS) enables multiple architectures to be evaluated prior to selection of the optimal architecture. A system integrating open-source tools for Neural Architecture Search (OpenNAS) of image classification problems has been developed and made available to the open-source community. OpenNAS takes any dataset of grayscale, or RGB images, and generates the optimal CNN architecture. The training and optimization of neural networks, using super learner and ensemble approaches, is explored in this research. Particle Swarm Optimization (PSO), Ant Colony Optimization (ACO) and pretrained models serve as base learners for network ensembles. Meta learner algorithms are subsequently applied to these base learners and the ensemble performance on image classification problems is evaluated. Our results show that a stacked generalization ensemble of heterogeneous models is the most effective approach to image classification within OpenNAS.","Super Learner, PSO, Stacking, CNN, ACO, Ensemble, AutoML",137–143,7,2021 2nd Asia Service Sciences and Software Engineering Conference
40,@inproceedings: 10.1145/3457784.3457788,"FU Covid-19 AI Agent Built on Attention Algorithm Using a Combination of Transformer, ALBERT Model, and RASA Framework","Quy Tran, Ban and Van Nguyen, Thai and Duc Phung, Thang and Tan Nguyen, Viet and Duy Tran, Dat and Tung Ngo, Son",2021,Association for Computing Machinery,,https://doi.org/10.1145/3457784.3457788,10.1145/3457784.3457788,"Potentialized by Natural Language Processing (NLP) technology, we can build a chatbot or an AI Agent to automatically address the need to automatically get credible and timely information, especially in the fight against epidemics. However, Vietnamese understanding is still a big challenge for NLP. This paper introduces an AI Agent using the Attention algorithm and Albert model to implement the question/answering task in the Covid-19 field for the Vietnamese language. In the end, we also built two other modules, one for Vietnamese diacritic auto-correction and another for updating Covid-19 statistics (using RASA framework), to deploy a Covid-19 chatbot application on mobile devices.","Transformer, AI Agent, Application, Natural Language Processing, Albert, Rasa, Covid-19, Attention, Question Answering",22–31,10,2021 10th International Conference on Software and Computer Applications
41,@inproceedings: 10.1145/3457784.3457811,Face Translation Based on Semantic Style Transfer and Rendering from One Single Image,"Lin, Peizhen and Liu, Baoyu and Wang, Lei and Lei, Zetong and Cheng, Jun",2021,Association for Computing Machinery,,https://doi.org/10.1145/3457784.3457811,10.1145/3457784.3457811,"Many avatar characters have been animated in films or games, which always need a lot of time for post-processing with the computer graphics technologies. In recent years, lots of deep learning based methods have been proposed for face translation and image generation, which always require a large amount of data for training. However, there are few samples for special characters' prototype. In this paper, we present one face translation framework for translating human faces to that with visual effects from one single prototype image. The proposed framework consists of three modules. We first design one module to generate semantic face mask–the semantic mask generating (SMG) module. According to the semantic mask, the face color tone can be changed to that of the prototype. So we design the semantic color transfer (SCT) module. For the local textures, we design the deformation and rendering (DR) module. Experiments show that the proposed framework can generate images with prototype's visual effects while preserving the original person's identification and expression information.","Deep learning, Face translation, Mask Generative Adversarial Networks,",166–172,7,2021 10th International Conference on Software and Computer Applications
42,@inproceedings: 10.1145/3457784.3457827,A Deep Learning Based Implementation for Self-Driving Car,"Thanh Dat, Vu and Quang Huy, Phan and Dinh Tra, Nguyen and Hai Anh, Tran and Ngoc Anh, Bui and Tung Son, Ngo",2021,Association for Computing Machinery,,https://doi.org/10.1145/3457784.3457827,10.1145/3457784.3457827,"Today, self-driving cars are a part of our life. It has received much attention in recent years. Many big companies and developers have invested a lot in this area and developed their own autonomous driving car platforms. The intriguing area of self-driving car motivates us to build a self-driving platform. This paper proposes the self-driving car's architecture and its software components that have been solved in FPT's contest. Lane detection in different environmental conditions, dodging obstacles, and detecting traffic signs. In this competition, the vehicle is equipped with limited hardware such as a single low-cost camera, an Nvidia Jetson TX2 board. We analyze the results obtained in the game in the simulator. We see that our method has overcome limited hardware but still achieved good results in complex problems. The final product has been used to compete in the Digital Race competition 2020 - a competition held annually by FPT Corporation.","CCS CONCEPTS • Computer systems organization∼Embedded and cyber-physical systems∼Embedded systems, Redundancy, Robotics",279–287,9,2021 10th International Conference on Software and Computer Applications
43,@inproceedings: 10.1145/3457784.3457791,Research on Data Generation Model Based on Improved SeqGAN,"Dou, Jian and Qie, Shuang and Lu, Jizhe and Ren, Yi",2021,Association for Computing Machinery,,https://doi.org/10.1145/3457784.3457791,10.1145/3457784.3457791,"With the demand of integrated energy metering business and the rise of artificial intelligence technology, the data generation model of digital equipment has become the focus of attention. As the most widely used method in the field of image generation, the implicit method based on GAN has great development potential and strong domain expansion ability. The addition of reinforcement learning method makes the GAN correlation algorithm suitable for data generation of discrete data. This paper proposes an improved SeqGAN model, reconstructs the original SeqGAN model, improves the roll-out module of the original model, uses model parameters lagging behind the generator, and increases the stability of long sequence reinforcement learning. Compared with some existing popular algorithms, the performance of the proposed model algorithm is significantly better than that of the comparison algorithm when the training times are enough (more than 150 times), which lays a foundation for its application in data generation of digital equipment.","SeqGAN, Roll-out module, Data generation, Reinforcement learning",45–50,6,2021 10th International Conference on Software and Computer Applications
44,@inproceedings: 10.1145/3448290.3448560,CompactNet: Platform-Aware Automatic Optimization for Convolutional Neural Networks,"Li, Weicheng and Wang, Rui and Qian, Depei",2021,Association for Computing Machinery,,https://doi.org/10.1145/3448290.3448560,10.1145/3448290.3448560,"Convolutional Neural Network (CNN) based Deep Learning (DL) has achieved great progress in many real-life applications. Meanwhile, due to the complex model structures against strict latency and memory restriction, the implementation of CNN models on the resource-limited platforms is becoming more challenging. This work proposes a solution, called CompactNet, which automatically optimizes a pre-trained CNN model on a specific resource-limited platform given a specific target of inference speedup. Guided by a simulator of the target platform, CompactNet progressively trims a pre-trained network by removing certain redundant filters until the target speedup is reached and generates an optimal platform-specific model while maintaining the accuracy. We evaluate our work on two platforms of a mobile ARM CPU and a machine learning accelerator NPU (Cambricon-1A ISA) on a Huawei Mate10 smartphone. For the state-of-the-art slim CNN model made for the embedded platform, MobileNetV2, CompactNet achieves up to a 1.8x kernel computation speedup with equal or even higher accuracy for image classification tasks on the ImageNet dataset, which outperforms other successful CNN optimizing techniques. Compared with the state-of-the-art Neural Architecture Searching (NAS) work, the optimal model generated through our CompactNet is faster and can be applied to bigger datasets like ImageNet. Furthermore, the optimizing process is much faster than those searching approaches.","mobility, model compression, deep learning",11–20,10,Proceedings of the 12th International Workshop on Programming Models and Applications for Multicores and Manycores
45,@inbook: 10.1145/3431920.3439295,CoDeNet: Efficient Deployment of Input-Adaptive Object Detection on Embedded FPGAs,"Huang, Qijing and Wang, Dequan and Dong, Zhen and Gao, Yizhao and Cai, Yaohui and Li, Tian and Wu, Bichen and Keutzer, Kurt and Wawrzynek, John",2021,Association for Computing Machinery,,https://doi.org/10.1145/3431920.3439295,,"Deploying deep learning models on embedded systems for computer vision tasks has been challenging due to limited compute resources and strict energy budgets. The majority of existing work focuses on accelerating image classification, while other fundamental vision problems, such as object detection, have not been adequately addressed. Compared with image classification, detection problems are more sensitive to the spatial variance of objects, and therefore, require specialized convolutions to aggregate spatial information. To address this need, recent work introduces dynamic deformable convolution to augment regular convolutions. Regular convolutions process a fixed grid of pixels across all the spatial locations in an image, while dynamic deformable convolution may access arbitrary pixels in the image with the access pattern being input-dependent and varying with spatial location. These properties lead to inefficient memory accesses of inputs with existing hardware.In this work, we harness the flexibility of FPGAs to develop a novel object detection pipeline with deformable convolutions. We show the speed-accuracy tradeoffs for a set of algorithm modifications including irregular-access versus limited-range and fixed-shape on a flexible hardware accelerator. We evaluate these algorithmic changes with corresponding hardware optimizations and show a 1.36x and 9.76x speedup respectively for the full and depthwise deformable convolution on hardware with minor accuracy loss. We then co-design a network called CoDeNet with the modified deformable convolution for object detection and quantize the network to 4-bit weights and 8-bit activations. With our high-efficiency implementation, our solution reaches 26.9 frames per second with a tiny model size of 0.76 MB while achieving 61.7 AP50 on the standard object detection dataset, Pascal VOC. With our higher-accuracy implementation, our model gets to 67.1 AP50 on Pascal VOC with only 2.9 MB of parameters--20.9x smaller but 10% more accurate than Tiny-YOLO.",,206–216,1,The 2021 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays
46,@inbook: 10.1145/3437801.3441578,TurboTransformers: An Efficient GPU Serving System for Transformer Models,"Fang, Jiarui and Yu, Yang and Zhao, Chengduo and Zhou, Jie",2021,Association for Computing Machinery,,https://doi.org/10.1145/3437801.3441578,,"The transformer is the most critical algorithm innovation of the Nature Language Processing (NLP) field in recent years. Unlike the Recurrent Neural Network (RNN) models, transformers are able to process on dimensions of sequence lengths in parallel, therefore leads to better accuracy on long sequences. However, efficient deployments of them for online services in data centers equipped with GPUs are not easy. First, more computation introduced by transformer structures makes it more challenging to meet the latency and throughput constraints of serving. Second, NLP tasks take in sentences of variable length. The variability of input dimensions brings a severe problem to efficient memory management and serving optimization.To solve the above challenges, this paper designed a transformer serving system called TurboTransformers, which consists of a computing runtime and a serving framework. Three innovative features make it stand out from other similar works. An efficient parallel algorithm is proposed for GPU-based batch reduction operations, like Softmax and LayerNorm, which are major hot spots besides BLAS routines. A memory allocation algorithm, which better balances the memory footprint and allocation/free efficiency, is designed for variable-length input situations. A serving framework equipped with a new batch scheduler using dynamic programming achieves the optimal throughput on variable-length requests. The system can achieve the state-of-the-art transformer model serving performance on GPU platforms and can be seamlessly integrated into your PyTorch code with a few lines of code.",,389–402,1,Proceedings of the 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming
47,@inbook: 10.1145/3431920.3439296,FracBNN: Accurate and FPGA-Efficient Binary Neural Networks with Fractional Activations,"Zhang, Yichi and Pan, Junhao and Liu, Xinheng and Chen, Hongzheng and Chen, Deming and Zhang, Zhiru",2021,Association for Computing Machinery,,https://doi.org/10.1145/3431920.3439296,,"Binary neural networks (BNNs) have 1-bit weights and activations. Such networks are well suited for FPGAs, as their dominant computations are bitwise arithmetic and the memory requirement is also significantly reduced. However, compared to start-of-the-art compact convolutional neural network (CNN) models, BNNs tend to produce a much lower accuracy on realistic datasets such as ImageNet. In addition, the input layer of BNNs has gradually become a major compute bottleneck, because it is conventionally excluded from binarization to avoid a large accuracy loss.This work proposes FracBNN, which exploits fractional activations to substantially improve the accuracy of BNNs. Specifically, our approach employs a dual-precision activation scheme to compute features with up to two bits, using an additional sparse binary convolution. We further binarize the input layer using a novel thermometer encoding. Overall, FracBNN preserves the key benefits of conventional BNNs, where all convolutional layers are computed in pure binary MAC operations (BMACs). We design an efficient FPGA-based accelerator for our novel BNN model that supports the fractional activations. To evaluate the performance of FracBNN under a resource-constrained scenario, we implement the entire optimized network architecture on an embedded FPGA (Xilinx Ultra96 v2). Our experiments on ImageNet show that FracBNN achieves an accuracy comparable to MobileNetV2, surpassing the best-known BNN design on FPGAs with an increase of 28.9% in top-1 accuracy and a 2.5x reduction in model size. FracBNN also outperforms a recently introduced BNN model with an increase of 2.4% in top-1 accuracy while using the same model size. On the embedded FPGA device, FracBNN demonstrates the ability of real-time image classification.",,171–182,1,The 2021 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays
48,@article: 10.1145/3418461,Beyond Tests: Program Vulnerability Repair via Crash Constraint Extraction,"Gao, Xiang and Wang, Bo and Duck, Gregory J. and Ji, Ruyi and Xiong, Yingfei and Roychoudhury, Abhik",2021,Association for Computing Machinery,1049-331X,https://doi.org/10.1145/3418461,10.1145/3418461,"Automated program repair is an emerging technology that seeks to automatically rectify program errors and vulnerabilities. Repair techniques are driven by a correctness criterion that is often in the form of a test suite. Such test-based repair may produce overfitting patches, where the patches produced fail on tests outside the test suite driving the repair. In this work, we present a repair method that fixes program vulnerabilities without the need for a voluminous test suite. Given a vulnerability as evidenced by an exploit, the technique extracts a constraint representing the vulnerability with the help of sanitizers. The extracted constraint serves as a proof obligation that our synthesized patch should satisfy. The proof obligation is met by propagating the extracted constraint to locations that are deemed to be “suitable” fix locations. An implementation of our approach (ExtractFix) on top of the KLEE symbolic execution engine shows its efficacy in fixing a wide range of vulnerabilities taken from the ManyBugs benchmark, real-world CVEs and Google’s OSS-Fuzz framework. We believe that our work presents a way forward for the overfitting problem in program repair by generalizing observable hazards/vulnerabilities (as constraint) from a single failing test or exploit.","constraint extraction and propagation, overfitting, Automated program repai",,27,ACM Trans. Softw. Eng. Methodol.
49,@article: 10.1145/3434280,Why My Code Summarization Model Does Not Work: Code Comment Improvement with Category Prediction,"Chen, Qiuyuan and Xia, Xin and Hu, Han and Lo, David and Li, Shanping",2021,Association for Computing Machinery,1049-331X,https://doi.org/10.1145/3434280,10.1145/3434280,"Code summarization aims at generating a code comment given a block of source code and it is normally performed by training machine learning algorithms on existing code block-comment pairs. Code comments in practice have different intentions. For example, some code comments might explain how the methods work, while others explain why some methods are written. Previous works have shown that a relationship exists between a code block and the category of a comment associated with it. In this article, we aim to investigate to which extent we can exploit this relationship to improve code summarization performance. We first classify comments into six intention categories and manually label 20,000 code-comment pairs. These categories include “what,” “why,” “how-to-use,” “how-it-is-done,” “property,” and “others.” Based on this dataset, we conduct an experiment to investigate the performance of different state-of-the-art code summarization approaches on the categories. We find that the performance of different code summarization approaches varies substantially across the categories. Moreover, the category for which a code summarization model performs the best is different for the different models. In particular, no models perform the best for “why” and “property” comments among the six categories. We design a composite approach to demonstrate that comment category prediction can boost code summarization to reach better results. The approach leverages classified code-category labeled data to train a classifier to infer categories. Then it selects the most suitable models for inferred categories and outputs the composite results. Our composite approach outperforms other approaches that do not consider comment categories and obtains a relative improvement of 8.57% and 16.34% in terms of ROUGE-L and BLEU-4 score, respectively.","comment classification, code comment, Code summarizatio",,29,ACM Trans. Softw. Eng. Methodol.
50,@article: 10.1145/3476995,Intermittent-Aware Neural Architecture Search,"Mendis, Hashan Roshantha and Kang, Chih-Kai and Hsiu, Pi-cheng",2021,Association for Computing Machinery,1539-9087,https://doi.org/10.1145/3476995,10.1145/3476995,"The increasing paradigm shift towards intermittent computing has made it possible to intermittently execute deep neural network (DNN) inference on edge devices powered by ambient energy. Recently, neural architecture search (NAS) techniques have achieved great success in automatically finding DNNs with high accuracy and low inference latency on the deployed hardware. We make a key observation, where NAS attempts to improve inference latency by primarily maximizing data reuse, but the derived solutions when deployed on intermittently-powered systems may be inefficient, such that the inference may not satisfy an end-to-end latency requirement and, more seriously, they may be unsafe given an insufficient energy budget. This work proposes iNAS, which introduces intermittent execution behavior into NAS to find accurate network architectures with corresponding execution designs, which can safely and efficiently execute under intermittent power. An intermittent-aware execution design explorer is presented, which finds the right balance between data reuse and the costs related to intermittent inference, and incorporates a preservation design search space into NAS, while ensuring the power-cycle energy budget is not exceeded. To assess an intermittent execution design, an intermittent-aware abstract performance model is presented, which formulates the key costs related to progress preservation and recovery during intermittent inference. We implement iNAS on top of an existing NAS framework and evaluate their respective solutions found for various datasets, energy budgets and latency requirements, on a Texas Instruments device. Compared to those NAS solutions that can safely complete the inference, the iNAS solutions reduce the intermittent inference latency by 60% on average while achieving comparable accuracy, with an average 7% increase in search overhead.","design space exploration, edge computing, energy harvesting, Deep neural networks, neural architecture search, intermittent system",,27,ACM Trans. Embed. Comput. Syst.
51,@inbook: 10.1145/3461001.3471149,The Interplay of Compile-Time and Run-Time Options for Performance Prediction,"Lesoil, Luc and Acher, Mathieu and T\'{e}rnava, Xhevahire and Blouin, Arnaud and J\'{e}z\'{e}quel, Jean-Marc",2021,Association for Computing Machinery,,https://doi.org/10.1145/3461001.3471149,,"Many software projects are configurable through compile-time options (e.g., using ./configure) and also through run-time options (e.g., command-line parameters, fed to the software at execution time). Several works have shown how to predict the effect of run-time options on performance. However it is yet to be studied how these prediction models behave when the software is built with different compile-time options. For instance, is the best run-time configuration always the best w.r.t. the chosen compilation options? In this paper, we investigate the effect of compile-time options on the performance distributions of 4 software systems. There are cases where the compiler layer effect is linear which is an opportunity to generalize performance models or to tune and measure runtime performance at lower cost. We also prove there can exist an interplay by exhibiting a case where compile-time options significantly alter the performance distributions of a configurable system.",,100–111,1,Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A
52,@inproceedings: 10.1145/3472674.3473979,Unsupervised Learning of General-Purpose Embeddings for Code Changes,"Pravilov, Mikhail and Bogomolov, Egor and Golubev, Yaroslav and Bryksin, Timofey",2021,Association for Computing Machinery,,https://doi.org/10.1145/3472674.3473979,10.1145/3472674.3473979,"Applying machine learning to tasks that operate with code changes requires their numerical representation. In this work, we propose an approach for obtaining such representations during pre-training and evaluate them on two different downstream tasks — applying changes to code and commit message generation. During pre-training, the model learns to apply the given code change in a correct way. This task requires only code changes themselves, which makes it unsupervised. In the task of applying code changes, our model outperforms baseline models by 5.9 percentage points in accuracy. As for the commit message generation, our model demonstrated the same results as supervised models trained for this specific task, which indicates that it can encode code changes well and can be improved in the future by pre-training on a larger dataset of easily gathered code changes.","Unsupervised learning, Commit message generation, Code changes",7–12,6,Proceedings of the 5th International Workshop on Machine Learning Techniques for Software Quality Evolution
53,@inproceedings: 10.1145/3472674.3473978,Comparing Within- and Cross-Project Machine Learning Algorithms for Code Smell Detection,"De Stefano, Manuel and Pecorelli, Fabiano and Palomba, Fabio and De Lucia, Andrea",2021,Association for Computing Machinery,,https://doi.org/10.1145/3472674.3473978,10.1145/3472674.3473978,"Code smells represent a well-known problem in software engineering, since they are a notorious cause of loss of comprehensibility and maintainability. The most recent efforts in devising automatic machine learning-based code smell detection techniques have achieved unsatisfying results so far. This could be explained by the fact that all these approaches follow a within-project classification, i.e. training and test data are taken from the same source project, which combined with the imbalanced nature of the problem, produces datasets with a very low number of instances belonging to the minority class (i.e. smelly instances). In this paper, we propose a cross-project machine learning approach and compare its performance with a within-project alternative. The core idea is to use transfer learning to increase the overall number of smelly instances in the training datasets. Our results have shown that cross-project classification provides very similar performance with respect to within-project. Despite this finding does not yet provide a step forward in increasing the performance of ML techniques for code smell detection, it sets the basis for further investigations.","Code smells, Empirical Software Engineering, Transfer Learning",1–6,6,Proceedings of the 5th International Workshop on Machine Learning Techniques for Software Quality Evolution
54,@inbook: 10.1145/3468264.3473921,Domain Adaptation for an Automated Classification of Deontic Modalities in Software Engineering Contracts,"Joshi, Vivek and Anish, Preethu Rose and Ghaisas, Smita",2021,Association for Computing Machinery,,https://doi.org/10.1145/3468264.3473921,,"Contracts are agreements between parties engaging in economic transactions. They specify deontic modalities that the signatories should be held responsible for and state the penalties or actions to be taken if the stated agreements are not met. Additionally, contracts have also been known to be source of Software Engineering (SE) requirements. Identifying the deontic modalities in contracts can therefore add value to the Requirements Engineering (RE) phase of SE. The complex and ambiguous language of contracts make it difficult and time-consuming to identify the deontic modalities (obligations, permissions, prohibitions), embedded in the text. State-of-art neural network models are effective for text classification; however, they require substantial amounts of training data. The availability of contracts data is sparse owing to the confidentiality concerns of customers. In this paper, we leverage the linguistic and taxonomical similarities between regulations (available abundantly in the public domain) and contracts to demonstrate that it is possible to use regulations as training data for classifying deontic modalities in real-life contracts. We discuss the results of a range of experiments from the use of rule-based approach to Bidirectional Encoder Representations from Transformers (BERT) for automating the classification of deontic modalities. With BERT, we obtained an average precision and recall of 90% and 89.66% respectively.",,1275–1280,,Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
55,@inbook: 10.1145/3468264.3468607,StateFormer: Fine-Grained Type Recovery from Binaries Using Generative State Modeling,"Pei, Kexin and Guan, Jonas and Broughton, Matthew and Chen, Zhongtian and Yao, Songchen and Williams-King, David and Ummadisetty, Vikas and Yang, Junfeng and Ray, Baishakhi and Jana, Suman",2021,Association for Computing Machinery,,https://doi.org/10.1145/3468264.3468607,,"Binary type inference is a critical reverse engineering task supporting many security applications, including vulnerability analysis, binary hardening, forensics, and decompilation. It is a difficult task because source-level type information is often stripped during compilation, leaving only binaries with untyped memory and register accesses. Existing approaches rely on hand-coded type inference rules defined by domain experts, which are brittle and require nontrivial effort to maintain and update. Even though machine learning approaches have shown promise at automatically learning the inference rules, their accuracy is still low, especially for optimized binaries.  We present StateFormer, a new neural architecture that is adept at accurate and robust type inference. StateFormer follows a two-step transfer learning paradigm. In the pretraining step, the model is trained with Generative State Modeling (GSM), a novel task that we design to teach the model to statically approximate execution effects of assembly instructions in both forward and backward directions. In the finetuning step, the pretrained model learns to use its knowledge of operational semantics to infer types.  We evaluate StateFormer's performance on a corpus of 33 popular open-source software projects containing over 1.67 billion variables of different types. The programs are compiled with GCC and LLVM over 4 optimization levels O0-O3, and 3 obfuscation passes based on LLVM. Our model significantly outperforms state-of-the-art ML-based tools by 14.6% in recovering types for both function arguments and variables. Our ablation studies show that GSM improves type inference accuracy by 33%.",,690–702,1,Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
56,@inbook: 10.1145/3468264.3473925,A Comprehensive Study on Learning-Based PE Malware Family Classification Methods,"Ma, Yixuan and Liu, Shuang and Jiang, Jiajun and Chen, Guanhong and Li, Keqiu",2021,Association for Computing Machinery,,https://doi.org/10.1145/3468264.3473925,,"Driven by the high profit, Portable Executable (PE) malware has been consistently evolving in terms of both volume and sophistication. PE malware family classification has gained great attention and a large number of approaches have been proposed. With the rapid development of machine learning techniques and the exciting results they achieved on various tasks, machine learning algorithms have also gained popularity in the PE malware family classification task. Three mainstream approaches that use learning based algorithms, as categorized by the input format the methods take, are image-based, binary-based and disassembly-based approaches. Although a large number of approaches are published, there is no consistent comparisons on those approaches, especially from the practical industry adoption perspective. Moreover, there is no comparison in the scenario of concept drift, which is a fact for the malware classification task due to the fast evolving nature of malware. In this work, we conduct a thorough empirical study on learning-based PE malware classification approaches on 4 different datasets and consistent experiment settings. Based on the experiment results and an interview with our industry partners, we find that (1) there is no individual class of methods that significantly outperforms the others; (2) All classes of methods show performance degradation on concept drift (by an average F1-score of 32.23%); and (3) the prediction time and high memory consumption hinder existing approaches from being adopted for industry usage.",,1314–1325,1,Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
57,@inproceedings: 10.1145/3468264.3468604,Data-Driven Accessibility Repair Revisited: On the Effectiveness of Generating Labels for Icons in Android Apps,"Mehralian, Forough and Salehnamadi, Navid and Malek, Sam",2021,Association for Computing Machinery,,https://doi.org/10.1145/3468264.3468604,10.1145/3468264.3468604,"Mobile apps are playing an increasingly important role in our daily lives, including the lives of approximately 304 million users worldwide that are either completely blind or suffer from some form of visual impairment. These users rely on screen readers to interact with apps. Screen readers, however, cannot describe the image icons that appear on the screen, unless those icons are accompanied with developer-provided textual labels. A prior study of over 5,000 Android apps found that in around 50% of the apps, less than 10% of the icons are labeled. To address this problem, a recent award-winning approach, called LabelDroid, employed deep-learning techniques to train a model on a dataset of existing icons with labels to automatically generate labels for visually similar, unlabeled icons. In this work, we empirically study the nature of icon labels in terms of distribution and their dependency on different sources of information. We then assess the effectiveness of LabelDroid in predicting labels for unlabeled icons. We find that icon images are insufficient in representing icon labels, while other sources of information from the icon usage context can enrich images in determining proper tokens for labels. We propose the first context-aware label generation approach, called COALA, that incorporates several sources of information from the icon in generating accurate labels. Our experiments show that although COALA significantly outperforms LabelDroid in both user study and automatic evaluation, further research is needed. We suggest that future studies should be more cautious when basing their approach on automatically extracted labeled data.","Android, Accessibility, Deep Learning, Screen Reader, Alternative Text",107–118,12,Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
58,@inproceedings: 10.1145/3468264.3468569,Validation on Machine Reading Comprehension Software without Annotated Labels: A Property-Based Method,"Chen, Songqiang and Jin, Shuo and Xie, Xiaoyuan",2021,Association for Computing Machinery,,https://doi.org/10.1145/3468264.3468569,10.1145/3468264.3468569,"Machine Reading Comprehension (MRC) in Natural Language Processing has seen great progress recently. But almost all the current MRC software is validated with a reference-based method, which requires well-annotated labels for test cases and tests the software by checking the consistency between the labels and the outputs. However, labeling test cases of MRC could be very costly due to their complexity, which makes reference-based validation hard to be extensible and sufficient. Furthermore, solely checking the consistency and measuring the overall score may not be sensible and flexible for assessing the language understanding capability. In this paper, we propose a property-based validation method for MRC software with Metamorphic Testing to supplement the reference-based validation. It does not refer to the labels and hence can make much data available for testing. Besides, it validates MRC software against various linguistic properties to give a specific and in-depth picture on linguistic capabilities of MRC software. Comprehensive experimental results show that our method can successfully reveal violations to the target linguistic properties without the labels. Moreover, it can reveal problems that have been concealed by the traditional validation. Comparison according to the properties provides deeper and more concrete ideas about different language understanding capabilities of the MRC software.","property-based validation, metamorphic relation, machine reading comprehension, language understanding capability",590–602,13,Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
59,@inproceedings: 10.1145/3468264.3473121,CsDetector: An Open Source Tool for Community Smells Detection,"Almarimi, Nuri and Ouni, Ali and Chouchen, Moataz and Mkaouer, Mohamed Wiem",2021,Association for Computing Machinery,,https://doi.org/10.1145/3468264.3473121,10.1145/3468264.3473121,"Community smells represent symptoms of sub-optimal organizational and social issues within software development communities that often lead to additional project costs and reduced software quality. Previous research identified a variety of community smells that are connected to sub-optimal patterns under different perspectives of organizational-social structures in the software development community. To detect community smells and understanding the characteristics of such organizational-social structures in a project, we propose csDetector, an open source tool that is able to automatically detect community smells within a project and provide relevant socio-technical metrics. csDetector uses a machine learning based detection approach that learns from various existing bad community development practices to provide automated support in detecting related community smells. We evaluate the effectiveness of csDetector on a benchmark of 143 open source projects from GitHub. Our results show that the csDetector tool can detect ten commonly occurring community smells in open software projects with an average F1 score of 84%. csDetector is publicly available, with a demo video, at: https://github.com/Nuri22/csDetector.","Software projects, Detection Tool, community smells",1560–1564,5,Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
60,@inproceedings: 10.1145/3468264.3468623,Semantic Bug Seeding: A Learning-Based Approach for Creating Realistic Bugs,"Patra, Jibesh and Pradel, Michael",2021,Association for Computing Machinery,,https://doi.org/10.1145/3468264.3468623,10.1145/3468264.3468623,"When working on techniques to address the wide-spread problem of software bugs, one often faces the need for a large number of realistic bugs in real-world programs. Such bugs can either help evaluate an approach, e.g., in form of a bug benchmark or a suite of program mutations, or even help build the technique, e.g., in learning-based bug detection. Because gathering a large number of real bugs is difficult, a common approach is to rely on automatically seeded bugs. Prior work seeds bugs based on syntactic transformation patterns, which often results in unrealistic bugs and typically cannot introduce new, application-specific code tokens.  This paper presents SemSeed, a technique for automatically seeding bugs in a semantics-aware way. The key idea is to imitate how a given real-world bug would look like in other programs by semantically adapting the bug pattern to the local context. To reason about the semantics of pieces of code, our approach builds on learned token embeddings that encode the semantic similarities of identifiers and literals. Our evaluation with real-world JavaScript software shows that the approach effectively reproduces real bugs and clearly outperforms a semantics-unaware approach. The seeded bugs are useful as training data for learning-based bug detection, where they significantly improve the bug detection ability. Moreover, we show that SemSeed-created bugs complement existing mutation testing operators, and that our approach is efficient enough to seed hundreds of thousands of bugs within an hour.","bugs, bug injection, token embeddings, machine learning, dataset",906–918,13,Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
61,@inproceedings: 10.1145/3468264.3468553,Automating the Removal of Obsolete TODO Comments,"Gao, Zhipeng and Xia, Xin and Lo, David and Grundy, John and Zimmermann, Thomas",2021,Association for Computing Machinery,,https://doi.org/10.1145/3468264.3468553,10.1145/3468264.3468553,"TODO comments are very widely used by software developers to describe their pending tasks during software development. However, after performing the task developers sometimes neglect or simply forget to remove the TODO comment, resulting in obsolete TODO comments. These obsolete TODO comments can confuse development teams and may cause the introduction of bugs in the future, decreasing the software's quality and maintainability. Manually identifying obsolete TODO comments is time-consuming and expensive. It is thus necessary to detect obsolete TODO comments and remove them automatically before they cause any unwanted side effects. In this work, we propose a novel model, named TDCleaner, to identify obsolete TODO comments in software projects. TDCleaner can assist developers in just-in-time checking of TODO comments status and avoid leaving obsolete TODO comments. Our approach has two main stages: offline learning and online prediction. During offline learning, we first automatically establish <code_change, todo_comment,="""" commit_msg=""""> training samples and leverage three neural encoders to capture the semantic features of TODO comment, code change and commit message respectively. TDCleaner then automatically learns the correlations and interactions between different encoders to estimate the final status of the TODO comment. For online prediction, we check a TODO comment's status by leveraging the offline trained model to judge the TODO comment's likelihood of being obsolete. We built our dataset by collecting TODO comments from the top-10,000 Python and Java Github repositories and evaluated TDCleaner on them. Extensive experimental results show the promising performance of our model over a set of benchmarks. We also performed an in-the-wild evaluation with real-world software projects, we reported 18 obsolete TODO comments identified by TDCleaner to Github developers and 9 of them have already been confirmed and removed by the developers, demonstrating the practical usage of our approach.","TODO comment, Code-Comment Inconsistency, Bert",218–229,12,Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
62,@inproceedings: 10.1145/3468264.3468603,Generalizable and Interpretable Learning for Configuration Extrapolation,"Ding, Yi and Pervaiz, Ahsan and Carbin, Michael and Hoffmann, Henry",2021,Association for Computing Machinery,,https://doi.org/10.1145/3468264.3468603,10.1145/3468264.3468603,"Modern software applications are increasingly configurable, which puts a burden on users to tune these configurations for their target hardware and workloads. To help users, machine learning techniques can model the complex relationships between software configuration parameters and performance. While powerful, these learners have two major drawbacks: (1) they rarely incorporate prior knowledge and (2) they produce outputs that are not interpretable by users. These limitations make it difficult to (1) leverage information a user has already collected (e.g., tuning for new hardware using the best configurations from old hardware) and (2) gain insights into the learner’s behavior (e.g., understanding why the learner chose different configurations on different hardware or for different workloads). To address these issues, this paper presents two configuration optimization tools, GIL and GIL+, using the proposed generalizable and interpretable learning approaches. To incorporate prior knowledge, the proposed tools (1) start from known configurations, (2) iteratively construct a new linear model, (3) extrapolate better performance configurations from that model, and (4) repeat. Since the base learners are linear models, these tools are inherently interpretable. We enhance this property with a graphical representation of how they arrived at the highest performance configuration. We evaluate GIL and GIL+ by using them to configure Apache Spark workloads on different hardware platforms and find that, compared to prior work, GIL and GIL+ produce comparable, and sometimes even better performance configurations, but with interpretable results.","machine learning, generalizability, Configuration, interpretability",728–740,13,Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
63,@inproceedings: 10.1145/3468264.3473114,Code2Que: A Tool for Improving Question Titles from Mined Code Snippets in Stack Overflow,"Gao, Zhipeng and Xia, Xin and Lo, David and Grundy, John and Li, Yuan-Fang",2021,Association for Computing Machinery,,https://doi.org/10.1145/3468264.3473114,10.1145/3468264.3473114,"Stack Overflow is one of the most popular technical Q&amp;A sites used by software developers. Seeking help from Stack Overflow has become an essential part of software developers’ daily work for solving programming-related questions. Although the Stack Overflow community has provided quality assurance guidelines to help users write better questions, we observed that a significant number of questions submitted to Stack Overflow are of low quality. In this paper, we introduce a new web-based tool, Code2Que, which can help developers in writing higher quality questions for a given code snippet. Code2Que consists of two main stages: offline learning and online recommendation. In the offline learning phase, we first collect a set of good quality ⟨code snippet, question⟩ pairs as training samples. We then train our model on these training samples via a deep sequence-to-sequence approach, enhanced with an attention mechanism, a copy mechanism and a coverage mechanism. In the online recommendation phase, for a given code snippet, we use the offline trained model to generate question titles to assist less experienced developers in writing questions more effectively. To evaluate Code2Que, we first sampled 50 low quality ⟨code snippet, question⟩ pairs from the Python and Java datasets on Stack Overflow. Then we conducted a user study to evaluate the question titles generated by our approach as compared to human-written ones using three metrics: Clearness, Fitness and Willingness to Respond. Our experimental results show that for a large number of low-quality questions in Stack Overflow, Code2Que can improve the question titles in terms of Clearness, Fitness and Willingness measures.","Seq2Seq Model, Question Quality, Stack Overflow, Deep Learning",1525–1529,5,Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
64,@inbook: 10.1145/3468264.3468563,Sustainability Forecasting for Apache Incubator Projects,"Yin, Likang and Chen, Zhuangzhi and Xuan, Qi and Filkov, Vladimir",2021,Association for Computing Machinery,,https://doi.org/10.1145/3468264.3468563,,"Although OSS development is very popular, ultimately more than 80% of OSS projects fail. Identifying the factors associated with OSS success can help in devising interventions when a project takes a downturn. OSS success has been studied from a variety of angles, more recently in empirical studies of large numbers of diverse projects, using proxies for sustainability, e.g., internal metrics related to productivity and external ones, related to community popularity. The internal socio-technical structure of projects has also been shown important, especially their dynamics. This points to another angle on evaluating software success, from the perspective of self-sustaining and self-governing communities. To uncover the dynamics of how a project at a nascent development stage gradually evolves into a sustainable one, here we apply a socio-technical network modeling perspective to a dataset of Apache Software Foundation Incubator (ASFI), sustainability-labeled projects. To identify and validate the determinants of sustainability, we undertake a mix of quantitative and qualitative studies of ASFI projects’ socio-technical network trajectories. We develop interpretable models which can forecast a project becoming sustainable with 93+% accuracy, within 8 months of incubation start. Based on the interpretable models we describe a strategy for real-time monitoring and suggesting actions, which can be used by projects to correct their sustainability trajectories.",,1056–1067,1,Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
65,@inproceedings: 10.1145/3468264.3473117,BiasRV: Uncovering Biased Sentiment Predictions at Runtime,"Yang, Zhou and Asyrofi, Muhammad Hilmi and Lo, David",2021,Association for Computing Machinery,,https://doi.org/10.1145/3468264.3473117,10.1145/3468264.3473117,"Sentiment analysis (SA) systems, though widely applied in many domains, have been demonstrated to produce biased results. Some research works have been done in automatically generating test cases to reveal unfairness in SA systems, but the community still lacks tools that can monitor and uncover biased predictions at runtime. This paper fills this gap by proposing BiasRV, the first tool to raise an alarm when a deployed SA system makes a biased prediction on a given input text. To implement this feature, BiasRV dynamically extracts a template from an input text and generates gender-discriminatory mutants (semantically-equivalent texts that only differ in gender information) from the template. Based on popular metrics used to evaluate the overall fairness of an SA system, we define the distributional fairness property for an individual prediction of an SA system. This property specifies a requirement that for one piece of text, mutants from different gender classes should be treated similarly. Verifying the distributional fairness property causes much overhead to the running system. To run more efficiently, BiasRV adopts a two-step heuristic: (1) sampling several mutants from each gender and checking if the system predicts them as of the same sentiment, (2) checking distributional fairness only when sampled mutants have conflicting results. Experiments show that when compared to directly checking the distributional fairness property for each input text, our two-step heuristic can decrease the overhead used for analyzing mutants by 73.81% while only resulting in 6.7% of biased predictions being missed. Besides, BiasRV can be used conveniently without knowing the implementation of SA systems. Future researchers can easily extend BiasRV to detect more types of bias, e.g., race and occupation. The demo video for BiasRV can be viewed at https://youtu.be/WPe4Ml77d3U and the source code can be found at https://github.com/soarsmu/BiasRV.","Runtime Verification, Ethical AI, Fairness, Sentiment Analysis",1540–1544,5,Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
66,@inbook: 10.1145/3468264.3468618,Learning-Based Extraction of First-Order Logic Representations of API Directives,"Liu, Mingwei and Peng, Xin and Marcus, Andrian and Treude, Christoph and Bai, Xuefang and Lyu, Gang and Xie, Jiazhan and Zhang, Xiaoxin",2021,Association for Computing Machinery,,https://doi.org/10.1145/3468264.3468618,,"Developers often rely on API documentation to learn API directives, i.e., constraints and guidelines related to API usage. Failing to follow API directives may cause defects or improper implementations. Since there are no industry-wide standards on how to document API directives, they take many forms and are often hard to understand by developers or challenging to parse with tools.  In this paper, we propose a learning based approach for extracting first-order logic representations of API directives (FOL directives for short). The approach, called LEADFOL, uses a joint learning method to extract atomic formulas by identifying the predicates and arguments involved in directive sentences, and recognizes the logical relations between atomic formulas, by parsing the sentence structures. It then parses the arguments and uses a learning based method to link API references to their corresponding API elements. Finally, it groups the formulas of the same class or method together and transforms them into conjunctive normal form. Our evaluation shows that LEADFOL can accurately extract more FOL directives than a state-of-the-art approach and that the extracted FOL directives are useful in supporting code reviews.",,491–502,1,Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
67,@inproceedings: 10.1145/3468264.3473935,An Empirical Study of GUI Widget Detection for Industrial Mobile Games,"Ye, Jiaming and Chen, Ke and Xie, Xiaofei and Ma, Lei and Huang, Ruochen and Chen, Yingfeng and Xue, Yinxing and Zhao, Jianjun",2021,Association for Computing Machinery,,https://doi.org/10.1145/3468264.3473935,10.1145/3468264.3473935,"With the widespread adoption of smartphones in our daily life, mobile games experienced increasing demand over the past years. Meanwhile, the quality of mobile games has been continuously drawing more and more attention, which can greatly affect the player experience. For better quality assurance, general-purpose testing has been extensively studied for mobile apps. However, due to the unique characteristic of mobile games, existing mobile testing techniques may not be directly suitable and applicable. To better understand the challenges in mobile game testing, in this paper, we first initiate an early step to conduct an empirical study towards understanding the challenges and pain points of mobile game testing process at our industrial partner NetEase Games. Specifically, we first conduct a survey from the mobile test development team at NetEase Games via both scrum interviews and questionnaires. We found that accurate and effective GUI widget detection for mobile games could be the pillar to boost the automation of mobile game testing and other downstream analysis tasks in practice.  We then continue to perform comparative studies to investigate the effectiveness of state-of-the-art general-purpose mobile app GUI widget detection methods in the context of mobile games. To this end, we also develop a technique to automatically collect GUI widgets region information of industrial mobile games, which is equipped with a heuristic-based data cleaning method for quality refinement of the labeling results. Our evaluation shows that: (1) Existing GUI widget detection methods for general-purpose mobile apps cannot perform well on industrial mobile games. (2) Mobile game exhibits obvious difference from other general-purpose mobile apps in the perspective GUI widgets. Our further in-depth analysis reveals high diversity and density characteristics of mobile game GUI widgets could be the major reasons that post the challenges for existing methods, which calls for new research methods and better industry practices. To enable further research along this line, we construct the very first GUI widget detection benchmark, specially designed for mobile games, incorporating both our collected dataset and the state-of-the-art widget detection methods for mobile apps, which could also be the basis for further study of many downstream quality assurance tasks (e.g., testing and analysis) for mobile games.","Game Testing, Deep Learning, GUI Detection",1427–1437,11,Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
68,@inproceedings: 10.1145/3468264.3468539,Understanding Neural Code Intelligence through Program Simplification,"Rabin, Md Rafiqul Islam and Hellendoorn, Vincent J. and Alipour, Mohammad Amin",2021,Association for Computing Machinery,,https://doi.org/10.1145/3468264.3468539,10.1145/3468264.3468539,"A wide range of code intelligence (CI) tools, powered by deep neural networks, have been developed recently to improve programming productivity and perform program analysis. To reliably use such tools, developers often need to reason about the behavior of the underlying models and the factors that affect them. This is especially challenging for tools backed by deep neural networks. Various methods have tried to reduce this opacity in the vein of ""transparent/interpretable-AI"". However, these approaches are often specific to a particular set of network architectures, even requiring access to the network's parameters. This makes them difficult to use for the average programmer, which hinders the reliable adoption of neural CI systems. In this paper, we propose a simple, model-agnostic approach to identify critical input features for models in CI systems, by drawing on software debugging research, specifically delta debugging. Our approach, SIVAND, uses simplification techniques that reduce the size of input programs of a CI model while preserving the predictions of the model. We show that this approach yields remarkably small outputs and is broadly applicable across many model architectures and problem domains. We find that the models in our experiments often rely heavily on just a few syntactic features in input programs. We believe that SIVAND's extracted features may help understand neural CI systems' predictions and learned behavior.","Models of Code, Program Simplification, Interpretable AI",441–452,12,Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
69,@inproceedings: 10.1145/3468264.3468593,Symbolic Parallel Adaptive Importance Sampling for Probabilistic Program Analysis,"Luo, Yicheng and Filieri, Antonio and Zhou, Yuan",2021,Association for Computing Machinery,,https://doi.org/10.1145/3468264.3468593,10.1145/3468264.3468593,"Probabilistic software analysis aims at quantifying the probability of a target event occurring during the execution of a program processing uncertain incoming data or written itself using probabilistic programming constructs. Recent techniques combine symbolic execution with model counting or solution space quantification methods to obtain accurate estimates of the occurrence probability of rare target events, such as failures in a mission-critical system. However, they face several scalability and applicability limitations when analyzing software processing with high-dimensional and correlated multivariate input distributions. In this paper, we present SYMbolic Parallel Adaptive Importance Sampling (SYMPAIS), a new inference method tailored to analyze path conditions generated from the symbolic execution of programs with high-dimensional, correlated input distributions. SYMPAIS combines results from importance sampling and constraint solving to produce accurate estimates of the satisfaction probability for a broad class of constraints that cannot be analyzed by current solution space quantification methods. We demonstrate SYMPAIS's generality and performance compared with state-of-the-art alternatives on a set of problems from different application domains.","Markov chain Monte Carlo, symbolic execution, probabilistic analysis, probabilistic programming, importance sampling",1166–1177,12,Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
70,@inbook: 10.1145/3468264.3468588,Reassessing Automatic Evaluation Metrics for Code Summarization Tasks,"Roy, Devjeet and Fakhoury, Sarah and Arnaoudova, Venera",2021,Association for Computing Machinery,,https://doi.org/10.1145/3468264.3468588,,"In recent years, research in the domain of source code summarization has adopted data-driven techniques pioneered in machine translation (MT). Automatic evaluation metrics such as BLEU, METEOR, and ROUGE, are fundamental to the evaluation of MT systems and have been adopted as proxies of human evaluation in the code summarization domain. However, the extent to which automatic metrics agree with the gold standard of human evaluation has not been evaluated on code summarization tasks. Despite this, marginal improvements in metric scores are often used to discriminate between the performance of competing summarization models. In this paper, we present a critical exploration of the applicability and interpretation of automatic metrics as evaluation techniques for code summarization tasks. We conduct an empirical study with 226 human annotators to assess the degree to which automatic metrics reflect human evaluation. Results indicate that metric improvements of less than 2 points do not guarantee systematic improvements in summarization quality, and are unreliable as proxies of human evaluation. When the difference between metric scores for two summarization approaches increases but remains within 5 points, some metrics such as METEOR and chrF become highly reliable proxies, whereas others, such as corpus BLEU, remain unreliable. Based on these findings, we make several recommendations for the use of automatic metrics to discriminate model performance in code summarization.",,1105–1116,1,Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
71,@inproceedings: 10.1145/3468264.3468611,Empirical Study of Transformers for Source Code,"Chirkova, Nadezhda and Troshin, Sergey",2021,Association for Computing Machinery,,https://doi.org/10.1145/3468264.3468611,10.1145/3468264.3468611,"Initially developed for natural language processing (NLP), Transformers are now widely used for source code processing, due to the format similarity between source code and text. In contrast to natural language, source code is strictly structured, i.e., it follows the syntax of the programming language. Several recent works develop Transformer modifications for capturing syntactic information in source code. The drawback of these works is that they do not compare to each other and consider different tasks. In this work, we conduct a thorough empirical study of the capabilities of Transformers to utilize syntactic information in different tasks. We consider three tasks (code completion, function naming and bug fixing) and re-implement different syntax-capturing modifications in a unified framework. We show that Transformers are able to make meaningful predictions based purely on syntactic information and underline the best practices of taking the syntactic information into account for improving the performance of the model.","transformer, variable misuse detection, code completion, neural networks, function naming",703–715,13,Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
72,@inproceedings: 10.1145/3475960.3475988,Multi-Stream Online Transfer Learning for Software Effort Estimation: Is It Necessary?,"Minku, Leandro L.",2021,Association for Computing Machinery,,https://doi.org/10.1145/3475960.3475988,10.1145/3475960.3475988,"Software Effort Estimation (SEE) may suffer from changes in the relationship between features describing software projects and their required effort over time, hindering predictive performance of machine learning models. To cope with that, most machine learning-based SEE approaches rely on receiving a large number of Within-Company (WC) projects for training over time, being prohibitively expensive. The approach Dycom reduces the number of required WC training projects by transferring knowledge from Cross-Company (CC) projects. However, it assumes that CC projects have no chronology and are entirely available before WC projects start being estimated. Given the importance of taking chronology into account to cope with changes, it may be beneficial to also take the chronology of CC projects into account. This paper thus investigates whether and under what circumstances treating CC projects as multiple data streams to be learned over time may be useful for improving SEE. For that, an extension of Dycom called OATES is proposed to enable multi-stream online learning, so that both incoming WC and CC data streams can be learnt over time. OATES is then compared against Dycom and five other approaches on a case study using four different scenarios derived from the ISBSG Repository. The results show that OATES improved predictive performance over the state-of-the-art when the number of CC projects available beforehand was small. Learning CC projects over time as multiple data streams is thus recommended for improving SEE in such scenario. When the number of CC projects available beforehand was large, OATES obtained similar predictive performance to the state-of-the-art. Therefore, CC data streams are unnecessary in this scenario, but are not detrimental either.","ensembles, data streams, Software effort estimation, transfer learning, cross-company learning, concept drift",11–20,10,Proceedings of the 17th International Conference on Predictive Models and Data Analytics in Software Engineering
73,@article: 10.1145/3453478,Understanding Software-2.0: A Study of Machine Learning Library Usage and Evolution,"Dilhara, Malinda and Ketkar, Ameya and Dig, Danny",2021,Association for Computing Machinery,1049-331X,https://doi.org/10.1145/3453478,10.1145/3453478,"Enabled by a rich ecosystem of Machine Learning (ML) libraries, programming using learned models, i.e., Software-2.0, has gained substantial adoption. However, we do not know what challenges developers encounter when they use ML libraries. With this knowledge gap, researchers miss opportunities to contribute to new research directions, tool builders do not invest resources where automation is most needed, library designers cannot make informed decisions when releasing ML library versions, and developers fail to use common practices when using ML libraries.We present the first large-scale quantitative and qualitative empirical study to shed light on how developers in Software-2.0 use ML libraries, and how this evolution affects their code. Particularly, using static analysis we perform a longitudinal study of 3,340 top-rated open-source projects with 46,110 contributors. To further understand the challenges of ML library evolution, we survey 109 developers who introduce and evolve ML libraries. Using this rich dataset we reveal several novel findings.Among others, we found an increasing trend of using ML libraries: The ratio of new Python projects that use ML libraries increased from 2% in 2013 to 50% in 2018. We identify several usage patterns including the following: (i) 36% of the projects use multiple ML libraries to implement various stages of the ML workflows, (ii) developers update ML libraries more often than the traditional libraries, (iii) strict upgrades are the most popular for ML libraries among other update kinds, (iv) ML library updates often result in cascading library updates, and (v) ML libraries are often downgraded (22.04% of cases). We also observed unique challenges when evolving and maintaining Software-2.0 such as (i) binary incompatibility of trained ML models and (ii) benchmarking ML models. Finally, we present actionable implications of our findings for researchers, tool builders, developers, educators, library vendors, and hardware vendors.","Machine learning libraries, Software-2.0, empirical studie",,42,ACM Trans. Softw. Eng. Methodol.
74,@article: 10.1145/3460352,SLO-Aware Inference Scheduler for Heterogeneous Processors in Edge Platforms,"Seo, Wonik and Cha, Sanghoon and Kim, Yeonjae and Huh, Jaehyuk and Park, Jongse",2021,Association for Computing Machinery,1544-3566,https://doi.org/10.1145/3460352,10.1145/3460352,"With the proliferation of applications with machine learning (ML), the importance of edge platforms has been growing to process streaming sensor, data locally without resorting to remote servers. Such edge platforms are commonly equipped with heterogeneous computing processors such as GPU, DSP, and other accelerators, but their computational and energy budget are severely constrained compared to the data center servers. However, as an edge platform must perform the processing of multiple machine learning models concurrently for multimodal sensor data, its scheduling problem poses a new challenge to map heterogeneous machine learning computation to heterogeneous computing processors. Furthermore, processing of each input must provide a certain level of bounded response latency, making the scheduling decision critical for the edge platform. This article proposes a set of new heterogeneity-aware ML inference scheduling policies for edge platforms. Based on the regularity of computation in common ML tasks, the scheduler uses the pre-profiled behavior of each ML model and routes requests to the most appropriate processors. It also aims to satisfy the service-level objective (SLO) requirement while reducing the energy consumption for each request. For such SLO supports, the challenge of ML computation on GPUs and DSP is its inflexible preemption capability. To avoid the delay caused by a long task, the proposed scheduler decomposes a large ML task to sub-tasks by its layer in the DNN model.","heterogeneous computing, machine learning, Edge computing, task scheduling, inferenc",,26,ACM Trans. Archit. Code Optim.
75,@inproceedings: 10.1145/3464970.3468413,First-Class Concepts: Reifying Architectural Knowledge beyond the Dominant Decomposition,"Mattis, Toni and Beckmann, Tom and Rein, Patrick and Hirschfeld, Robert",2021,Association for Computing Machinery,,https://doi.org/10.1145/3464970.3468413,10.1145/3464970.3468413,"In software engineering, programs are ideally partitioned into independently maintainable and understandable modules. As a system grows, its architecture gradually loses the capability to modularly accommodate new concepts. While refactoring is expensive and the language might lack appropriate primary language constructs to express certain cross-cutting concerns, programmers are still able to explain and delineate convoluted concepts through secondary means: code comments, use of whitespace and arrangement of code, documentation, or communicating tacit knowledge. Secondary constructs are easy to change and provide high flexibility in communicating cross-cutting concerns and other concepts among programmers. However, they have no reified representation that can be explored and maintained through tools. In this exploratory work, we discuss novel ways to express a wide range of concepts, including cross-cutting concerns, patterns, and lifecycle artifacts independently of the dominant decomposition imposed by an existing architecture. Our concepts are first-class objects inside the programming environment that retain the capability to change as easily as code comments. We explore new tools that allow programmers to view and change programs from conceptual perspectives rather than scattering their attention across existing modules. Our designs are geared towards facilitating multiple secondary perspectives on a system to co-exist alongside the original architecture, hence making it easier to explore, understand, and explain complex contexts and narratives not expressible in traditional modularity constructs.","software engineering, program comprehension, remodularization, modularity, exploratory programming",9–15,7,Proceedings of the 13th ACM International Workshop on Context-Oriented Programming and Advanced Modularity
76,@inbook: 10.1145/3460319.3464816,ModelDiff: Testing-Based DNN Similarity Comparison for Model Reuse Detection,"Li, Yuanchun and Zhang, Ziqi and Liu, Bingyan and Yang, Ziyue and Liu, Yunxin",2021,Association for Computing Machinery,,https://doi.org/10.1145/3460319.3464816,,"The knowledge of a deep learning model may be transferred to a student model, leading to intellectual property infringement or vulnerability propagation. Detecting such knowledge reuse is nontrivial because the suspect models may not be white-box accessible and/or may serve different tasks. In this paper, we propose ModelDiff, a testing-based approach to deep learning model similarity comparison. Instead of directly comparing the weights, activations, or outputs of two models, we compare their behavioral patterns on the same set of test inputs. Specifically, the behavioral pattern of a model is represented as a decision distance vector (DDV), in which each element is the distance between the model's reactions to a pair of inputs. The knowledge similarity between two models is measured with the cosine similarity between their DDVs. To evaluate ModelDiff, we created a benchmark that contains 144 pairs of models that cover most popular model reuse methods, including transfer learning, model compression, and model stealing. Our method achieved 91.7% correctness on the benchmark, which demonstrates the effectiveness of using ModelDiff for model reuse detection. A study on mobile deep learning apps has shown the feasibility of ModelDiff on real-world models.",,139–151,1,Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis
77,@inbook: 10.1145/3460319.3464829,DialTest: Automated Testing for Recurrent-Neural-Network-Driven Dialogue Systems,"Liu, Zixi and Feng, Yang and Chen, Zhenyu",2021,Association for Computing Machinery,,https://doi.org/10.1145/3460319.3464829,,"With the tremendous advancement of recurrent neural networks(RNN), dialogue systems have achieved significant development. Many RNN-driven dialogue systems, such as Siri, Google Home, and Alexa, have been deployed to assist various tasks. However, accompanying this outstanding performance, RNN-driven dialogue systems, which are essentially a kind of software, could also produce erroneous behaviors and result in massive losses. Meanwhile, the complexity and intractability of RNN models that power the dialogue systems make their testing challenging. In this paper, we design and implement DialTest, the first RNN-driven dialogue system testing tool. DialTest employs a series of transformation operators to make realistic changes on seed data while preserving their oracle information properly. To improve the efficiency of detecting faults, DialTest further adopts Gini impurity to guide the test generation process. We conduct extensive experiments to validate DialTest. We first experiment it on two fundamental tasks, i.e., intent detection and slot filling, of natural language understanding. The experiment results show that DialTest can effectively detect hundreds of erroneous behaviors for different RNN-driven natural language understanding (NLU) modules of dialogue systems and improve their accuracy via retraining with the generated data. Further, we conduct a case study on an industrial dialogue system to investigate the performance of DialTest under the real usage scenario. The study shows DialTest can detect errors and improve the robustness of RNN-driven dialogue systems effectively.",,115–126,1,Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis
78,@inproceedings: 10.1145/3460319.3464825,DeepCrime: Mutation Testing of Deep Learning Systems Based on Real Faults,"Humbatova, Nargiz and Jahangirova, Gunel and Tonella, Paolo",2021,Association for Computing Machinery,,https://doi.org/10.1145/3460319.3464825,10.1145/3460319.3464825,"Deep Learning (DL) solutions are increasingly adopted, but how to test them remains a major open research problem. Existing and new testing techniques have been proposed for and adapted to DL systems, including mutation testing. However, no approach has investigated the possibility to simulate the effects of real DL faults by means of mutation operators. We have defined 35 DL mutation operators relying on 3 empirical studies about real faults in DL systems. We followed a systematic process to extract the mutation operators from the existing fault taxonomies, with a formal phase of conflict resolution in case of disagreement. We have implemented 24 of these DL mutation operators into DeepCrime, the first source-level pre-training mutation tool based on real DL faults. We have assessed our mutation operators to understand their characteristics: whether they produce interesting, i.e., killable but not trivial, mutations. Then, we have compared the sensitivity of our tool to the changes in the quality of test data with that of DeepMutation++, an existing post-training DL mutation tool.","real faults, deep learning, mutation testing",67–78,12,Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis
79,@inproceedings: 10.1145/3460319.3464827,Semantic Matching of GUI Events for Test Reuse: Are We There Yet?,"Mariani, Leonardo and Mohebbi, Ali and Pezz\`{e}, Mauro and Terragni, Valerio",2021,Association for Computing Machinery,,https://doi.org/10.1145/3460319.3464827,10.1145/3460319.3464827,"GUI testing is an important but expensive activity. Recently, research on test reuse approaches for Android applications produced interesting results. Test reuse approaches automatically migrate human-designed GUI tests from a source app to a target app that shares similar functionalities. They achieve this by exploiting semantic similarity among textual information of GUI widgets. Semantic matching of GUI events plays a crucial role in these approaches. In this paper, we present the first empirical study on semantic matching of GUI events. Our study involves 253 configurations of the semantic matching, 337 unique queries, and 8,099 distinct GUI events. We report several key findings that indicate how to improve semantic matching of test reuse approaches, propose SemFinder a novel semantic matching algorithm that outperforms existing solutions, and identify several interesting research directions.","GUI testing, Android applications, test reuse, word embedding, mobile testing, NLP",177–190,14,Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis
80,@inproceedings: 10.1145/3460319.3464801,Exposing Previously Undetectable Faults in Deep Neural Networks,"Dunn, Isaac and Pouget, Hadrien and Kroening, Daniel and Melham, Tom",2021,Association for Computing Machinery,,https://doi.org/10.1145/3460319.3464801,10.1145/3460319.3464801,"Existing methods for testing DNNs solve the oracle problem by constraining the raw features (e.g. image pixel values) to be within a small distance of a dataset example for which the desired DNN output is known. But this limits the kinds of faults these approaches are able to detect. In this paper, we introduce a novel DNN testing method that is able to find faults in DNNs that other methods cannot. The crux is that, by leveraging generative machine learning, we can generate fresh test inputs that vary in their high-level features (for images, these include object shape, location, texture, and colour). We demonstrate that our approach is capable of detecting deliberately injected faults as well as new faults in state-of-the-art DNNs, and that in both cases, existing methods are unable to find these faults.","Deep Learning, Robustness, Generative Adversarial Networks",56–66,11,Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis
81,@inproceedings: 10.1145/3460319.3464804,A Lightweight Framework for Function Name Reassignment Based on Large-Scale Stripped Binaries,"Gao, Han and Cheng, Shaoyin and Xue, Yinxing and Zhang, Weiming",2021,Association for Computing Machinery,,https://doi.org/10.1145/3460319.3464804,10.1145/3460319.3464804,"Software in the wild is usually released as stripped binaries that contain no debug information (e.g., function names). This paper studies the issue of reassigning descriptive names for functions to help facilitate reverse engineering. Since the essence of this issue is a data-driven prediction task, persuasive research should be based on sufficiently large-scale and diverse data. However, prior studies can only be based on small-scale datasets because their techniques suffer from heavyweight binary analysis, making them powerless in the face of big-size and large-scale binaries.  This paper presents the Neural Function Rename Engine (NFRE), a lightweight framework for function name reassignment that utilizes both sequential and structural information of assembly code. NFRE uses fine-grained and easily acquired features to model assembly code, making it more effective and efficient than existing techniques. In addition, we construct a large-scale dataset and present two data-preprocessing approaches to help improve its usability. Benefiting from the lightweight design, NFRE can be efficiently trained on the large-scale dataset, thereby having better generalization capability for unknown functions. The comparative experiments show that NFRE outperforms two existing techniques by a relative improvement of 32% and 16%, respectively, while the time cost for binary analysis is much less.","Binary Analysis, Neural Networks, Reverse Engineering",607–619,13,Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis
82,@inproceedings: 10.1145/3460319.3469078,SCStudio: A Secure and Efficient Integrated Development Environment for Smart Contracts,"Ren, Meng and Ma, Fuchen and Yin, Zijing and Li, Huizhong and Fu, Ying and Chen, Ting and Jiang, Yu",2021,Association for Computing Machinery,,https://doi.org/10.1145/3460319.3469078,10.1145/3460319.3469078,"With the increasing popularity of block-chain technologies, more and more engineers use smart contracts for application implementation. Traditional supporting tools can either provide code completions based on static libraries or detect a limited set of vulnerabilities, which results in the manpower waste during coding and miss-detection of bugs. In this work, we propose SCStudio, a unified smart contract development platform, which aims to help developers implement more secure smart contracts easily. The core idea is to realize real-time security-reinforced recommendation through pattern-based learning; and to perform security-oriented validation via integrated testing. SCStudio was implemented as a plug-in of VS Code. It has been used as the official development tool of WeBank and integrated as the recommended development tool by FISCO-BCOS community. In practice, it outperforms existing contract development environments, such as Remix, improving the average word suggestion accuracy by 30%-60% and helping detect about 25% more vulnerabilities.  The video is presented at https://youtu.be/l6hW3Ds5Tkg.","Smart Contract, Security-Reinforced Code Suggestion, Validation",666–669,4,Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis
83,@inbook: 10.1145/3460319.3464843,Predoo: Precision Testing of Deep Learning Operators,"Zhang, Xufan and Sun, Ning and Fang, Chunrong and Liu, Jiawei and Liu, Jia and Chai, Dong and Wang, Jiang and Chen, Zhenyu",2021,Association for Computing Machinery,,https://doi.org/10.1145/3460319.3464843,,"Deep learning(DL) techniques attract people from various fields with superior performance in making progressive breakthroughs. To ensure the quality of DL techniques, researchers have been working on testing and verification approaches. Some recent studies reveal that the underlying DL operators could cause defects inside a DL model. DL operators work as fundamental components in DL libraries. Library developers still work on practical approaches to ensure the quality of operators they provide. However, the variety of DL operators and the implementation complexity make it challenging to evaluate their quality. Operator testing with limited test cases may fail to reveal hidden defects inside the implementation. Besides, the existing model-to-library testing approach requires extra labor and time cost to identify and locate errors, i.e., developers can only react to the exposed defects. This paper proposes a fuzzing-based operator-level precision testing approach to estimate individual DL operators' precision errors to bridge this gap. Unlike conventional fuzzing techniques, valid shape variable inputs and fine-grained precision error evaluation are implemented. The testing of DL operators is treated as a searching problem to maximize output precision errors. We implement our approach in a tool named Predoo and conduct an experiment on seven DL operators from TensorFlow. The experiment result shows that Predoo can trigger larger precision errors compared to the error threshold declared in the testing scripts from the TensorFlow repository.",,400–412,1,Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis
84,@inbook: 10.1145/3460319.3464815,Synthesize Solving Strategy for Symbolic Execution,"Chen, Zhenbang and Chen, Zehua and Shuai, Ziqi and Zhang, Guofeng and Pan, Weiyu and Zhang, Yufeng and Wang, Ji",2021,Association for Computing Machinery,,https://doi.org/10.1145/3460319.3464815,,"Symbolic execution is powered by constraint solving. The advancement of constraint solving boosts the development and the applications of symbolic execution. Modern SMT solvers provide the mechanism of solving strategy that allows the users to control the solving procedure, which significantly improves the solver's generalization ability. We observe that the symbolic executions of different programs are actually different constraint solving problems. Therefore, we propose synthesizing a solving strategy for a program to fit the program's symbolic execution best. To achieve this, we divide symbolic execution into two stages. The SMT formulas solved in the first stage are used to online synthesize a solving strategy, which is then employed during the constraint solving in the second stage. We propose novel synthesis algorithms that combine offline trained deep learning models and online tuning to synthesize the solving strategy. The algorithms balance the synthesis overhead and the improvement achieved by the synthesized solving strategy.  We have implemented our method on the state-of-the-art symbolic execution engine KLEE for C programs. The results of the extensive experiments indicate that our method effectively improves the efficiency of symbolic execution. On average, our method increases the numbers of queries and paths by 58.76% and 66.11%, respectively. Besides, we applied our method to a Java Pathfinder-based concolic execution engine to validate the generalization ability. The results indicate that our method has a good generalization ability and increases the numbers of queries and paths by 100.24% and 102.6% for the benchmark Java programs, respectively.",,348–360,1,Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis
85,@inproceedings: 10.1145/3460319.3464809,AdvDoor: Adversarial Backdoor Attack of Deep Learning System,"Zhang, Quan and Ding, Yifeng and Tian, Yongqiang and Guo, Jianmin and Yuan, Min and Jiang, Yu",2021,Association for Computing Machinery,,https://doi.org/10.1145/3460319.3464809,10.1145/3460319.3464809,"Deep Learning (DL) system has been widely used in many critical applications, such as autonomous vehicles and unmanned aerial vehicles. However, their security is threatened by backdoor attack, which is achieved by adding artificial patterns on specific training data. Existing attack methods normally poison the data using a patch, and they can be easily detected by existing detection methods. In this work, we propose the Adversarial Backdoor, which utilizes the Targeted Universal Adversarial Perturbation (TUAP) to hide the anomalies in DL models and confuse existing powerful detection methods. With extensive experiments, it is demonstrated that Adversarial Backdoor can be injected stably with an attack success rate around 98%. Moreover, Adversarial Backdoor can bypass state-of-the-art backdoor detection methods. More specifically, only around 37% of the poisoned models can be caught, and less than 29% of the poisoned data cannot bypass the detection. In contrast, for the patch backdoor, all the poisoned models and more than 80% of the poisoned data will be detected. This work intends to alarm the researchers and developers of this potential threat and to inspire the designing of effective detection methods.","Backdoor Attack, Deep Learning System, Adversarial Attack",127–138,12,Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis
86,@inproceedings: 10.1145/3460319.3464822,Attack as Defense: Characterizing Adversarial Examples Using Robustness,"Zhao, Zhe and Chen, Guangke and Wang, Jingyi and Yang, Yiwei and Song, Fu and Sun, Jun",2021,Association for Computing Machinery,,https://doi.org/10.1145/3460319.3464822,10.1145/3460319.3464822,"As a new programming paradigm, deep learning has expanded its application to many real-world problems. At the same time, deep learning based software are found to be vulnerable to adversarial attacks. Though various defense mechanisms have been proposed to improve robustness of deep learning software, many of them are ineffective against adaptive attacks. In this work, we propose a novel characterization to distinguish adversarial examples from benign ones based on the observation that adversarial examples are significantly less robust than benign ones. As existing robustness measurement does not scale to large networks, we propose a novel defense framework, named attack as defense (A2D), to detect adversarial examples by effectively evaluating an example’s robustness. A2D uses the cost of attacking an input for robustness evaluation and identifies those less robust examples as adversarial since less robust examples are easier to attack. Extensive experiment results on MNIST, CIFAR10 and ImageNet show that A2D is more effective than recent promising approaches. We also evaluate our defense against potential adaptive attacks and show that A2D is effective in defending carefully designed adaptive attacks, e.g., the attack success rate drops to 0% on CIFAR10.","Deep learning, neural networks, defense, adversarial examples",42–55,14,Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis
87,@inproceedings: 10.1145/3460319.3464820,Efficient White-Box Fairness Testing through Gradient Search,"Zhang, Lingfeng and Zhang, Yueling and Zhang, Min",2021,Association for Computing Machinery,,https://doi.org/10.1145/3460319.3464820,10.1145/3460319.3464820,"Deep learning (DL) systems are increasingly deployed for autonomous decision-making in a wide range of applications. Apart from the robustness and safety, fairness is also an important property that a well-designed DL system should have. To evaluate and improve individual fairness of a model, systematic test case generation for identifying individual discriminatory instances in the input space is essential. In this paper, we propose a framework EIDIG for efficiently discovering individual fairness violation. Our technique combines a global generation phase for rapidly generating a set of diverse discriminatory seeds with a local generation phase for generating as many individual discriminatory instances as possible around these seeds under the guidance of the gradient of the model output. In each phase, prior information at successive iterations is fully exploited to accelerate convergence of iterative optimization or reduce frequency of gradient calculation. Our experimental results show that, on average, our approach EIDIG generates 19.11% more individual discriminatory instances with a speedup of 121.49% when compared with the state-of-the-art method and mitigates individual discrimination by 80.03% with a limited accuracy loss after retraining.","neural networks, fairness testing, test case generation, software bias",103–114,12,Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis
88,@inproceedings: 10.1145/3489449.3489999,Designing Systems for Mobile Collaboration,"Seidel, Niels",2021,Association for Computing Machinery,,https://doi.org/10.1145/3489449.3489999,10.1145/3489449.3489999," Due to the widespread use of mobile communication devices, the mobility in everyday life including mobile working habits, a variety of collaborative software applications accompany us every day. In this paper, we aim to capture and describe the essence of the design solutions implemented in these applications in the terms of design patterns. To achieve this goal, we examined relevant services and applications in literature that enable people to collaborate on the go. Furthermore, the geographic characteristics of place, space, and mobility are discussed with regard to location-based collaboration including references to the theory of relativity. As result 11 design patterns for mobile collaboration have been elaborated. The design patterns describe, in addition to basic technologies, generic map services and privacy preserving solutions. In particular the patterns Geo Fencing, Locale Me, Geo Tracing, and Substitute Position are presented in detail. These patterns are intended to guide computer science teachers and software developers on how to design mobile collaboration systems that mediate collaboration in physical space through interactions in virtual space.","Mobile Interaction, CSCW, Mobile Collaboration, Design Patterns",,13,26th European Conference on Pattern Languages of Programs
89,@inproceedings: 10.1145/3463274.3463358,Towards an Automated Classification Approach for Software Engineering Research,"Kaplan, Angelika and Keim, Jan",2021,Association for Computing Machinery,,https://doi.org/10.1145/3463274.3463358,10.1145/3463274.3463358," The rapid growth of software engineering research publications forces an amount of scholarly knowledge that needs to be managed, organized and communicated in digital libraries and scientific search engines. Thus, there is a need for classified papers to accomplish these tasks, but the classification process is cumbersome. Moreover, in case of new schemas, one would need to reclassify previously published research. We propose to automate the classification and present different possible techniques for doing so: Using natural language models, a rule-based approach, or an approach based on topic-labeling. In this proposal paper, we initially implemented a prototype for text classification of software engineering research papers.","NLP, Research knowledge organization and management, text classification, neural machine learning, scholarly knowledge communication, information extraction",347–352,6,Evaluation and Assessment in Software Engineering
90,@inproceedings: 10.1145/3460945.3464951,Generating Bug-Fixes Using Pretrained Transformers,"Drain, Dawn and Wu, Chen and Svyatkovskiy, Alexey and Sundaresan, Neel",2021,Association for Computing Machinery,,https://doi.org/10.1145/3460945.3464951,10.1145/3460945.3464951,"Detecting and fixing bugs are two of the most important yet frustrating parts of the software development cycle. Existing bug detection tools are based mainly on static analyzers, which rely on mathematical logic and symbolic reasoning about the program execution to detect common types of bugs. Fixing bugs is typically left out to the developer. In this work we introduce DeepDebug: a data-driven program repair approach which learns to detect and fix bugs in Java methods mined from real-world GitHub repositories. We frame bug-patching as a sequence-to-sequence learning task consisting of two steps: (i) denoising pretraining, and (ii) supervised finetuning on the target translation task. We show that pretraining on source code programs improves the number of patches found by 33% as compared to supervised training from scratch, while domain-adaptive pretraining from natural language to code further improves the accuracy by another 32%. We refine the standard accuracy evaluation metric into non-deletion and deletion-only fixes, and show that our best model generates 75% more non-deletion fixes than the previous state of the art. In contrast to prior work, we attain our best results when generating raw code, as opposed to working with abstracted code that tends to only benefit smaller capacity models. Finally, we observe a subtle improvement from adding syntax embeddings along with the standard positional embeddings, as well as with adding an auxiliary task to predict each token's syntactic class. Despite focusing on Java, our approach is language agnostic, requiring only a general-purpose parser such as tree-sitter.","program repair, transformers, bug-fixing, bugpatching",1–8,8,Proceedings of the 5th ACM SIGPLAN International Symposium on Machine Programming
91,@inproceedings: 10.1145/3463274.3463330,Towards a Corpus for Credibility Assessment in Software Practitioner Blog Articles,"Williams, Ashley and Shardlow, Matthew and Rainer, Austen",2021,Association for Computing Machinery,,https://doi.org/10.1145/3463274.3463330,10.1145/3463274.3463330,"Background: Blogs are a source of grey literature which are widely adopted by software practitioners for disseminating opinion and experience. Analysing such articles can provide useful insights into the state–of–practice for software engineering research. However, there are challenges in identifying higher quality content from the large quantity of articles available. Credibility assessment can help in identifying quality content, though there is a lack of existing corpora. Credibility is typically measured through a series of conceptual criteria, with ’argumentation’ and ’evidence’ being two important criteria. Objective: We create a corpus labelled for argumentation and evidence that can aid the credibility community. The corpus consists of articles from the blog of a single software practitioner and is publicly available. Method: Three annotators label the corpus with a series of conceptual credibility criteria, reaching an agreement of 0.82 (Fleiss’ Kappa). We present preliminary analysis of the corpus by using it to investigate the identification of claim sentences (one of our ten labels). Results: We train four systems (Bert, KNN, Decision Tree and SVM) using three feature sets (Bag of Words, Topic Modelling and InferSent), achieving an F1 score of 0.64 using InferSent and a Linear SVM. Conclusions: Our preliminary results are promising, indicating that the corpus can help future studies in detecting the credibility of grey literature. Future research will investigate the degree to which the sentence level annotations can infer the credibility of the overall document. ","experience mining, credibility assessment, argumentation mining, text mining",100–108,9,Evaluation and Assessment in Software Engineering
92,@inproceedings: 10.1145/3463274.3463311,How Should Developers Respond to App Reviews? Features Predicting the Success of Developer Responses,"Srisopha, Kamonphop and Link, Daniel and Boehm, Barry",2021,Association for Computing Machinery,,https://doi.org/10.1145/3463274.3463311,10.1145/3463274.3463311,"Context: The Google Play Store allows app developers to respond to user reviews. Existing research shows that response strategies vary considerably. In addition, while responding to reviews can lead to several types of favorable outcomes, not every response leads to success, which we define as increased user ratings. Aims: This work has two objectives. The first is to investigate the potential to predict early whether a developer response to a review is likely to be successful. The second is to pinpoint how developers can increase the chance of their responses to achieve success. Method: We track changes in user reviews of the 1,600 top free apps over a ten-week period, and find that in 11,034 out of 228,274 one- to four-star reviews, the ratings increase after a response. We extract three groups of features, namely time, presentation and tone, from the responses given to these reviews. We apply the extreme gradient boosting (XGBoost) algorithm to model the success of developer responses using these features. We employ model interpretation techniques to derive insights from the model. Results: Our model can achieve an AUC of 0.69, thus demonstrating that feature engineering and machine learning have the potential to enable developers to estimate the probability of success of their responses at composition time. We learn from it that the ratio between the length of the review and response, the textual similarity between the review and response, and the timeliness and the politeness of the response have the highest predictive power for distinguishing successful and unsuccessful developer responses. Conclusions: Based on our findings, we provide recommendations that developers can follow to increase the chance of success of their responses. Tools may also leverage our findings to support developers in writing more effective responses to reviews on the app store.","Partial Dependence Plot, Extreme Gradient Boosting, Feature Importance, Developer Response, App Store Mining, User Reviews",119–128,10,Evaluation and Assessment in Software Engineering
93,@inproceedings: 10.1145/3463274.3463352,DID-EFed: Facilitating Federated Learning as a Service with Decentralized Identities,"Geng, Jiahui and Kanwal, Neel and Jaatun, Martin Gilje and Rong, Chunming",2021,Association for Computing Machinery,,https://doi.org/10.1145/3463274.3463352,10.1145/3463274.3463352," We have entered the era of big data, and it is considered to be the ”fuel” for the flourishing of artificial intelligence applications. The enactment of the EU General Data Protection Regulation (GDPR) raises concerns about individuals’ privacy in big data. Federated learning (FL) emerges as a functional solution that can help build high-performance models shared among multiple parties while still complying with user privacy and data confidentiality requirements. Although FL has been intensively studied and used in real applications, there is still limited research related to its prospects and applications as a FLaaS (Federated Learning as a Service) to interested 3rd parties. In this paper, we present a FLaaS system: DID-eFed, where FL is facilitated by decentralized identities (DID) and a smart contract. DID enables a more flexible and credible decentralized access management in our system, while the smart contract offers a frictionless and less error-prone process. We describe particularly the scenario where our DID-eFed enables the FLaaS among hospitals and research institutions. ","blockchain, decentralized identity, FLaaS, federated learning",329–335,7,Evaluation and Assessment in Software Engineering
94,@inproceedings: 10.1145/3463274.3463333,Self-Claimed Assumptions in Deep Learning Frameworks: An Exploratory Study,"Yang, Chen and Liang, Peng and Fu, Liming and Li, Zengyang",2021,Association for Computing Machinery,,https://doi.org/10.1145/3463274.3463333,10.1145/3463274.3463333," Deep learning (DL) frameworks have been extensively designed, implemented, and used in software projects across many domains. However, due to the lack of knowledge or information, time pressure, complex context, etc., various uncertainties emerge during the development, leading to assumptions made in DL frameworks. Though not all the assumptions are negative to the frameworks, being unaware of certain assumptions can result in critical problems (e.g., system vulnerability and failures). As the first step of addressing the critical problems, there is a need to explore and understand the assumptions made in DL frameworks. To this end, we conducted an exploratory study to understand self-claimed assumptions (SCAs) about their distribution, classification, and impacts using code comments from nine popular DL framework projects on GitHub. The results are that: (1) 3,084 SCAs are scattered across 1,775 files in the nine DL frameworks, ranging from 1,460 (TensorFlow) to 8 (Keras) SCAs. (2) There are four types of validity of SCAs: Valid SCA, Invalid SCA, Conditional SCA, and Unknown SCA, and four types of SCAs based on their content: Configuration and Context SCA, Design SCA, Tensor and Variable SCA, and Miscellaneous SCA. (3) Both valid and invalid SCAs may have an impact within a specific scope (e.g., in a function) on the DL frameworks. Certain technical debt is induced when making SCAs. There are source code written and decisions made based on SCAs. This is the first study on investigating SCAs in DL frameworks, which helps researchers and practitioners to get a comprehensive understanding on the assumptions made. We also provide the first dataset of SCAs for further research and practice in this area.","Self-Claimed Assumption, GitHub, Deep Learning Framework",139–148,10,Evaluation and Assessment in Software Engineering
95,@inproceedings: 10.1145/3463274.3463315,Human-Level Ordinal Maintainability Prediction Based on Static Code Metrics,"Schnappinger, Markus and Fietzke, Arnaud and Pretschner, Alexander",2021,Association for Computing Machinery,,https://doi.org/10.1145/3463274.3463315,10.1145/3463274.3463315," One of the greatest challenges in software quality control is the efficient and effective measurement of maintainability. Thorough expert assessments are precise yet slow and expensive, whereas automated static analysis yields imprecise yet rapid feedback. Several machine learning approaches aim to integrate the advantages of both concepts. However, most prior studies did not adhere to expert judgment and predicted the number of changed lines as a proxy for maintainability, or were biased towards a small group of experts. In contrast, the present study builds on a manually labeled and validated dataset. Prediction is done using static code metrics where we found simple structural metrics such as the size of a class and its methods to yield the highest predictive power towards maintainability. Using just a small set of these metrics, our models can distinguish easy from hard to maintain code with an F-score of 91.3% and AUC of 82.3%. In addition, we perform a more detailed ordinal classification and compare the quality of the classification with the performance of experts. Here, we use the deviations between the individual expert’s ratings and the eventually determined consensus of all experts. In sum, our models achieve the same level of performance as an average human expert. In fact, the obtained accuracy and mean squared error outperform human performance. We hence argue that our models provide an automated and trustworthy prediction of software maintainability.","Expert Judgment, Software Maintainability, Ordinal Classification, Maintainability Prediction, Machine Learning",160–169,10,Evaluation and Assessment in Software Engineering
96,@inproceedings: 10.1145/3460945.3464952,Learning to Make Compiler Optimizations More Effective,"Mammadli, Rahim and Selakovic, Marija and Wolf, Felix and Pradel, Michael",2021,Association for Computing Machinery,,https://doi.org/10.1145/3460945.3464952,10.1145/3460945.3464952,"Because loops execute their body many times, compiler developers place much emphasis on their optimization. Nevertheless, in view of highly diverse source code and hardware, compilers still struggle to produce optimal target code. The sheer number of possible loop optimizations, including their combinations, exacerbates the problem further. Today's compilers use hard-coded heuristics to decide when, whether, and which of a limited set of optimizations to apply. Often, this leads to highly unstable behavior, making the success of compiler optimizations dependent on the precise way a loop has been written. This paper presents LoopLearner, which addresses the problem of compiler instability by predicting which way of writing a loop will lead to efficient compiled code. To this end, we train a neural network to find semantically invariant source-level transformations for loops that help the compiler generate more efficient code. Our model learns to extract useful features from the raw source code and predicts the speedup that a given transformation is likely to yield. We evaluate LoopLearner with 1,895 loops from various performance-relevant benchmarks. Applying the transformations that our model deems most favorable prior to compilation yields an average speedup of 1.14x. When trying the top-3 suggested transformations, the average speedup even increases to 1.29x. Comparing the approach with an exhaustive search through all available code transformations shows that LoopLearner helps to identify the most beneficial transformations in several orders of magnitude less time.","compiler optimizations, loop transformation, deep learning",9–20,12,Proceedings of the 5th ACM SIGPLAN International Symposium on Machine Programming
97,@inbook: 10.1145/3453483.3454109,Bliss: Auto-Tuning Complex Applications Using a Pool of Diverse Lightweight Learning Models,"Roy, Rohan Basu and Patel, Tirthak and Gadepally, Vijay and Tiwari, Devesh",2021,Association for Computing Machinery,,https://doi.org/10.1145/3453483.3454109,,"As parallel applications become more complex, auto-tuning becomes more desirable, challenging, and time-consuming. We propose, Bliss, a novel solution for auto-tuning parallel applications without requiring apriori information about applications, domain-specific knowledge, or instrumentation. Bliss demonstrates how to leverage a pool of Bayesian Optimization models to find the near-optimal parameter setting 1.64\texttimes{} faster than the state-of-the-art approaches.",,1280–1295,1,Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation
98,@inbook: 10.1145/3453483.3454047,Web Question Answering with Neurosymbolic Program Synthesis,"Chen, Qiaochu and Lamoreaux, Aaron and Wang, Xinyu and Durrett, Greg and Bastani, Osbert and Dillig, Isil",2021,Association for Computing Machinery,,https://doi.org/10.1145/3453483.3454047,,"In this paper, we propose a new technique based on program synthesis for extracting information from webpages. Given a natural language query and a few labeled webpages, our method synthesizes a program that can be used to extract similar types of information from other unlabeled webpages. To handle websites with diverse structure, our approach employs a neurosymbolic DSL that incorporates both neural NLP models as well as standard language constructs for tree navigation and string manipulation. We also propose an optimal synthesis algorithm that generates all DSL programs that achieve optimal F1 score on the training examples. Our synthesis technique is compositional, prunes the search space by exploiting a monotonicity property of the DSL, and uses transductive learning to select programs with good generalization power. We have implemented these ideas in a new tool called WebQA and evaluate it on 25 different tasks across multiple domains. Our experiments show that WebQA significantly outperforms existing tools such as state-of-the-art question answering models and wrapper induction systems.",,328–343,1,Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation
99,@inproceedings: 10.1145/3453483.3454054,Automated Conformance Testing for JavaScript Engines via Deep Compiler Fuzzing,"Ye, Guixin and Tang, Zhanyong and Tan, Shin Hwei and Huang, Songfang and Fang, Dingyi and Sun, Xiaoyang and Bian, Lizhong and Wang, Haibo and Wang, Zheng",2021,Association for Computing Machinery,,https://doi.org/10.1145/3453483.3454054,10.1145/3453483.3454054,"JavaScript (JS) is a popular, platform-independent programming language. To ensure the interoperability of JS programs across different platforms, the implementation of a JS engine should conform to the ECMAScript standard. However, doing so is challenging as there are many subtle definitions of API behaviors, and the definitions keep evolving.  We present COMFORT, a new compiler fuzzing framework for detecting JS engine bugs and behaviors that deviate from the ECMAScript standard. COMFORT leverages the recent advance in deep learning-based language models to automatically generate JS test code. As a departure from prior fuzzers, COMFORT utilizes the well-structured ECMAScript specifications to automatically generate test data along with the test programs to expose bugs that could be overlooked by the developers or manually written test cases. COMFORT then applies differential testing methodologies on the generated test cases to expose standard conformance bugs. We apply COMFORT to ten mainstream JS engines. In 200 hours of automated concurrent testing runs, we discover bugs in all tested JS engines. We had identified 158 unique JS engine bugs, of which 129 have been verified, and 115 have already been fixed by the developers. Furthermore, 21 of the COMFORT-generated test cases have been added to Test262, the official ECMAScript conformance test suite.","Compiler fuzzing, Differential testing, JavaScript, Conformance bugs, Deep learning",435–450,16,Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation
100,@article: 10.1145/3491211,An Empirical Study of the Effectiveness of an Ensemble of Stand-Alone Sentiment Detection Tools for Software Engineering Datasets,"Uddin, Gias and Gu\'{e}h\'{e}nuc, Yann-Ga\""{e}l and Khomh, Foutse and Roy, Chanchal K.",2021,Association for Computing Machinery,1049-331X,https://doi.org/10.1145/3491211,10.1145/3491211,"Sentiment analysis in software engineering (SE) has shown promise to analyze and support diverse development activities. Recently, several tools are proposed to detect sentiments in software artifacts. While the tools improve accuracy over off-the-shelf tools, recent research shows that their performance could still be unsatisfactory. A more accurate sentiment detector for SE can help reduce noise in analysis of software scenarios where sentiment analysis is required. Recently, combinations, i.e., hybrids of stand-alone classifiers are found to offer better performance than the stand-alone classifiers for fault detection. However, we are aware of no such approach for sentiment detection for software artifacts. We report the results of an empirical study that we conducted to determine the feasibility of developing an ensemble engine by combining the polarity labels of stand-alone SE-specific sentiment detectors. Our study has two phases. In the first phase, we pick five SE-specific sentiment detection tools from two recently published papers by Lin et&nbsp;al.&nbsp;[31, 32], who first reported negative results with stand alone sentiment detectors and then proposed an improved SE-specific sentiment detector, POME&nbsp;[31]. We report the study results on 17,581 units (sentences/documents) coming from six currently available sentiment benchmarks for software engineering. We find that the existing tools can be complementary to each other in 85-95% of the cases, i.e., one is wrong but another is right. However, a majority voting-based ensemble of those tools fails to improve the accuracy of sentiment detection. We develop Sentisead, a supervised tool by combining the polarity labels and bag of words as features. Sentisead improves the performance (F1-score) of the individual tools by 4% (over Senti4SD&nbsp;[5]) – 100% (over POME&nbsp;[31]). The initial development of Sentisead occurred before we observed the use of deep learning models for SE-specific sentiment detection. In particular, recent papers show the superiority of advanced language-based pre-trained transformer models (PTM) over rule-based and shallow learning models. Consequently, in a second phase, we compare and improve Sentisead infrastructure using the PTMs. We find that a Sentisead infrastructure with RoBERTa as the ensemble of the five stand-alone rule-based and shallow learning SE-specific tools from Lin et&nbsp;al.&nbsp;[31, 32] offers the best F1-score of 0.805 across the six datasets, while a stand-alone RoBERTa shows an F1-score of 0.801.","Ensemble Classifier, Machine Learning, Sentiment Analysi",,,ACM Trans. Softw. Eng. Methodol.
101,@article: 10.1145/3511887,"Towards Robustness of Deep Program Processing Models – Detection, Estimation and Enhancement","Zhang, Huangzhao and Fu, Zhiyi and Li, Ge and Ma, Lei and Zhao, Zhehao and Yang, Hua’an and Sun, Yizhe and Liu, Yang and Jin, Zhi",2021,Association for Computing Machinery,1049-331X,https://doi.org/10.1145/3511887,10.1145/3511887,"Deep learning (DL) has recently been widely applied to diverse source code processing tasks in the software engineering (SE) community, which achieves competitive performance (e.g., accuracy). However, the robustness, which requires the model to produce consistent decisions given minorly perturbed code inputs, still lacks systematic investigation as an important quality indicator. This paper initiates an early step and proposes a framework CARROT for robustness detection, measurement, and enhancement of DL models for source code processing. We first propose an optimization-based attack technique CARROTA to generate valid adversarial source code examples effectively and efficiently. Based on this, we define the robustness metrics and propose robustness measurement toolkit CARROTM, which employs the worst-case performance approximation under the allowable perturbations. We further propose to improve the robustness of the DL models by adversarial training (CARROTT) with our proposed attack techniques. Our in-depth evaluations on three source code processing tasks (i.e., functionality classification, code clone detection, defect prediction) containing more than 3 million lines of code and the classic or SOTA DL models, including GRU, LSTM, ASTNN, LSCNN, TBCNN, CodeBERT and CDLH, demonstrate the usefulness of our techniques for ❶ effective and efficient adversarial example detection, ❷ tight robustness estimation, and ❸ effective robustness enhancement.","source code processing, adversarial attack, big code, robustness enhancemen",,,ACM Trans. Softw. Eng. Methodol.
102,@article: 10.1145/3498537,Detecting and Augmenting Missing Key Aspects in Vulnerability Descriptions,"Guo, Hao and Chen, Sen and Xing, Zhenchang and Li, Xiaohong and Bai, Yude and Sun, Jiamou",2021,Association for Computing Machinery,1049-331X,https://doi.org/10.1145/3498537,10.1145/3498537,"Security vulnerabilities have been continually disclosed and documented. For the effective understanding, management, and mitigation of the fast-growing number of vulnerabilities, an important practice in documenting vulnerabilities is to describe the key vulnerability aspects, such as vulnerability type, root cause, affected product, impact, attacker type, and attack vector. In this paper, we first investigate 133,639 vulnerability reports in the Common Vulnerabilities and Exposures (CVE) database over the past 20 years. We find that 56%, 85%, 38%, and 28% of CVEs miss vulnerability type, root cause, attack vector, and attacker type, respectively. By comparing the differences of the latest updated CVE reports across different databases, we observe that 1,476 missing key aspects in 1,320 CVE descriptions were augmented manually in the National Vulnerability Database (NVD), which indicates that the vulnerability database maintainers try to complete the vulnerability descriptions in practice to mitigate such a problem. To help complete the missing information of key vulnerability aspects and reduce human efforts, we propose a neural network based approach named PMA to predict the missing key aspects of a vulnerability based on its known aspects. We systematically explore the design space of the neural network models and empirically identify the most effective model design in the scenario. Our ablation study reveals the prominent correlations among vulnerability aspects when predicting. Trained with historical CVEs, our model achieves 88%, 71%, 61%, and 81% in F1 for predicting the missing vulnerability type, root cause, attacker type, and attack vector of 8,623 “future” CVEs across 3 years, respectively. Furthermore, we validate the predicting performance of key aspect augmentation of CVEs based on the manually augmented CVE data collected from NVD, which confirms the practicality of our approach. We finally highlight that PMA has the ability to reduce human efforts by recommending and augmenting missing key aspects for vulnerability databases, and to facilitate other research works such as severity level prediction of CVEs based on the vulnerability descriptions.","Data augmentation, Vulnerability description, CVE, Deep neural networ",,,ACM Trans. Softw. Eng. Methodol.
103,@article: 10.1145/3501256,Adversarial Robustness of Deep Code Comment Generation,"Zhou, Yu and Zhang, Xiaoqing and Shen, Juanjuan and Han, Tingting and Chen, Taolue and Gall, Harald",2021,Association for Computing Machinery,1049-331X,https://doi.org/10.1145/3501256,10.1145/3501256,"Deep neural networks (DNNs) have shown remarkable performance in a variety of domains such as computer vision, speech recognition, and natural language processing. Recently they also have been applied to various software engineering tasks, typically involving processing source code. DNNs are well-known to be vulnerable to adversarial examples, i.e., fabricated inputs that could lead to various misbehaviors of the DNN model while being perceived as benign by humans. In this paper, we focus on the code comment generation task in software engineering and study the robustness issue of the DNNs when they are applied to this task. We propose ACCENT (Adversarial Code Comment gENeraTor), an identifier substitution approach to craft adversarial code snippets, which are syntactically correct and semantically close to the original code snippet, but may mislead the DNNs to produce completely irrelevant code comments. In order to improve the robustness, ACCENT also incorporates a novel training method, which can be applied to existing code comment generation models. We conduct comprehensive experiments to evaluate our approach by attacking the mainstream encoder-decoder architectures on two large-scale publicly available datasets. The results show that ACCENT efficiently produces stable attacks with functionality-preserving adversarial examples, and the generated examples have better transferability compared with the baselines. We also confirm, via experiments, the effectiveness in improving model robustness with our training method.","Deep Learning, Robustness, Code Comment Generation, Adversarial Attac",,,ACM Trans. Softw. Eng. Methodol.
104,@article: 10.1145/3502868,Deep Reinforcement Learning for Black-Box Testing of Android Apps,"Romdhana, Andrea and Merlo, Alessio and Ceccato, Mariano and Tonella, Paolo",2021,Association for Computing Machinery,1049-331X,https://doi.org/10.1145/3502868,10.1145/3502868,"The state space of Android apps is huge, and its thorough exploration during testing remains a significant challenge. The best exploration strategy is highly dependent on the features of the app under test. Reinforcement Learning (RL) is a machine learning technique that learns the optimal strategy to solve a task by trial and error, guided by positive or negative reward, rather than explicit supervision. Deep RL is a recent extension of RL that takes advantage of the learning capabilities of neural networks. Such capabilities make Deep RL suitable for complex exploration spaces such as one of Android apps. However, state-of-the-art, publicly available tools only support basic, Tabular RL. We have developed ARES, a Deep RL approach for black-box testing of Android apps. Experimental results show that it achieves higher coverage and fault revelation than the baselines, including state-of-the-art tools, such as TimeMachine and Q-Testing. We also investigated the reasons behind such performance qualitatively, and we have identified the key features of Android apps that make Deep RL particularly effective on them to be the presence of chained and blocking activities. Moreover, we have developed FATE to fine-tune the hyperparameters of Deep RL algorithms on simulated apps, since it is computationally expensive to carry it out on real apps.","Deep reinforcement learning, Android testin",,,ACM Trans. Softw. Eng. Methodol.
105,@article: 10.1145/3491039,Using Personality Detection Tools for Software Engineering Research: How Far Can We Go?,"Calefato, Fabio and Lanubile, Filippo",2021,Association for Computing Machinery,1049-331X,https://doi.org/10.1145/3491039,10.1145/3491039,"Assessing the personality of software engineers may help to match individual traits with the characteristics of development activities such as code review and testing, as well as support managers in team composition. However, self-assessment questionnaires are not a practical solution for collecting multiple observations on a large scale. Instead, automatic personality detection, while overcoming these limitations, is based on off-the-shelf solutions trained on non-technical corpora, which might not be readily applicable to technical domains like software engineering. In this paper, we first assess the performance of general-purpose personality detection tools when applied to a technical corpus of developers’ emails retrieved from the public archives of the Apache Software Foundation. We observe a general low accuracy of predictions and an overall disagreement among the tools. Second, we replicate two previous research studies in software engineering by replacing the personality detection tool used to infer developers’ personalities from pull-request discussions and emails. We observe that the original results are not confirmed, i.e., changing the tool used in the original study leads to diverging conclusions. Our results suggest a need for personality detection tools specially targeted for the software engineering domain.","Five-Factor Model, IBM Personality Insights., automatic personality recognition, Big Five, LIWC, negative results, Computational personality detection, replicatio",,,ACM Trans. Softw. Eng. Methodol.
106,@article: 10.1145/3502771.3502777,MaLTeSQuE 2021 Workshop Summary,"Feitosa, Daniel and Catolino, Gemma and Lenarduzzi, Valentina and Ampatzoglou, Apostolos",2022,Association for Computing Machinery,0163-5948,https://doi.org/10.1145/3502771.3502777,10.1145/3502771.3502777,"The 5th edition of the Machine Learning Techniques for Software Quality Evaluation (MaLTeSQuE 2021) workshop was held virtually (originally in Athens, Greece) on August 23rd, 2021, colocated with the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE 2021). MaLTeSQuE received a total of ten submissions from all over the world, from which six papers and three abstracts were included in the program. The program also featured a keynote by Tim Menzies on the future of machine learning techniques for software quality. This report summarizes the event and insights stemmed from the keynote and presentations in the three sessions of the workshop.",,15–17,,SIGSOFT Softw. Eng. Notes
107,@article: 10.1145/3500917,Optimizing Small-Sample Disk Fault Detection Based on LSTM-GAN Model,"Wang, Yufei and Dsong, Xiaoshe and Wang, Longxiang and Chen, Weiduo and Zhang, Xingjun",2022,Association for Computing Machinery,1544-3566,https://doi.org/10.1145/3500917,10.1145/3500917,"In recent years, researches on disk fault detection based on SMART data combined with different machine learning algorithms have been proven to be effective. However, these methods require a large amount of data. In the early stages of the establishment of a data center or the deployment of new storage devices, the amount of reliability data for disks is relatively limited, and the amount of failed disk data is even less, resulting in the unsatisfactory detection performances of machine learning algorithms.To solve the above problems, we propose a novel small sample disk fault detection (SSDFD)1 optimizing method based on Generative Adversarial Networks (GANs). Combined with the characteristics of hard disk reliability data, the generator of the original GAN is improved based on Long Short-Term Memory (LSTM), making it suitable for the generation of failed disk data. To alleviate the problem of data imbalance and expand the failed disk dataset with reduced amounts of original data, the proposed model is trained through adversarial training, which focuses on the generation of failed disk data. Experimental results on real HDD datasets show that SSDFD can generate enough virtual failed disk data to enable the machine learning algorithm to detect disk faults with increased accuracy under the condition of a few original failed disk data. Furthermore, the model trained with 300 original failed disk data has a significant effect on improving the accuracy of HDD fault detection. The optimal amount of generated virtual data are, 20–30 times that of the original data.","hard disk drives, deep learning, Fault detection, generative adversarial networks, reliabilit",,24,ACM Trans. Archit. Code Optim.
108,@inproceedings: 10.1145/3493700.3493730,Diagnosing Covid-19 Using AI Based Medical Image Analysis,"Srivastava, Varad and Ruchilekha",2022,Association for Computing Machinery,,https://doi.org/10.1145/3493700.3493730,10.1145/3493700.3493730," The pandemic of COVID-19 is currently one of the most significant problems being dealt with, all around the world. It mainly affects the lungs of the infected person which can further result in serious threats. So to avoid this life threatening condition, we have used chest radiological images for COVID-19 detection. This infectious disease is communicable and is spreading rapidly throughout the world. Hence, fast and accurate detection of COVID-19 is mandatory, so one can be given proper treatment well before time. In this paper, the proposed work aims to develop a web application, namely CovSADs(Covid-19 Smart A.I. Diagnosis System), using deep learning approach for faster and efficient detection of COVID-19. This web application uses X-ray and CT scan images for the evaluation. Here, we have developed DeepCovX and DeepCovCT models by incorporating Transfer Learning (TL) approach for COVID-19 detection via chest X-ray and CT scan images respectively. Further, we have used GradCam in case of X-ray to make sure our model is looking at relevant information to make decisions and image-segmentation is used in case of CT scan to extract and localize Region-of-interest (ROI) from binary image. Our proposed models show the accuracy of 95.89% and 98.01% for X-ray and CT scan images respectively. We have obtained specificity of 99.57%, sensitivity of 100%, and AUC of 0.998 in case of X-ray and specificity of 98.80%, sensitivity of 97.06%, and AUC of 0.9875 in case of CT scan images. F1-score is obtained as 0.98 for COVID-19 and 0.98 for Non-COVID-19 in case of CT scan images. Both quantitative and qualitative results demonstrate promising results for COVID-19 detection and extraction of infected lung regions. The primary objective of the web application is to assist the radiologists not only for mass screening but also to help in planning treatment process. ","Deep Learning, etc., GradCam, Transfer Learning, CovSADs, DeepCovX, DeepCovCT, COVID-19",204–212,9,5th Joint International Conference on Data Science &amp; Management of Data (9th ACM IKDD CODS and 27th COMAD)
109,@inproceedings: 10.1145/3493700.3493729,Customer Support Chat Intent Classification Using Weak Supervision and Data Augmentation,"Prabhu, Sumanth and Brahma, Aditya Kiran and Misra, Hemant",2022,Association for Computing Machinery,,https://doi.org/10.1145/3493700.3493729,10.1145/3493700.3493729," Understanding the actual intent of customers is an essential step in automating the conversational experience on a chat platform. Typically, chatbots are powered by machine learning algorithms that rely on the acquisition of a large amount of high quality labeled training data which can be prohibitively expensive. To overcome this dependence on labeled training data, weaker forms of supervision have been recently exploited to generate samples in a more cost effective manner though the samples may be noisy. In this paper, we analyse a use-case specific to food delivery services where customer-agent conversations in incoherent English and code-mixed language (Hindi mixed with English, commonly referred to as Hinglish) are associated with a single customer chosen noisy label (referred to as “Conversation Level Intent” in the current paper) for the entire conversation. However, in reality, a conversation can have several messages and can be comprised of multiple labels. Moreover, each label may be associated with one or more messages in the conversation. In this paper, we demonstrate how simple light-weight word embeddings based weak supervision techniques can be used to tag individual customer messages with the most relevant label. We also show that simple augmentation techniques can significantly improve performance on code-mixed messages. On an internal benchmark dataset, we show that our sampling approach achieves an absolute performance gain of 33% in F1 score on random sampling strategy, 19% in F1 score over an approach using entire raw samples and 4% over a Snorkel (a state-of-the-art weak supervision framework) based approach.","Weak Supervision, Text Classification",144–152,9,5th Joint International Conference on Data Science &amp; Management of Data (9th ACM IKDD CODS and 27th COMAD)
110,@inproceedings: 10.1145/3493700.3493713,Extraction of Product Specifications from the Web - Going Beyond Tables and Lists,"Gangadhar, Govind Krishnan and Kulkarni, Ashish",2022,Association for Computing Machinery,,https://doi.org/10.1145/3493700.3493713,10.1145/3493700.3493713," E-commerce product pages on the web often present product specification data in structured tabular blocks. Extraction of these product attribute-value specifications has benefited applications like product catalogue curation, search, question answering, and others. However, across different Websites, there is a wide variety of HTML elements (like , <ul>, <div>, <span>, <dl>, etc.) typically used to render these blocks that makes their automatic extraction a challenge. Most of the current research has focused on extracting product specifications from tables and lists and, therefore, suffers from recall when applied to a large-scale extraction setting. In this paper, we present a product specification extraction approach that goes beyond tables or lists and generalizes across the diverse HTML elements used for rendering specification blocks. Using a combination of hand-coded features and deep learned spatial and token features, we first identify the specification blocks on a product page. We then extract the product attribute-value pairs from these blocks following an approach inspired by wrapper induction. We created a labeled dataset of product specifications extracted from 14,111 diverse specification blocks taken from a range of different product websites. Our experiments show the efficacy of our approach compared to the current specification extraction models and support our claim about its application to large-scale product specification extraction.","Text Classification, Information Retrieval, Data Mining, Extraction",19–27,9,5th Joint International Conference on Data Science &amp; Management of Data (9th ACM IKDD CODS and 27th COMAD)
111,@inproceedings: 10.1145/3493700.3493733,ComParE: A User Behavior Centric Framework for Personalized Recommendations in Skill Gaming Platforms,"Mukherjee, Koyel and Seth, Deepanshi and Mittal, Prachi and Gowtham, Nuthi S. and Mukherjee, Tridib and Biswas, Dattatreya and Agrawal, Sanjay",2022,Association for Computing Machinery,,https://doi.org/10.1145/3493700.3493733,10.1145/3493700.3493733," Recommendation in outcome based platforms (eg. skill gaming, financial trading) where users get rewards based on their choices and the subsequent non-deterministic outcomes from such choices presents a unique set of challenges. Unlike other online services (e.g., e-commerce, social media), these platforms often see: (a) distinctive longitudinal behavior patterns in users’ transactions, (b) enormous content generated by the user interactions, (c) a dynamic interplay between the users’ transactional history and outcomes towards their future behavior, (d) ordinal nature of transaction elements. Motivated by these observations, we propose ComParE (Competing Parallel Networks with Expert Network), a novel personalized recommendation framework that: (i) exploits the distinct behavioral trends in the data, by training competing parallel networks as local experts, (ii) trains a global expert network to get the overall picture for the final prediction, and (iii) introduces custom loss functions to learn inherent ordering and interpretations of the classes being predicted. With the example of personalizing entry fee choices for the game of Rummy on the RummyCircle platform we show: (i) distinct and robust user Personas found based on historical entry fee selections; (ii) significant boosts in the entry fee predictions through ComParE with both neural networks and classical ML algorithms for local and global experts; (iii) substantial lifts over platform default as shown through offline analysis. ComParE significantly outperforms other baselines including well known deep learning models, as shown through offline experiments. ","user experience, neural networks, persona",186–194,9,5th Joint International Conference on Data Science &amp; Management of Data (9th ACM IKDD CODS and 27th COMAD)
112,@inproceedings: 10.1145/3493700.3493727,GCFI++: Embedding and Frequent Itemset Based Incremental Hierarchical Clustering with Labels and Outliers,"Dixit, Vini",2022,Association for Computing Machinery,,https://doi.org/10.1145/3493700.3493727,10.1145/3493700.3493727," Customer support is an essential part of any company to ensure customer satisfaction and business growth. The responsibility and scale get even higher for enterprise companies like Freshworks that build customer support platforms for its enterprise customers from multiple and previously unseen domains. A ticket may carry issues never seen before, therefore static, and predefined problem labels will not always work. These predefined problem labels are often created manually by admins. For a large customer account, roughly 10-35% of the problem labels could get discovered by this manual process and only 5-9% of the available open tickets could get linked to a problem label. So, there is a need for a system for a new incoming ticket. The system should either automatically detect an existing problem topic with its possible suggestions or systematically generate new problem topic(s) for its assignment. Our system is based on an incremental approach to mine ticket data streams and assigns them to hierarchical soft clusters with auto-generated problem/issue topics. Our system achieves an improvement in ticket assignments up to 97%. To compare with recent clustering advancements, we’ve also evaluated our approach on a public dataset [9] and achieved comparable results. Source code and results of the evaluation are available in detail at: https://github.com/vinidixit/hierarchical-labelled-clustering-evaluation.","Microtext, Customer Support Systems, Frequent Itemset Embeddings, Hierarchical Labeling, Topic Detection, Hierarchical Clustering, Outlier Detection, Incremental Systems",135–143,9,5th Joint International Conference on Data Science &amp; Management of Data (9th ACM IKDD CODS and 27th COMAD)
113,@inproceedings: 10.1145/3493700.3493724,An Intelligent Recommendation-Cum-Reminder System,"Saxena, Rohan and Chaudhary, Maheep and Maurya, Chandresh Kumar and Prasad, Shitala",2022,Association for Computing Machinery,,https://doi.org/10.1145/3493700.3493724,10.1145/3493700.3493724," Intelligent recommendation and reminder systems are the need of the fast-pacing life. Current intelligent systems such as Siri, Google Assistant, Microsoft Cortona, etc., have limited capability. For example, if you want to wake up at 6 am because you have an upcoming trip, you have to set the alarm manually. Besides, these systems do not recommend or remind what else to carry, such as carrying an umbrella during a likely rain. The present work proposes a system that takes an email as input and returns a recommendation-cum-reminder list. As a first step, we parse the emails, recognize the entities using named entity recognition (NER). In the second step, information retrieval over the web is done to identify nearby places, climatic conditions, etc. Imperative sentences from the reviews of all places are extracted and passed to the object extraction module. The main challenge lies in extracting the objects (items) of interest from the review. To solve it, a modified Machine Reading Comprehension-NER (MRC-NER) model is trained to tag objects of interest by formulating annotation rules as a query. The objects so found are recommended to the user one day in advance. The final reminder list of objects is pruned by our proposed model for tracking objects kept during the ”packing activity.” Eventually, when the user leaves for the event/trip, an alert is sent containing the reminding list items. Our approach achieves superior performance compared to several baselines by as much as 30% on recall and 10% on precision.",,169–177,9,5th Joint International Conference on Data Science &amp; Management of Data (9th ACM IKDD CODS and 27th COMAD)
114,@inproceedings: 10.1145/3493700.3493709,Identifying Efficient Curricula for Reinforcement Learning in Complex Environments with a Fixed Computational Budget,"Shelke, Omkar and Meisheri, Hardik and Khadilkar, Harshad",2022,Association for Computing Machinery,,https://doi.org/10.1145/3493700.3493709,10.1145/3493700.3493709," Pommerman is a hybrid cooperative/adversarial multi-agent environment, with challenging characteristics in terms of partial observability, limited or no communication, sparse and delayed rewards, and restrictive inference time limits. This makes it a challenging environment for reinforcement learning (RL) approaches. In this paper, we focus on developing a curriculum for learning a robust and promising policy in a constrained computational budget of 100,000 games, starting from a fixed base policy (which itself imitates a noisy expert policy). All RL algorithms starting from the base policy use vanilla proximal-policy optimization (PPO) with the same reward function, and the only difference between their training is the mix and sequence of opponent policies. One expects that beginning training with simpler opponents and then gradually increasing the opponent difficulty will facilitate faster learning, leading to more robust policies compared against a baseline where all available opponent policies are introduced from the start. We test this hypothesis and show that within constrained computational budgets, it is in fact better to “learn in the school of hard knocks”, i.e., against all available opponent policies nearly from the start. We also include ablation studies where we study the effect of modifying the base environment properties of ammo and bomb blast strength on the agent performance.","Imitation Learning, Reinforcement Learning, Curriculum Design",81–89,9,5th Joint International Conference on Data Science &amp; Management of Data (9th ACM IKDD CODS and 27th COMAD)
115,@inproceedings: 10.1145/3493700.3493712,An Active Learning Framework for Efficient Robust Policy Search,"Narayanaswami, Sai Kiran and Sudarsanam, Nandan and Ravindran, Balaraman",2022,Association for Computing Machinery,,https://doi.org/10.1145/3493700.3493712,10.1145/3493700.3493712," Robust Policy Search is the problem of learning policies that do not degrade in performance when subject to unseen environment model parameters. It is particularly relevant for transferring policies learned in a simulation environment to the real world. Several existing approaches involve sampling large batches of trajectories which reflect the differences in various possible environments, and then selecting some subset of these to learn robust policies, such as the ones that result in the worst performance. We propose an active learning based framework, EffAcTS, to selectively choose model parameters for this purpose so as to collect only as much data as necessary to select such a subset. We apply this framework using Linear Bandits, and experimentally validate the gains in sample efficiency and the performance of our approach on standard continuous control tasks. We also present a Multi-Task Learning perspective to the problem of Robust Policy Search, and draw connections from our proposed framework to existing work on Multi-Task Learning.","robust learning, robotics, deep reinforcement learning, active learning",1–9,9,5th Joint International Conference on Data Science &amp; Management of Data (9th ACM IKDD CODS and 27th COMAD)
116,@inproceedings: 10.1145/3493700.3493718,Universalization of Any Adversarial Attack Using Very Few Test Examples,"Kamath, Sandesh and Deshpande, Amit and Subrahmanyam, K V and Balasubramanian, Vineeth N",2022,Association for Computing Machinery,,https://doi.org/10.1145/3493700.3493718,10.1145/3493700.3493718," Deep learning models are known to be vulnerable not only to input-dependent adversarial attacks but also to input-agnostic or universal adversarial attacks. Dezfooli et al.&nbsp;[8, 9] construct universal adversarial attack on a given model by looking at a large number of training data points and the geometry of the decision boundary near them. Subsequent work&nbsp;[5] constructs universal attack by looking only at test examples and intermediate layers of the given model. In this paper, we propose a simple universalization technique to take any input-dependent adversarial attack and construct a universal attack by only looking at very few adversarial test examples. We do not require details of the given model and have negligible computational overhead for universalization. We theoretically justify our universalization technique by a spectral property common to many input-dependent adversarial perturbations, e.g., gradients, Fast Gradient Sign Method (FGSM) and DeepFool. Using matrix concentration inequalities and spectral perturbation bounds, we show that the top singular vector of input-dependent adversarial directions on a small test sample gives an effective and simple universal adversarial attack. For standard models on CIFAR10 and ImageNet, our simple universalization of Gradient, FGSM, and DeepFool perturbations using a test sample of 64 images gives fooling rates comparable to state-of-the-art universal attacks&nbsp;[5, 9] for reasonable norms of perturbation.","neural networks, adversarial, universal",72–80,9,5th Joint International Conference on Data Science &amp; Management of Data (9th ACM IKDD CODS and 27th COMAD)
117,@article: 10.1145/3485135,What You See is What It Means! Semantic Representation Learning of Code Based on Visualization and Transfer Learning,"Keller, Patrick and Kabor\'{e}, Abdoul Kader and Plein, Laura and Klein, Jacques and Le Traon, Yves and Bissyand\'{e}, Tegawend\'{e} F.",2021,Association for Computing Machinery,1049-331X,https://doi.org/10.1145/3485135,10.1145/3485135,"Recent successes in training word embeddings for Natural Language Processing (NLP) tasks have encouraged a wave of research on representation learning for source code, which builds on similar NLP methods. The overall objective is then to produce code embeddings that capture the maximum of program semantics. State-of-the-art approaches invariably rely on a syntactic representation (i.e., raw lexical tokens, abstract syntax trees, or intermediate representation tokens) to generate embeddings, which are criticized in the literature as non-robust or non-generalizable. In this work, we investigate a novel embedding approach based on the intuition that source code has visual patterns of semantics. We further use these patterns to address the outstanding challenge of identifying semantic code clones. We propose the WySiWiM&nbsp;(‘‘What You See Is What It Means”) approach where visual representations of source code are fed into powerful pre-trained image classification neural networks from the field of computer vision to benefit from the practical advantages of transfer learning. We evaluate the proposed embedding approach on the task of vulnerable code prediction in source code and on two variations of the task of semantic code clone identification: code clone detection (a binary classification problem), and code classification (a multi-classification problem). We show with experiments on the BigCloneBench (Java), Open Judge (C) that although simple, our WySiWiM &nbsp;approach performs as effectively as state-of-the-art approaches such as ASTNN or TBCNN. We also showed with data from NVD and SARD that WySiWiM &nbsp;representation can be used to learn a vulnerable code detector with reasonable performance (accuracy ∼90%). We further explore the influence of different steps in our approach, such as the choice of visual representations or the classification algorithm, to eventually discuss the promises and limitations of this research direction.","Semantic clones, representation learning, visual representation, embeddin",,34,ACM Trans. Softw. Eng. Methodol.
118,@inproceedings: 10.1145/3489849.3489887,Towards Context-Aware Automatic Haptic Effect Generation for Home Theatre Environments,"Li, Yaxuan and Yoo, Yongjae and Weill-Duflos, Antoine and Cooperstock, Jeremy",2021,Association for Computing Machinery,,https://doi.org/10.1145/3489849.3489887,10.1145/3489849.3489887," The application of haptic technology in entertainment systems, such as Virtual Reality and 4D cinema, enables novel experiences for users and drives the demand for efficient haptic authoring systems. Here, we propose an automatic multimodal vibrotactile content creation pipeline that substantially improves the overall hapto-audiovisual (HAV) experience based on contextual audio and visual content from movies. Our algorithm is implemented on a low-cost system with nine actuators attached to a viewing chair and extracts significant features from video files to generate corresponding haptic stimuli. We implemented this pipeline and used the resulting system in a user study (n = 16), quantifying user experience according to the sense of immersion, preference, harmony, and discomfort. The results indicate that the haptic patterns generated by our algorithm complement the movie content and provide an immersive and enjoyable HAV user experience. This further suggests that the pipeline can facilitate the efficient creation of 4D effects and could therefore be applied to improve the viewing experience in home theatre environments. ","4D effect generation, Haptics, automatic haptic effect authoring, home theatre, immersive experience",,11,Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology
119,@inproceedings: 10.5555/3507788.3507811,Text Classification on Software Requirements Specifications Using Transformer Models,"Kici, Derya and Bozanta, Aysun and Cevik, Mucahit and Parikh, Devang and Ba\c{s}ar, Ay\c{s}e",2021,IBM Corp.,,,,"Text classification in Software Requirements Specifications (SRS) documents is an essential task for various purposes including automatically extracting requirements and their types as well as identification of duplicate or conflicting information, which all contribute to avoiding potential issues in the later stages of the software development life cycle. While a variety of machine learning approaches have been considered for text classification over SRS documents, many of these fail to provide adequate performance as they often ignore the meaning of software artifacts or integrate domain knowledge for the classification task. Recent advances in deep learning methodology have significantly contributed to Natural Language Processing (NLP) and text classification. One of the main challenges in using deep learning models for various NLP tasks in the software engineering domain is the scarcity of labeled textual data. In addition, even with sufficient data, training from the scratch still requires significant training time and computational resources. Transfer learning is a novel approach that proposes a solution to such reservations by providing pre-trained models that enable fine-tuning with the customized data. In this research, we conduct an empirical analysis on multi-class text classification over SRS documents using different pre-trained transformer models including BERT, DistilBERT, Roberta, AlBERT, and XLNet, and compare their performance. We test the performance of these models using three SRS datasets: DOORS, NFR-PROMISE, and PURE. Our numerical study shows that the transformer models are able to generate highly accurate results to classify all categories except Priority of the requirements. While all models provide a 80% or higher accuracy for other classification tasks, the accuracy of the models to classify the Priority does not exceed 60%.","software requirement specifications, text classification, NLP, transfer learning, BERT",163–172,10,Proceedings of the 31st Annual International Conference on Computer Science and Software Engineering
120,@inproceedings: 10.5555/3507788.3507794,Text Classification for Predicting Multi-Level Product Categories,"Jahanshahi, Hadi and Ozyegen, Ozan and Cevik, Mucahit and Bulut, Beste and Yigit, Deniz and Gonen, Fahrettin F. and Ba\c{s}ar, Ay\c{s}e",2021,IBM Corp.,,,,"In an online shopping platform, a detailed classification of the products facilitates user navigation. It also helps online retailers keep track of the price fluctuations in a certain industry or special discounts on a specific product category. Moreover, an automated classification system may help to pinpoint incorrect or subjective categories suggested by an operator. In this study, we focus on product title classification of the grocery products. We perform a comprehensive comparison of six different text classification models to establish a strong baseline for this task, which involves testing both traditional and recent machine learning methods. In our experiments, we investigate the generalizability of the trained models to the products of other online retailers, the dynamic masking of infeasible subcategories for pretrained language models, and the benefits of incorporating product titles in multiple languages. Our numerical results indicate that dynamic masking of subcategories is effective in improving prediction accuracy. In addition, we observe that using bilingual product titles is generally beneficial, and neural network-based models perform significantly better than SVM and XGBoost models. Lastly, we investigate the reasons for the misclassified products and propose future research directions to further enhance the prediction models.","multi-level classification, product category classification, supervised learning, machine learning",33–42,10,Proceedings of the 31st Annual International Conference on Computer Science and Software Engineering
121,@inproceedings: 10.5555/3507788.3507856,Natural Language Processing for Software Requirement Specifications,"Cevik, Mucahit and Yildirim, Savas and Ba\c{s}ar, Ay\c{s}e",2021,IBM Corp.,,,,"Software Requirement Specifications (SRS) describe the functionality and expected performance for software products. It is one of the most important documents in the software development life cycle process that affects all the subsequent phases in product development. To manage the software requirements, requirement management tools such as IBM DOORS have been used to facilitate the communication, collaboration and verification throughout the process. Such tools generate a large amount of textual data, which can be processed through various NLP techniques to automate the software development processes. In this regard, the amalgamation of NLP and requirement engineering can catalyze the process of requirement classification (e.g., classifying requirements as functional and non-functional requirements), categorization of software documents, and topic modeling over SRS documents. Moreover, NLP-based approaches can be expanded to analyze the deeper semantics of software requirements for various purposes such as information extraction, automation of UML diagrams, AI-enabled software requirement analysis, defect and error detection in SRS, and solving the natural language ambiguities present in the requirements. In this workshop, university and IBM researchers will give insights into various existing NLP methodologies applied to SRS documents, e.g., for detecting the ambiguities and defects in the SRS documents. The prospects of various NLP-based automation techniques for conflict/duplicate detection as well as other NLP applications in requirement engineering will also be presented.","deep learning, software requirement specifications, natural language processing, requirement engineering, machine learning",308–309,2,Proceedings of the 31st Annual International Conference on Computer Science and Software Engineering
122,@inproceedings: 10.5555/3507788.3507815,Exploring Multi-View Perspectives on Deep Reinforcement Learning Agents for Embodied Object Navigation in Virtual Home Environments,"Liu, Xiaotian and Armstrong, Victoria and Nabil, Sara and Muise, Christian",2021,IBM Corp.,,,,"Recent years have brought the exploration of embodied reinforcement learning agents in a variety of domains. One of the advantages of artificial agents is that they can obtain visual inputs simultaneously using multiple input devices. This work explores multi-view reinforcement learning for object navigation tasks in 3D rendered virtual home environments using AI2-THOR. We trained CNN based Deep Q-learning embodied agents with egocentric, allocentric, and combined egocentric-allocentric perspectives to locate an object in an unknown environment. We compared the results of the three RL agents, and evaluated them by both reward improvement rate, and reward obtained. We demonstrate that the egocentric perspective allows for faster reward accumulation in the earlier episodes, whereas the allocentric agents obtained better long-term rewards. Interesting results arise from the combined allocentric and egocentric perspective, where we found that the agent had the best overall results by harnessing the benefits of each perspective. The results show that while single perspective embodied agents each have their own advantages, combining both inputs yields the best overall reward. Our findings provide a foundation and benchmark for building embodied RL agents with multi-view perspectives.","multi-view reinforcement learning, computer vision, embodied agent, object navigation, deep reinforcement learning",190–195,6,Proceedings of the 31st Annual International Conference on Computer Science and Software Engineering
123,@inproceedings: 10.5555/3507788.3507792,Deep Learning Models and Techniques for Facial Emotion Recognition,"Coreau, Rachel and P\'{e}pin, Ian and Duffy, Cameron and Abbas, Hazem M. and Hassanein, Hossam",2021,IBM Corp.,,,,"Automatic Facial Emotion Recognition (FER) is currently in high demand, with many industries trying to incorporate emotion-aware technologies into their products. This process involves analyzing images of human faces and detecting features in order to determine which emotion they are displaying. This study investigates data augmentation techniques, different Convolutional Neural Networks (CNNs), and the incorporation of Recurrent Neural Networks (RNNs) into models using JAFFE, CK+, KDEF, and FER2013 datasets. The study finds optimal solutions for FER models which can achieve high accuracy and have the potential for implementation in embedded systems with limited memory, such as cars. Using the most successful data augmentation techniques found for each dataset, a simple CNN model is proposed, exhibiting 100% accuracy on JAFFE and CK+ datasets and 60% on FER2013. A second CNN model utilizes EfficientNetB1 and is proposed for use on more complex datasets, reaching 65% on FER2013. A third model which uses a CNN combined with a RNN is proposed for use on sequences of images, resulting in 82.4% accuracy on the KDEF dataset.","model memory, data augmentation, datasets, recurrent neural networks, convolutional neural networks",14–22,9,Proceedings of the 31st Annual International Conference on Computer Science and Software Engineering
124,@article: 10.1145/3474827,Classifying Mobile Applications Using Word Embeddings,"Ebrahimi, Fahimeh and Tushev, Miroslav and Mahmoud, Anas",2021,Association for Computing Machinery,1049-331X,https://doi.org/10.1145/3474827,10.1145/3474827,"Modern application stores enable developers to classify their apps by choosing from a set of generic categories, or genres, such as health, games, and music. These categories are typically static—new categories do not necessarily emerge over time to reflect innovations in the mobile software landscape. With thousands of apps classified under each category, locating apps that match a specific consumer interest can be a challenging task. To overcome this challenge, in this article, we propose an automated approach for classifying mobile apps into more focused categories of functionally related application domains. Our aim is to enhance apps visibility and discoverability. Specifically, we employ word embeddings to generate numeric semantic representations of app descriptions. These representations are then classified to generate more cohesive categories of apps. Our empirical investigation is conducted using a dataset of 600 apps, sampled from the Education, Health&amp;Fitness, and Medical categories of the Apple App Store. The results show that our classification algorithms achieve their best performance when app descriptions are vectorized using GloVe, a count-based model of word embeddings. Our findings are further validated using a dataset of Sharing Economy apps and the results are evaluated by 12 human subjects. The results show that GloVe combined with Support Vector Machines can produce app classifications that are aligned to a large extent with human-generated classifications.","Word2Vec, GloVe, App classification, app store, word embeddings, fastTex",,30,ACM Trans. Softw. Eng. Methodol.
125,@article: 10.1145/3488269,Towards a Consistent Interpretation of AIOps Models,"Lyu, Yingzhe and Rajbahadur, Gopi Krishnan and Lin, Dayi and Chen, Boyuan and Jiang, Zhen Ming (Jack)",2021,Association for Computing Machinery,1049-331X,https://doi.org/10.1145/3488269,10.1145/3488269,"Artificial Intelligence for IT Operations (AIOps) has been adopted in organizations in various tasks, including interpreting models to identify indicators of service failures. To avoid misleading practitioners, AIOps model interpretations should be consistent (i.e., different AIOps models on the same task agree with one another on feature importance). However, many AIOps studies violate established practices in the machine learning community when deriving interpretations, such as interpreting models with suboptimal performance, though the impact of such violations on the interpretation consistency has not been studied.In this article, we investigate the consistency of AIOps model interpretation along three dimensions: internal consistency, external consistency, and time consistency. We conduct a case study on two AIOps tasks: predicting Google cluster job failures and Backblaze hard drive failures. We find that the randomness from learners, hyperparameter tuning, and data sampling should be controlled to generate consistent interpretations. AIOps models with AUCs greater than 0.75 yield more consistent interpretation compared to low-performing models. Finally, AIOps models that are constructed with the Sliding Window or Full History approaches have the most consistent interpretation with the trends presented in the entire datasets. Our study provides valuable guidelines for practitioners to derive consistent AIOps model interpretation.","model interpretation, AIOp",,38,ACM Trans. Softw. Eng. Methodol.
126,@inbook: 10.1145/3493229.3493306,FADE: FaaS-Inspired Application Decomposition and Energy-Aware Function Placement on the Edge,"Tzenetopoulos, Achilleas and Marantos, Charalampos and Gavrielides, Giannos and Xydis, Sotirios and Soudris, Dimitrios",2021,Association for Computing Machinery,,https://doi.org/10.1145/3493229.3493306,,"Lately, more and more applications are deployed on heterogeneous, power-constrained edge-computing devices. Bringing computation closer to the data, contributes both to latency and energy consumption reduction due to the elimination of excessive data transfers. However, while the main concern in such environments is the minimization of energy consumption, the heterogeneity in compute resources found at the edge may lead to Quality of Service (QoS) violations. At the same time, Serverless computing, the next frontier of Cloud computing has emerged to offer unprecedented elasticity by utilizing fine-grained, stateless functions. The reduction in the execution time and the modest memory footprint of such decomposed applications, allow for fine-grained resource multiplexing. In this work, we propose a methodology for application decomposition into fine-grained functions and energy-aware function placement on a cluster of edge devices subject to user-specified QoS guarantees.",,7–10,,Proceedings of the 24th International Workshop on Software and Compilers for Embedded Systems
127,@inproceedings: 10.1145/3493229.3493302,How to Exploit Sparsity in RNNs on Event-Driven Architectures,"Brils, Jarno and Waeijen, Luc and Pourtaherian, Arash",2021,Association for Computing Machinery,,https://doi.org/10.1145/3493229.3493302,10.1145/3493229.3493302,"Event-driven architectures have been shown to provide low-power, low-latency artificial neural network (ANN) inference. This is especially beneficial on Edge devices, particularly when combined with sparse execution. Recurrent neural networks (RNNs) are ANNs that emulate memory. Their recurrent connection enables the reuse of previous output for the generation of new output. However, when trying to use RNNs in a sparse context on event-driven architectures, novel challenges in synchronization and the usage of sparse data are encountered. In this work, these challenges are systematically analyzed, and mechanisms to overcome them are proposed. Experimental results of a monocular depth estimation use case on the NeuronFlow architecture show that sparsity in RNNs can be exploited effectively on event-driven architectures.","asynchronous, recurrent neural networks, event-driven, neural networks, sparsity, monocular depth estimation",17–22,6,Proceedings of the 24th International Workshop on Software and Compilers for Embedded Systems
128,@article: 10.1145/3477535,On the Reproducibility and Replicability of Deep Learning in Software Engineering,"Liu, Chao and Gao, Cuiyun and Xia, Xin and Lo, David and Grundy, John and Yang, Xiaohu",2021,Association for Computing Machinery,1049-331X,https://doi.org/10.1145/3477535,10.1145/3477535,"Context: Deep learning (DL) techniques have gained significant popularity among software engineering (SE) researchers in recent years. This is because they can often solve many SE challenges without enormous manual feature engineering effort and complex domain knowledge.Objective: Although many DL studies have reported substantial advantages over other state-of-the-art models on effectiveness, they often ignore two factors: (1) reproducibility—whether the reported experimental results can be obtained by other researchers using authors’ artifacts (i.e., source code and datasets) with the same experimental setup; and (2) replicability—whether the reported experimental result can be obtained by other researchers using their re-implemented artifacts with a different experimental setup. We observed that DL studies commonly overlook these two factors and declare them as minor threats or leave them for future work. This is mainly due to high model complexity with many manually set parameters and the time-consuming optimization process, unlike classical supervised machine learning (ML) methods (e.g., random forest). This study aims to investigate the urgency and importance of reproducibility and replicability for DL studies on SE tasks.Method: In this study, we conducted a literature review on 147 DL studies recently published in 20 SE venues and 20 AI (Artificial Intelligence) venues to investigate these issues. We also re-ran four representative DL models in SE to investigate important factors that may strongly affect the reproducibility and replicability of a study.Results: Our statistics show the urgency of investigating these two factors in SE, where only 10.2% of the studies investigate any research question to show that their models can address at least one issue of replicability and/or reproducibility. More than 62.6% of the studies do not even share high-quality source code or complete data to support the reproducibility of their complex models. Meanwhile, our experimental results show the importance of reproducibility and replicability, where the reported performance of a DL model could not be reproduced for an unstable optimization process. Replicability could be substantially compromised if the model training is not convergent, or if performance is sensitive to the size of vocabulary and testing data.Conclusion: It is urgent for the SE community to provide a long-lasting link to a high-quality reproduction package, enhance DL-based solution stability and convergence, and avoid performance sensitivity on different sampled data.","software engineering, replicability, reproducibility, Deep learnin",,46,ACM Trans. Softw. Eng. Methodol.
129,@article: 10.1145/3464969,Automating App Review Response Generation Based on Contextual Knowledge,"Gao, Cuiyun and Zhou, Wenjie and Xia, Xin and Lo, David and Xie, Qi and Lyu, Michael R.",2021,Association for Computing Machinery,1049-331X,https://doi.org/10.1145/3464969,10.1145/3464969,"User experience of mobile apps is an essential ingredient that can influence the user base and app revenue. To ensure good user experience and assist app development, several prior studies resort to analysis of app reviews, a type of repository that directly reflects user opinions about the apps. Accurately responding to the app reviews is one of the ways to relieve user concerns and thus improve user experience. However, the response quality of the existing method relies on the pre-extracted features from other tools, including manually labelled keywords and predicted review sentiment, which may hinder the generalizability and flexibility of the method. In this article, we propose a novel neural network approach, named CoRe, with the contextual knowledge naturally incorporated and without involving external tools. Specifically, CoRe integrates two types of contextual knowledge in the training corpus, including official app descriptions from app store and responses of the retrieved semantically similar reviews, for enhancing the relevance and accuracy of the generated review responses. Experiments on practical review data show that CoRe can outperform the state-of-the-art method by 12.36% in terms of BLEU-4, an accuracy metric that is widely used to evaluate text generation systems.","app descriptions, User reviews, retrieved responses, pointer-generator networ",,36,ACM Trans. Softw. Eng. Methodol.
130,@inproceedings: 10.1145/3494885.3494907,Self-Supervised Fine-Grained Image Classification via Progressive Global Disturbance,"Yang, Xuebin and Hu, Jianguo and Wang, Ziming and Xu, Fanglei and Zhu, Lu",2021,Association for Computing Machinery,,https://doi.org/10.1145/3494885.3494907,10.1145/3494885.3494907," Recently, self-supervised learning has shown to surpass the supervised counterpart for finetuning downstream applications such as ImageNet classification. However, in the area of fine-grained image recognition, self-supervised learning still lags far behind supervised learning. In this work, we study the self-supervised learning for fine-grained visual recognition, and propose an effective method called PGD, which consist of two mutually coordinated operations: Global Disturbance and Progressive Multi-granularity Learning. For the problem of small inter-class variance, Global Disturbance forces the neural network to pay more attention to the local discriminative areas by destroying global semantics but retaining local semantics. For the problem of high intra-class variance, progressive multi-granularity learning integrates different granularity information to locate the discriminative regions more accurately. Extensive experiments show that whether on large-scale datasets or small-scale datasets, PGD can obtain consistency gains by transfering to fine-grained recognition downstream tasks. In addition, we demonstrate the possibility of getting rid of weights pretrained on large-scale dataset. Specifically, we conduct sufficient experiments on three standard small-scall fine-grained classification datasets (CUB200-2011, Aircraft, Car), and the results show that PGD can reach or exceed the results of ImageNet supervised learning without using any additional data or annotations, which further proves the effectiveness of our method, and also shows that self-supervised learning has great potential in the domain of fine-grained image classification. ","Progressive Global Disturbance, Fine-grained Image Classification., Self-supervised Learning",119–125,7,2021 4th International Conference on Computer Science and Software Engineering (CSSE 2021)
131,@inproceedings: 10.1145/3494885.3494888,Multi-Interest Sequence Recommendation Algorithm Based on BERT,"Wang, Fei and Feng, WeiSen",2021,Association for Computing Machinery,,https://doi.org/10.1145/3494885.3494888,10.1145/3494885.3494888,"Many studies use a fixed vector to represent the user ' s various interest preferences, and this embedding vector does not have enough ability to effectively capture the user ' s different interests, resulting in a lot of user interest representation missing. Therefore, A model based on BERT multi-interest sequence recommendation (BMISREC) is proposed. The model uses BERT for pre-training, and then divides the users historical interaction sequence into t time windows according to the timestamp. By fusing the cavity convolution and forward attention, the local preference within each time window and the global preference between time windows are obtained. Fusion local and global preferences for each window. Multiple different interest embeddings are generated for each user. In the target item preference module, multiple interests learned can adaptively perceive the correlation between different target items through the attention mechanism, and the target item preference changes with the different target items. Then, the deep neural network is used to recommend the fusion of user and target item preferences in a nonlinear manner. Finally, the effectiveness of the model is verified on two real datasets.","Interaction sequence,Diversity,Dilated convolution,Self-attention mechanism,Personalized recommendation",13–17,5,2021 4th International Conference on Computer Science and Software Engineering (CSSE 2021)
132,@inproceedings: 10.1145/3494885.3494887,Determining the Rumour Stance with Ensemble Method Based on BSAF Model,"Wang, Xi and Feng, Weisen and Wang, Fei",2021,Association for Computing Machinery,,https://doi.org/10.1145/3494885.3494887,10.1145/3494885.3494887,"With the development of social platforms, the public can easily access lots of online information and express personal opinions. However, various unconfirmed rumors are flooding on social networks and leading public opinion which caused serious social problems. In view of this, scholars pay more and more attention to the research of rumors, such as the rumor stance classification. The rumor stance classification task aims to determine the stance of the target tweet about specific social media posts. The stance is divided into four categories: support, deny, query and comment. SemEval-2019 Task7 provided a dataset of dubious posts and ensuing conversations in social network which annotated the stance and accuracy of posts and subsequent conversations. We proposed a ensemble method based on BSAF model for the rumor stance classification. Pre-processed the original data and extracted the word-level features and tweet-level features of the target tweet are the first step. The pre-training model uses the fine-tuned BERT model to enrich semantic information and the key words are labeled via the self-attention mechanism. Finally the stance of target tweet is predicted with tweet vector, word-level features and tweet-level features. A number of BSAF models are obtained by adjusting the training parameters. According to the results of the validation set, the excellent BSAF model is added to the ensemble to predict the stance of target tweet of test set. In experiments, the ensemble method based on BSAF model has achieved the state-of-the-art results in F1-macro score and accuracy.","rumour stance, BERT model, self-attention mechanism, model ensemble, feature extraction",6–12,7,2021 4th International Conference on Computer Science and Software Engineering (CSSE 2021)
133,@inproceedings: 10.1145/3494885.3494919,Using Contextual Text Mining Algorithm to Analysis Yearly Trend for Population Ageing and Declining Fertility with Government Science and Technology Projects in Taiwan,"Chuang, C. Y. and Huang, M. C. and Lin, Y. Y. and Hsiao, Y. H. and Wang, T. C.",2021,Association for Computing Machinery,,https://doi.org/10.1145/3494885.3494919,10.1145/3494885.3494919,,"word segmentation, sBERT, TF-IDF, Word2Vec, contextual text mining, declining fertility, population ageing",184–189,6,2021 4th International Conference on Computer Science and Software Engineering (CSSE 2021)
134,@inproceedings: 10.1145/3494885.3494895,Synthetic Aperture Radar Image Target Recognition Based on Improved Fusion of R-FCN and SRC,"Liu, Yutong and Wang, Yumei",2021,Association for Computing Machinery,,https://doi.org/10.1145/3494885.3494895,10.1145/3494885.3494895,"In order to solve the problem of inaccurate classification caused by enhanced recognition ability and coherent speckle noise in synthetic spatial aperture radar image target recognition, a synthetic aperture radar target recognition algorithm based on the fusion of improved R-FCN structure and SRC is proposed. The algorithm improves the R-FCN structure by optimizing the location-sensitive score map to improve the pooling area, in order to better obtain the target features, and combines the sparse representation classifier as the basic classifier to classify the categories. At the same time, the correct classification can be added to the original test sample through the decision criteria, which can increase the number of samples and enhance the recognition ability at the same time. Through the improvement of the structure of R-FCN, the effect of coherent speckle noise in SAR images can be suppressed based on the effective feature extraction of target features, so as to achieve better target detection. Experiments based on the MSTAR data set prove that the algorithm has better recognition capabilities.","sparse representation classification, target recognition, small sample, synthetic aperture radar image, improved R-FCN structure",53–60,8,2021 4th International Conference on Computer Science and Software Engineering (CSSE 2021)
135,@inproceedings: 10.1145/3494885.3494925,Military Target Recognition Technology Based on WGAN-GP and XGBoost,"Zhao, Kainan and Dong, Baoliang and Yang, Cheng",2021,Association for Computing Machinery,,https://doi.org/10.1145/3494885.3494925,10.1145/3494885.3494925," This paper proposes a military target recognition method based on WGAN-GP and XGBoost, which expands and improves the quality of military target samples by constructing WGAN-GP, then sampling iteratively based on heuristic learning to construct an effective sample training set. On the basis of the quality training set, XGBoost is used for supervised learning, and finally a classification network model for military target recognition is obtained. Compared with methods such as CNN, KNN, and SVM, the method proposed in the article has a 1.01% to 58.84% higher overall accuracy of target sample recognition, and the overall accuracy is 27.36% &nbsp; 57.46% higher under the conditions of different small-scale samples.","gradient boosting decision tree, generative adversarial network, target recognition, heuristic learning",216–220,5,2021 4th International Conference on Computer Science and Software Engineering (CSSE 2021)
136,@inproceedings: 10.1145/3494885.3494943,Image Super-Resolution for Arthropod Identification,"Wani, Duhita and Maul, Tomas",2021,Association for Computing Machinery,,https://doi.org/10.1145/3494885.3494943,10.1145/3494885.3494943," Super-resolution techniques have recently made great strides, especially in the context of deep learning. In spite of this, not much research has been conducted on the explicit application of these techniques to biodiversity related problems such as species identification. We took a state-of-the-art super-resolution model (enhanced deep super-resolution network, i.e. EDSR), and enhanced it further with perceptual and texture losses, and a test-time-augmentation solution. Furthermore, we designed a qualitative assessment framework and studied its relationship with automated performance metrics. Our results show that our proposed modifications to EDSR improve the recovery of details, and that current automated metrics (e.g. Peak Signal-to-Noise Ratio) are inadequate in the context of super-resolution for species identification.","Deep Learning, Computer Vision, Neural Networks, Super-Resolution, Biodiversity",317–324,8,2021 4th International Conference on Computer Science and Software Engineering (CSSE 2021)
137,@inproceedings: 10.1145/3494885.3494953,GAN Theft Auto: Autonomous Texturing of Procedurally Generated Interactive Cities,"Dadfar, Oscar and Huang, Lingdong and \c{C}elik, Hizal",2021,Association for Computing Machinery,,https://doi.org/10.1145/3494885.3494953,10.1145/3494885.3494953," We explore the possibility of producing photo-realistic and stylized videos from semantically segmented image sequences drawn from a procedurally generated interactive 3D environment. We evaluate our environment using the Cityscape and ACDC weather dataset to obtain swappable daytime, nighttime, and various weather texturings from our city. We further use the GTA V image dataset to showcase its feasibility on large interactive scenes for 3D animation and game texturing, demonstrating the ability to repurpose existing video game textures when generating our city. Our algorithm can be used to support video games by autonomizing both the environment generation process, as well as supporting researchers by providing a semantic testing environment for many city style-transfer algorithms. Video documentation at: https://tinyurl.com/vb63k87x","Computer Games., Autonomous Texturing, Style-Transfer",362–366,5,2021 4th International Conference on Computer Science and Software Engineering (CSSE 2021)
138,@inproceedings: 10.1145/3486607.3486749,Natural Language-Guided Programming,"Heyman, Geert and Huysegems, Rafael and Justen, Pascal and Van Cutsem, Tom",2021,Association for Computing Machinery,,https://doi.org/10.1145/3486607.3486749,10.1145/3486607.3486749,"In today’s software world with its cornucopia of reusable software libraries, when a programmer is faced with a programming task that they suspect can be completed through the use of a library, they often look for code examples using a search engine and then manually adapt found examples to their specific context of use. We put forward a vision based on a new breed of developer tools that have the potential to largely automate this process. The key idea is to adapt code autocompletion tools such that they take into account not only the developer’s already-written code but also the intent of the task the developer is trying to achieve next, formulated in plain natural language. We call this practice of enriching the code with natural language intent to facilitate its completion natural language-guided programming. To show that this idea is feasible we design, implement and benchmark a tool that solves this problem in the context of a specific domain (data science) and a specific programming language (Python). Central to the tool is the use of language models trained on a large corpus of documented code. Our initial experiments confirm the feasibility of the idea but also make it clear that we have only scratched the surface of what may become possible in the future. We end the paper with a comprehensive research agenda to stimulate additional research in the budding area of natural language-guided programming.","natural language-guided programming, code completion, example-centric programming, code prediction",39–55,17,"Proceedings of the 2021 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software"
139,@inproceedings: 10.1145/3486607.3486748,Programming with Neural Surrogates of Programs,"Renda, Alex and Ding, Yi and Carbin, Michael",2021,Association for Computing Machinery,,https://doi.org/10.1145/3486607.3486748,10.1145/3486607.3486748,"Surrogates, models that mimic the behavior of programs, form the basis of a variety of development workflows. We study three surrogate-based design patterns, evaluating each in case studies on a large-scale CPU simulator. With surrogate compilation, programmers develop a surrogate that mimics the behavior of a program to deploy to end-users in place of the original program. Surrogate compilation accelerates the CPU simulator under study by 1.6\texttimes{}. With surrogate adaptation, programmers develop a surrogate of a program then retrain that surrogate on a different task. Surrogate adaptation decreases the simulator’s error by up to 50%. With surrogate optimization, programmers develop a surrogate of a program, optimize input parameters of the surrogate, then plug the optimized input parameters back into the original program. Surrogate optimization finds simulation parameters that decrease the simulator’s error by 5% compared to the error induced by expert-set parameters. In this paper we formalize this taxonomy of surrogate-based design patterns. We further describe the programming methodology common to all three design patterns. Our work builds a foundation for the emerging class of workflows based on programming with surrogates of programs.","machine learning, neural networks, programming languages, surrogate models",18–38,21,"Proceedings of the 2021 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software"
140,@inproceedings: 10.1145/3486609.3487199,Artifact and Reference Models for Generative Machine Learning Frameworks and Build Systems,"Atouani, Abdallah and Kirchhof, J\""{o}rg Christian and Kusmenko, Evgeny and Rumpe, Bernhard",2021,Association for Computing Machinery,,https://doi.org/10.1145/3486609.3487199,10.1145/3486609.3487199,"Machine learning is a discipline which has become ubiquitous in the last few years. While the research of machine learning algorithms is very active and continues to reveal astonishing possibilities on a regular basis, the wide usage of these algorithms is shifting the research focus to the integration, maintenance, and evolution of AI-driven systems. Although there is a variety of machine learning frameworks on the market, there is little support for process automation and DevOps in machine learning-driven projects. In this paper, we discuss how metamodels can support the development of deep learning frameworks and help deal with the steadily increasing variety of learning algorithms. In particular, we present a deep learning-oriented artifact model which serves as a foundation for build automation and data management in iterative, machine learning-driven development processes. Furthermore, we show how schema and reference models can be used to structure and maintain a versatile deep learning framework. Feasibility is demonstrated on several state-of-the-art examples from the domains of image and natural language processing as well as decision making and autonomous driving.","compiler, artificial intelligence, build systems, reference models, artifact models, metamodeling, training, machine learning",55–68,14,Proceedings of the 20th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences
141,@article: 10.1145/3485477,Semantic Programming by Example with Pre-Trained Models,"Verbruggen, Gust and Le, Vu and Gulwani, Sumit",2021,Association for Computing Machinery,,https://doi.org/10.1145/3485477,10.1145/3485477,"The ability to learn programs from few examples is a powerful technology with disruptive applications in many domains, as it allows users to automate repetitive tasks in an intuitive way. Existing frameworks on inductive synthesis only perform syntactic manipulations, where they rely on the syntactic structure of the given examples and not their meaning. Any semantic manipulations, such as transforming dates, have to be manually encoded by the designer of the inductive programming framework. Recent advances in large language models have shown these models to be very adept at performing semantic transformations of its input by simply providing a few examples of the task at hand. When it comes to syntactic transformations, however, these models are limited in their expressive power. In this paper, we propose a novel framework for integrating inductive synthesis with few-shot learning language models to combine the strength of these two popular technologies. In particular, the inductive synthesis is tasked with breaking down the problem in smaller subproblems, among which those that cannot be solved syntactically are passed to the language model. We formalize three semantic operators that can be integrated with inductive synthesizers. To minimize invoking expensive semantic operators during learning, we introduce a novel deferred query execution algorithm that considers the operators to be oracles during learning. We evaluate our approach in the domain of string transformations: the combination methodology can automate tasks that cannot be handled using either technologies by themselves. Finally, we demonstrate the generality of our approach via a case study in the domain of string profiling.","programming by example, language models, program synthesi",,25,Proc. ACM Program. Lang.
142,@article: 10.1145/3485535,Multi-Modal Program Inference: A Marriage of Pre-Trained Language Models and Component-Based Synthesis,"Rahmani, Kia and Raza, Mohammad and Gulwani, Sumit and Le, Vu and Morris, Daniel and Radhakrishna, Arjun and Soares, Gustavo and Tiwari, Ashish",2021,Association for Computing Machinery,,https://doi.org/10.1145/3485535,10.1145/3485535,"Multi-modal program synthesis refers to the task of synthesizing programs (code) from their specification given in different forms, such as a combination of natural language and examples. Examples provide a precise but incomplete specification, and natural language provides an ambiguous but more ""complete"" task description. Machine-learned pre-trained models (PTMs) are adept at handling ambiguous natural language, but struggle with generating syntactically and semantically precise code. Program synthesis techniques can generate correct code, often even from incomplete but precise specifications, such as examples, but they are unable to work with the ambiguity of natural languages. We present an approach that combines PTMs with component-based synthesis (CBS): PTMs are used to generate candidates programs from the natural language description of the task, which are then used to guide the CBS procedure to find the program that matches the precise examples-based specification. We use our combination approach to instantiate multi-modal synthesis systems for two programming domains: the domain of regular expressions and the domain of CSS selectors. Our evaluation demonstrates the effectiveness of our domain-agnostic approach in comparison to a state-of-the-art specialized system, and the generality of our approach in providing multi-modal program synthesis from natural language and examples in different programming domains.","GPT-3, Program Inference, Natural Language Model",,29,Proc. ACM Program. Lang.
143,@article: 10.1145/3485486,Efficient Automatic Scheduling of Imaging and Vision Pipelines for the GPU,"Anderson, Luke and Adams, Andrew and Ma, Karima and Li, Tzu-Mao and Jin, Tian and Ragan-Kelley, Jonathan",2021,Association for Computing Machinery,,https://doi.org/10.1145/3485486,10.1145/3485486,"We present a new algorithm to quickly generate high-performance GPU implementations of complex imaging and vision pipelines, directly from high-level Halide algorithm code. It is fully automatic, requiring no schedule templates or hand-optimized kernels. We address the scalability challenge of extending search-based automatic scheduling to map large real-world programs to the deep hierarchies of memory and parallelism on GPU architectures in reasonable compile time. We achieve this using (1) a two-phase search algorithm that first ‘freezes’ decisions for the lowest cost sections of a program, allowing relatively more time to be spent on the important stages, (2) a hierarchical sampling strategy that groups schedules based on their structural similarity, then samples representatives to be evaluated, allowing us to explore a large space with few samples, and (3) memoization of repeated partial schedules, amortizing their cost over all their occurrences. We guide the process with an efficient cost model combining machine learning, program analysis, and GPU architecture knowledge. We evaluate our method’s performance on a diverse suite of real-world imaging and vision pipelines. Our scalability optimizations lead to average compile time speedups of 49x (up to 530x). We find schedules that are on average 1.7x faster than existing automatic solutions (up to 5x), and competitive with what the best human experts were able to achieve in an active effort to beat our automatic results.","Halide, optimizing compiler",,28,Proc. ACM Program. Lang.
144,@inbook: 10.1145/3472749.3474743,Taming FNIRS-Based BCI Input for Better Calibration and Broader Use,"Wang, Liang and Huang, Zhe and Zhou, Ziyu and McKeon, Devon and Blaney, Giles and Hughes, Michael C. and Jacob, Robert J. K.",2021,Association for Computing Machinery,,https://doi.org/10.1145/3472749.3474743,,"Brain-computer interfaces (BCI) are an emerging technology with many potential applications. Functional near-infrared spectroscopy (fNIRS) can provide a convenient and unobtrusive real time input for BCI. fNIRS is especially promising as a signal that could be used to automatically classify a user’s current cognitive workload. However, the data needed to train such a classifier is currently not widely available, difficult to collect, and difficult to interpret due to noise and cross-subject variation. A further challenge is the need for significant user-specific calibration. To address these issues, we introduce a new dataset gathered from 15 subjects and a new multi-stage supervised machine learning pipeline. Our approach learns from both observed data and augmented data derived from multiple subjects in its early stages, and then fine-tunes predictions to an individual subject in its last stage. We show promising gains in accuracy in a standard “n-back” cognitive workload classification task compared to baselines that use only subject-specific data or only group-level data, even when our approach is given much less subject-specific data. Even though these experiments analyzed the data retrospectively, we carefully removed anything from our process that could not have been done in real time, because our process is targeted at future real-time operation. This paper contributes a new dataset, a new multi-stage training pipeline, results showing significant improvement compared to alternative pipelines, and discussion of the implications for user interface design. Our complete dataset and software are publicly available at https://tufts-hci-lab.github.io/code_and_datasets/. We hope these results make fNIRS-based interactive brain input easier for a wide range of future researchers and designers to explore.",,179–197,1,The 34th Annual ACM Symposium on User Interface Software and Technology
145,@inbook: 10.1145/3472749.3474817,HandyTrak: Recognizing the Holding Hand on a Commodity Smartphone from Body Silhouette Images,"Lim, Hyunchul and Lin, David and Tweneboah, Jessica and Zhang, Cheng",2021,Association for Computing Machinery,,https://doi.org/10.1145/3472749.3474817,," Understanding which hand a user holds a smartphone with can help improve the mobile interaction experience. For instance, the layout of the user interface (UI) can be adapted to the holding hand. In this paper, we present HandyTrak, an AI-powered software system that recognizes the holding hand on a commodity smartphone using body silhouette images captured by the front-facing camera. The silhouette images are processed and sent to a customized user-dependent deep learning model (CNN) to infer how the user holds the smartphone (left, right, or both hands). We evaluated our system on each participant’s smartphone at five possible front camera positions in a user study with ten participants under two hand positions (in the middle and skewed) and three common usage cases (standing, sitting, and resting against a desk). The results showed that HandyTrak was able to continuously recognize the holding hand with an average accuracy of 89.03% (SD: 8.98%) at a 2 Hz sampling rate. We also discuss the challenges and opportunities to deploy HandyTrak on different commodity smartphones and potential applications in real-world scenarios.",,1210–1220,1,The 34th Annual ACM Symposium on User Interface Software and Technology
146,@inbook: 10.1145/3472749.3474759,DextrEMS: Increasing Dexterity in Electrical Muscle Stimulation by Combining It with Brakes,"Nith, Romain and Teng, Shan-Yuan and Li, Pengyu and Tao, Yujie and Lopes, Pedro",2021,Association for Computing Machinery,,https://doi.org/10.1145/3472749.3474759,,"Electrical muscle stimulation (EMS) is an emergent technique that miniaturizes force feedback, especially popular for untethered haptic devices, such as mobile gaming, VR, or AR. However, the actuation displayed by interactive systems based on EMS is coarse and imprecise. EMS systems mostly focus on inducing movements in large muscle groups such as legs, arms, and wrists; whereas individual finger poses, which would be required, for example, to actuate a user's fingers to fingerspell even the simplest letters in sign language, are not possible. The lack of dexterity in EMS stems from two fundamental limitations: (1) lack of independence: when a particular finger is actuated by EMS, the current runs through nearby muscles, causing unwanted actuation of adjacent fingers; and, (2) unwanted oscillations: while it is relatively easy for EMS to start moving a finger, it is very hard for EMS to stop and hold that finger at a precise angle; because, to stop a finger, virtually all EMS systems contract the opposing muscle, typically achieved via controllers (e.g., PID)—unfortunately, even with the best controller tuning, this often results in unwanted oscillations. To tackle these limitations, we propose dextrEMS, an EMS-based haptic device featuring mechanical brakes attached to each finger joint. The key idea behind dextrEMS is that while the EMS actuates the fingers, it is our mechanical brake that stops the finger in a precise position. Moreover, it is also the brakes that allow dextrEMS to select which fingers are moved by EMS, eliminating unwanted movements by preventing adjacent fingers from moving. We implemented dextrEMS as an untethered haptic device, weighing only 68g, that actuates eight finger joints independently (metacarpophalangeal and proximal interphalangeal joints for four fingers), which we demonstrate in a wide range of haptic applications, such as assisted fingerspelling, a piano tutorial, guitar tutorial, and a VR game. Finally, in our technical evaluation, we found that dextrEMS outperformed EMS alone by doubling its independence and reducing unwanted oscillations.",,414–430,1,The 34th Annual ACM Symposium on User Interface Software and Technology
147,@article: 10.1145/3468854,SPI: Automated Identification of Security Patches via Commits,"Zhou, Yaqin and Siow, Jing Kai and Wang, Chenyu and Liu, Shangqing and Liu, Yang",2021,Association for Computing Machinery,1049-331X,https://doi.org/10.1145/3468854,10.1145/3468854,"Security patches in open source software, providing security fixes to identified vulnerabilities, are crucial in protecting against cyber attacks. Security advisories and announcements are often publicly released to inform the users about potential security vulnerability. Despite the National Vulnerability Database (NVD) publishes identified vulnerabilities, a vast majority of vulnerabilities and their corresponding security patches remain beyond public exposure, e.g., in the open source libraries that are heavily relied on by developers. As many of these patches exist in open sourced projects, the problem of curating and gathering security patches can be difficult due to their hidden nature. An extensive and complete security patches dataset could help end-users such as security companies, e.g., building a security knowledge base, or researcher, e.g., aiding in vulnerability research.To efficiently curate security patches including undisclosed patches at large scale and low cost, we propose a deep neural-network-based approach built upon commits of open source repositories. First, we design and build security patch datasets that include 38,291 security-related commits and 1,045 Common Vulnerabilities and Exposures (CVE) patches from four large-scale C programming language libraries. We manually verify each commit, among the 38,291 security-related commits, to determine if they are security related.We devise and implement a deep learning-based security patch identification system that consists of two composite neural networks: one commit-message neural network that utilizes pretrained word representations learned from our commits dataset and one code-revision neural network that takes code before revision and after revision and learns the distinction on the statement level. Our system leverages the power of the two networks for Security Patch Identification. Evaluation results show that our system significantly outperforms SVM and K-fold stacking algorithms. The result on the combined dataset achieves as high as 87.93% F1-score and precision of 86.24%.We deployed our pipeline and learned model in an industrial production environment to evaluate the generalization ability of our approach. The industrial dataset consists of 298,917 commits from 410 new libraries that range from a wide functionalities. Our experiment results and observation on the industrial dataset proved that our approach can identify security patches effectively among open sourced projects.","software security, deep learning, Machine learnin",,27,ACM Trans. Softw. Eng. Methodol.
148,@inproceedings: 10.1145/3475061.3475083,A Graph-Based Model for Building Optimization Sequences: A Study Case on Code Size Reduction,"Queiroz Junior, Nilton Luiz and Faustino da Silva, Anderson",2021,Association for Computing Machinery,,https://doi.org/10.1145/3475061.3475083,10.1145/3475061.3475083," Embedded Systems applications have several limitations, one of these limitations is the memory size. Modern compilers provide optimization sequences that reduce the code size, contributing to solve this memory issue. This paper presents a new approach to predict optimization sequences for unseen programs. Our strategy builds a graph-based model, which describes the relationship between two optimization, and an engine to traversal the graph and build an optimization sequence. We propose four traverse algorithms. The best traversal algorithm overcomes OPT Oz in 4%. Moreover, our algorithms have a performance similar to state-of-the-art algorithms, through a single evaluation. ","Code Size, Program reasoning, Compiler",92–99,8,25th Brazilian Symposium on Programming Languages
149,@article: 10.1145/3476986,Learning to Train CNNs on Faulty ReRAM-Based Manycore Accelerators,"Joardar, Biresh Kumar and Doppa, Janardhan Rao and Li, Hai and Chakrabarty, Krishnendu and Pande, Partha Pratim",2021,Association for Computing Machinery,1539-9087,https://doi.org/10.1145/3476986,10.1145/3476986,"The growing popularity of convolutional neural networks (CNNs) has led to the search for efficient computational platforms to accelerate CNN training. Resistive random-access memory (ReRAM)-based manycore architectures offer a promising alternative to commonly used GPU-based platforms for training CNNs. However, due to the immature fabrication process and limited write endurance, ReRAMs suffer from different types of faults. This makes training of CNNs challenging as weights are misrepresented when they are mapped to faulty ReRAM cells. This results in unstable training, leading to unacceptably low accuracy for the trained model. Due to the distributed nature of the mapping of the individual bits of a weight to different ReRAM cells, faulty weights often lead to exploding gradients. This in turn introduces a positive feedback in the training loop, resulting in extremely large and unstable weights. In this paper, we propose a lightweight and reliable CNN training methodology using weight clipping to prevent this phenomenon and enable training even in the presence of many faults. Weight clipping prevents large weights from destabilizing CNN training and provides the backpropagation algorithm with the opportunity to compensate for the weights mapped to faulty cells. The proposed methodology achieves near-GPU accuracy without introducing significant area or performance overheads. Experimental evaluation indicates that weight clipping enables the successful training of CNNs in the presence of faults, while also reducing training time by 4X on average compared to a conventional GPU platform. Moreover, we also demonstrate that weight clipping outperforms a recently proposed error correction code (ECC)-based method when training is carried out using faulty ReRAMs.","hardware faults, ReRAM, reliable training, CNN",,23,ACM Trans. Embed. Comput. Syst.
189,@inbook: 10.1145/3368089.3409731,API Method Recommendation via Explicit Matching of Functionality Verb Phrases,"Xie, Wenkai and Peng, Xin and Liu, Mingwei and Treude, Christoph and Xing, Zhenchang and Zhang, Xiaoxin and Zhao, Wenyun",2020,Association for Computing Machinery,,https://doi.org/10.1145/3368089.3409731,,"Due to the lexical gap between functionality descriptions and user queries, documentation-based API retrieval often produces poor results.Verb phrases and their phrase patterns are essential in both describing API functionalities and interpreting user queries. Thus we hypothesize that API retrieval can be facilitated by explicitly recognizing and matching between the fine-grained structures of functionality descriptions and user queries. To verify this hypothesis, we conducted a large-scale empirical study on the functionality descriptions of 14,733 JDK and Android API methods. We identified 356 different functionality verbs from the descriptions, which were grouped into 87 functionality categories, and we extracted 523 phrase patterns from the verb phrases of the descriptions. Building on these findings, we propose an API method recommendation approach based on explicit matching of functionality verb phrases in functionality descriptions and user queries, called PreMA. Our evaluation shows that PreMA can accurately recognize the functionality categories (92.8%) and phrase patterns (90.4%) of functionality description sentences; and when used for API retrieval tasks, PreMA can help participants complete their tasks more accurately and with fewer retries compared to a baseline approach.",,1015–1026,1,Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
190,@inbook: 10.1145/3368089.3409754,Is Neuron Coverage a Meaningful Measure for Testing Deep Neural Networks?,"Harel-Canada, Fabrice and Wang, Lingxiao and Gulzar, Muhammad Ali and Gu, Quanquan and Kim, Miryung",2020,Association for Computing Machinery,,https://doi.org/10.1145/3368089.3409754,,"Recent effort to test deep learning systems has produced an intuitive and compelling test criterion called neuron coverage (NC), which resembles the notion of traditional code coverage. NC measures the proportion of neurons activated in a neural network and it is implicitly assumed that increasing NC improves the quality of a test suite. In an attempt to automatically generate a test suite that increases NC, we design a novel diversity promoting regularizer that can be plugged into existing adversarial attack algorithms. We then assess whether such attempts to increase NC could generate a test suite that (1) detects adversarial attacks successfully, (2) produces natural inputs, and (3) is unbiased to particular class predictions. Contrary to expectation, our extensive evaluation finds that increasing NC actually makes it harder to generate an effective test suite: higher neuron coverage leads to fewer defects detected, less natural inputs, and more biased prediction preferences. Our results invoke skepticism that increasing neuron coverage may not be a meaningful objective for generating tests for deep neural networks and call for a new test generation technique that considers defect detection, naturalness, and output impartiality in tandem.",,851–862,1,Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
191,@inbook: 10.1145/3368089.3417928,BEE: A Tool for Structuring and Analyzing Bug Reports,"Song, Yang and Chaparro, Oscar",2020,Association for Computing Machinery,,https://doi.org/10.1145/3368089.3417928,,"This paper introduces BEE, a tool that automatically analyzes user-written bug reports and provides feedback to reporters and developers about the system’s observed behavior (OB), expected behavior (EB), and the steps to reproduce the bug (S2R). BEE employs machine learning to (i) detect if an issue describes a bug, an enhancement, or a question; (ii) identify the structure of bug descriptions by automatically labeling the sentences that correspond to the OB, EB, or S2R; and (iii) detect when bug reports fail to provide these elements. BEE is integrated with GitHub and offers a public web API that researchers can use to investigate bug management tasks based on bug reports. We evaluated BEE’s underlying models on more than 5k existing bug reports and found they can correctly detect OB, EB, and S2R sentences as well as missing information in bug reports. BEE is an open-source project that can be found at <a>https://git.io/JfFnN</a>. A screencast showing the full capabilities of BEE can be found at <a>https://youtu.be/8pC48f_hClw</a>.",,1551–1555,,Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
192,@inbook: 10.1145/3368089.3409671,Correlations between Deep Neural Network Model Coverage Criteria and Model Quality,"Yan, Shenao and Tao, Guanhong and Liu, Xuwei and Zhai, Juan and Ma, Shiqing and Xu, Lei and Zhang, Xiangyu",2020,Association for Computing Machinery,,https://doi.org/10.1145/3368089.3409671,,"Inspired by the great success of using code coverage as guidance in software testing, a lot of neural network coverage criteria have been proposed to guide testing of neural network models (e.g., model accuracy under adversarial attacks). However, while the monotonic relation between code coverage and software quality has been supported by many seminal studies in software engineering, it remains largely unclear whether similar monotonicity exists between neural network model coverage and model quality. This paper sets out to answer this question. Specifically, this paper studies the correlation between DNN model quality and coverage criteria, effects of coverage guided adversarial example generation compared with gradient decent based methods, effectiveness of coverage based retraining compared with existing adversarial training, and the internal relationships among coverage criteria.",,775–787,1,Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
193,@inbook: 10.1145/3368089.3409723,MTFuzz: Fuzzing with a Multi-Task Neural Network,"She, Dongdong and Krishna, Rahul and Yan, Lu and Jana, Suman and Ray, Baishakhi",2020,Association for Computing Machinery,,https://doi.org/10.1145/3368089.3409723,,"Fuzzing is a widely used technique for detecting software bugs and vulnerabilities. Most popular fuzzers generate new inputs using an evolutionary search to maximize code coverage. Essentially, these fuzzers start with a set of seed inputs, mutate them to generate new inputs, and identify the promising inputs using an evolutionary fitness function for further mutation.Despite their success, evolutionary fuzzers tend to get stuck in long sequences of unproductive mutations. In recent years, machine learning (ML) based mutation strategies have reported promising results. However, the existing ML-based fuzzers are limited by the lack of quality and diversity of the training data. As the input space of the target programs is high dimensional and sparse, it is prohibitively expensive to collect many diverse samples demonstrating successful and unsuccessful mutations to train the model.In this paper, we address these issues by using a Multi-Task Neural Network that can learn a compact embedding of the input space based on diverse training samples for multiple related tasks (i.e.,predicting for different types of coverage). The compact embedding can guide the mutation process by focusing most of the mutations on the parts of the embedding where the gradient is high. MTFuzz uncovers 11 previously unseen bugs and achieves an average of 2\texttimes{} more edge coverage compared with 5 state-of-the-art fuzzer on 10 real-world programs",,737–749,1,Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
194,@inbook: 10.1145/3368089.3409761,Deep Learning Library Testing via Effective Model Generation,"Wang, Zan and Yan, Ming and Chen, Junjie and Liu, Shuang and Zhang, Dongdi",2020,Association for Computing Machinery,,https://doi.org/10.1145/3368089.3409761,,"Deep learning (DL) techniques are rapidly developed and have been widely adopted in practice. However, similar to traditional software systems, DL systems also contain bugs, which could cause serious impacts especially in safety-critical domains. Recently, many research approaches have focused on testing DL models, while little attention has been paid for testing DL libraries, which is the basis of building DL models and directly affects the behavior of DL systems. In this work, we propose a novel approach, LEMON, to testing DL libraries. In particular, we (1) design a series of mutation rules for DL models, with the purpose of exploring different invoking sequences of library code and hard-to-trigger behaviors; and (2) propose a heuristic strategy to guide the model generation process towards the direction of amplifying the inconsistent degrees of the inconsistencies between different DL libraries caused by bugs, so as to mitigate the impact of potential noise introduced by uncertain factors in DL libraries. We conducted an empirical study to evaluate the effectiveness of LEMON with 20 release versions of 4 widely-used DL libraries, i.e., TensorFlow, Theano, CNTK, MXNet. The results demonstrate that LEMON detected 24 new bugs in the latest release versions of these libraries, where 7 bugs have been confirmed and one bug has been fixed by developers. Besides, the results confirm that the heuristic strategy for model generation indeed effectively guides LEMON in amplifying the inconsistent degrees for bugs.",,788–799,1,Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
195,@inproceedings: 10.1145/3368089.3417919,Threshy: Supporting Safe Usage of Intelligent Web Services,"Cummaudo, Alex and Barnett, Scott and Vasa, Rajesh and Grundy, John",2020,Association for Computing Machinery,,https://doi.org/10.1145/3368089.3417919,10.1145/3368089.3417919,"Increased popularity of ‘intelligent’ web services provides end-users with machine-learnt functionality at little effort to developers. However, these services require a decision threshold to be set which is dependent on problem-specific data. Developers lack a systematic approach for evaluating intelligent services and existing evaluation tools are predominantly targeted at data scientists for pre-development evaluation. This paper presents a workflow and supporting tool, Threshy, to help software developers select a decision threshold suited to their problem domain. Unlike existing tools, Threshy is designed to operate in multiple workflows including pre-development, pre-release, and support. Threshy is designed for tuning the confidence scores returned by intelligent web services and does not deal with hyper-parameter optimisation used in ML models. Additionally, it considers the financial impacts of false positives. Threshold configuration files exported by Threshy can be integrated into client applications and monitoring infrastructure. Demo: <a>https://bit.ly/2YKeYhE</a>.","tooling, decision theory, thresholding, intelligent services",1645–1649,5,Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
196,@inbook: 10.1145/3368089.3409768,Identifying Linked Incidents in Large-Scale Online Service Systems,"Chen, Yujun and Yang, Xian and Dong, Hang and He, Xiaoting and Zhang, Hongyu and Lin, Qingwei and Chen, Junjie and Zhao, Pu and Kang, Yu and Gao, Feng and Xu, Zhangwei and Zhang, Dongmei",2020,Association for Computing Machinery,,https://doi.org/10.1145/3368089.3409768,,"In large-scale online service systems, incidents occur frequently due to a variety of causes, from updates of software and hardware to changes in operation environment. These incidents could significantly degrade system’s availability and customers’ satisfaction. Some incidents are linked because they are duplicate or inter-related. The linked incidents can greatly help on-call engineers find mitigation solutions and identify the root causes. In this work, we investigate the incidents and their links in a representative real-world incident management (IcM) system. Based on the identified indicators of linked incidents, we further propose LiDAR (Linked Incident identification with DAta-driven Representation), a deep learning based approach to incident linking. More specifically, we incorporate the textual description of incidents and structural information extracted from historical linked incidents to identify possible links among a large number of incidents. To show the effectiveness of our method, we apply our method to a real-world IcM system and find that our method outperforms other state-of-the-art methods.",,304–314,1,Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
197,@inbook: 10.1145/3368089.3417051,Enhancing the Interoperability between Deep Learning Frameworks by Model Conversion,"Liu, Yu and Chen, Cheng and Zhang, Ru and Qin, Tingting and Ji, Xiang and Lin, Haoxiang and Yang, Mao",2020,Association for Computing Machinery,,https://doi.org/10.1145/3368089.3417051,,"Deep learning (DL) has become one of the most successful machine learning techniques. To achieve the optimal development result, there are emerging requirements on the interoperability between DL frameworks that the trained model files and training/serving programs can be re-utilized. Faithful model conversion is a promising technology to enhance the framework interoperability in which a source model is transformed into the semantic equivalent in another target framework format. However, several major challenges need to be addressed. First, there are apparent discrepancies between DL frameworks. Second, understanding the semantics of a source model could be difficult due to the framework scheme and optimization. Lastly, there exist a large number of DL frameworks, bringing potential significant engineering efforts.  In this paper, we propose MMdnn, an open-sourced, comprehensive, and faithful model conversion tool for popular DL frameworks. MMdnn adopts a novel unified intermediate representation (IR)-based methodology to systematically handle the conversion challenges. The source model is first transformed into an intermediate computation graph represented by the simple graph-based IR of MMdnn and then to the target framework format, which greatly reduces the engineering complexity. Since the model structure expressed by developers may have been changed by DL frameworks (e.g., graph optimization), MMdnn tries to recover the original high-level neural network layers for better semantic comprehension via a pattern matching similar method. In the meantime, a piece of model construction code is generated to facilitate later retraining or serving. MMdnn implements an extensible conversion architecture from the compilation point of view, which eases contribution from the community to support new DL operators and frameworks. MMdnn has reached good maturity and quality, and is applied for converting production models.",,1320–1330,1,Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
198,@inbook: 10.1145/3368089.3409676,Dynamic Slicing for Deep Neural Networks,"Zhang, Ziqi and Li, Yuanchun and Guo, Yao and Chen, Xiangqun and Liu, Yunxin",2020,Association for Computing Machinery,,https://doi.org/10.1145/3368089.3409676,,"Program slicing has been widely applied in a variety of software engineering tasks. However, existing program slicing techniques only deal with traditional programs that are constructed with instructions and variables, rather than neural networks that are composed of neurons and synapses. In this paper, we introduce NNSlicer, the first approach for slicing deep neural networks based on data-flow analysis. Our method understands the reaction of each neuron to an input based on the difference between its behavior activated by the input and the average behavior over the whole dataset. Then we quantify the neuron contributions to the slicing criterion by recursively backtracking from the output neurons, and calculate the slice as the neurons and the synapses with larger contributions. We demonstrate the usefulness and effectiveness of NNSlicer with three applications, including adversarial input detection, model pruning, and selective model protection. In all applications, NNSlicer significantly outperforms other baselines that do not rely on data flow analysis.",,838–850,1,Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
199,@inbook: 10.1145/3368089.3409750,DeepSearch: A Simple and Effective Blackbox Attack for Deep Neural Networks,"Zhang, Fuyuan and Chowdhury, Sankalan Pal and Christakis, Maria",2020,Association for Computing Machinery,,https://doi.org/10.1145/3368089.3409750,,"Although deep neural networks have been very successful in image-classification tasks, they are prone to adversarial attacks. To generate adversarial inputs, there has emerged a wide variety of techniques, such as black- and whitebox attacks for neural networks. In this paper, we present DeepSearch, a novel fuzzing-based, query-efficient, blackbox attack for image classifiers. Despite its simplicity, DeepSearch is shown to be more effective in finding adversarial inputs than state-of-the-art blackbox approaches. DeepSearch is additionally able to generate the most subtle adversarial inputs in comparison to these approaches.",,800–812,1,Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
200,@inbook: 10.1145/3368089.3417054,How to Mitigate the Incident? An Effective Troubleshooting Guide Recommendation Technique for Online Service Systems,"Jiang, Jiajun and Lu, Weihai and Chen, Junjie and Lin, Qingwei and Zhao, Pu and Kang, Yu and Zhang, Hongyu and Xiong, Yingfei and Gao, Feng and Xu, Zhangwei and Dang, Yingnong and Zhang, Dongmei",2020,Association for Computing Machinery,,https://doi.org/10.1145/3368089.3417054,,"In recent years, more and more traditional shrink-wrapped software is provided as 7x24 online services. Incidents (events that lead to service disruptions or outages) could affect service availability and cause great financial loss. Therefore, mitigating the incidents is important and time critical. In practice, a document describing a mitigation process, called a troubleshooting guide (TSG), is usually used to reduce the Time To Mitigate (TTM). To investigate the usage of TSGs in real-world online services, we conduct the first empirical study on 18 real-world, large-scale online service systems in Microsoft. We analyze the distribution and characteristics of TSGs among all incident records in the past two years. According to our study, 27.2% incidents have TSG records and 36.2% of them occurred at least twice. Besides, on average developers spend around 36.3% of the entire mitigation time on locating the desired TSGs.  Our study shows that incidents could occur repeatedly and TSGs could be reused to facilitate incident mitigation. Motivated by our empirical study, we propose an automated TSG recommendation approach, DeepRmd, by leveraging the textual similarity between incident description and its corresponding TSG using deep learning techniques. We evaluate the effectiveness of DeepRmd on 18 online service systems. The results show that DeepRmd can recommend the correct TSG as the Top 1 returned result for 80.3% incidents, which significantly outperforms two baseline approaches.",,1410–1420,1,Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
201,@inbook: 10.1145/3368089.3409700,AMS: Generating AutoML Search Spaces from Weak Specifications,"Cambronero, Jos\'{e} P. and Cito, J\""{u}rgen and Rinard, Martin C.",2020,Association for Computing Machinery,,https://doi.org/10.1145/3368089.3409700,,"We consider a usage model for automated machine learning (AutoML) in which users can influence the generated pipeline by providing a weak pipeline specification: an unordered set of API components from which the AutoML system draws the components it places into the generated pipeline. Such specifications allow users to express preferences over the components that appear in the pipeline, for example a desire for interpretable components to appear in the pipeline. We present AMS, an approach to automatically strengthen weak specifications to include unspecified complementary and functionally related API components, populate the space of hyperparameters and their values, and pair this configuration with a search procedure to produce a strong pipeline specification: a full description of the search space for candidate pipelines. ams uses normalized pointwise mutual information on a code corpus to identify complementary components, BM25 as a lexical similarity score over the target API's documentation to identify functionally related components, and frequency distributions in the code corpus to extract key hyperparameters and values. We show that strengthened specifications can produce pipelines that outperform the pipelines generated from the initial weak specification and an expert-annotated variant, while producing pipelines that still reflect the user preferences captured in the original weak specification.",,763–774,1,Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
202,@inbook: 10.1145/3368089.3417925,Enhancing Developer Interactions with Programming Screencasts through Accurate Code Extraction,"Bao, Lingfeng and Pan, Shengyi and Xing, Zhenchang and Xia, Xin and Lo, David and Yang, Xiaohu",2020,Association for Computing Machinery,,https://doi.org/10.1145/3368089.3417925,,"Programming screencasts have become a pervasive resource on the Internet, which is favoured by many developers for learning new programming skills. For developers, the source code in screencasts is valuable and important. However, the streaming nature of screencasts limits the choice that they have for interacting with the code. Many studies apply the Optical Character Recognition (OCR) technique to convert screen images into text, which can be easily searched and indexed. However, we observe that the noise in the screen images significantly affects the quality of OCRed code. In this paper, we develop a tool named psc2code, which has two components, denoising code extraction from screencasts and enhancing programming video interaction. Experiment results on 1142 programming screencasts from YouTube show psc2code can effectively identify frames containing valid code region with a F1-score of 0.88 and improve the quality of OCRed code by fixing 46% of the errors. We also conduct a user study to evaluate the applicability of psc2code in enhancing video interaction, which shows it helps participants learn the knowledge in tutorials more efficiently.",,1581–1585,,Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
203,@inproceedings: 10.1145/3416508.3417118,An Exploratory Study on Applicability of Cross Project Defect Prediction Approaches to Cross-Company Effort Estimation,"Amasaki, Sousuke and Aman, Hirohisa and Yokogawa, Tomoyuki",2020,Association for Computing Machinery,,https://doi.org/10.1145/3416508.3417118,10.1145/3416508.3417118,"BACKGROUND: Research on software effort estimation has been active for decades, especially in developing effort estimation models. Effort estimation models need a dataset collected from completed projects similar to a project to be estimated. The similarity suffers from dataset shift, and cross-company software effort estimation (CCSEE) gets an attractive research topic. A recent study on the dataset shift problem examined the applicability and the effectiveness of cross-project defect prediction (CPDP) approaches. It was insufficient to bring a conclusion due to a limited number of examined approaches. AIMS: To investigate the characteristics of CPDP approaches that are applicable and effective for dataset shift problem in effort estimation. METHOD: We first reviewed the characteristics of 24 CPDP approaches to find applicable approaches. Next, we investigated their effectiveness in effort estimation performance with ten dataset configurations. RESULTS: 16 out of 24 CPDP approaches implemented in CrossPare framework were found to be applicable to CCSEE. However, only one approach could improve the effort estimation performance. Most of the others degraded it and were harmful. CONCLUSIONS: Most of the CPDP approaches we examined were helpless for CCSEE.","cross-project defect prediction, cross-company effort estimation, empirical evaluation",71–80,10,Proceedings of the 16th ACM International Conference on Predictive Models and Data Analytics in Software Engineering
204,@inproceedings: 10.1145/3368089.3409759,A Comprehensive Study on Challenges in Deploying Deep Learning Based Software,"Chen, Zhenpeng and Cao, Yanbin and Liu, Yuanqiang and Wang, Haoyu and Xie, Tao and Liu, Xuanzhe",2020,Association for Computing Machinery,,https://doi.org/10.1145/3368089.3409759,10.1145/3368089.3409759,"Deep learning (DL) becomes increasingly pervasive, being used in a wide range of software applications. These software applications, named as DL based software (in short as DL software), integrate DL models trained using a large data corpus with DL programs written based on DL frameworks such as TensorFlow and Keras. A DL program encodes the network structure of a desirable DL model and the process by which the model is trained using the training data. To help developers of DL software meet the new challenges posed by DL, enormous research efforts in software engineering have been devoted. Existing studies focus on the development of DL software and extensively analyze faults in DL programs. However, the deployment of DL software has not been comprehensively studied. To fill this knowledge gap, this paper presents a comprehensive study on understanding challenges in deploying DL software. We mine and analyze 3,023 relevant posts from Stack Overflow, a popular Q&amp;A website for developers, and show the increasing popularity and high difficulty of DL software deployment among developers. We build a taxonomy of specific challenges encountered by developers in the process of DL software deployment through manual inspection of 769 sampled posts and report a series of actionable implications for researchers, developers, and DL framework vendors.","Stack Overflow, software deployment, deep learning",750–762,13,Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
205,@inproceedings: 10.1145/3368089.3409675,Dimensions of Software Configuration: On the Configuration Context in Modern Software Development,"Siegmund, Norbert and Ruckel, Nicolai and Siegmund, Janet",2020,Association for Computing Machinery,,https://doi.org/10.1145/3368089.3409675,10.1145/3368089.3409675,"With the rise of containerization, cloud development, and continuous integration and delivery, configuration has become an essential aspect not only to tailor software to user requirements, but also to configure a software system’s environment and infrastructure. This heterogeneity of activities, domains, and processes blurs the term configuration, as it is not clear anymore what tasks, artifacts, or stakeholders are involved and intertwined. However, each re- search study and each paper involving configuration places their contributions and findings in a certain context without making the context explicit. This makes it difficult to compare findings, translate them to practice, and to generalize the results. Thus, we set out to evaluate whether these different views on configuration are really distinct or can be summarized under a common umbrella. By interviewing practitioners from different domains and in different roles about the aspects of configuration and by analyzing two qualitative studies in similar areas, we derive a model of configuration that provides terminology and context for research studies, identifies new research opportunities, and allows practitioners to spot possible challenges in their current tasks. Although our interviewees have a clear view about configuration, it substantially differs due to their personal experience and role. This indicates that the term configuration might be overloaded. However, when taking a closer look, we see the interconnections and dependencies among all views, arriving at the conclusion that we need to start considering the entire spectrum of dimensions of configuration.","variability, Dimensions of software configuration, developer study, configuration management and life cycle",338–349,12,Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
206,@inbook: 10.1145/3368089.3409715,TypeWriter: Neural Type Prediction with Search-Based Validation,"Pradel, Michael and Gousios, Georgios and Liu, Jason and Chandra, Satish",2020,Association for Computing Machinery,,https://doi.org/10.1145/3368089.3409715,,"Maintaining large code bases written in dynamically typed languages, such as JavaScript or Python, can be challenging due to the absence of type annotations: simple data compatibility errors proliferate, IDE support is limited, and APIs are hard to comprehend. Recent work attempts to address those issues through either static type inference or probabilistic type prediction. Unfortunately, static type inference for dynamic languages is inherently limited, while probabilistic approaches suffer from imprecision. This paper presents TypeWriter, the first combination of probabilistic type prediction with search-based refinement of predicted types. TypeWriter’s predictor learns to infer the return and argument types for functions from partially annotated code bases by combining the natural language properties of code with programming language-level information. To validate predicted types, TypeWriter invokes a gradual type checker with different combinations of the predicted types, while navigating the space of possible type combinations in a feedback-directed manner. We implement the TypeWriter approach for Python and evaluate it on two code corpora: a multi-million line code base at Facebook and a collection of 1,137 popular open-source projects. We show that TypeWriter’s type predictor achieves an F1 score of 0.64 (0.79) in the top-1 (top-5) predictions for return types, and 0.57 (0.80) for argument types, which clearly outperforms prior type prediction models. By combining predictions with search-based validation, TypeWriter can fully annotate between 14% to 44% of the files in a randomly selected corpus, while ensuring type correctness. A comparison with a static type inference tool shows that TypeWriter adds many more non-trivial types. TypeWriter currently suggests types to developers at Facebook and several thousands of types have already been accepted with minimal changes.",,209–220,1,Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
207,@inproceedings: 10.1145/3393822.3432314,Deep Image Compositing,"Aneja, Shivangi and Mazumder, Soham",2020,Association for Computing Machinery,,https://doi.org/10.1145/3393822.3432314,10.1145/3393822.3432314,"In image editing, the most common task is pasting objects from one image to the other and then eventually adjusting the manifestation of the foreground object with the background object. This task is called image compositing. But image compositing is a challenging problem which requires professional editing skills and a considerable amount of time. Not only these professionals are expensive to hire, but the tools (like Adobe Photoshop) used for doing such tasks are also expensive to purchase making the overall task of image compositing difficult for people without this skillset. In this work we aim to cater to this problem by making composite images look realistic. To achieve this, we are using Generative Adversarial Networks (GANS). By training the network with a diverse range of filters applied to the images and special loss functions, the model is able to decode the color histogram of foreground and background part of the image and also learns to blend the foreground object with the background. The hue and saturation values of the image plays an important role as discussed in this paper. To the best of our knowledge, this is the first work that uses GANs for that task of image compositing. Currently, there is no benchmark dataset available for image compositing. So we created the dataset and will also make the dataset publicly available for benchmarking. Experimental results on this dataset show that our method outperforms all current state-of-the-art methods.","GAN, Computer Vision, Deep Learning, Image Editing",101–104,4,Proceedings of the 2020 European Symposium on Software Engineering
208,@inproceedings: 10.1145/3393822.3432325,Improving Fairness in Speaker Recognition,"Fenu, Gianni and Medda, Giacomo and Marras, Mirko and Meloni, Giacomo",2020,Association for Computing Machinery,,https://doi.org/10.1145/3393822.3432325,10.1145/3393822.3432325,"The human voice conveys unique characteristics of an individual, making voice biometrics a key technology for verifying identities in various industries. Despite the impressive progress of speaker recognition systems in terms of accuracy, a number of ethical and legal concerns has been raised, specifically relating to the fairness of such systems. In this paper, we aim to explore the disparity in performance achieved by state-of-the-art deep speaker recognition systems, when different groups of individuals characterized by a common sensitive attribute (e.g., gender) are considered. In order to mitigate the unfairness we uncovered by means of an exploratory study, we investigate whether balancing the representation of the different groups of individuals in the training set can lead to a more equal treatment of these demographic groups. Experiments on two state-of-the-art neural architectures and a large-scale public dataset show that models trained with demographically-balanced training sets exhibit a fairer behavior on different groups, while still being accurate. Our study is expected to provide a solid basis for instilling beyond-accuracy objectives (e.g., fairness) in speaker recognition.","Deep Learning, Bias, X-Vector, Fairness, Speaker Recognition, Speaker Verification, Discrimination, ResNet",129–136,8,Proceedings of the 2020 European Symposium on Software Engineering
209,@inproceedings: 10.1145/3379337.3415825,Predicting Visual Importance Across Graphic Design Types,"Fosco, Camilo and Casser, Vincent and Bedi, Amish Kumar and O'Donovan, Peter and Hertzmann, Aaron and Bylinskii, Zoya",2020,Association for Computing Machinery,,https://doi.org/10.1145/3379337.3415825,10.1145/3379337.3415825,"This paper introduces a Unified Model of Saliency and Importance (UMSI), which learns to predict visual importance in input graphic designs, and saliency in natural images, along with a new dataset and applications. Previous methods for predicting saliency or visual importance are trained individually on specialized datasets, making them limited in application and leading to poor generalization on novel image classes, while requiring a user to know which model to apply to which input. UMSI is a deep learning-based model simultaneously trained on images from different design classes, including posters, infographics, mobile UIs, as well as natural images, and includes an automatic classification module to classify the input. This allows the model to work more effectively without requiring a user to label the input. We also introduce Imp1k, a new dataset of designs annotated with importance information. We demonstrate two new design interfaces that use importance prediction, including a tool for adjusting the relative importance of design elements, and a tool for reflowing designs to new aspect ratios while preserving visual importance.","graphic designs, saliency, automated design, user interface for design, human attention, importance, deep learning",249–260,12,Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
210,@inbook: 10.1145/3379337.3415821,Brain Relevance Feedback for Interactive Image Generation,"de la Torre-Ortiz, Carlos and Spap\'{e}, Michiel M. and Kangassalo, Lauri and Ruotsalo, Tuukka",2020,Association for Computing Machinery,,https://doi.org/10.1145/3379337.3415821,,"Brain-computer interfaces (BCIs) are increasingly used to perform simple operations such as a moving a cursor, but have remained of limited use for more complex tasks. In our new approach to BCI, we use brain relevance feedback to control a generative adversarial network (GAN). We obtained EEG data from 31 participants who viewed face images while concentrating on particular facial features. Following, an EEG relevance classifier was trained and propagated as feedback on the latent image representation provided by the GAN. Estimates for individual vectors matching the relevant criteria were iteratively updated to optimize an image generation process towards mental targets. A double-blind evaluation showed high performance (86.26% accuracy) against random feedback (18.71%), and not significantly lower than explicit feedback (93.30%). Furthermore, we show the feasibility of the method with simultaneous task targets demonstrating BCI operation beyond individual task constraints. Thus, brain relevance feedback can validly control a generative model, overcoming a critical limitation of current BCI approaches.",,1060–1070,1,Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
211,@inbook: 10.1145/3379337.3415822,PoseTween: Pose-Driven Tween Animation,"Liu, Jingyuan and Fu, Hongbo and Tai, Chiew-Lan",2020,Association for Computing Machinery,,https://doi.org/10.1145/3379337.3415822,,"Augmenting human action videos with visual effects often requires professional tools and skills. To make this more accessible by novice users, existing attempts have focused on automatically adding visual effects to faces and hands, or let virtual objects strictly track certain body parts, resulting in rigid-looking effects. We present PoseTween, an interactive system that allows novice users to easily add vivid virtual objects with their movement interacting with a moving subject in an input video. Our key idea is to leverage the motion of the subject to create pose-driven tween animations of virtual objects. With our tool, a user only needs to edit the properties of a virtual object with respect to the subject's movement at keyframes, and the object is associated with certain body parts automatically. The properties of the object at intermediate frames are then determined by both the body movement and the interpolated object keyframe properties, producing natural object movements and interactions with the subject. We design a user interface to facilitate editing of keyframes and previewing animation results. Our user study shows that PoseTween significantly requires less editing time and fewer keyframes than using the traditional tween animation in making pose-driven tween animations for novice users.",,791–804,1,Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
212,@inbook: 10.1145/3379337.3415844,MYND: Unsupervised Evaluation of Novel BCI Control Strategies on Consumer Hardware,"Hohmann, Matthias R. and Konieczny, Lisa and Hackl, Michelle and Wirth, Brian and Zaman, Talha and Enficiaud, Raffi and Grosse-Wentrup, Moritz and Sch\""{o}lkopf, Bernhard",2020,Association for Computing Machinery,,https://doi.org/10.1145/3379337.3415844,,"Neurophysiological laboratory studies are often constraint to immediate geographical surroundings and access to equipment may be temporally restricted. Limitations of ecological validity, scalability, and generalizability of findings pose a significant challenge for the development of brain-computer interfaces (BCIs), which ultimately need to function in any context, on consumer-grade hardware. We introduce MYND: An open-source framework that couples consumer-grade recording hardware with an easy-to-use application for the unsupervised evaluation of BCI control strategies. Subjects are guided through experiment selection, hardware fitting, recording, and data upload in order to self-administer multi-day studies that include neurophysiological recordings and questionnaires at home. As a use case, thirty subjects evaluated two BCI control strategies ""Positive memories"" and ""Music imagery"" by using a four-channel electroencephalogram (EEG) with MYND. Neural activity in both control strategies could be decoded with an average offline accuracy of 68.5% and 64.0% across all days.",,1071–1084,1,Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
213,@inbook: 10.1145/3379337.3415835,GrabAR: Occlusion-Aware Grabbing Virtual Objects in AR,"Tang, Xiao and Hu, Xiaowei and Fu, Chi-Wing and Cohen-Or, Daniel",2020,Association for Computing Machinery,,https://doi.org/10.1145/3379337.3415835,,"Existing augmented reality (AR) applications often ignore the occlusion between real hands and virtual objects when incorporating virtual objects in user's views. The challenges come from the lack of accurate depth and mismatch between real and virtual depth. This paper presents GrabAR1, a new approach that directly predicts the real-and-virtual occlusion and bypasses the depth acquisition and inference. Our goal is to enhance AR applications with interactions between hand (real) and grabbable objects (virtual). With paired images of hand and object as inputs, we formulate a compact deep neural network that learns to generate the occlusion mask. To train the network, we compile a large dataset, including synthetic data and real data. We then embed the trained network in a prototyping AR system to support real-time grabbing of virtual objects. Further, we demonstrate the performance of our method on various virtual objects, compare our method with others through two user studies, and showcase a rich variety of interaction scenarios, in which we can use bare hand to grab virtual objects and directly manipulate them.",,697–708,1,Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
214,@inbook: 10.1145/3379337.3415886,ZebraSense: A Double-Sided Textile Touch Sensor for Smart Clothing,"Wu, Tony and Fukuhara, Shiho and Gillian, Nicholas and Sundara-Rajan, Kishore and Poupyrev, Ivan",2020,Association for Computing Machinery,,https://doi.org/10.1145/3379337.3415886,,"ZebraSense is a novel dual-sided woven touch sensor that can recognize and differentiate interactions on the top and bottom surfaces of the sensor. ZebraSense is based on an industrial multi-layer textile weaving technique, yet it enables a novel capacitive sensing paradigm, where each sensing element contributes to touch detection on both surfaces of the sensor simultaneously. Unlike the common ""sensor sandwich"" approach used in previous work, ZebraSense inherently minimizes the number of sensing elements, which drastically simplifies both sensor construction and its integration into soft goods, while preserving maximum sensor resolution. The experimental evaluation confirmed the validity of our approach and demonstrated that ZebraSense is a reliable, efficient, and accurate solution for detecting user gestures in various dual-sided interaction scenarios, allowing for new use cases in smart apparel, home decoration, toys, and other textile objects.",,662–674,1,Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
215,@inbook: 10.1145/3379337.3415864,Rescribe: Authoring and Automatically Editing Audio Descriptions,"Pavel, Amy and Reyes, Gabriel and Bigham, Jeffrey P.",2020,Association for Computing Machinery,,https://doi.org/10.1145/3379337.3415864,,"Audio descriptions make videos accessible to those who cannot see them by describing visual content in audio. Producing audio descriptions is challenging due to the synchronous nature of the audio description that must fit into gaps of other video content. An experienced audio description author will produce content that fits narration necessary to understand, enjoy, or experience the video content into the time available. This can be especially tricky for novices to do well. In this paper, we introduce a tool, Rescribe, that helps authors create and refine their audio descriptions. Using Rescribe, authors first create a draft of all the content they would like to include in the audio description. Rescribe then uses a dynamic programming approach to optimize between the length of the audio description, available automatic shortening approaches, and source track lengthening approaches. Authors can iteratively visualize and refine the audio descriptions produced by Rescribe, working in concert with the tool. We evaluate the effectiveness of Rescribe through interviews with blind and visually impaired audio description users who give feedback on Rescribe results. In addition, we invite novice users to create audio descriptions with Rescribe and another tool, finding that users produce audio descriptions with fewer placement errors using Rescribe.",,747–759,1,Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
216,@inbook: 10.1145/3379337.3415902,MechanoBeat: Monitoring Interactions with Everyday Objects Using 3D Printed Harmonic Oscillators and Ultra-Wideband Radar,"Tasnim Oshim, Md. Farhan and Killingback, Julian and Follette, Dave and Peng, Huaishu and Rahman, Tauhidur",2020,Association for Computing Machinery,,https://doi.org/10.1145/3379337.3415902,,"In this paper we present MechanoBeat, a 3D printed mechanical tag that oscillates at a unique frequency upon user interaction. With the help of an ultra-wideband (UWB) radar array, MechanoBeat can unobtrusively monitor interactions with both stationary and mobile objects. MechanoBeat consists of small, scalable, and easy-to-install tags that do not require any batteries, silicon chips, or electronic components. Tags can be produced using commodity desktop 3D printers with cheap materials. We develop an efficient signal processing and deep learning method to locate and identify tags using only the signals reflected from the tag vibrations. MechanoBeat is capable of detecting simultaneous interactions with high accuracy, even in noisy environments. We leverage UWB radar signals' high penetration property to sense interactions behind walls in a non-line-of-sight (NLOS) scenario. A number of applications using MechanoBeat have been explored and the results have been presented in the paper.",,430–444,1,Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
217,@inbook: 10.1145/3379337.3415820,Multi-Modal Repairs of Conversational Breakdowns in Task-Oriented Dialogs,"Li, Toby Jia-Jun and Chen, Jingya and Xia, Haijun and Mitchell, Tom M. and Myers, Brad A.",2020,Association for Computing Machinery,,https://doi.org/10.1145/3379337.3415820,,"A major problem in task-oriented conversational agents is the lack of support for the repair of conversational breakdowns. Prior studies have shown that current repair strategies for these kinds of errors are often ineffective due to: (1) the lack of transparency about the state of the system's understanding of the user's utterance; and (2) the system's limited capabilities to understand the user's verbal attempts to repair natural language understanding errors. This paper introduces SOVITE, a new multi-modal speech plus direct manipulation interface that helps users discover, identify the causes of, and recover from conversational breakdowns using the resources of existing mobile app GUIs for grounding. SOVITE displays the system's understanding of user intents using GUI screenshots, allows users to refer to third-party apps and their GUI screens in conversations as inputs for intent disambiguation, and enables users to repair breakdowns using direct manipulation on these screenshots. The results from a remote user study with 10 users using SOVITE in 7 scenarios suggested that SOVITE's approach is usable and effective.",,1094–1107,1,Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
218,@inbook: 10.1145/3379337.3415815,CAPturAR: An Augmented Reality Tool for Authoring Human-Involved Context-Aware Applications,"Wang, Tianyi and Qian, Xun and He, Fengming and Hu, Xiyun and Huo, Ke and Cao, Yuanzhi and Ramani, Karthik",2020,Association for Computing Machinery,,https://doi.org/10.1145/3379337.3415815,,"Recognition of human behavior plays an important role in context-aware applications. However, it is still a challenge for end-users to build personalized applications that accurately recognize their own activities. Therefore, we present CAPturAR, an in-situ programming tool that supports users to rapidly author context-aware applications by referring to their previous activities. We customize an AR head-mounted device with multiple camera systems that allow for non-intrusive capturing of user's daily activities. During authoring, we reconstruct the captured data in AR with an animated avatar and use virtual icons to represent the surrounding environment. With our visual programming interface, users create human-centered rules for the applications and experience them instantly in AR. We further demonstrate four use cases enabled by CAPturAR. Also, we verify the effectiveness of the AR-HMD and the authoring workflow with a system evaluation using our prototype. Moreover, we conduct a remote user study in an AR simulator to evaluate the usability.",,328–341,1,Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
219,@inproceedings: 10.1145/3379337.3415897,Back-Hand-Pose: 3D Hand Pose Estimation for a Wrist-Worn Camera via Dorsum Deformation Network,"Wu, Erwin and Yuan, Ye and Yeo, Hui-Shyong and Quigley, Aaron and Koike, Hideki and Kitani, Kris M.",2020,Association for Computing Machinery,,https://doi.org/10.1145/3379337.3415897,10.1145/3379337.3415897,"The automatic recognition of how people use their hands and fingers in natural settings -- without instrumenting the fingers -- can be useful for many mobile computing applications. To achieve such an interface, we propose a vision-based 3D hand pose estimation framework using a wrist-worn camera. The main challenge is the oblique angle of the wrist-worn camera, which makes the fingers scarcely visible. To address this, a special network that observes deformations on the back of the hand is required. We introduce DorsalNet, a two-stream convolutional neural network to regress finger joint angles from spatio-temporal features of the dorsal hand region (the movement of bones, muscle, and tendons). This work is the first vision-based real-time 3D hand pose estimator using visual features from the dorsal hand region. Our system achieves a mean joint-angle error of 8.81 degree for user-specific models and 9.77 degree for a general model. Further evaluation shows that our system outperforms previous work with an average of 20% higher accuracy in recognizing dynamic gestures, and achieves a 75% accuracy of detecting 11 different grasp types. We also demonstrate 3 applications which employ our system as a control device, an input device, and a grasped object recognizer.","dorsal hand, wrist-worn devices, 3d hand pose estimation",1147–1160,14,Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
220,@inbook: 10.1145/3379337.3415856,MonoEye: Multimodal Human Motion Capture System Using A Single Ultra-Wide Fisheye Camera,"Hwang, Dong-Hyun and Aso, Kohei and Yuan, Ye and Kitani, Kris and Koike, Hideki",2020,Association for Computing Machinery,,https://doi.org/10.1145/3379337.3415856,,"We present MonoEye, a multimodal human motion capture system using a single RGB camera with an ultra-wide fisheye lens, mounted on the user's chest. Existing optical motion capture systems use multiple cameras, which are synchronized and require camera calibration. These systems also have usability constraints that limit the user's movement and operating space. Since the MonoEye system is based on a wearable single RGB camera, the wearer's 3D body pose can be captured without space and environment limitations. The body pose, captured with our system, is aware of the camera orientation and therefore it is possible to recognize various motions that existing egocentric motion capture systems cannot recognize. Furthermore, the proposed system captures not only the wearer's body motion but also their viewport using the head pose estimation and an ultra-wide image. To implement robust multimodal motion capture, we design three deep neural networks: BodyPoseNet, HeadPoseNet, and CameraPoseNet, that estimate 3D body pose, head pose, and camera pose in real-time, respectively. We train these networks with our new extensive synthetic dataset providing 680K frames of renderings of people with a wide range of body shapes, clothing, actions, backgrounds, and lighting conditions. To demonstrate the interactive potential of the MonoEye system, we present several application examples from common body gestural to context-aware interactions.",,98–111,1,Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
221,@inbook: 10.1145/3379337.3415859,ZoomWalls: Dynamic Walls That Simulate Haptic Infrastructure for Room-Scale VR World,"Yixian, Yan and Takashima, Kazuki and Tang, Anthony and Tanno, Takayuki and Fujita, Kazuyuki and Kitamura, Yoshifumi",2020,Association for Computing Machinery,,https://doi.org/10.1145/3379337.3415859,,"We focus on the problem of simulating the haptic infrastructure of a virtual environment (i.e. walls, doors). Our approach relies on multiple ZoomWalls---autonomous robotic encounter-type haptic wall-shaped props---that coordinate to provide haptic feedback for room-scale virtual reality. Based on a user's movement through the physical space, ZoomWall props are coordinated through a predict-and-dispatch architecture to provide just-in-time haptic feedback for objects the user is about to touch. To refine our system, we conducted simulation studies of different prediction algorithms, which helped us to refine our algorithmic approach to realize the physical ZoomWall prototype. Finally, we evaluated our system through a user experience study, which showed that participants found that ZoomWalls increased their sense of presence in the VR environment. ZoomWalls represents an instance of autonomous mobile reusable props, which we view as an important design direction for haptics in VR.",,223–235,1,Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology
222,@inproceedings: 10.1145/3382025.3414968,Automated Extraction of Domain Knowledge in Practice: The Case of Feature Extraction from Requirements at Danfoss,"Li, Yang and Schulze, Sandro and Scherrebeck, Helene Hvidegaard and Fogdal, Thomas Sorensen",2020,Association for Computing Machinery,,https://doi.org/10.1145/3382025.3414968,10.1145/3382025.3414968,"Software product line supports structured reuse of software artifacts in order to realize the maintenance and evolution of the typically large number of variants, which promotes the industrialization of software development, especially for software-intensive products. However, for a legacy system, it is non-trivial to gain information about commonalities and differences of the variants. Meanwhile, software requirements specifications as the initial artifacts can be used to achieve this information to generate a domain model. Unfortunately, manually analyzing these requirements is time-consuming and inefficient. To address this problem, we explored the usage of feature extraction techniques to automatically extract domain knowledge from requirements to assist domain engineers. In detail, we applied Doc2Vec and a clustering algorithm to process the requirements for achieving the initial feature tree. Moreover, we utilized key words/phrases extraction techniques to provide key information to domain engineers for further analyzing the extraction results. In particular, we developed a GUI to support the extraction process. The empirical evaluation indicates that most of the extracted features and terms are beneficial to improve the process of feature extraction.","feature extraction, software product lines, requirement documents, reverse engineering",,11,Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A
223,@inproceedings: 10.1145/3382494.3422159,Beyond Accuracy: ROI-Driven Data Analytics of Empirical Data,"Deshpande, Gouri and Ruhe, Guenther",2020,Association for Computing Machinery,,https://doi.org/10.1145/3382494.3422159,10.1145/3382494.3422159,"Background: The unprecedented access to data has rendered a remarkable opportunity to analyze, understand, and optimize the investigation approaches in almost all the areas of (Empirical) Software Engineering. However, data analytics is time and effort consuming, thus, expensive, and not automatically valuable.Objective: This vision paper demonstrates that it is crucial to consider Return-on-Investment (ROI) when performing Data Analytics. Decisions on ""How much analytics is needed""? are hard to answer. ROI could guide for decision support on the What?, How?, and How Much? analytics for a given problem.Method: The proposed conceptual framework is validated through two empirical studies that focus on requirements dependencies extraction in the Mozilla Firefox project. The two case studies are (i) Evaluation of fine-tuned BERT against Naive Bayes and Random Forest machine learners for binary dependency classification and (ii) Active Learning against passive Learning (random sampling) for REQUIRES dependency extraction. For both the cases, their analysis investment (cost) is estimated, and the achievable benefit from DA is predicted, to determine a break-even point of the investigation.Results: For the first study, fine-tuned BERT performed superior to the Random Forest, provided that more than 40% of training data is available. For the second, Active Learning achieved higher F1 accuracy within fewer iterations and higher ROI compared to Baseline (Random sampling based RF classifier). In both the studies, estimate on, How much analysis likely would pay off for the invested efforts?, was indicated by the break-even point.Conclusions: Decisions for the depth and breadth of DA of empirical data should not be made solely based on the accuracy measures. Since ROI-driven Data Analytics provides a simple yet effective direction to discover when to stop further investigation while considering the cost and value of the various types of analysis, it helps to avoid over-analyzing empirical data.","Data Analytics, Requirements Engineering, Dependency extraction, BERT, Return-on-Investment, Mozilla",,6,Proceedings of the 14th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)
224,@inproceedings: 10.1145/3382494.3410694,Quest for the Golden Approach: An Experimental Evaluation of Duplicate Crowdtesting Reports Detection,"Huang, Yuekai and Wang, Junjie and Wang, Song and Liu, Zhe and Hu, Yuanzhe and Wang, Qing",2020,Association for Computing Machinery,,https://doi.org/10.1145/3382494.3410694,10.1145/3382494.3410694,"Background: Given the invisibility and unpredictability of distributed crowdtesting processes, there is a large number of duplicate reports, and detecting these duplicate reports is an important task to help save testing effort. Although, many approaches have been proposed to automatically detect the duplicates, the comparison among them and the practical guidelines to adopt these approaches in crowdtesting remain vague.Aims: We aim at conducting the first experimental evaluation of the commonly-used and state-of-the-art approaches for duplicate detection in crowdtesting reports, and exploring which is the golden approach.Method: We begin with a systematic review of approaches for duplicate detection, and select ten state-of-the-art approaches for our experimental evaluation. We conduct duplicate detection with each approach on 414 crowdtesting projects with 59,289 reports collected from one of the largest crowdtesting platforms.Results: Machine learning based approach, i.e., ML-REP, and deep learning based approach, i.e., DL-BiMPM, are the best two approaches for duplicate reports detection in crowdtesting, while the later one is more sensitive to the size of training data and more time-consuming for model training and prediction.Conclusions: This paper provides new insights and guidelines to select appropriate duplicate detection techniques for duplicate crowdtesting reports detection.","information retrieval, Crowdtesting, duplicate detection, deep learning, machine learning",,12,Proceedings of the 14th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)
225,@inproceedings: 10.1145/3425329.3425331,Intelligent Safety Monitoring and Early Warning System for Construction Site,"Hu, Zheyuan and Cai, Jun and Wang, Huiwei and Zhang, Dehao and Liu, Yang and Li, Xin and Li, Yanlong and Zhao, Fengyan and Zhang, Hongjun",2020,Association for Computing Machinery,,https://doi.org/10.1145/3425329.3425331,10.1145/3425329.3425331,"Faced with the complex environment and difficult construction of infrastructure projects, designing an intelligent safety monitoring and early warning system for construction sites can effectively detect existing violations and reduce the probability of accidents. Existing violation detection methods for construction sites mainly include hand-crafted feature extraction and deep neural network. However, the method of extracting features is usually difficult to design and the architecture of the deep learning-based method is simple, which might lead to poor detection performance in extreme cases (Insufficient light, small detection object, occlusion, etc.) and cannot be used in actual detection environment. Therefore, we improve the existing target detection algorithm by adding image preprocessing module, multi-scale feature fusion module, and repulsion loss term. We also use the KCF algorithm to continuously track targets to identify specific violations. On this basis, we develop an intelligent safety monitoring and early warning system to classify the detected violations and send the information to the responsible person in time, which significantly improves the management capacity at the construction site. Through a series of experiments, we compared the impact of different modules on detection accuracy. The results show that our model has a significant improvement compared to existing methods on our dataset, especially in harsh environments.","Intelligent violation recognition, Intelligent early warning, Object detection, Video analysis",6–11,6,Proceedings of the 2020 The 2nd World Symposium on Software Engineering
226,@inbook: 10.1145/3417113.3422998,Gesture Driven Smart Home Solution for Bedridden People,"Jayaweera, Nimna and Gamage, Binura and Samaraweera, Mihiri and Liyanage, Sachintha and Lokuliyana, Shashika and Kuruppu, Thilmi",2020,Association for Computing Machinery,,https://doi.org/10.1145/3417113.3422998,,"Conversion of ordinary houses into smart homes has been a rising trend for past years. Smart house development is based on the enhancement of the quality of the daily activities of normal people. But many smart homes have not been designed in a way that is user friendly for differently-abled people such as immobile, bedridden (disabled people with at least one hand movable). Due to negligence and forgetfulness, there are cases where the electrical devices are left switched on, regardless of any necessity. It is one of the most occurred examples of domestic energy wastage. To overcome those challenges, this research represents the improved smart home design: MobiGO that uses cameras to capture gestures, smart sockets to deliver gesture-driven outputs to home appliances, etc. The camera captures the gestures done by the user and the system processes those images through advanced gesture recognition and image processing technologies. The commands relevant to the gesture are sent to the specific appliance through a specific IoT device attached to them. The basic literature survey content, which contains technical words, is analyzed using Deep Learning, Convolutional Neural Network (CNN), Image Processing, Gesture recognition, smart homes, IoT. Finally, the authors conclude that the MobiGO solution proposes a smart home system that is safer and easier for people with disabilities.",,152–158,,Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering Workshops
227,@inbook: 10.1145/3417113.3423375,Emotion Detection in Roman Urdu Text Using Machine Learning,"Majeed, Adil and Mujtaba, Hasan and Beg, Mirza Omer",2020,Association for Computing Machinery,,https://doi.org/10.1145/3417113.3423375,,"Emotion detection is playing a very important role in our life. People express their emotions in different ways i.e face expression, gestures, speech, and text. This research focuses on detecting emotions from the Roman Urdu text. Previously, A lot of work has been done on different languages for emotion detection but there is limited work done in Roman Urdu. Therefore, there is a need to explore Roman Urdu as it is the most widely used language on social media platforms for communication. One major issue for the Roman Urdu is the absence of benchmark corpora for emotion detection from text because language assets are essential for different natural language processing (NLP) tasks. There are many useful applications of the emotional analysis of a text such as improving the quality of products, dialog systems, investment trends, mental health. In this research, to focus on the emotional polarity of the Roman Urdu sentence we develop a comprehensive corpus of 18k sentences that are gathered from different domains and annotate it with six different classes. We applied different baseline algorithms like KNN, Decision tree, SVM, and Random Forest on our corpus. After experimentation and evaluation, the results showed that the SVM model achieves a better F-measure score.",,125–130,,Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering Workshops
228,@inbook: 10.1145/3417113.3423370,Boosting Component-Based Synthesis with API Usage Knowledge,"Liu, Jiaxin and Dong, Wei and Liu, Binbin",2020,Association for Computing Machinery,,https://doi.org/10.1145/3417113.3423370,,"Component-based synthesis is one of the hottest research areas in automated software engineering. It aims to generate programs from a collection of components like Java library. However, the program space constituted by all the components in the library is fairly large, which leads to a vast number of candidate programs generated for a long time. The intractability of the program space affects the synthesis efficiency of the program and the size of the program generated. In this paper, we propose Itas, a framework of iterative program synthesis via API usage knowledge from the Internet, which can significantly improve the efficiency of program synthesis. Itas aims to constrain the program space by combining two main ideas. First, narrow down the program space from the outside via the guidance of API usage knowledge. Second, expand the program space from the inside via iterative strategy based on knowledge. For evaluation, we collect a set of programming tasks and compare our approach with a program synthesis tool on synthesizing these tasks. The experiment results show that Itas can significantly improve the efficiency of program synthesis, which can reduce the synthesis time by 97.1% than the original synthesizer.",,91–97,,Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering Workshops
229,@inproceedings: 10.1145/3395363.3397376,Active Fuzzing for Testing and Securing Cyber-Physical Systems,"Chen, Yuqi and Xuan, Bohan and Poskitt, Christopher M. and Sun, Jun and Zhang, Fan",2020,Association for Computing Machinery,,https://doi.org/10.1145/3395363.3397376,10.1145/3395363.3397376,"Cyber-physical systems&nbsp;(CPSs) in critical infrastructure face a pervasive threat from attackers, motivating research into a variety of countermeasures for securing them. Assessing the effectiveness of these countermeasures is challenging, however, as realistic benchmarks of attacks are difficult to manually construct, blindly testing is ineffective due to the enormous search spaces and resource requirements, and intelligent fuzzing approaches require impractical amounts of data and network access. In this work, we propose active fuzzing, an automatic approach for finding test suites of packet-level CPS network attacks, targeting scenarios in which attackers can observe sensors and manipulate packets, but have no existing knowledge about the payload encodings. Our approach learns regression models for predicting sensor values that will result from sampled network packets, and uses these predictions to guide a search for payload manipulations (i.e.&nbsp;bit flips) most likely to drive the CPS into an unsafe state. Key to our solution is the use of online active learning, which iteratively updates the models by sampling payloads that are estimated to maximally improve them. We evaluate the efficacy of active fuzzing by implementing it for a water purification plant testbed, finding it can automatically discover a test suite of flow, pressure, and over/underflow attacks, all with substantially less time, data, and network access than the most comparable approach. Finally, we demonstrate that our prediction models can also be utilised as countermeasures themselves, implementing them as anomaly detectors and early warning systems.","fuzzing, benchmark generation, Cyber-physical systems, testing defence mechanisms, active learning",14–26,13,Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis
230,@inproceedings: 10.1145/3395363.3397346,Effective White-Box Testing of Deep Neural Networks with Adaptive Neuron-Selection Strategy,"Lee, Seokhyun and Cha, Sooyoung and Lee, Dain and Oh, Hakjoo",2020,Association for Computing Machinery,,https://doi.org/10.1145/3395363.3397346,10.1145/3395363.3397346,"We present Adapt, a new white-box testing technique for deep neural networks. As deep neural networks are increasingly used in safety-first applications, testing their behavior systematically has become a critical problem. Accordingly, various testing techniques for deep neural networks have been proposed in recent years. However, neural network testing is still at an early stage and existing techniques are not yet sufficiently effective. In this paper, we aim to advance this field, in particular white-box testing approaches for neural networks, by identifying and addressing a key limitation of existing state-of-the-arts. We observe that the so-called neuron-selection strategy is a critical component of white-box testing and propose a new technique that effectively employs the strategy by continuously adapting it to the ongoing testing process. Experiments with real-world network models and datasets show that Adapt is remarkably more effective than existing testing techniques in terms of coverage and adversarial inputs found.","Deep neural networks, Online learning, White-box testing",165–176,12,Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis
231,@inproceedings: 10.1145/3395363.3397356,Scaffle: Bug Localization on Millions of Files,"Pradel, Michael and Murali, Vijayaraghavan and Qian, Rebecca and Machalica, Mateusz and Meijer, Erik and Chandra, Satish",2020,Association for Computing Machinery,,https://doi.org/10.1145/3395363.3397356,10.1145/3395363.3397356,"Despite all efforts to avoid bugs, software sometimes crashes in the field, leaving crash traces as the only information to localize the problem. Prior approaches on localizing where to fix the root cause of a crash do not scale well to ultra-large scale, heterogeneous code bases that contain millions of code files written in multiple programming languages. This paper presents Scaffle, the first scalable bug localization technique, which is based on the key insight to divide the problem into two easier sub-problems. First, a trained machine learning model predicts which lines of a raw crash trace are most informative for localizing the bug. Then, these lines are fed to an information retrieval-based search engine to retrieve file paths in the code base, predicting which file to change to address the crash. The approach does not make any assumptions about the format of a crash trace or the language that produces it. We evaluate Scaffle with tens of thousands of crash traces produced by a large-scale industrial code base at Facebook that contains millions of possible bug locations and that powers tools used by billions of people. The results show that the approach correctly predicts the file to fix for 40% to 60% (50% to 70%) of all crash traces within the top-1 (top-5) predictions. Moreover, Scaffle improves over several baseline approaches, including an existing classification-based approach, a scalable variant of existing information retrieval-based approaches, and a set of hand-tuned, industrially deployed heuristics.","machine learning, software crashes, Bug localization",225–236,12,Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis
232,@inproceedings: 10.1145/3395363.3397354,Reinforcement Learning Based Curiosity-Driven Testing of Android Applications,"Pan, Minxue and Huang, An and Wang, Guoxin and Zhang, Tian and Li, Xuandong",2020,Association for Computing Machinery,,https://doi.org/10.1145/3395363.3397354,10.1145/3395363.3397354,"Mobile applications play an important role in our daily life, while it still remains a challenge to guarantee their correctness. Model-based and systematic approaches have been applied to Android GUI testing. However, they do not show significant advantages over random approaches because of limitations such as imprecise models and poor scalability. In this paper, we propose Q-testing, a reinforcement learning based approach which benefits from both random and model-based approaches to automated testing of Android applications. Q-testing explores the Android apps with a curiosity-driven strategy that utilizes a memory set to record part of previously visited states and guides the testing towards unfamiliar functionalities. A state comparison module, which is a neural network trained by plenty of collected samples, is novelly employed to divide different states at the granularity of functional scenarios. It can determine the reinforcement learning reward in Q-testing and help the curiosity-driven strategy explore different functionalities efficiently. We conduct experiments on 50 open-source applications where Q-testing outperforms the state-of-the-art and state-of-practice Android GUI testing tools in terms of code coverage and fault detection. So far, 22 of our reported faults have been confirmed, among which 7 have been fixed.","Android app testing, functional scenario division, reinforcement learning",153–164,12,Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis
233,@inbook: 10.1145/3387904.3389278,Adaptive Deep Code Search,"Ling, Chunyang and Lin, Zeqi and Zou, Yanzhen and Xie, Bing",2020,Association for Computing Machinery,,https://doi.org/10.1145/3387904.3389278,,"Searching code in a large-scale codebase using natural language queries is a common practice during software development. Deep learning-based code search methods demonstrate superior performance if models are trained with large amount of text-code pairs. However, few deep code search models can be easily transferred from one codebase to another. It can be very costly to prepare training data for a new codebase and re-train an appropriate deep learning model. In this paper, we propose AdaCS, an adaptive deep code search method that can be trained once and transferred to new codebases. AdaCS decomposes the learning process into embedding domain-specific words and matching general syntactic patterns. Firstly, an unsupervised word embedding technique is used to construct a matching matrix to represent the lexical similarities. Then, a recurrent neural network is used to capture latent syntactic patterns from these matching matrices in a supervised way. As the supervised task learns general syntactic patterns that exist across domains, AdaCS is transferable to new codebases. Experimental results show that: when extended to new software projects never seen in the training data, AdaCS is more robust and significantly outperforms state-of-the-art deep code search methods.",,48–59,1,Proceedings of the 28th International Conference on Program Comprehension
234,@inbook: 10.1145/3387904.3389260,Supporting Program Comprehension through Fast Query Response in Large-Scale Systems,"Lin, Jinfeng and Liu, Yalin and Cleland-Huang, Jane",2020,Association for Computing Machinery,,https://doi.org/10.1145/3387904.3389260,,"Software traceability provides support for various engineering activities including Program Comprehension; however, it can be challenging and arduous to complete in large industrial projects. Researchers have proposed automated traceability techniques to create, maintain and leverage trace links. Computationally intensive techniques, such as repository mining and deep learning, have showed the capability to deliver accurate trace links. The objective of achieving trusted, automated tracing techniques at industrial scale has not yet been successfully accomplished due to practical performance challenges. This paper evaluates high-performance solutions for deploying effective, computationally expensive trace-ability algorithms in large scale industrial projects and leverages generated trace links to answer Program Comprehension Queries. We comparatively evaluate four different platforms for supporting industrial-scale tracing solutions, capable of tackling software projects with millions of artifacts. We demonstrate that tracing solutions built using big data frameworks scale well for large projects and that our Spark implementation outperforms relational database, graph database (GraphDB), and plain Java implementations. These findings contradict earlier results which suggested that GraphDB solutions should be adopted for large-scale tracing problems.",,285–295,1,Proceedings of the 28th International Conference on Program Comprehension
235,@inproceedings: 10.1145/3424771.3424778,Video Segmentation as an Example for Elaborating Design Patterns through Empirical Studies,"Seidel, Niels",2020,Association for Computing Machinery,,https://doi.org/10.1145/3424771.3424778,10.1145/3424771.3424778,"While pattern mining is characterized by empirical examinations the applied solutions propagated in the patterns are only rarely the subject or the result of empirical tests. However, confidence that a pattern will serve its purpose can only be obtained through testing and using them in practice.This article uses the pattern VIDEO SEGMENTATION as a use case to present an iterative approach on how to integrate iterative empirical tests into the elaboration of design patterns. This approach aims to explore the design space and specifies the context of a pattern. With the approach presented here, we want to encourage researchers from different disciplines to communicate their empirical research results in the form of design patterns.Using learning analytics methods, two studies were conducted for this purpose. In the first study 10 popular educational YouTube channels were analysed regarding the length and sequence of 4.136 videos. By using a cluster analysis three groups of videos of different length could be identified: videos under 8 minutes playing time, videos between 8 and 20 minutes duration and longer videos.For the second study 22 participants were split up into two groups using a video player supporting segmented videos in two different ways, and a control group watching a non-segmented video. Segmented videos with sequences of up to 10 minutes resulted in higher learning effects than the non-segmented version of the same video.","pattern mining, design pattern theory, video segmentation, learning analytics",,15,Proceedings of the European Conference on Pattern Languages of Programs 2020
236,@inbook: 10.1145/3379597.3387470,A Soft Alignment Model for Bug Deduplication,"Rodrigues, Irving Muller and Aloise, Daniel and Fernandes, Eraldo Rezende and Dagenais, Michel",2020,Association for Computing Machinery,,https://doi.org/10.1145/3379597.3387470,,"Bug tracking systems (BTS) are widely used in software projects. An important task in such systems consists of identifying duplicate bug reports, i.e., distinct reports related to the same software issue. For several reasons, reporting bugs that have already been reported is quite frequent, making their manual triage impractical in large BTSs. In this paper, we present a novel deep learning network based on soft-attention alignment to improve duplicate bug report detection. For a given pair of possibly duplicate reports, the attention mechanism computes interdependent representations for each report, which is more powerful than previous approaches. We evaluate our model on four well-known datasets derived from BTSs of four popular open-source projects. Our evaluation is based on a ranking-based metric, which is more realistic than decision-making metrics used in many previous works. Achieved results demonstrate that our model outperforms state-of-the-art systems and strong baselines in different scenarios. Finally, an ablation study is performed to confirm that the proposed architecture improves the duplicate bug reports detection.",,43–53,1,Proceedings of the 17th International Conference on Mining Software Repositories
237,@inbook: 10.1145/3379597.3387440,Traceability Support for Multi-Lingual Software Projects,"Liu, Yalin and Lin, Jinfeng and Cleland-Huang, Jane",2020,Association for Computing Machinery,,https://doi.org/10.1145/3379597.3387440,,"Software traceability establishes associations between diverse software artifacts such as requirements, design, code, and test cases. Due to the non-trivial costs of manually creating and maintaining links, many researchers have proposed automated approaches based on information retrieval techniques. However, many globally distributed software projects produce software artifacts written in two or more languages. The use of intermingled languages reduces the efficacy of automated tracing solutions. In this paper, we first analyze and discuss patterns of intermingled language use across multiple projects, and then evaluate several different tracing algorithms including the Vector Space Model (VSM), Latent Semantic Indexing (LSI), Latent Dirichlet Allocation (LDA), and various models that combine mono-and cross-lingual word embeddings with the Generative Vector Space Model (GVSM). Based on an analysis of 14 Chinese-English projects, our results show that best performance is achieved using mono-lingual word embeddings integrated into GVSM with machine translation as a preprocessing step.",,443–454,1,Proceedings of the 17th International Conference on Mining Software Repositories
238,@inbook: 10.1145/3379597.3387445,Embedding Java Classes with Code2vec: Improvements from Variable Obfuscation,"Compton, Rhys and Frank, Eibe and Patros, Panos and Koay, Abigail",2020,Association for Computing Machinery,,https://doi.org/10.1145/3379597.3387445,,"Automatic source code analysis in key areas of software engineering, such as code security, can benefit from Machine Learning (ML). However, many standard ML approaches require a numeric representation of data and cannot be applied directly to source code. Thus, to enable ML, we need to embed source code into numeric feature vectors while maintaining the semantics of the code as much as possible. code2vec is a recently released embedding approach that uses the proxy task of method name prediction to map Java methods to feature vectors. However, experimentation with code2vec shows that it learns to rely on variable names for prediction, causing it to be easily fooled by typos or adversarial attacks. Moreover, it is only able to embed individual Java methods and cannot embed an entire collection of methods such as those present in a typical Java class, making it difficult to perform predictions at the class level (e.g., for the identification of malicious Java classes). Both shortcomings are addressed in the research presented in this paper. We investigate the effect of obfuscating variable names during training of a code2vec model to force it to rely on the structure of the code rather than specific names and consider a simple approach to creating class-level embeddings by aggregating sets of method embeddings. Our results, obtained on a challenging new collection of source-code classification problems, indicate that obfuscating variable names produces an embedding model that is both impervious to variable naming and more accurately reflects code semantics. The datasets, models, and code are shared1 for further ML research on source code.",,243–253,1,Proceedings of the 17th International Conference on Mining Software Repositories
239,@inproceedings: 10.1145/3442391.3442402,Deep Software Variability: Towards Handling Cross-Layer Configuration,"Lesoil, Luc and Acher, Mathieu and Blouin, Arnaud and J\'{e}z\'{e}quel, Jean-Marc",2021,Association for Computing Machinery,,https://doi.org/10.1145/3442391.3442402,10.1145/3442391.3442402,"Configuring software is a powerful means to reach functional and performance goals of a system. However, many layers (hardware, operating system, input data, etc.), themselves subject to variability, can alter performances of software configurations. For instance, configurations’ options of the x264 video encoder may have very different effects on x264’s encoding time when used with different input videos, depending on the hardware on which it is executed. In this vision paper, we coin the term deep software variability to refer to the interaction of all external layers modifying the behavior or non-functional properties of a software. Deep software variability challenges practitioners and researchers: the combinatorial explosion of possible executing environments complicates the understanding, the configuration, the maintenance, the debug, and the test of configurable systems. There are also opportunities: harnessing all variability layers (and not only the software layer) can lead to more efficient systems and configuration knowledge that truly generalizes to any usage and context. ",,,8,15th International Working Conference on Variability Modelling of Software-Intensive Systems
240,@article: 10.1145/3424308,Emoji-Powered Sentiment and Emotion Detection from Software Developers’ Communication Data,"Chen, Zhenpeng and Cao, Yanbin and Yao, Huihan and Lu, Xuan and Peng, Xin and Mei, Hong and Liu, Xuanzhe",2021,Association for Computing Machinery,1049-331X,https://doi.org/10.1145/3424308,10.1145/3424308,"Sentiment and emotion detection from textual communication records of developers have various application scenarios in software engineering (SE). However, commonly used off-the-shelf sentiment/emotion detection tools cannot obtain reliable results in SE tasks and misunderstanding of technical knowledge is demonstrated to be the main reason. Then researchers start to create labeled SE-related datasets manually and customize SE-specific methods. However, the scarce labeled data can cover only very limited lexicon and expressions. In this article, we employ emojis as an instrument to address this problem. Different from manual labels that are provided by annotators, emojis are self-reported labels provided by the authors themselves to intentionally convey affective states and thus are suitable indications of sentiment and emotion in texts. Since emojis have been widely adopted in online communication, a large amount of emoji-labeled texts can be easily accessed to help tackle the scarcity of the manually labeled data. Specifically, we leverage Tweets and GitHub posts containing emojis to learn representations of SE-related texts through emoji prediction. By predicting emojis containing in each text, texts that tend to surround the same emoji are represented with similar vectors, which transfers the sentiment knowledge contained in emoji usage to the representations of texts. Then we leverage the sentiment-aware representations as well as manually labeled data to learn the final sentiment/emotion classifier via transfer learning. Compared to existing approaches, our approach can achieve significant improvement on representative benchmark datasets, with an average increase of 0.036 and 0.049 in macro-F1 in sentiment and emotion detection, respectively. Further investigations reveal that the large-scale Tweets make a key contribution to the power of our approach. This finding informs future research not to unilaterally pursue the domain-specific resource but try to transform knowledge from the open domain through ubiquitous signals such as emojis. Finally, we present the open challenges of sentiment and emotion detection in SE through a qualitative analysis of texts misclassified by our approach.","Emoji, sentiment, emotion, software engineerin",,48,ACM Trans. Softw. Eng. Methodol.
241,@inproceedings: 10.1145/3451471.3451506,Relation Extraction: A Brief Survey on Deep Neural Network Based Methods,"Wang, Hailin and Lu, Guoming and Yin, Jin and Qin, Ke",2021,Association for Computing Machinery,,https://doi.org/10.1145/3451471.3451506,10.1145/3451471.3451506,"Knowledge, data, algorithms and computing power are the foundations of artificial intelligence (AI), for which knowledge is the most powerful support. An effective way to acquire knowledge, called Relation Extraction (RE), is beneficial to knowledge graph completion, path inference, logical rule reasoning, and many other AI tasks. In this survey, we provide a comprehensive review on deep neural networks (DNNs) based RE covering some main research topics about: 1) general framework, 2) supervised and distant supervised RE, 3) future research directions. General framework provides a full-view summarization on traditional methods, main component and basic conceptions of DNN-based RE. For supervised and distant supervised RE, this part summarizes DNN-based RE methods into a new taxonomies on these topics. In the end, to facilitate future research on RE, we also discuss some obstacles.","Neural Networks, Supervised Relation Extraction, Distant Supervised Relation Extraction, Information Extraction",220–228,9,2021 The 4th International Conference on Software Engineering and Information Management
242,@inproceedings: 10.1145/3430984.3431022,"Domain-Specific, Semi-Supervised Transfer Learning for Medical Imaging","Virk, Jitender Singh and Bathula, Deepti R.",2021,Association for Computing Machinery,,https://doi.org/10.1145/3430984.3431022,10.1145/3430984.3431022," Limited availability of annotated medical imaging data poses a challenge for deep learning algorithms. Although transfer learning minimizes this hurdle in general, knowledge transfer across disparate domains is shown to be less effective. On the other hand, smaller architectures were found to be more compelling in learning better features. Consequently, we propose a lightweight architecture that uses mixed asymmetric kernels (MAKNet) to reduce the number of parameters significantly. Additionally, we train the proposed architecture using semi-supervised learning to provide pseudo-labels for a large medical dataset to assist with transfer learning. The proposed MAKNet provides better classification performance with fewer parameters than popular architectures. Experimental results also highlight the importance of domain-specific knowledge for effective transfer learning. Additionally, we interrogate the proposed network with integrated gradients and perturbation methods to establish the superior quality of the learned features.","Semi-supervised learning, mixed-kernels neural networks, image perturbations, Neural networks, integrated gradients, transfer learning, domain-specific, pseudo-labelling, CT scans",145–153,9,8th ACM IKDD CODS and 26th COMAD
243,@inproceedings: 10.1145/3430984.3431007,Fine-Tune Longformer for Jointly Predicting Rumor Stance and Veracity,"Khandelwal, Anant",2021,Association for Computing Machinery,,https://doi.org/10.1145/3430984.3431007,10.1145/3430984.3431007," Increased usage of social media caused the popularity of news and events that are not even verified, resulting in the spread of rumors all over the web. Due to widely available social media platforms and increased usage caused the data to be available in large amounts. The manual methods to process such data is costly and time-taking, so there has been increased attention to process and verify such content automatically for the presence of rumors. Lots of research studies reveal that identifying the stances of posts in the discussion thread of such events and news is an important preceding step before detecting the rumor veracity. In this paper, we propose a multi-task learning framework for jointly predicting rumor stance and veracity on the dataset released at SemEval 2019 RumorEval: Determining rumor veracity and support for rumors (SemEval 2019 Task 7), which includes social media rumors stem from a variety of breaking news stories from Reddit as well as Twitter. Our framework consists of two parts: a) The bottom part of our framework classifies the stance for each post in the conversation thread (discussing a rumor) via modeling the multi-turn conversation so that each post aware of its neighboring posts. b) The upper part predicts the rumor veracity of the conversation thread respecting the stance evolution obtained from the bottom part. Experimental results on SemEval 2019 Task 7 dataset show that our method outperforms previous methods on both rumor stance classification and veracity prediction.","Rumor Stance, Rumor Veracity, Longformer, Model Averaging, Multi-turn Conversation, Transformer, Roberta",10–19,10,8th ACM IKDD CODS and 26th COMAD
244,@inproceedings: 10.1145/3430984.3431024,Adversarial and Auxiliary Features-Aware BERT for Sarcasm Detection,"Kumar, Avinash and Narapareddy, Vishnu Teja and Gupta, Pranjal and Srikanth, Veerubhotla Aditya and Neti, Lalita Bhanu Murthy and Malapati, Aruna",2021,Association for Computing Machinery,,https://doi.org/10.1145/3430984.3431024,10.1145/3430984.3431024," Sarcasm is a way to express a negative viewpoint using positive or intensified positive words in social media. This intentional ambiguity makes sarcasm detection, a key task of sentiment analysis. Sarcasm detection is modeled as a binary classification problem wherein both feature-rich traditional models and deep learning models have been successfully built to detect sarcastic comments. To help deep learning models generalize better huge labeled data collection is required, which can be laborious and time-consuming. An adversarial process which is carried out in the embedding space provides an alternative mechanism to generate real-world like examples. However, these examples are not real sentences, but act as a regularization method and can make neural networks more robust. In this paper, we present a novel model, Adversarial and Auxiliary Features-Aware BERT (AAFAB), which utilizes contextual word embedding (BERT) for encoding the semantic meaning of the sentence and then combine it with high quality manually extracted auxiliary features for sarcasm detection. Further, adversarial training is performed by adding perturbations to the input word embedding which enables AAFAB to generalize parameters in a better way. The experiment results show that the inclusion of auxiliary features and adversarial training enhances the performance of AAFAB, and it performs better than various baseline traditional models and deep learning models. ","sarcasm detection, Feature engineering, BERT, SVM, Attention mechanism, Deep neural network",163–170,8,8th ACM IKDD CODS and 26th COMAD
245,@inproceedings: 10.1145/3430984.3431000,Mining Domain-Specific Component-Action Links for Technical Support Documents,"Aggarwal, Pooja and Bansal, Sahil and Mohapatra, Prateeti and Kumar, Atul",2021,Association for Computing Machinery,,https://doi.org/10.1145/3430984.3431000,10.1145/3430984.3431000,"IT service support data such as user queries, resolutions done by agents, technical documents are very complex. It is very important to accurately identify the key components and the actions performed on them to better assist applications, such as knowledge extraction and ingestion, search, query, and support chat-bots. Extracting components and correctly linking them to the required actions would also benefit the problem remediation process by drastically reducing the time to resolve and helping the engineers to focus on the steps or section of the document that are relevant and needed to solve the issue. In this paper, we evaluate the performance of four approaches to establish links between mentioned components and actions on technical support documents.","Component-Action Links, Semantic Parsing, Unsupervised Learning",323–331,9,8th ACM IKDD CODS and 26th COMAD
246,@inproceedings: 10.1145/3430984.3431001,MoDest: Multi-Module Design Validation for Documents,"Pegu, Bhanupriya and Singh, Maneet and Kant, Kamal and Singh, Karamjit and Bhowmik, Tanmoy",2021,Association for Computing Machinery,,https://doi.org/10.1145/3430984.3431001,10.1145/3430984.3431001,"Information extraction (IE) from Visually Rich Documents (VRDs) is a common need for businesses, where extracted information is used for various purposes such as verification, design validation, or compliance. Most of the research in IE from VRDs has focused on textual documents such as invoices and receipts, while extracting information from multi-modal VRDs remains a challenging task. This research presents a novel end-to-end design validation framework for multi-modal VRDs containing textual and visual components, for compliance against a pre-defined set of rules. The proposed Multi-mOdule DESign validaTion (referred to as MoDest) framework constitutes two steps: (i) information extraction using five modules for obtaining the textual and visual components, followed by (ii) validating the extracted components against a pre-defined set of design rules. Given an input multi-modal VRD image, the MoDest framework either accepts or rejects its design while providing an explanation for the decision. The proposed framework is tested for design validation for a particular type of VRDs: banking cards, under the real-world constraint of limited and highly imbalance training data with more than 99% of card designs belonging to one class (accepted). Experimental evaluation on real world images from our in-house dataset demonstrates the effectiveness of the proposed MoDest framework. Analysis drawn from the real-world deployment of the framework further strengthens its utility for design validation. ","Design Validation, Multi-modal Document, Computer Vision",332–340,9,8th ACM IKDD CODS and 26th COMAD
247,@inproceedings: 10.1145/3430984.3430994,Anticipatory Decisions in Retail E-Commerce Warehouses Using Reinforcement Learning,"Shelke, Omkar and Baniwal, Vinita and Khadilkar, Harshad",2021,Association for Computing Machinery,,https://doi.org/10.1145/3430984.3430994,10.1145/3430984.3430994," This paper describes the use of Reinforcement Learning (RL) in retail warehouses serving e-commerce demand, for (i) reducing fulfilment time (from order receipt to shipping hand-off), and (ii) improving labour utilisation. A complex sequence of operations is performed in order to pick the products from shelves and deliver them to the customer. This process requires a large amount of labour (manual or robotic), and its poor utilisation leads to high operating costs. In this paper, we focus on effectual labour utilisation for performing these tasks and for serving more orders with existing labour. We define our problem in RL framework and use the Deep-Deterministic Policy Gradient (DDPG) algorithm for computing anticipatory picking decisions simultaneously for a large number of products. These picking (retrieval from storage) decisions are generated before orders are actually received, and can be executed overnight in bulk batches, significantly improving labour efficiency. Experiments show that our approach allows the system to process more orders in comparison to baseline heuristic and imitation learning algorithms.","E-Commerce, Reinforcement&nbsp;Learning, Multi&nbsp;Product, Supply&nbsp;Chain, Scalability",272–280,9,8th ACM IKDD CODS and 26th COMAD
248,@inproceedings: 10.1145/3430984.3431019,Learn to Bind and Grow Neural Structures,"Shaikh, Azhar and Sinha, Nishant",2021,Association for Computing Machinery,,https://doi.org/10.1145/3430984.3431019,10.1145/3430984.3431019," Task-incremental learning involves the challenging problem of learning new tasks continually, without forgetting past knowledge. Many approaches address the problem by expanding the structure of a shared neural network as tasks arrive, but struggle to grow optimally, without losing past knowledge. We present a new framework, Learn to Bind and Grow, which learns a neural architecture for a new task incrementally, either by binding with layers of a similar task or by expanding layers which are more likely to conflict between tasks. Central to our approach is a novel, interpretable, parameterization of the shared, multi-task architecture space, which then enables computing globally optimal architectures using Bayesian optimization. Experiments on continual learning benchmarks show that our framework performs comparably with earlier expansion based approaches and is able to flexibly compute multiple optimal solutions with performance-size trade-offs.","dynamic network expansion, continual learning, neural networks",119–126,8,8th ACM IKDD CODS and 26th COMAD
249,@article: 10.1145/3412845,Technical Q8A Site Answer Recommendation via Question Boosting,"Gao, Zhipeng and Xia, Xin and Lo, David and Grundy, John",2021,Association for Computing Machinery,1049-331X,https://doi.org/10.1145/3412845,10.1145/3412845,"Software developers have heavily used online question-and-answer platforms to seek help to solve their technical problems. However, a major problem with these technical Q8A sites is “answer hungriness,” i.e., a large number of questions remain unanswered or unresolved, and users have to wait for a long time or painstakingly go through the provided answers with various levels of quality. To alleviate this time-consuming problem, we propose a novel DEEPANS neural network–based approach to identify the most relevant answer among a set of answer candidates. Our approach follows a three-stage process: question boosting, label establishment, and answer recommendation. Given a post, we first generate a clarifying question as a way of question boosting. We automatically establish the positive, neutral+, neutral-, and negative training samples via label establishment. When it comes to answer recommendation, we sort answer candidates by the matching scores calculated by our neural network–based model. To evaluate the performance of our proposed model, we conducted a large-scale evaluation on four datasets, collected from the real-world technical Q8A sites (i.e., Ask Ubuntu, Super User, Stack Overflow Python, and Stack Overflow Java). Our experimental results show that our approach significantly outperforms several state-of-the-art baselines in automatic evaluation. We also conducted a user study with 50 solved/unanswered/unresolved questions. The user-study results demonstrate that our approach is effective in solving the answer-hungry problem by recommending the most relevant answers from historical archives.","question answering, CQA, sequence-to-sequence, deep neural network, weakly supervised learning, question boostin",,34,ACM Trans. Softw. Eng. Methodol.
250,@article: 10.1145/3412376,Automated Patch Transplantation,"Shariffdeen, Ridwan Salihin and Tan, Shin Hwei and Gao, Mingyuan and Roychoudhury, Abhik",2021,Association for Computing Machinery,1049-331X,https://doi.org/10.1145/3412376,10.1145/3412376,"Automated program repair is an emerging area that attempts to patch software errors and vulnerabilities. In this article, we formulate and study a problem related to automated repair, namely automated patch transplantation. A patch for an error in a donor program is automatically adapted and inserted into a “similar” target program. We observe that despite standard procedures for vulnerability disclosures and publishing of patches, many un-patched occurrences remain in the wild. One of the main reasons is the fact that various implementations of the same functionality may exist and, hence, published patches need to be modified and adapted. In this article, we therefore propose and implement a workflow for transplanting patches. Our approach centers on identifying patch insertion points, as well as namespaces translation across programs via symbolic execution. Experimental results to eliminate five classes of errors highlight our ability to fix recurring vulnerabilities across various programs through transplantation. We report that in 20 of 24 fixing tasks involving eight application subjects mostly involving file processing programs, we successfully transplanted the patch and validated the transplantation through differential testing. Since the publication of patches make an un-patched implementation more vulnerable, our proposed techniques should serve a long-standing need in practice.","patch transplantation, Program repair, code transplantation, dynamic program analysi",,36,ACM Trans. Softw. Eng. Methodol.
251,@inbook: 10.1145/3324884.3416591,Multi-Task Learning Based Pre-Trained Language Model for Code Completion,"Liu, Fang and Li, Ge and Zhao, Yunfei and Jin, Zhi",2020,Association for Computing Machinery,,https://doi.org/10.1145/3324884.3416591,,"Code completion is one of the most useful features in the Integrated Development Environments (IDEs), which can accelerate software development by suggesting the next probable token based on the contextual code in real-time. Recent studies have shown that statistical language modeling techniques can improve the performance of code completion tools through learning from large-scale software repositories. However, these models suffer from two major drawbacks: a) Existing research uses static embeddings, which map a word to the same vector regardless of its context. The differences in the meaning of a token in varying contexts are lost when each token is associated with a single representation; b) Existing language model based code completion models perform poor on completing identifiers, and the type information of the identifiers is ignored in most of these models. To address these challenges, in this paper, we develop a multi-task learning based pre-trained language model for code understanding and code generation with a Transformer-based neural architecture. We pre-train it with hybrid objective functions that incorporate both code understanding and code generation tasks. Then we fine-tune the pre-trained model on code completion. During the completion, our model does not directly predict the next token. Instead, we adopt multi-task learning to predict the token and its type jointly and utilize the predicted type to assist the token prediction. Experiments results on two real-world datasets demonstrate the effectiveness of our model when compared with state-of-the-art methods.",,473–485,1,Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering
252,@inbook: 10.1145/3324884.3418934,BugPecker: Locating Faulty Methods with Deep Learning on Revision Graphs,"Cao, Junming and Yang, Shouliang and Jiang, Wenhui and Zeng, Hushuang and Shen, Beijun and Zhong, Hao",2020,Association for Computing Machinery,,https://doi.org/10.1145/3324884.3418934,,"Given a bug report of a project, the task of locating the faults of the bug report is called fault localization. To help programmers in the fault localization process, many approaches have been proposed, and have achieved promising results to locate faulty files. However, it is still challenging to locate faulty methods, because many methods are short and do not have sufficient details to determine whether they are faulty. In this paper, we present BugPecker, a novel approach to locate faulty methods based on its deep learning on revision graphs. Its key idea includes (1) building revision graphs and capturing the details of past fixes as much as possible, and (2) discovering relations inside our revision graphs to expand the details for methods and calculating various features to assist our ranking. We have implemented BugPecker, and evaluated it on three open source projects. The early results show that BugPecker achieves a mean average precision (MAP) of 0.263 and mean reciprocal rank (MRR) of 0.291, which improve the prior approaches significantly. For example, BugPecker improves the MAP values of all three projects by five times, compared with two recent approaches such as DNNLoc-m and BLIA 1.5.",,1214–1218,,Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering
253,@inbook: 10.1145/3324884.3416532,Evaluating Representation Learning of Code Changes for Predicting Patch Correctness in Program Repair,"Tian, Haoye and Liu, Kui and Kabor\'{e}, Abdoul Kader and Koyuncu, Anil and Li, Li and Klein, Jacques and Bissyand\'{e}, Tegawend\'{e} F.",2020,Association for Computing Machinery,,https://doi.org/10.1145/3324884.3416532,,"A large body of the literature of automated program repair develops approaches where patches are generated to be validated against an oracle (e.g., a test suite). Because such an oracle can be imperfect, the generated patches, although validated by the oracle, may actually be incorrect. While the state of the art explore research directions that require dynamic information or that rely on manually-crafted heuristics, we study the benefit of learning code representations in order to learn deep features that may encode the properties of patch correctness. Our empirical work mainly investigates different representation learning approaches for code changes to derive embeddings that are amenable to similarity computations. We report on findings based on embeddings produced by pre-trained and re-trained neural networks. Experimental results demonstrate the potential of embeddings to empower learning algorithms in reasoning about patch correctness: a machine learning predictor with BERT transformer-based embeddings associated with logistic regression yielded an AUC value of about 0.8 in the prediction of patch correctness on a deduplicated dataset of 1000 labeled patches. Our investigations show that learned representations can lead to reasonable performance when comparing against the state-of-the-art, PATCH-SIM, which relies on dynamic information. These representations may further be complementary to features that were carefully (manually) engineered in the literature.",,981–992,1,Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering
254,@inbook: 10.1145/3324884.3416584,Metamorphic Object Insertion for Testing Object Detection Systems,"Wang, Shuai and Su, Zhendong",2020,Association for Computing Machinery,,https://doi.org/10.1145/3324884.3416584,,"Recent advances in deep neural networks (DNNs) have led to object detectors (ODs) that can rapidly process pictures or videos, and recognize the objects that they contain. Despite the promising progress by industrial manufacturers such as Amazon and Google in commercializing deep learning-based ODs as a standard computer vision service, ODs --- similar to traditional software --- may still produce incorrect results. These errors, in turn, can lead to severe negative outcomes for the users. For instance, an autonomous driving system that fails to detect pedestrians can cause accidents or even fatalities. However, despite their importance, principled, systematic methods for testing ODs do not yet exist.To fill this critical gap, we introduce the design and realization of MetaOD, a metamorphic testing system specifically designed for ODs to effectively uncover erroneous detection results. To this end, we (1) synthesize natural-looking images by inserting extra object instances into background images, and (2) design metamorphic conditions asserting the equivalence of OD results between the original and synthetic images after excluding the prediction results on the inserted objects. MetaOD is designed as a streamlined workflow that performs object extraction, selection, and insertion. We develop a set of practical techniques to realize an effective workflow, and generate diverse, natural-looking images for testing. Evaluated on four commercial OD services and four pretrained models provided by the TensorFlow API, MetaOD found tens of thousands of detection failures. To further demonstrate the practical usage of MetaOD, we use the synthetic images that cause erroneous detection results to retrain the model. Our results show that the model performance is significantly increased, from an mAP score of 9.3 to an mAP score of 10.5.",,1053–1065,1,Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering
255,@inbook: 10.1145/3324884.3416581,Automating Just-in-Time Comment Updating,"Liu, Zhongxin and Xia, Xin and Yan, Meng and Li, Shanping",2020,Association for Computing Machinery,,https://doi.org/10.1145/3324884.3416581,,"Code comments are valuable for program comprehension and software maintenance, and also require maintenance with code evolution. However, when changing code, developers sometimes neglect updating the related comments, bringing in inconsistent or obsolete comments (aka., bad comments). Such comments are detrimental since they may mislead developers and lead to future bugs. Therefore, it is necessary to fix and avoid bad comments. In this work, we argue that bad comments can be reduced and even avoided by automatically performing comment updates with code changes. We refer to this task as ""Just-In-Time (JIT) Comment Updating"" and propose an approach named CUP (<u>C</u>omment <u>UP</u>dater) to automate this task. CUP can be used to assist developers in updating comments during code changes and can consequently help avoid the introduction of bad comments. Specifically, CUP leverages a novel neural sequence-to-sequence model to learn comment update patterns from extant code-comment co-changes and can automatically generate a new comment based on its corresponding old comment and code change. Several customized enhancements, such as a special tokenizer and a novel co-attention mechanism, are introduced in CUP by us to handle the characteristics of this task. We build a dataset with over 108K comment-code co-change samples and evaluate CUP on it. The evaluation results show that CUP outperforms an information-retrieval-based and a rule-based baselines by substantial margins, and can reduce developers' edits required for JIT comment updating. In addition, the comments generated by our approach are identical to those updated by developers in 1612 (16.7%) test samples, 7 times more than the best-performing baseline.",,585–597,1,Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering
256,@inbook: 10.1145/3324884.3416617,BiLO-CPDP: Bi-Level Programming for Automated Model Discovery in Cross-Project Defect Prediction,"Li, Ke and Xiang, Zilin and Chen, Tao and Tan, Kay Chen",2020,Association for Computing Machinery,,https://doi.org/10.1145/3324884.3416617,,"Cross-Project Defect Prediction (CPDP), which borrows data from similar projects by combining a transfer learner with a classifier, have emerged as a promising way to predict software defects when the available data about the target project is insufficient. However, developing such a model is challenge because it is difficult to determine the right combination of transfer learner and classifier along with their optimal hyper-parameter settings. In this paper, we propose a tool, dubbed BiLO-CPDP, which is the first of its kind to formulate the automated CPDP model discovery from the perspective of bi-level programming. In particular, the bi-level programming proceeds the optimization with two nested levels in a hierarchical manner. Specifically, the upper-level optimization routine is designed to search for the right combination of transfer learner and classifier while the nested lower-level optimization routine aims to optimize the corresponding hyper-parameter settings. To evaluate BiLO-CPDP, we conduct experiments on 20 projects to compare it with a total of 21 existing CPDP techniques, along with its single-level optimization variant and Auto-Sklearn, a state-of-the-art automated machine learning tool. Empirical results show that BiLO-CPDP champions better prediction performance than all other 21 existing CPDP techniques on 70% of the projects, while being overwhelmingly superior to Auto-Sklearn and its single-level optimization variant on all cases. Furthermore, the unique bi-level formalization in BiLO-CPDP also permits to allocate more budget to the upper-level, which significantly boosts the performance.",,573–584,1,Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering
257,@inbook: 10.1145/3324884.3416627,A Deep Multitask Learning Approach for Requirements Discovery and Annotation from Open Forum,"Li, Mingyang and Shi, Lin and Yang, Ye and Wang, Qing",2020,Association for Computing Machinery,,https://doi.org/10.1145/3324884.3416627,,"The ability in rapidly learning and adapting to evolving user needs is key to modern business successes. Existing methods are based on text mining and machine learning techniques to analyze user comments and feedback, and often constrained by heavy reliance on manually codified rules or insufficient training data. Multitask learning (MTL) is an effective approach with many successful applications, with the potential to address these limitations associated with requirements analysis tasks. In this paper, we propose a deep MTL-based approach, DEMAR, to address these limitations when discovering requirements from massive issue reports and annotating the sentences in support of automated requirements analysis. DEMAR consists of three main phases: (1) data augmentation phase, for data preparation and allowing data sharing beyond single task learning; (2) model construction phase, for constructing the MTL-based model for requirements discovery and requirements annotation tasks; and (3) model training phase, enabling eavesdropping by shared loss function between the two related tasks. Evaluation results from eight open-source projects show that, the proposed multitask learning approach outperforms two state-of-the-art approaches (CNC and FRA) and six common machine learning algorithms, with the precision of 91% and the recall of 83% for requirements discovery task, and the overall accuracy of 83% for requirements annotation task. The proposed approach provides a novel and effective way to jointly learn two related requirements analysis tasks. We believe that it also sheds light on further directions of exploring multitask learning in solving other software engineering problems.",,336–348,1,Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering
258,@inbook: 10.1145/3324884.3416583,Detecting and Explaining Self-Admitted Technical Debts with Attention-Based Neural Networks,"Wang, Xin and Liu, Jin and Li, Li and Chen, Xiao and Liu, Xiao and Wu, Hao",2020,Association for Computing Machinery,,https://doi.org/10.1145/3324884.3416583,,"Self-Admitted Technical Debt (SATD) is a sub-type of technical debt. It is introduced to represent such technical debts that are intentionally introduced by developers in the process of software development. While being able to gain short-term benefits, the introduction of SATDs often requires to be paid back later with a higher cost, e.g., introducing bugs to the software or increasing the complexity of the software.To cope with these issues, our community has proposed various machine learning-based approaches to detect SATDs. These approaches, however, are either not generic that usually require manual feature engineering efforts or do not provide promising means to explain the predicted outcomes. To that end, we propose to the community a novel approach, namely HATD (Hybrid Attention-based method for self-admitted Technical Debt detection), to detect and explain SATDs using attention-based neural networks. Through extensive experiments on 445,365 comments in 20 projects, we show that HATD is effective in detecting SATDs on both in-the-lab and in-the-wild datasets under both within-project and cross-project settings. HATD also outperforms the state-of-the-art approaches in detecting and explaining SATDs.",,871–882,1,Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering
259,@inbook: 10.1145/3324884.3416624,How Incidental Are the Incidents? Characterizing and Prioritizing Incidents for Large-Scale Online Service Systems,"Chen, Junjie and Zhang, Shu and He, Xiaoting and Lin, Qingwei and Zhang, Hongyu and Hao, Dan and Kang, Yu and Gao, Feng and Xu, Zhangwei and Dang, Yingnong and Zhang, Dongmei",2020,Association for Computing Machinery,,https://doi.org/10.1145/3324884.3416624,,"Although tremendous efforts have been devoted to the quality assurance of online service systems, in reality, these systems still come across many incidents (i.e., unplanned interruptions and outages), which can decrease user satisfaction or cause economic loss. To better understand the characteristics of incidents and improve the incident management process, we perform the first large-scale empirical analysis of incidents collected from 18 real-world online service systems in Microsoft. Surprisingly, we find that although a large number of incidents could occur over a short period of time, many of them actually do not matter, i.e., engineers will not fix them with a high priority after manually identifying their root cause. We call these incidents incidental incidents. Our qualitative and quantitative analyses show that incidental incidents are significant in terms of both number and cost. Therefore, it is important to prioritize incidents by identifying incidental incidents in advance to optimize incident management efforts. In particular, we propose an approach, called DeepIP (Deep learning based Incident Prioritization), to prioritizing incidents based on a large amount of historical incident data. More specifically, we design an attention-based Convolutional Neural Network (CNN) to learn a prediction model to identify incidental incidents. We then prioritize all incidents by ranking the predicted probabilities of incidents being incidental. We evaluate the performance of DeepIP using real-world incident data. The experimental results show that DeepIP effectively prioritizes incidents by identifying incidental incidents and significantly outperforms all the compared approaches. For example, the AUC of DeepIP achieves 0.808, while that of the best compared approach is only 0.624 on average.",,373–384,1,Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering
260,@inbook: 10.1145/3324884.3416633,Attend and Represent: A Novel View on Algorithm Selection for Software Verification,"Richter, Cedric and Wehrheim, Heike",2020,Association for Computing Machinery,,https://doi.org/10.1145/3324884.3416633,,"Today, a plethora of different software verification tools exist. When having a concrete verification task at hand, software developers thus face the problem of algorithm selection. Existing algorithm selectors for software verification typically use handpicked program features together with (1) either manually designed selection heuristics or (2) machine learned strategies. While the first approach suffers from not being transferable to other selection problems, the second approach lacks interpretability, i.e., insights into reasons for choosing particular tools.In this paper, we propose a novel approach to algorithm selection for software verification. Our approach employs representation learning together with an attention mechanism. Representation learning circumvents feature engineering, i.e., avoids the handpicking of program features. Attention permits a form of interpretability of the learned selectors. We have implemented our approach and have experimentally evaluated and compared it with existing approaches. The evaluation shows that representation learning does not only outperform manual feature engineering, but also enables transferability of the learning model to other selection tasks.",,1016–1028,1,Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering
261,@inbook: 10.1145/3324884.3416564,Marble: Model-Based Robustness Analysis of Stateful Deep Learning Systems,"Du, Xiaoning and Li, Yi and Xie, Xiaofei and Ma, Lei and Liu, Yang and Zhao, Jianjun",2020,Association for Computing Machinery,,https://doi.org/10.1145/3324884.3416564,,"State-of-the-art deep learning (DL) systems are vulnerable to adversarial examples, which hinders their potential adoption in safety-and security-critical scenarios. While some recent progress has been made in analyzing the robustness of feed-forward neural networks, the robustness analysis for stateful DL systems, such as recurrent neural networks (RNNs), still remains largely uncharted. In this paper, we propose Marble, a model-based approach for quantitative robustness analysis of real-world RNN-based DL systems. Marble builds a probabilistic model to compactly characterize the robustness of RNNs through abstraction. Furthermore, we propose an iterative refinement algorithm to derive a precise abstraction, which enables accurate quantification of the robustness measurement. We evaluate the effectiveness of Marble on both LSTM and GRU models trained separately with three popular natural language datasets. The results demonstrate that (1) our refinement algorithm is more efficient in deriving an accurate abstraction than the random strategy, and (2) Marble enables quantitative robustness analysis, in rendering better efficiency, accuracy, and scalability than the state-of-the-art techniques.",,423–435,1,Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering
262,@inbook: 10.1145/3324884.3416545,Problems and Opportunities in Training Deep Learning Software Systems: An Analysis of Variance,"Pham, Hung Viet and Qian, Shangshu and Wang, Jiannan and Lutellier, Thibaud and Rosenthal, Jonathan and Tan, Lin and Yu, Yaoliang and Nagappan, Nachiappan",2020,Association for Computing Machinery,,https://doi.org/10.1145/3324884.3416545,,"Deep learning (DL) training algorithms utilize nondeterminism to improve models' accuracy and training efficiency. Hence, multiple identical training runs (e.g., identical training data, algorithm, and network) produce different models with different accuracies and training times. In addition to these algorithmic factors, DL libraries (e.g., TensorFlow and cuDNN) introduce additional variance (referred to as implementation-level variance) due to parallelism, optimization, and floating-point computation.This work is the first to study the variance of DL systems and the awareness of this variance among researchers and practitioners. Our experiments on three datasets with six popular networks show large overall accuracy differences among identical training runs. Even after excluding weak models, the accuracy difference is 10.8%. In addition, implementation-level factors alone cause the accuracy difference across identical training runs to be up to 2.9%, the per-class accuracy difference to be up to 52.4%, and the training time difference to be up to 145.3%. All core libraries (TensorFlow, CNTK, and Theano) and low-level libraries (e.g., cuDNN) exhibit implementation-level variance across all evaluated versions.Our researcher and practitioner survey shows that 83.8% of the 901 participants are unaware of or unsure about any implementation-level variance. In addition, our literature survey shows that only 19.5±3% of papers in recent top software engineering (SE), artificial intelligence (AI), and systems conferences use multiple identical training runs to quantify the variance of their DL approaches. This paper raises awareness of DL variance and directs SE researchers to challenging tasks such as creating deterministic DL implementations to facilitate debugging and improving the reproducibility of DL software and results.",,771–783,1,Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering
263,@inbook: 10.1145/3324884.3416537,Identifying and Describing Information Seeking Tasks,"Satterfield, Chris and Fritz, Thomas and Murphy, Gail C.",2020,Association for Computing Machinery,,https://doi.org/10.1145/3324884.3416537,,"A software developer works on many tasks per day, frequently switching between these tasks back and forth. This constant churn of tasks makes it difficult for a developer to know the specifics of when they worked on what task, complicating task resumption, planning, retrospection, and reporting activities. In a first step towards an automated aid to this issue, we introduce a new approach to help identify the topic of work during an information seeking task --- one of the most common types of tasks that software developers face --- that is based on capturing the contents of the developer's active window at regular intervals and creating a vector representation of key information the developer viewed. To evaluate our approach, we created a data set with multiple developers working on the same set of six information seeking tasks that we also make available for other researchers to investigate similar approaches. Our analysis shows that our approach enables: 1) segments of a developer's work to be automatically associated with a task from a known set of tasks with average accuracy of 70.6%, and 2) a word cloud describing a segment of work that a developer can use to recognize a task with average accuracy of 67.9%.",,797–808,1,Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering
264,@inbook: 10.1145/3324884.3416628,Generating Concept Based API Element Comparison Using a Knowledge Graph,"Liu, Yang and Liu, Mingwei and Peng, Xin and Treude, Christoph and Xing, Zhenchang and Zhang, Xiaoxin",2020,Association for Computing Machinery,,https://doi.org/10.1145/3324884.3416628,,"Developers are concerned with the comparison of similar APIs in terms of their commonalities and (often subtle) differences. Our empirical study of Stack Overflow questions and API documentation confirms that API comparison questions are common and can often be answered by knowledge contained in API reference documentation. Our study also identifies eight types of API statements that are useful for API comparison. Based on these findings, we propose a knowledge graph based approach APIComp that automatically extracts API knowledge from API reference documentation to support the comparison of a pair of API classes or methods from different aspects. Our approach includes an offline phase for constructing an API knowledge graph, and an online phase for generating an API comparison result for a given pair of API elements. Our evaluation shows that the quality of different kinds of extracted knowledge in the API knowledge graph is generally high. Furthermore, the comparison results generated by APIComp are significantly better than those generated by a baseline approach based on heuristic rules and text similarity, and our generated API comparison results are useful for helping developers in API selection tasks.",,834–845,1,Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering
265,@inbook: 10.1145/3324884.3416559,Hybrid Deep Neural Networks to Infer State Models of Black-Box Systems,"Mashhadi, Mohammad Jafar and Hemmati, Hadi",2020,Association for Computing Machinery,,https://doi.org/10.1145/3324884.3416559,,"Inferring behavior model of a running software system is quite useful for several automated software engineering tasks, such as program comprehension, anomaly detection, and testing. Most existing dynamic model inference techniques are white-box, i.e., they require source code to be instrumented to get run-time traces. However, in many systems, instrumenting the entire source code is not possible (e.g., when using black-box third-party libraries) or might be very costly. Unfortunately, most black-box techniques that detect states over time are either univariate, or make assumptions on the data distribution, or have limited power for learning over a long period of past behavior. To overcome the above issues, in this paper, we propose a hybrid deep neural network that accepts as input a set of time series, one per input/output signal of the system, and applies a set of convolutional and recurrent layers to learn the non-linear correlations between signals and the patterns, over time. We have applied our approach on a real UAV auto-pilot solution from our industry partner with half a million lines of C code. We ran 888 random recent system-level test cases and inferred states, over time. Our comparison with several traditional time series change point detection techniques showed that our approach improves their performance by up to 102%, in terms of finding state change points, measured by F1 score. We also showed that our state classification algorithm provides on average 90.45% F1 score, which improves traditional classification algorithms by up to 17%.",,299–311,1,Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering
266,@inbook: 10.1145/3324884.3416530,OCoR: An Overlapping-Aware Code Retriever,"Zhu, Qihao and Sun, Zeyu and Liang, Xiran and Xiong, Yingfei and Zhang, Lu",2020,Association for Computing Machinery,,https://doi.org/10.1145/3324884.3416530,,"Code retrieval helps developers reuse code snippets in the open-source projects. Given a natural language description, code retrieval aims to search for the most relevant code relevant among a set of code snippets. Existing state-of-the-art approaches apply neural networks to code retrieval. However, these approaches still fail to capture an important feature: overlaps. The overlaps between different names used by different people indicate that two different names may be potentially related (e.g., ""message"" and ""msg""), and the overlaps between identifiers in code and words in natural language descriptions indicate that the code snippet and the description may potentially be related.To address this problem, we propose a novel neural architecture named OCoR1, where we introduce two specifically-designed components to capture overlaps: the first embeds names by characters to capture the overlaps between names, and the second introduces a novel overlap matrix to represent the degrees of overlaps between each natural language word and each identifier.The evaluation was conducted on two established datasets. The experimental results show that OCoR significantly outperforms the existing state-of-the-art approaches and achieves 13.1% to 22.3% improvements. Moreover, we also conducted several in-depth experiments to help understand the performance of the different components in OCoR.",,883–894,1,Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering
267,@article: 10.1145/3418463,"IR2V<span class=""smallcaps SmallerCapital"">EC</span>: LLVM IR Based Scalable Program Embeddings","VenkataKeerthy, S. and Aggarwal, Rohit and Jain, Shalini and Desarkar, Maunendra Sankar and Upadrasta, Ramakrishna and Srikant, Y. N.",2020,Association for Computing Machinery,1544-3566,https://doi.org/10.1145/3418463,10.1145/3418463,"We propose IR2VEC, a Concise and Scalable encoding infrastructure to represent programs as a distributed embedding in continuous space. This distributed embedding is obtained by combining representation learning methods with flow information to capture the syntax as well as the semantics of the input programs. As our infrastructure is based on the Intermediate Representation (IR) of the source code, obtained embeddings are both language and machine independent. The entities of the IR are modeled as relationships, and their representations are learned to form a seed embedding vocabulary. Using this infrastructure, we propose two incremental encodings: Symbolic and Flow-Aware. Symbolic encodings are obtained from the seed embedding vocabulary, and Flow-Aware encodings are obtained by augmenting the Symbolic encodings with the flow information.We show the effectiveness of our methodology on two optimization tasks (Heterogeneous device mapping and Thread coarsening). Our way of representing the programs enables us to use non-sequential models resulting in orders of magnitude of faster training time. Both the encodings generated by IR2VEC outperform the existing methods in both the tasks, even while using simple machine learning models. In particular, our results improve or match the state-of-the-art speedup in 11/14 benchmark-suites in the device mapping task across two platforms and 53/68 benchmarks in the thread coarsening task across four different platforms. When compared to the other methods, our embeddings are more scalable, is non-data-hungry, and has better Out-Of-Vocabulary (OOV) characteristics.","representation learning, compiler optimizations, heterogeneous systems, intermediate representations, LLV",,27,ACM Trans. Archit. Code Optim.
268,@inproceedings: 10.1145/3439231.3439257,Word-Sense Disambiguation System for Text Readability,"AlarconLourdes Alarcon, Rodrigo and Moreno, Lourdes and Mart\'{\i}nez, Paloma",2020,Association for Computing Machinery,,https://doi.org/10.1145/3439231.3439257,10.1145/3439231.3439257,"People with cognitive, language and learning disabilities face accessibility barriers when reading texts with complex or specialized words. In order for these needs to be addressed, and in accordance with accessibility guidelines, it is beneficial to provide the definitions of complex words to the user. In this sense, human language can, at times, be ambiguous, and many words may be interpreted in multiple ways depending on the context. To offer a correct definition, it is often necessary to carry out Word Sense Disambiguation. In this paper, a system that is based on Natural Language Processing and Language Resources in the field of easy reading to provide a contextualized definition for a complex word is presented. An expert linguistic, specialized in easy reading and plain language, has evaluated the Word Sense Disambiguation system. This research work is part of the EASIER project that offers an accessible platform to provide users with the easiest possible Spanish text to understand and read.","Web accessibility, Readability, Word-sense disambiguation, Tool, Cognitive disabilities, WCAG, Natural Language Processing",147–152,6,9th International Conference on Software Development and Technologies for Enhancing Accessibility and Fighting Info-Exclusion
269,@inproceedings: 10.1145/3439961.3440004,Does Gamification Improve the Training of Software Testers? A Preliminary Study from the Industry Perspective✱,"Valle, Pedro Henrique Dias and Vilela, Ricardo Ferreira and Hernandes, Elis C. Montoro",2020,Association for Computing Machinery,,https://doi.org/10.1145/3439961.3440004,10.1145/3439961.3440004,"Background: Software testing is an essential activity in software companies to ensure the quality of their systems. Nevertheless, the most recent studies demonstrate that there is a lack of qualified professionals in this area. Aim: To check whether the gamification contributes to the training of software testers in the industry, analyzing their motivation and test cases designed by them. Method: For this, we conducted an experimental study with junior testers in a software company to compare gamified training against non-gamified training. Results: We observed significant contributions from gamification in the training of software testers, especially in their motivation and quality of the test cases. We noted that non-gamified training still offers contributions to software testing training. This result intensified when we embedded gamified elements in the training since we noticed a difference between groups at scenario coverage analysis. Conclusions: Our results should encourage software companies to emphasize training in software testing and adopt gamified elements because software testers are more motivated in training and design better test cases from the theories of software testing.","Industry, Software Testing Training, Gamification",,10,19th Brazilian Symposium on Software Quality
270,@inproceedings: 10.1145/3439961.3439991,Deployment of a Machine Learning System for Predicting Lawsuits Against Power Companies: Lessons Learned from an Agile Testing Experience for Improving Software Quality,"Rivero, Luis and Diniz, Jo\~{a}o and Silva, Giovanni and Borralho, Gabriel and Braz Junior, Geraldo and Paiva, Anselmo and Alves, Erika and Oliveira, Milton",2020,Association for Computing Machinery,,https://doi.org/10.1145/3439961.3439991,10.1145/3439961.3439991," The advances in Machine Learning (ML) require software organizations to evolve their development processes in order to improve the quality of ML systems. Within the software development process, the testing stage of an ML system is more critical, considering that it is necessary to add data validation, trained model quality evaluation, and model validation to traditional unit, integration tests and system tests. In this paper, we focus on reporting the lessons learned of using model testing and exploratory testing within the context of the agile development process of an ML system that predicts lawsuits proneness in energy supply companies. Through the development of the project, the SCRUM agile methodology was applied and activities related to the development of the ML model and the development of the end-user application were defined. After the testing process of the ML model, we managed to achieve 93.89 accuracy; 95.58 specificity; 88.84 sensitivity; and 87.09 precision. Furthermore, we focused on the quality of use of the application embedding the ML model, by carrying out exploratory testing. As a result, through several iterations, different types of defects were identified and corrected. Our lessons learned support software engineers willing to develop ML systems that consider both the ML model and the end-user application.","Methods, Software Processes, Validation, and Tools, Verification, and Testing",,10,19th Brazilian Symposium on Software Quality
271,@inproceedings: 10.1145/3426425.3426937,Towards the Optical Character Recognition of DSLs,"Perianez-Pascual, Jorge and Rodriguez-Echeverria, Roberto and Burgue\~{n}o, Loli and Cabot, Jordi",2020,Association for Computing Machinery,,https://doi.org/10.1145/3426425.3426937,10.1145/3426425.3426937,"OCR engines aim to identify and extract text strings fromdocuments or images. While current efforts focus mostly inmainstream languages, there is little support for program-ming or domain-specific languages (DSLs). In this paper, wepresent our vision about the current state of OCR recognitionfor DSLs and its challenges. We discuss some strategies toimprove the OCR quality applied to DSL textual expressionsby leveraging DSL specifications and domain data. To bettersupport our ideas we present the preliminary results of anempirical study and outline a research roadmap.","text recognition, optical character recognition, domain-specific languages",126–132,7,Proceedings of the 13th ACM SIGPLAN International Conference on Software Language Engineering
272,@inbook: 10.1145/3425898.3426962,VarSem: Declarative Expression and Automated Inference of Variable Usage Semantics,"Liu, Yin and Tilevich, Eli",2020,Association for Computing Machinery,,https://doi.org/10.1145/3425898.3426962,,"Programmers declare variables to serve specific implementation purposes that we refer to as variable usage semantics (VUS). Understanding VUS is required for various software engineering tasks, including program comprehension, code audits, and vulnerability detection. To help programmers understand VUS, we present a new program analysis that infers a variable's usage semantics from its textual and context information (e.g., symbolic name, type, scope, information flow). To support this analysis, we introduce VarSem, a domain-specific language, in which a variable's semantic category is expressed as a set of declarative rules. VarSem's execution determines which program variables belong to a given semantic category. VarSem translates high-level declarative rules into low-level program analysis techniques, including natural language processing and data flow, and provides a highly extensible architecture for specifying new rules and analysis techniques. We evaluate VarSem with eight real-world systems to identify their personally identifiable information variables. The evaluation results show that VarSem infers variable semantics with satisfying accuracy/precision and passable recall, thus potentially benefiting both software and security engineers.",,84–97,1,Proceedings of the 19th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences
273,@article: 10.1145/3428295,Just-in-Time Learning for Bottom-up Enumerative Synthesis,"Barke, Shraddha and Peleg, Hila and Polikarpova, Nadia",2020,Association for Computing Machinery,,https://doi.org/10.1145/3428295,10.1145/3428295,"A key challenge in program synthesis is the astronomical size of the search space the synthesizer has to explore. In response to this challenge, recent work proposed to guide synthesis using learned probabilistic models. Obtaining such a model, however, might be infeasible for a problem domain where no high-quality training data is available. In this work we introduce an alternative approach to guided program synthesis: instead of training a model ahead of time we show how to bootstrap one just in time, during synthesis, by learning from partial solutions encountered along the way. To make the best use of the model, we also propose a new program enumeration algorithm we dub guided bottom-up search, which extends the efficient bottom-up search with guidance from probabilistic models.  We implement this approach in a tool called Probe, which targets problems in the popular syntax-guided synthesis (SyGuS) format. We evaluate Probe on benchmarks from the literature and show that it achieves significant performance gains both over unguided bottom-up search and over a state-of-the-art probability-guided synthesizer, which had been trained on a corpus of existing solutions. Moreover, we show that these performance gains do not come at the cost of solution quality: programs generated by Probe are only slightly more verbose than the shortest solutions and perform no unnecessary case-splitting.","Probabilistic models, Domain-specific languages, Program Synthesi",,29,Proc. ACM Program. Lang.
274,@article: 10.1145/3428283,A Structural Model for Contextual Code Changes,"Brody, Shaked and Alon, Uri and Yahav, Eran",2020,Association for Computing Machinery,,https://doi.org/10.1145/3428283,10.1145/3428283,"We address the problem of predicting edit completions based on a learned model that was trained on past edits. Given a code snippet that is partially edited, our goal is to predict a completion of the edit for the rest of the snippet. We refer to this task as the EditCompletion task and present a novel approach for tackling it. The main idea is to directly represent structural edits. This allows us to model the likelihood of the edit itself, rather than learning the likelihood of the edited code. We represent an edit operation as a path in the program’s Abstract Syntax Tree (AST), originating from the source of the edit to the target of the edit. Using this representation, we present a powerful and lightweight neural model for the EditCompletion task. We conduct a thorough evaluation, comparing our approach to a variety of representation and modeling approaches that are driven by multiple strong models such as LSTMs, Transformers, and neural CRFs. Our experiments show that our model achieves a 28% relative gain over state-of-the-art sequential models and 2\texttimes{} higher accuracy than syntactic models that learn to generate the edited code, as opposed to modeling the edits directly. Our code, dataset, and trained models are publicly available at <a>https://github.com/tech-srl/c3po/</a> .","Edit Completions, Neural Models of Code, Machine Learnin",,28,Proc. ACM Program. Lang.
275,@article: 10.1145/3417709,NNBench-X: A Benchmarking Methodology for Neural Network Accelerator Designs,"Xie, Xinfeng and Hu, Xing and Gu, Peng and Li, Shuangchen and Ji, Yu and Xie, Yuan",2020,Association for Computing Machinery,1544-3566,https://doi.org/10.1145/3417709,10.1145/3417709,"The tremendous impact of deep learning algorithms over a wide range of application domains has encouraged a surge of neural network (NN) accelerator research. Facilitating the NN accelerator design calls for guidance from an evolving benchmark suite that incorporates emerging NN models. Nevertheless, existing NN benchmarks are not suitable for guiding NN accelerator designs. These benchmarks are either selected for general-purpose processors without considering unique characteristics of NN accelerators or lack quantitative analysis to guarantee their completeness during the benchmark construction, update, and customization.In light of the shortcomings of prior benchmarks, we propose a novel benchmarking methodology for NN accelerators with a quantitative analysis of application performance features and a comprehensive awareness of software-hardware co-design. Specifically, we decouple the benchmarking process into three stages: First, we characterize the NN workloads with quantitative metrics and select the representative applications for the benchmark suite to ensure diversity and completeness. Second, we refine the selected applications according to the customized model compression techniques provided by specific software-hardware co-design. Finally, we evaluate a variety of accelerator designs on the generated benchmark suite. To demonstrate the effectiveness of our benchmarking methodology, we conduct a case study of composing an NN benchmark from the TensorFlow Model Zoo and compress these selected models with various model compression techniques. Finally, we evaluate compressed models on various architectures, including GPU, Neurocube, DianNao, and Cambricon-X.","software-hardware co-designs, benchmark, accelerator, Neural network",,25,ACM Trans. Archit. Code Optim.
276,@inproceedings: 10.5555/3432601.3432628,Evaluating the Effectiveness of Static Word Embeddings on the Classification of IT Support Tickets,"Wahba, Yasmen and Madhavji, Nazim H. and Steinbacher, John",2020,IBM Corp.,,,,"Support tickets are service requests, initiated by a system's end-users when they encounter issues with their system. With a wide user-base and system issues, there will be an ongoing influx of generated support tickets. Manual classification and prioritization is effortful and error-prone, that can lead to incorrect routing and delays in the resolution of the issues.Recently, various state-of-the-art machine learning and deep learning methods have been applied to automate the process of text classification. Because the quality of these methods highly depends on the quality of the associated ""features"", in this paper we focus on the ""feature engineering"" step in the classification process. In particular, we evaluate the effectiveness of using different static word embeddings on the accuracy of classifying IT support tickets.In collaboration with an industrial partner, we were able to train and evaluate our machine learning model on 1.6 million support tickets and 32 ticket categories.The experimental results show that the traditional Term Frequency Inverse Document Frequency (TFIDF) bag-of-words along with Support Vector Machines (SVM) provides competitive results and sometimes outperforms static word embedding models such as word2vec while maintaining low computational cost.","word embedding, customer support tickets, machine learning, natural language processing, feature engineering",198–206,9,Proceedings of the 30th Annual International Conference on Computer Science and Software Engineering
277,@inproceedings: 10.5555/3432601.3432619,Moving from Cross-Project Defect Prediction to Heterogeneous Defect Prediction: A Partial Replication Study,"Jahanshahi, Hadi and Cevik, Mucahit and Ba\c{s}ar, Ay\c{s}e",2020,IBM Corp.,,,,"Software defect prediction heavily relies on the metrics collected from software projects. Earlier studies often used machine learning techniques to build, validate, and improve bug prediction models using either a set of metrics collected within a project or across different projects. However, techniques applied and conclusions derived by those models are restricted by how identical those metrics are. Knowledge coming from those models will not be extensible to a target project if no sufficient overlapping metrics have been collected in the source projects. To explore the feasibility of transferring knowledge across projects without common labeled metrics, we systematically integrated Heterogeneous Defect Prediction (HDP) by replicating and validating the obtained results. Our main goal is to extend prior research and explore the feasibility of HDP and finally to compare its performance with that of its predecessor, Cross-Project Defect Prediction. We construct an HDP model on different publicly available datasets. Moreover, we propose a new ensemble voting approach in the HDP context to utilize the predictive power of multiple available datasets. The result of our experiment is comparable to that of the original study. However, we also explored the feasibility of HDP in real cases. Our results shed light on the infeasibility of many cases for the HDP algorithm due to its sensitivity to the parameter selection. In general, our analysis gives a deep insight into why and how to perform transfer learning from one domain to another, and in particular, provides a set of guidelines to help researchers and practitioners to disseminate knowledge to the defect prediction domain.","heterogeneous metrics, software quality, defect prediction, transfer learning",133–142,10,Proceedings of the 30th Annual International Conference on Computer Science and Software Engineering
278,@inproceedings: 10.5555/3432601.3432629,Deep Learning Approaches to Classify the Relevance and Sentiment of News Articles to the Economy,"Wang, Jingli and Bhowmick, Ashok and Cevik, Mucahit and Basar, Ayse",2020,IBM Corp.,,,,"We consider a text classification task over an open source dataset involving news snippets and their relevance to the US economy. Text classification and sentiment analysis have been performed using nine different classifiers among which three are the traditional machine learning models, namely, support vector machine, extreme gradient boosting and logistic regression, and six neural network-based methods. The neural net frameworks include long short-term memory (LSTM), bidirectional long short-term memory (BiLSTM) and an ensemble of one dimensional convolution network (1D CNN) with LSTM/BiLSTM. Both word-to-vector and term-frequency inverse-document-frequency vectors are used in our analysis with text and sentiment classification tasks. A detailed comparative study is provided to assess the relative performance of different classification approaches. It is observed that the ensemble with 1D CNN performs better in both binary and multiclass classifications. Specifically, in the multinomial sentiment classification, 1D CNN with BiLSTM has the best performance as opposed to 1D CNN with LSTM in the binary text classification. BiLSTM architecture which incorporates the backward dependencies turns out as superior to LSTM by a margin of 30% in multiclass classification even though the considered dataset is small and inherently challenging. Further analysis to evaluate the impact of successive increases in percentage of augmented data reveals that such augmentation has a limit up to 180% in this dataset beyond which the performance starts decreasing.","news snippet, sentiment analysis, 1D CNN, NLP, US economy, tf-idf, text classification, LSTM, BiLSTM",207–216,10,Proceedings of the 30th Annual International Conference on Computer Science and Software Engineering
279,@inproceedings: 10.5555/3432601.3432605,Towards Interpretable and Maintainable Supervised Learning Using Shapley Values in Arrhythmia,"Krishnakumar, Sanjena and Abdou, Tamer",2020,IBM Corp.,,,,"This paper investigates the application of a model-agnostic interpretability technique, Shapley Additive Explanations (SHAP), to understand and hence, enhance machine learning classification models using Shapley values in the prediction of arrhythmias1. Using the Arrhythmia dataset2, three different feature selection techniques, Information Gain (IG), Recursive Feature Elimination-Random Forest (RFE-RF), and AutoSpearman, were used to select features for machine learning models to predict the arrhythmia class. Four multi-class classification models, Na\""{\i}ve Bayes (NB), k-Nearest Neighbours (kNN), Random Forest (RF), and stacking heterogeneous ensemble (Ensemble) were built, evaluated, and compared. SHAP interpretation method was applied to find reliable explanations for the predictions of the classification models. Additionally, SHAP values were used to find `bellwether' instances to enhance the training of our models in order to improve their performances in the prediction of arrhythmia. The most stable and top-performing classification model was RF, followed by Ensemble in comparison to NB and kNN. SHAP provided robust and reliable explanations for the classification models. Furthermore, improving the training of our models with `bellwether' instances, found using SHAP values, enhanced the overall model performances in terms of accuracy, AUC, and F1 score. In conclusion, we recommend using SHAP value explanations as a robust and reliable method for local model-agnostic interpretability and to enhance machine learning models for arrhythmia prediction.","SHAP, bellwether, arrhythmia, healthcare, machine learning, LIME, multi-class classification, local model-agnostic interpretation, shapley value",23–32,10,Proceedings of the 30th Annual International Conference on Computer Science and Software Engineering
280,@inbook: 10.1145/3412453.3423197,Developing Smart Edutainment for Preschoolers: A Multidisciplinary Approach,"Guran, Adriana-Mihaela and Cojocar, Grigoreta Sofia and Dio\c{s}an, Laura Silvia",2020,Association for Computing Machinery,,https://doi.org/10.1145/3412453.3423197,,"Education has suffered multiple changes due to technological progress. Even though the current generation of preschoolers (aged 3 to 6 years in our country) is called digital native, there is a lack of focus on introducing technology enabled learning tools for them. This paper presents our approach to designing smart learning experiences for a fringe users group, the preschoolers. We present our method proposal for designing edutainment applications for preschoolers, based on a User Centered Design process and how we may integrate Artificial Intelligence to complete and support the capabilities of our little users.",,20–26,,Proceedings of the 2nd ACM SIGSOFT International Workshop on Education through Advanced Software Engineering and Artificial Intelligence
281,@inbook: 10.1145/3368089.3417058,IntelliCode Compose: Code Generation Using Transformer,"Svyatkovskiy, Alexey and Deng, Shao Kun and Fu, Shengyu and Sundaresan, Neel",2020,Association for Computing Machinery,,https://doi.org/10.1145/3368089.3417058,,"In software development through integrated development environments (IDEs), code completion is one of the most widely used features. Nevertheless, majority of integrated development environments only support completion of methods and APIs, or arguments. In this paper, we introduce IntelliCode Compose – a general-purpose multilingual code completion tool which is capable of predicting sequences of code tokens of arbitrary types, generating up to entire lines of syntactically correct code. It leverages state-of-the-art generative transformer model trained on 1.2 billion lines of source code in Python, C#, JavaScript and TypeScript programming languages. IntelliCode Compose is deployed as a cloud-based web service. It makes use of client-side tree-based caching, efficient parallel implementation of the beam search decoder, and compute graph optimizations to meet edit-time completion suggestion requirements in the Visual Studio Code IDE and Azure Notebook. Our best model yields an average edit similarity of 86.7% and a perplexity of 1.82 for Python programming language.",,1433–1443,1,Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
282,@inbook: 10.1145/3368089.3417061,Efficient Customer Incident Triage via Linking with System Incidents,"Gu, Jiazhen and Wen, Jiaqi and Wang, Zijian and Zhao, Pu and Luo, Chuan and Kang, Yu and Zhou, Yangfan and Yang, Li and Sun, Jeffrey and Xu, Zhangwei and Qiao, Bo and Li, Liqun and Lin, Qingwei and Zhang, Dongmei",2020,Association for Computing Machinery,,https://doi.org/10.1145/3368089.3417061,,"In cloud service systems, customers will report the service issues they have encountered to cloud service providers. Despite many issues can be handled by the support team, sometimes the customer issues can not be easily solved, thus raising customer incidents. Quick troubleshooting of a customer incident is critical. To this end, a customer incident should be assigned to its responsible team accurately in a timely manner. Our industrial experiences show that linking customer incidents with detected system incidents can help the customer incident triage. In particular, our empirical study on 7 real cloud service systems shows that with the additional information about the system incidents (i.e., incident reports generated by system monitors), the triage time of customer incidents can be accelerated 13.1\texttimes{} on average. Based on this observation, in this paper, we propose LinkCM, a learning based approach to automatically link customer incidents to monitor reported system incidents. LinkCM incorporates a novel learning-based model that effectively extracts related information from two resources, and a transfer learning strategy is proposed to help LinkCM achieve better performance without huge amount of data. The experimental results indicate that LinkCM is able to achieve accurate link prediction. Furthermore, case studies are presented to demonstrate how LinkCM can help the customer incident triage procedure in real production cloud service systems.",,1296–1307,1,Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
283,@inbook: 10.1145/3368089.3409674,Automatically Identifying Performance Issue Reports with Heuristic Linguistic Patterns,"Zhao, Yutong and Xiao, Lu and Babvey, Pouria and Sun, Lei and Wong, Sunny and Martinez, Angel A. and Wang, Xiao",2020,Association for Computing Machinery,,https://doi.org/10.1145/3368089.3409674,,"Performance issues compromise the response time and resource consumption of a software system. Modern software systems use issue tracking systems to manage all kinds of issue reports, including performance issues. The problem is that performance issues are often not explicitly tagged. The tagging mechanism, if exists, is completely voluntary, depending on the project’s convention and on submitters’ discipline. For example, the performance tag rate in Apache’s Jira system is below 1%. This paper contributes a hybrid classification approach that combines linguistic patterns and machine/deep learning techniques to automatically detect performance issue reports. We manually analyzed 980 real-life performance issue reports and derived 80 project-agnostic linguistic patterns that recur in the reports. Our approach uses these linguistic patterns to construct the sentence-level and issue-level learning features for training effective machine/deep learning classifiers. We test our approach on two separate datasets, each consisting of 980 unclassified issue reports, and compare the results with 31 baseline methods. Our approach can reach up to 83% precision and up to 59% recall. The only comparable baseline method is BERT, which is still 25% lower in the F1-score.",,964–975,1,Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
284,@inbook: 10.1145/3368089.3409691,Object Detection for Graphical User Interface: Old Fashioned or Deep Learning or a Combination?,"Chen, Jieshan and Xie, Mulong and Xing, Zhenchang and Chen, Chunyang and Xu, Xiwei and Zhu, Liming and Li, Guoqiang",2020,Association for Computing Machinery,,https://doi.org/10.1145/3368089.3409691,,"Detecting Graphical User Interface (GUI) elements in GUI images is a domain-specific object detection task. It supports many software engineering tasks, such as GUI animation and testing, GUI search and code generation. Existing studies for GUI element detection directly borrow the mature methods from computer vision (CV) domain, including old fashioned ones that rely on traditional image processing features (e.g., canny edge, contours), and deep learning models that learn to detect from large-scale GUI data. Unfortunately, these CV methods are not originally designed with the awareness of the unique characteristics of GUIs and GUI elements and the high localization accuracy of the GUI element detection task. We conduct the first large-scale empirical study of seven representative GUI element detection methods on over 50k GUI images to understand the capabilities, limitations and effective designs of these methods. This study not only sheds the light on the technical challenges to be addressed but also informs the design of new GUI element detection methods. We accordingly design a new GUI-specific old-fashioned method for non-text GUI element detection which adopts a novel top-down coarse-to-fine strategy, and incorporate it with the mature deep learning model for GUI text detection.Our evaluation on 25,000 GUI images shows that our method significantly advances the start-of-the-art performance in GUI element detection.",,1202–1214,1,Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
285,@inproceedings: 10.1145/3368089.3409668,On Decomposing a Deep Neural Network into Modules,"Pan, Rangeet and Rajan, Hridesh",2020,Association for Computing Machinery,,https://doi.org/10.1145/3368089.3409668,10.1145/3368089.3409668,"Deep learning is being incorporated in many modern software systems. Deep learning approaches train a deep neural network (DNN) model using training examples, and then use the DNN model for prediction. While the structure of a DNN model as layers is observable, the model is treated in its entirety as a monolithic component. To change the logic implemented by the model, e.g. to add/remove logic that recognizes inputs belonging to a certain class, or to replace the logic with an alternative, the training examples need to be changed and the DNN needs to be retrained using the new set of examples. We argue that decomposing a DNN into DNN modules— akin to decomposing a monolithic software code into modules—can bring the benefits of modularity to deep learning. In this work, we develop a methodology for decomposing DNNs for multi-class problems into DNN modules. For four canonical problems, namely MNIST, EMNIST, FMNIST, and KMNIST, we demonstrate that such decomposition enables reuse of DNN modules to create different DNNs, enables replacement of one DNN module in a DNN with another without needing to retrain. The DNN models formed by composing DNN modules are at least as good as traditional monolithic DNNs in terms of test accuracy for our problems.","deep neural networks, decomposing, modules, modularity",889–900,12,Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
286,@inbook: 10.1145/3368089.3409696,Operational Calibration: Debugging Confidence Errors for DNNs in the Field,"Li, Zenan and Ma, Xiaoxing and Xu, Chang and Xu, Jingwei and Cao, Chun and L\""{u}, Jian",2020,Association for Computing Machinery,,https://doi.org/10.1145/3368089.3409696,,"Trained DNN models are increasingly adopted as integral parts of software systems, but they often perform deficiently in the field. A particularly damaging problem is that DNN models often give false predictions with high confidence, due to the unavoidable slight divergences between operation data and training data. To minimize the loss caused by inaccurate confidence, operational calibration, i.e., calibrating the confidence function of a DNN classifier against its operation domain, becomes a necessary debugging step in the engineering of the whole system.  Operational calibration is difficult considering the limited budget of labeling operation data and the weak interpretability of DNN models. We propose a Bayesian approach to operational calibration that gradually corrects the confidence given by the model under calibration with a small number of labeled operation data deliberately selected from a larger set of unlabeled operation data. The approach is made effective and efficient by leveraging the locality of the learned representation of the DNN model and modeling the calibration as Gaussian Process Regression. Comprehensive experiments with various practical datasets and DNN models show that it significantly outperformed alternative methods, and in some difficult tasks it eliminated about 71% to 97% high-confidence (&gt;0.9) errors with only about 10% of the minimal amount of labeled operation data needed for practical learning techniques to barely work",,901–913,1,Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
287,@inbook: 10.1145/3368089.3417926,DeepCommenter: A Deep Code Comment Generation Tool with Hybrid Lexical and Syntactical Information,"Li, Boao and Yan, Meng and Xia, Xin and Hu, Xing and Li, Ge and Lo, David",2020,Association for Computing Machinery,,https://doi.org/10.1145/3368089.3417926,,"As the scale of software projects increases, the code comments are more and more important for program comprehension. Unfortunately, many code comments are missing, mismatched or outdated due to tight development schedule or other reasons. Automatic code comment generation is of great help for developers to comprehend source code and reduce their workload. Thus, we propose a code comment generation tool (DeepCommenter) to generate descriptive comments for Java methods. DeepCommenter formulates the comment generation task as a machine translation problem and exploits a deep neural network that combines the lexical and structural information of Java methods. We implement DeepCommenter in the form of an Integrated Development Environment (i.e., Intellij IDEA) plug-in. Such plug-in is built upon a Client/Server architecture. The client formats the code selected by the user, sends request to the server and inserts the comment generated by the server above the selected code. The server listens for client’s request, analyzes the requested code using the pre-trained model and sends back the generated comment to the client. The pre-trained model learns both the lexical and syntactical information from source code tokens and Abstract Syntax Trees (AST) respectively and combines these two types of information together to generate comments. To evaluate DeepCommenter, we conduct experiments on a large corpus built from a large number of open source Java projects on GitHub. The experimental results on different metrics show that DeepCommenter outperforms the state-of-the-art approaches by a substantial margin.",,1571–1575,,Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
288,@inproceedings: 10.1145/3368089.3417067,Clustering Test Steps in Natural Language toward Automating Test Automation,"Li, Linyi and Li, Zhenwen and Zhang, Weijie and Zhou, Jun and Wang, Pengcheng and Wu, Jing and He, Guanghua and Zeng, Xia and Deng, Yuetang and Xie, Tao",2020,Association for Computing Machinery,,https://doi.org/10.1145/3368089.3417067,10.1145/3368089.3417067,"For large industrial applications, system test cases are still often described in natural language (NL), and their number can reach thousands. Test automation is to automatically execute the test cases. Achieving test automation typically requires substantial manual effort for creating executable test scripts from these NL test cases. In particular, given that each NL test case consists of a sequence of NL test steps, testers first implement a test API method for each test step and then write a test script for invoking these test API methods sequentially for test automation. Across different test cases, multiple test steps can share semantic similarities, supposedly mapped to the same API method. However, due to numerous test steps in various NL forms under manual inspection, testers may not realize those semantically similar test steps and thus waste effort to implement duplicate test API methods for them. To address this issue, in this paper, we propose a new approach based on natural language processing to cluster similar NL test steps together such that the test steps in each cluster can be mapped to the same test API method. Our approach includes domain-specific word embedding training along with measurement based on Relaxed Word Mover’sDistance to analyze the similarity of test steps. Our approach also includes a technique to combine hierarchical agglomerative clustering and K-means clustering post-refinement to derive high-quality and manually-adjustable clustering results. The evaluation results of our approach on a large industrial mobile app, WeChat, show that our approach can cluster the test steps with high accuracy, substantially reducing the number of clusters and thus reducing the downstream manual effort. In particular, compared with the baseline approach, our approach achieves 79.8% improvement on cluster quality, reducing 65.9% number of clusters, i.e., the number of test API methods to be implemented.","natural language processing, Clustering, software testing",1285–1295,11,Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
289,@inbook: 10.1145/3379597.3387459,Beyond the Code: Mining Self-Admitted Technical Debt in Issue Tracker Systems,"Xavier, Laerte and Ferreira, Fabio and Brito, Rodrigo and Valente, Marco Tulio",2020,Association for Computing Machinery,,https://doi.org/10.1145/3379597.3387459,,"Self-admitted technical debt (SATD) is a particular case of Technical Debt (TD) where developers explicitly acknowledge their sub-optimal implementation decisions. Previous studies mine SATD by searching for specific TD-related terms in source code comments. By contrast, in this paper we argue that developers can admit technical debt by other means, e.g., by creating issues in tracking systems and labelling them as referring to TD. We refer to this type of SATD as issue-based SATD or just SATD-I. We study a sample of 286 SATD-I instances collected from five open source projects, including Microsoft Visual Studio and GitLab Community Edition. We show that only 29% of the studied SATD-I instances can be tracked to source code comments. We also show that SATD-I issues take more time to be closed, compared to other issues, although they are not more complex in terms of code churn. Besides, in 45% of the studied issues TD was introduced to ship earlier, and in almost 60% it refers to DESIGN flaws. Finally, we report that most developers pay SATD-I to reduce its costs or interests (66%). Our findings suggest that there is space for designing novel tools to support technical debt management, particularly tools that encourage developers to create and label issues containing TD concerns.",,137–146,1,Proceedings of the 17th International Conference on Mining Software Repositories
290,@inbook: 10.1145/3379597.3387448,AIMMX: Artificial Intelligence Model Metadata Extractor,"Tsay, Jason and Braz, Alan and Hirzel, Martin and Shinnar, Avraham and Mummert, Todd",2020,Association for Computing Machinery,,https://doi.org/10.1145/3379597.3387448,,"Despite all of the power that machine learning and artificial intelligence (AI) models bring to applications, much of AI development is currently a fairly ad hoc process. Software engineering and AI development share many of the same languages and tools, but AI development as an engineering practice is still in early stages. Mining software repositories of AI models enables insight into the current state of AI development. However, much of the relevant metadata around models are not easily extractable directly from repositories and require deduction or domain knowledge. This paper presents a library called AIMMX that enables simplified AI Model Metadata eXtraction from software repositories. The extractors have five modules for extracting AI model-specific metadata: model name, associated datasets, references, AI frameworks used, and model domain. We evaluated AIMMX against 7,998 open-source models from three sources: model zoos, arXiv AI papers, and state-of-the-art AI papers. Our platform extracted metadata with 87% precision and 83% recall. As preliminary examples of how AI model metadata extraction enables studies and tools to advance engineering support for AI development, this paper presents an exploratory analysis for data and method reproducibility over the models in the evaluation dataset and a catalog tool for discovering and managing models. Our analysis suggests that while data reproducibility may be relatively poor with 42% of models in our sample citing their datasets, method reproducibility is more common at 72% of models in our sample, particularly state-of-the-art models. Our collected models are searchable in a catalog that uses existing metadata to enable advanced discovery features for efficiently finding models.",,81–92,1,Proceedings of the 17th International Conference on Mining Software Repositories
291,@inbook: 10.1145/3379597.3387449,Improved Automatic Summarization of Subroutines via Attention to File Context,"Haque, Sakib and LeClair, Alexander and Wu, Lingfei and McMillan, Collin",2020,Association for Computing Machinery,,https://doi.org/10.1145/3379597.3387449,,"Software documentation largely consists of short, natural language summaries of the subroutines in the software. These summaries help programmers quickly understand what a subroutine does without having to read the source code him or herself. The task of writing these descriptions is called ""source code summarization"" and has been a target of research for several years. Recently, AI-based approaches have superseded older, heuristic-based approaches. Yet, to date these AI-based approaches assume that all the content needed to predict summaries is inside subroutine itself. This assumption limits performance because many subroutines cannot be understood without surrounding context. In this paper, we present an approach that models the file context of subroutines (i.e. other subroutines in the same file) and uses an attention mechanism to find words and concepts to use in summaries. We show in an experiment that our approach extends and improves several recent baselines.",,300–310,1,Proceedings of the 17th International Conference on Mining Software Repositories
292,@inproceedings: 10.1145/3377811.3380389,Software Visualization and Deep Transfer Learning for Effective Software Defect Prediction,"Chen, Jinyin and Hu, Keke and Yu, Yue and Chen, Zhuangzhi and Xuan, Qi and Liu, Yi and Filkov, Vladimir",2020,Association for Computing Machinery,,https://doi.org/10.1145/3377811.3380389,10.1145/3377811.3380389,"Software defect prediction aims to automatically locate defective code modules to better focus testing resources and human effort. Typically, software defect prediction pipelines are comprised of two parts: the first extracts program features, like abstract syntax trees, by using external tools, and the second applies machine learning-based classification models to those features in order to predict defective modules. Since such approaches depend on specific feature extraction tools, machine learning classifiers have to be custom-tailored to effectively build most accurate models.To bridge the gap between deep learning and defect prediction, we propose an end-to-end framework which can directly get prediction results for programs without utilizing feature-extraction tools. To that end, we first visualize programs as images, apply the self-attention mechanism to extract image features, use transfer learning to reduce the difference in sample distributions between projects, and finally feed the image files into a pre-trained, deep learning model for defect prediction. Experiments with 10 open source projects from the PROMISE dataset show that our method can improve cross-project and within-project defect prediction. Our code and data pointers are available at https://zenodo.org/record/3373409#.XV0Oy5Mza35.","self-attention, within-project defect prediction, deep transfer learning, software visualization, cross-project defect prediction",578–589,12,Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering
293,@inbook: 10.1145/3387940.3391496,Sensemaking Practices in the Everyday Work of AI/ML Software Engineering,"Wolf, Christine T. and Paine, Drew",2020,Association for Computing Machinery,,https://doi.org/10.1145/3387940.3391496,,"This paper considers sensemaking as it relates to everyday software engineering (SE) work practices and draws on a multi-year ethnographic study of SE projects at a large, global technology company building digital services infused with artificial intelligence (AI) and machine learning (ML) capabilities. Our findings highlight the breadth of sensemaking practices in AI/ML projects, noting developers' efforts to make sense of AI/ML environments (e.g., algorithms/methods and libraries), of AI/ML model ecosystems (e.g., pre-trained models and ""upstream"" models), and of business-AI relations (e.g., how the AI/ML service relates to the domain context and business problem at hand). This paper builds on recent scholarship drawing attention to the integral role of sensemaking in everyday SE practices by empirically investigating how and in what ways AI/ML projects present software teams with emergent sensemaking requirements and opportunities.",,86–92,,Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops
294,@inbook: 10.1145/3377812.3381399,Variability Aware Requirements Reuse Analysis,"Abbas, Muhammad",2020,Association for Computing Machinery,,https://doi.org/10.1145/3377812.3381399,,"Problem: The goal of a software product line is to aid quick and quality delivery of software products, sharing common features. Effectively achieving the above-mentioned goals requires reuse analysis of the product line features. Existing requirements reuse analysis approaches are not focused on recommending product line features, that can be reused to realize new customer requirements. Hypothesis: Given that the customer requirements are linked to product line features' description satisfying them: then the customer requirements can be clustered based on patterns and similarities, preserving the historic reuse information. New customer requirements can be evaluated against existing customer requirements and reuse of product line features can be recommended. Contributions: We treated the problem of feature reuse analysis as a text classification problem at the requirements-level. We use Natural Language Processing and clustering to recommend reuse of features based on similarities and historic reuse information. The recommendations can be used to realize new customer requirements.",,190–193,,Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Companion Proceedings
295,@inproceedings: 10.1145/3387940.3391464,OffSide: Learning to Identify Mistakes in Boundary Conditions,"Briem, J\'{o}n Arnar and Smit, Jordi and Sellik, Hendrig and Rapoport, Pavel and Gousios, Georgios and Aniche, Maur\'{\i}cio",2020,Association for Computing Machinery,,https://doi.org/10.1145/3387940.3391464,10.1145/3387940.3391464,"Mistakes in boundary conditions are the cause of many bugs in software. These mistakes happen when, e.g., developers make use of '&lt;' or '&gt;' in cases where they should have used '&lt;=' or '&gt;='. Mistakes in boundary conditions are often hard to find and manually detecting them might be very time-consuming for developers. While researchers have been proposing techniques to cope with mistakes in the boundaries for a long time, the automated detection of such bugs still remains a challenge. We conjecture that, for a tool to be able to precisely identify mistakes in boundary conditions, it should be able to capture the overall context of the source code under analysis. In this work, we propose a deep learning model that learn mistakes in boundary conditions and, later, is able to identify them in unseen code snippets. We train and test a model on over 1.5 million code snippets, with and without mistakes in different boundary conditions. Our model shows an accuracy from 55% up to 87%. The model is also able to detect 24 out of 41 real-world bugs; however, with a high false positive rate. The existing state-of-the-practice linter tools are not able to detect any of the bugs. We hope this paper can pave the road towards deep learning models that will be able to support developers in detecting mistakes in boundary conditions.","software testing, boundary testing, deep learning for software testing, software engineering, machine learning for software engineering, machine learning for software testing",203–208,6,Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops
296,@inbook: 10.1145/3387940.3391470,Insights on Training Neural Networks for QUBO Tasks,"Gabor, Thomas and Feld, Sebastian and Safi, Hila and Phan, Thomy and Linnhoff-Popien, Claudia",2020,Association for Computing Machinery,,https://doi.org/10.1145/3387940.3391470,,"Current hardware limitations restrict the potential when solving quadratic unconstrained binary optimization (QUBO) problems via the quantum approximate optimization algorithm (QAOA) or quantum annealing (QA). Thus, we consider training neural networks in this context. We first discuss QUBO problems that originate from translated instances of the traveling salesman problem (TSP): Analyzing this representation via autoencoders shows that there is way more information included than necessary to solve the original TSP. Then we show that neural networks can be used to solve TSP instances from both QUBO input and autoencoders' hidden state representation. We finally generalize the approach and successfully train neural networks to solve arbitrary QUBO problems, sketching means to use neuromorphic hardware as a simulator or an additional co-processor for quantum computing.",,436–441,,Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops
297,@inproceedings: 10.1145/3377811.3380400,Testing DNN Image Classifiers for Confusion &amp; Bias Errors,"Tian, Yuchi and Zhong, Ziyuan and Ordonez, Vicente and Kaiser, Gail and Ray, Baishakhi",2020,Association for Computing Machinery,,https://doi.org/10.1145/3377811.3380400,10.1145/3377811.3380400,"Image classifiers are an important component of today's software, from consumer and business applications to safety-critical domains. The advent of Deep Neural Networks (DNNs) is the key catalyst behind such wide-spread success. However, wide adoption comes with serious concerns about the robustness of software systems dependent on DNNs for image classification, as several severe erroneous behaviors have been reported under sensitive and critical circumstances. We argue that developers need to rigorously test their software's image classifiers and delay deployment until acceptable. We present an approach to testing image classifier robustness based on class property violations.We found that many of the reported erroneous cases in popular DNN image classifiers occur because the trained models confuse one class with another or show biases towards some classes over others. These bugs usually violate some class properties of one or more of those classes. Most DNN testing techniques focus on perimage violations, so fail to detect class-level confusions or biases.We developed a testing technique to automatically detect class-based confusion and bias errors in DNN-driven image classification software. We evaluated our implementation, DeepInspect, on several popular image classifiers with precision up to 100% (avg. 72.6%) for confusion errors, and up to 84.3% (avg. 66.8%) for bias errors. DeepInspect found hundreds of classification mistakes in widely-used models, many exposing errors indicating confusion or bias.","whitebox testing, image classifiers, DNNs, bias, deep learning",1122–1134,13,Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering
298,@inbook: 10.1145/3387940.3392191,Recommendation of Move Method Refactoring Using Path-Based Representation of Code,"Kurbatova, Zarina and Veselov, Ivan and Golubev, Yaroslav and Bryksin, Timofey",2020,Association for Computing Machinery,,https://doi.org/10.1145/3387940.3392191,,"Software refactoring plays an important role in increasing code quality. One of the most popular refactoring types is the Move Method refactoring. It is usually applied when a method depends more on members of other classes than on its own original class. Several approaches have been proposed to recommend Move Method refactoring automatically. Most of them are based on heuristics and have certain limitations (e.g., they depend on the selection of metrics and manually-defined thresholds). In this paper, we propose an approach to recommend Move Method refactoring based on a path-based representation of code called code2vec that is able to capture the syntactic structure and semantic information of a code fragment. We use this code representation to train a machine learning classifier suggesting to move methods to more appropriate classes. We evaluate the approach on two publicly available datasets: a manually compiled dataset of well-known open-source projects and a synthetic dataset with automatically injected code smell instances. The results show that our approach is capable of recommending accurate refactoring opportunities and outperforms JDeodorant and JMove, which are state of the art tools in this field.",,315–322,,Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops
299,@inbook: 10.1145/3387940.3391463,Deep Learning for Software Defect Prediction: A Survey,"Omri, Safa and Sinz, Carsten",2020,Association for Computing Machinery,,https://doi.org/10.1145/3387940.3391463,,"Software fault prediction is an important and beneficial practice for improving software quality and reliability. The ability to predict which components in a large software system are most likely to contain the largest numbers of faults in the next release helps to better manage projects, including early estimation of possible release delays, and affordably guide corrective actions to improve the quality of the software. However, developing robust fault prediction models is a challenging task and many techniques have been proposed in the literature. Traditional software fault prediction studies mainly focus on manually designing features (e.g. complexity metrics), which are input into machine learning classifiers to identify defective code. However, these features often fail to capture the semantic and structural information of programs. Such information is needed for building accurate fault prediction models. In this survey, we discuss various approaches in fault prediction, also explaining how in recent studies deep learning algorithms for fault prediction help to bridge the gap between programs' semantics and fault prediction features and make accurate predictions.",,209–214,,Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops
300,@inproceedings: 10.1145/3377811.3380356,Detection of Hidden Feature Requests from Massive Chat Messages via Deep Siamese Network,"Shi, Lin and Xing, Mingzhe and Li, Mingyang and Wang, Yawen and Li, Shoubin and Wang, Qing",2020,Association for Computing Machinery,,https://doi.org/10.1145/3377811.3380356,10.1145/3377811.3380356,"Online chatting is gaining popularity and plays an increasingly significant role in software development. When discussing functionalities, developers might reveal their desired features to other developers. Automated mining techniques towards retrieving feature requests from massive chat messages can benefit the requirements gathering process. But it is quite challenging to perform such techniques because detecting feature requests from dialogues requires a thorough understanding of the contextual information, and it is also extremely expensive on annotating feature-request dialogues for learning. To bridge that gap, we recast the traditional text classification task of mapping single dialog to its class into the task of determining whether two dialogues are similar or not by incorporating few-shot learning. We propose a novel approach, named FRMiner, which can detect feature-request dialogues from chat messages via deep Siamese network. We design a BiLSTM-based dialog model that can learn the contextual information of a dialog in both forward and reverse directions. Evaluation on the real-world projects shows that our approach achieves average precision, recall and F1-score of 88.52%, 88.50% and 88.51%, which confirms that our approach could effectively detect hidden feature requests from chat messages, thus can facilitate gathering comprehensive requirements from the crowd in an automated way.","deep learning, siamese network, feature requests, requirements engineering",641–653,13,Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering
301,@inproceedings: 10.1145/3377811.3380342,Big Code != Big Vocabulary: Open-Vocabulary Models for Source Code,"Karampatsis, Rafael-Michael and Babii, Hlib and Robbes, Romain and Sutton, Charles and Janes, Andrea",2020,Association for Computing Machinery,,https://doi.org/10.1145/3377811.3380342,10.1145/3377811.3380342,"Statistical language modeling techniques have successfully been applied to large source code corpora, yielding a variety of new software development tools, such as tools for code suggestion, improving readability, and API migration. A major issue with these techniques is that code introduces new vocabulary at a far higher rate than natural language, as new identifier names proliferate. Both large vocabularies and out-of-vocabulary issues severely affect Neural Language Models (NLMs) of source code, degrading their performance and rendering them unable to scale.In this paper, we address this issue by: 1) studying how various modelling choices impact the resulting vocabulary on a large-scale corpus of 13,362 projects; 2) presenting an open vocabulary source code NLM that can scale to such a corpus, 100 times larger than in previous work; and 3) showing that such models outperform the state of the art on three distinct code corpora (Java, C, Python). To our knowledge, these are the largest NLMs for code that have been reported.All datasets, code, and trained models used in this work are publicly available.","neural language models, naturalness of code, byte-pair encoding",1073–1085,13,Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering
302,@inproceedings: 10.1145/3377811.3380362,An Empirical Study on Program Failures of Deep Learning Jobs,"Zhang, Ru and Xiao, Wencong and Zhang, Hongyu and Liu, Yu and Lin, Haoxiang and Yang, Mao",2020,Association for Computing Machinery,,https://doi.org/10.1145/3377811.3380362,10.1145/3377811.3380362,"Deep learning has made significant achievements in many application areas. To train and test models more efficiently, enterprise developers submit and run their deep learning programs on a shared, multi-tenant platform. However, some of the programs fail after a long execution time due to code/script defects, which reduces the development productivity and wastes expensive resources such as GPU, storage, and network I/O.This paper presents the first comprehensive empirical study on program failures of deep learning jobs. 4960 real failures are collected from a deep learning platform in Microsoft. We manually examine their failure messages and classify them into 20 categories. In addition, we identify the common root causes and bug-fix solutions on a sample of 400 failures. To better understand the current testing and debugging practices for deep learning, we also conduct developer interviews. Our major findings include: (1) 48.0% of the failures occur in the interaction with the platform rather than in the execution of code logic, mostly due to the discrepancies between local and platform execution environments; (2) Deep learning specific failures (13.5%) are mainly caused by inappropriate model parameters/structures and framework API misunderstanding; (3) Current debugging practices are not efficient for fault localization in many cases, and developers need more deep learning specific tools. Based on our findings, we further suggest possible research topics and tooling support that could facilitate future deep learning development.","empirical study, deep learning jobs, program failures",1159–1170,12,Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering
303,@inproceedings: 10.1145/3377811.3380378,Repairing Deep Neural Networks: Fix Patterns and Challenges,"Islam, Md Johirul and Pan, Rangeet and Nguyen, Giang and Rajan, Hridesh",2020,Association for Computing Machinery,,https://doi.org/10.1145/3377811.3380378,10.1145/3377811.3380378,"Significant interest in applying Deep Neural Network (DNN) has fueled the need to support engineering of software that uses DNNs. Repairing software that uses DNNs is one such unmistakable SE need where automated tools could be beneficial; however, we do not fully understand challenges to repairing and patterns that are utilized when manually repairing DNNs. What challenges should automated repair tools address? What are the repair patterns whose automation could help developers? Which repair patterns should be assigned a higher priority for building automated bug repair tools? This work presents a comprehensive study of bug fix patterns to address these questions. We have studied 415 repairs from Stack Overflow and 555 repairs from GitHub for five popular deep learning libraries Caffe, Keras, Tensorflow, Theano, and Torch to understand challenges in repairs and bug repair patterns. Our key findings reveal that DNN bug fix patterns are distinctive compared to traditional bug fix patterns; the most common bug fix patterns are fixing data dimension and neural network connectivity; DNN bug fixes have the potential to introduce adversarial vulnerabilities; DNN bug fixes frequently introduce new bugs; and DNN bug localization, reuse of trained model, and coping with frequent releases are major challenges faced by developers when fixing bugs. We also contribute a benchmark of 667 DNN (bug, repair) instances.","bug fix patterns, bugs, bug fix, deep neural networks",1135–1146,12,Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering
304,@inbook: 10.1145/3387940.3391465,Evaluating Surprise Adequacy for Question Answering,"Kim, Seah and Yoo, Shin",2020,Association for Computing Machinery,,https://doi.org/10.1145/3387940.3391465,,"With the wide and rapid adoption of Deep Neural Networks (DNNs) in various domains, an urgent need to validate their behaviour has risen, resulting in various test adequacy metrics for DNNs. One of the metrics, Surprise Adequacy (SA), aims to measure how surprising a new input is based on the similarity to the data used for training. While SA has been evaluated to be effective for image classifiers based on Convolutional Neural Networks (CNNs), it has not been studied for the Natural Language Processing (NLP) domain. This paper applies SA to NLP, in particular to the question answering task: the aim is to investigate whether SA correlates well with the correctness of answers. An empirical evaluation using the widely used Stanford Question Answering Dataset (SQuAD) shows that SA can work well as a test adequacy metric for the question answering task.",,197–202,,Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops
305,@inproceedings: 10.1145/3377811.3380360,Understanding the Automated Parameter Optimization on Transfer Learning for Cross-Project Defect Prediction: An Empirical Study,"Li, Ke and Xiang, Zilin and Chen, Tao and Wang, Shuo and Tan, Kay Chen",2020,Association for Computing Machinery,,https://doi.org/10.1145/3377811.3380360,10.1145/3377811.3380360,"Data-driven defect prediction has become increasingly important in software engineering process. Since it is not uncommon that data from a software project is insufficient for training a reliable defect prediction model, transfer learning that borrows data/konwledge from other projects to facilitate the model building at the current project, namely cross-project defect prediction (CPDP), is naturally plausible. Most CPDP techniques involve two major steps, i.e., transfer learning and classification, each of which has at least one parameter to be tuned to achieve their optimal performance. This practice fits well with the purpose of automated parameter optimization. However, there is a lack of thorough understanding about what are the impacts of automated parameter optimization on various CPDP techniques. In this paper, we present the first empirical study that looks into such impacts on 62 CPDP techniques, 13 of which are chosen from the existing CPDP literature while the other 49 ones have not been explored before. We build defect prediction models over 20 real-world software projects that are of different scales and characteristics. Our findings demonstrate that: (1) Automated parameter optimization substantially improves the defect prediction performance of 77% CPDP techniques with a manageable computational cost. Thus more efforts on this aspect are required in future CPDP studies. (2) Transfer learning is of ultimate importance in CPDP. Given a tight computational budget, it is more cost-effective to focus on optimizing the parameter configuration of transfer learning algorithms (3) The research on CPDP is far from mature where it is 'not difficult' to find a better alternative by making a combination of existing transfer learning and classification techniques. This finding provides important insights about the future design of CPDP techniques.","cross-project defect prediction, classification techniques, transfer learning, automated parameter optimization",566–577,12,Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering
306,@inproceedings: 10.1145/3377811.3380423,TRADER: Trace Divergence Analysis and Embedding Regulation for Debugging Recurrent Neural Networks,"Tao, Guanhong and Ma, Shiqing and Liu, Yingqi and Xu, Qiuling and Zhang, Xiangyu",2020,Association for Computing Machinery,,https://doi.org/10.1145/3377811.3380423,10.1145/3377811.3380423,"Recurrent Neural Networks (RNN) can deal with (textual) input with various length and hence have a lot of applications in software systems and software engineering applications. RNNs depend on word embeddings that are usually pre-trained by third parties to encode textual inputs to numerical values. It is well known that problematic word embeddings can lead to low model accuracy. In this paper, we propose a new technique to automatically diagnose how problematic embeddings impact model performance, by comparing model execution traces from correctly and incorrectly executed samples. We then leverage the diagnosis results as guidance to harden/repair the embeddings. Our experiments show that TRADER can consistently and effectively improve accuracy for real world models and datasets by 5.37% on average, which represents substantial improvement in the literature of RNN models.",,986–998,13,Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering
307,@inbook: 10.1145/3387940.3391487,Dialogue Act Classification for Virtual Agents for Software Engineers during Debugging,"Wood, Andrew and Eberhart, Zachary and McMillan, Collin",2020,Association for Computing Machinery,,https://doi.org/10.1145/3387940.3391487,,"A ""dialogue act"" is a written or spoken action during a conversation. Dialogue acts are usually only a few words long, and are often categorized by researchers into a relatively small set of dialogue act types, such as eliciting information, expressing an opinion, or making a greeting. Research interest into automatic classification of dialogue acts has grown recently due to the proliferation of Virtual Agents (VA) e.g. Siri, Cortana, Alexa. But unfortunately, the gains made into VA development in one domain are generally not applicable to other domains, since the composition of dialogue acts differs in different conversations. In this paper, we target the problem of dialogue act classification for a VA for software engineers repairing bugs. A problem in the SE domain is that very little sample data exists - the only public dataset is a recently-released Wizard of Oz study with 30 conversations. Therefore, we present a transfer-learning technique to learn on a much larger dataset for general business conversations, and apply the knowledge to the SE dataset. In an experiment, we observe between 8% and 20% improvement over two key baselines.",,462–469,,Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops
308,@inproceedings: 10.1145/3377811.3380368,Towards Characterizing Adversarial Defects of Deep Learning Software from the Lens of Uncertainty,"Zhang, Xiyue and Xie, Xiaofei and Ma, Lei and Du, Xiaoning and Hu, Qiang and Liu, Yang and Zhao, Jianjun and Sun, Meng",2020,Association for Computing Machinery,,https://doi.org/10.1145/3377811.3380368,10.1145/3377811.3380368,"Over the past decade, deep learning (DL) has been successfully applied to many industrial domain-specific tasks. However, the current state-of-the-art DL software still suffers from quality issues, which raises great concern especially in the context of safety- and security-critical scenarios. Adversarial examples (AEs) represent a typical and important type of defects needed to be urgently addressed, on which a DL software makes incorrect decisions. Such defects occur through either intentional attack or physical-world noise perceived by input sensors, potentially hindering further industry deployment. The intrinsic uncertainty nature of deep learning decisions can be a fundamental reason for its incorrect behavior. Although some testing, adversarial attack and defense techniques have been recently proposed, it still lacks a systematic study to uncover the relationship between AEs and DL uncertainty.In this paper, we conduct a large-scale study towards bridging this gap. We first investigate the capability of multiple uncertainty metrics in differentiating benign examples (BEs) and AEs, which enables to characterize the uncertainty patterns of input data. Then, we identify and categorize the uncertainty patterns of BEs and AEs, and find that while BEs and AEs generated by existing methods do follow common uncertainty patterns, some other uncertainty patterns are largely missed. Based on this, we propose an automated testing technique to generate multiple types of uncommon AEs and BEs that are largely missed by existing techniques. Our further evaluation reveals that the uncommon data generated by our method is hard to be defended by the existing defense techniques with the average defense success rate reduced by 35%. Our results call for attention and necessity to generate more diverse data for evaluating quality assurance solutions of DL software.","uncertainty, deep learning, software testing, adversarial attack",739–751,13,Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering
309,@inbook: 10.1145/3387940.3391490,Oracle Issues in Machine Learning and Where to Find Them,"Liem, Cynthia C. S. and Panichella, Annibale",2020,Association for Computing Machinery,,https://doi.org/10.1145/3387940.3391490,,"The rise in popularity of machine learning (ML), and deep learning in particular, has both led to optimism about achievements of artificial intelligence, as well as concerns about possible weaknesses and vulnerabilities of ML pipelines. Within the software engineering community, this has led to a considerable body of work on ML testing techniques, including white- and black-box testing for ML models. This means the oracle problem needs to be addressed. For supervised ML applications, oracle information is indeed available in the form of dataset 'ground truth', that encodes input data with corresponding desired output labels. However, while ground truth forms a gold standard, there still is no guarantee it is truly correct. Indeed, syntactic, semantic, and conceptual framing issues in the oracle may negatively affect the ML system's integrity. While syntactic issues may automatically be verified and corrected, the higher-level issues traditionally need human judgment and manual analysis. In this paper, we employ two heuristics based on information entropy and semantic analysis on well-known computer vision models and benchmark data from ImageNet. The heuristics are used to semi-automatically uncover potential higher-level issues in (i) the label taxonomy used to define the ground truth oracle (labels), and (ii) data encoding and representation. In doing this, beyond existing ML testing efforts, we illustrate the need for software engineering strategies that especially target and assess the oracle.",,483–488,,Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops
310,@inproceedings: 10.1145/3377811.3380422,DeepBillboard: Systematic Physical-World Testing of Autonomous Driving Systems,"Zhou, Husheng and Li, Wei and Kong, Zelun and Guo, Junfeng and Zhang, Yuqun and Yu, Bei and Zhang, Lingming and Liu, Cong",2020,Association for Computing Machinery,,https://doi.org/10.1145/3377811.3380422,10.1145/3377811.3380422,"Deep Neural Networks (DNNs) have been widely applied in autonomous systems such as self-driving vehicles. Recently, DNN testing has been intensively studied to automatically generate adversarial examples, which inject small-magnitude perturbations into inputs to test DNNs under extreme situations. While existing testing techniques prove to be effective, particularly for autonomous driving, they mostly focus on generating digital adversarial perturbations, e.g., changing image pixels, which may never happen in the physical world. Thus, there is a critical missing piece in the literature on autonomous driving testing: understanding and exploiting both digital and physical adversarial perturbation generation for impacting steering decisions. In this paper, we propose a systematic physical-world testing approach, namely DeepBillboard, targeting at a quite common and practical driving scenario: drive-by billboards. DeepBillboard is capable of generating a robust and resilient printable adversarial billboard test, which works under dynamic changing driving conditions including viewing angle, distance, and lighting. The objective is to maximize the possibility, degree, and duration of the steering-angle errors of an autonomous vehicle driving by our generated adversarial billboard. We have extensively evaluated the efficacy and robustness of DeepBillboard by conducting both experiments with digital perturbations and physical-world case studies. The digital experimental results show that DeepBillboard is effective for various steering models and scenes. Furthermore, the physical case studies demonstrate that DeepBillboard is sufficiently robust and resilient for generating physical-world adversarial billboard tests for real-world driving under various weather conditions, being able to mislead the average steering angle error up to 26.44 degrees. To the best of our knowledge, this is the first study demonstrating the possibility of generating realistic and continuous physical-world tests for practical autonomous driving systems; moreover, DeepBillboard can be directly generalized to a variety of other physical entities/surfaces along the curbside, e.g., a graffiti painted on a wall.",,347–358,12,Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering
311,@inproceedings: 10.1145/3387940.3391489,Improving Code Recommendations by Combining Neural and Classical Machine Learning Approaches,"Schumacher, Max Eric Henry and Le, Kim Tuyen and Andrzejak, Artur",2020,Association for Computing Machinery,,https://doi.org/10.1145/3387940.3391489,10.1145/3387940.3391489,"Code recommendation systems for software engineering are designed to accelerate the development of large software projects. A classical example is code completion or next token prediction offered by modern integrated development environments. A particular challenging case for such systems are dynamic languages like Python due to limited type information at editing time. Recently, researchers proposed machine learning approaches to address this challenge. In particular, the Probabilistic Higher Order Grammar technique (Bielik et al., ICML 2016) uses a grammar-based approach with a classical machine learning schema to exploit local context. A method by Li et al., (IJCAI 2018) uses deep learning methods, in detail a Recurrent Neural Network coupled with a Pointer Network. We compare these two approaches quantitatively on a large corpus of Python files from GitHub. We also propose a combination of both approaches, where a neural network decides which schema to use for each prediction. The proposed method achieves a slightly better accuracy than either of the systems alone. This demonstrates the potential of ensemble-like methods for code completion and recommendation tasks in dynamically typed languages.","neural networks, machine learning, code recommendations",476–482,7,Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops
312,@inproceedings: 10.1145/3377811.3380328,Translating Video Recordings of Mobile App Usages into Replayable Scenarios,"Bernal-C\'{a}rdenas, Carlos and Cooper, Nathan and Moran, Kevin and Chaparro, Oscar and Marcus, Andrian and Poshyvanyk, Denys",2020,Association for Computing Machinery,,https://doi.org/10.1145/3377811.3380328,10.1145/3377811.3380328,"Screen recordings of mobile applications are easy to obtain and capture a wealth of information pertinent to software developers (e.g., bugs or feature requests), making them a popular mechanism for crowdsourced app feedback. Thus, these videos are becoming a common artifact that developers must manage. In light of unique mobile development constraints, including swift release cycles and rapidly evolving platforms, automated techniques for analyzing all types of rich software artifacts provide benefit to mobile developers. Unfortunately, automatically analyzing screen recordings presents serious challenges, due to their graphical nature, compared to other types of (textual) artifacts. To address these challenges, this paper introduces V2S, a lightweight, automated approach for translating video recordings of Android app usages into replayable scenarios. V2S is based primarily on computer vision techniques and adapts recent solutions for object detection and image classification to detect and classify user actions captured in a video, and convert these into a replayable test scenario. We performed an extensive evaluation of V2S involving 175 videos depicting 3,534 GUI-based actions collected from users exercising features and reproducing bugs from over 80 popular Android apps. Our results illustrate that V2S can accurately replay scenarios from screen recordings, and is capable of reproducing ≈89% of our collected videos with minimal overhead. A case study with three industrial partners illustrates the potential usefulness of V2S from the viewpoint of developers.","screen recordings, bug reporting, object detection",309–321,13,Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering
313,@inproceedings: 10.1145/3377813.3381355,Automated Bug Reproduction from User Reviews for Android Applications,"Li, Shuyue and Guo, Jiaqi and Fan, Ming and Lou, Jian-Guang and Zheng, Qinghua and Liu, Ting",2020,Association for Computing Machinery,,https://doi.org/10.1145/3377813.3381355,10.1145/3377813.3381355,"Bug-related user reviews of mobile applications have negative influence on their reputation and competence, and thus these reviews are highly regarded by developers. Before bug fixing, developers need to manually reproduce the bugs reported in user reviews, which is an extremely time-consuming and tedious task. Hence, it is highly expected to automate this process. However, it is challenging to do so since user reviews are hard to understand and poorly informative for bug reproduction (especially lack of reproduction steps). In this paper, we propose RepRev to automatically Reproduce Android application bugs from user Reviews. Specifically, RepRev leverages natural language processing techniques to extract valuable information for bug reproduction. Then, it ranks GUI components by semantic similarity with the user review and dynamically searches on apps with a novel one-step exploration technique. To evaluate RepRev, we construct a benchmark including 63 crash-related user reviews from Google Play, which have been reproduced successfully by three graduate students. On this benchmark, RepRev presents comparable performance with humans, which successfully reproduces 44 user reviews in our benchmark (about 70%) with 432.2 seconds average time. We make the implementation of our approach publicly available, along with the artifacts and experimental data we used [4].","Android applications, user review analysis, bug reproduction",51–60,10,Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Software Engineering in Practice
314,@inbook: 10.1145/3387940.3391501,MSABot: A Chatbot Framework for Assisting in the Development and Operation of Microservice-Based Systems,"Lin, Chun-Ting and Ma, Shang-Pin and Huang, Yu-Wen",2020,Association for Computing Machinery,,https://doi.org/10.1145/3387940.3391501,,"Microservice architecture (MSA) has become a popular architectural style. The main advantages of MSA include modularization and scalability. However, the development and maintenance of Microservice-based systems are more complex than traditional monolithic architecture. This research plans to develop a novel Chatbot system, referred to as MSABot (Microservice Architecture Bot), to assist in the development and operation of Microservice-based systems by using Chatbots. MSABot integrates a variety of tools to allow users to understand the current status of Microservice development and operation, and to push the information of system errors or risks to users. For the operators who take over the maintenance of Microservices, MSABot also allows them to quickly understand the overall service architecture and the operation status of each service. Besides, we invited multiple users who are familiar with the technology of Microservice or ChapOps to evaluate MSABot. The results of the survey show that more than 90% of the respondents believe that MSABot can adequately support the development and maintenance of Microservice-based systems.",,36–40,,Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops
315,@inproceedings: 10.1145/3377811.3380395,Taxonomy of Real Faults in Deep Learning Systems,"Humbatova, Nargiz and Jahangirova, Gunel and Bavota, Gabriele and Riccio, Vincenzo and Stocco, Andrea and Tonella, Paolo",2020,Association for Computing Machinery,,https://doi.org/10.1145/3377811.3380395,10.1145/3377811.3380395,"The growing application of deep neural networks in safety-critical domains makes the analysis of faults that occur in such systems of enormous importance. In this paper we introduce a large taxonomy of faults in deep learning (DL) systems. We have manually analysed 1059 artefacts gathered from GitHub commits and issues of projects that use the most popular DL frameworks (TensorFlow, Keras and PyTorch) and from related Stack Overflow posts. Structured interviews with 20 researchers and practitioners describing the problems they have encountered in their experience have enriched our taxonomy with a variety of additional faults that did not emerge from the other two sources. Our final taxonomy was validated with a survey involving an additional set of 21 developers, confirming that almost all fault categories (13/15) were experienced by at least 50% of the survey participants.","taxonomy, deep learning, real faults, software testing",1110–1121,12,Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering
316,@inproceedings: 10.1145/3377813.3381367,Assessing Practitioner Beliefs about Software Defect Prediction,"Shrikanth, N. C. and Menzies, Tim",2020,Association for Computing Machinery,,https://doi.org/10.1145/3377813.3381367,10.1145/3377813.3381367,"Just because software developers say they believe in ""X"", that does not necessarily mean that ""X"" is true. As shown here, there exist numerous beliefs listed in the recent Software Engineering literature which are only supported by small portions of the available data. Hence we ask what is the source of this disconnect between beliefs and evidence?.To answer this question we look for evidence for ten beliefs within 300,000+ changes seen in dozens of open-source projects. Some of those beliefs had strong support across all the projects; specifically, ""A commit that involves more added and removed lines is more bug-prone"" and ""Files with fewer lines contributed by their owners (who contribute most changes) are bug-prone"".Most of the widely-held beliefs studied are only sporadically supported in the data; i.e. large effects can appear in project data and then disappear in subsequent releases. Such sporadic support explains why developers believe things that were relevant to their prior work, but not necessarily their current work.Our conclusion will be that we need to change the nature of the debate with Software Engineering. Specifically, while it is important to report the effects that hold right now, it is also important to report on what effects change over time.","beliefs, practitioner, defects, empirical software engineering",182–190,9,Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Software Engineering in Practice
317,@inproceedings: 10.1145/3377811.3380391,Importance-Driven Deep Learning System Testing,"Gerasimou, Simos and Eniser, Hasan Ferit and Sen, Alper and Cakan, Alper",2020,Association for Computing Machinery,,https://doi.org/10.1145/3377811.3380391,10.1145/3377811.3380391,"Deep Learning (DL) systems are key enablers for engineering intelligent applications due to their ability to solve complex tasks such as image recognition and machine translation. Nevertheless, using DL systems in safety- and security-critical applications requires to provide testing evidence for their dependable operation. Recent research in this direction focuses on adapting testing criteria from traditional software engineering as a means of increasing confidence for their correct behaviour. However, they are inadequate in capturing the intrinsic properties exhibited by these systems. We bridge this gap by introducing DeepImportance, a systematic testing methodology accompanied by an Importance-Driven (IDC) test adequacy criterion for DL systems. Applying IDC enables to establish a layer-wise functional understanding of the importance of DL system components and use this information to assess the semantic diversity of a test set. Our empirical evaluation on several DL systems, across multiple DL datasets and with state-of-the-art adversarial generation techniques demonstrates the usefulness and effectiveness of DeepImportance and its ability to support the engineering of more robust DL systems.","deep learning systems, test adequacy, safety-critical systems",702–713,12,Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering
318,@inproceedings: 10.1145/3377811.3380924,Caspar: Extracting and Synthesizing User Stories of Problems from App Reviews,"Guo, Hui and Singh, Munindar P.",2020,Association for Computing Machinery,,https://doi.org/10.1145/3377811.3380924,10.1145/3377811.3380924,"A user's review of an app often describes the user's interactions with the app. These interactions, which we interpret as mini stories, are prominent in reviews with negative ratings. In general, a story in an app review would contain at least two types of events: user actions and associated app behaviors. Being able to identify such stories would enable an app's developer in better maintaining and improving the app's functionality and enhancing user experience.We present Caspar, a method for extracting and synthesizing user-reported mini stories regarding app problems from reviews. By extending and applying natural language processing techniques, Caspar extracts ordered events from app reviews, classifies them as user actions or app problems, and synthesizes action-problem pairs. Our evaluation shows that Caspar is effective in finding action-problem pairs from reviews. First, Caspar classifies the events with an accuracy of 82.0% on manually labeled data. Second, relative to human evaluators, Caspar extracts event pairs with 92.9% precision and 34.2% recall. In addition, we train an inference model on the extracted action-problem pairs that automatically predicts possible app problems for different use cases. Preliminary evaluation shows that our method yields promising results. Caspar illustrates the potential for a deeper understanding of app reviews and possibly other natural language artifacts arising in software engineering.",,628–640,13,Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering
319,@inproceedings: 10.1145/3377811.3380403,An Investigation of Cross-Project Learning in Online Just-in-Time Software Defect Prediction,"Tabassum, Sadia and Minku, Leandro L. and Feng, Danyi and Cabral, George G. and Song, Liyan",2020,Association for Computing Machinery,,https://doi.org/10.1145/3377811.3380403,10.1145/3377811.3380403,"Just-In-Time Software Defect Prediction (JIT-SDP) is concerned with predicting whether software changes are defect-inducing or clean based on machine learning classifiers. Building such classifiers requires a sufficient amount of training data that is not available at the beginning of a software project. Cross-Project (CP) JIT-SDP can overcome this issue by using data from other projects to build the classifier, achieving similar (not better) predictive performance to classifiers trained on Within-Project (WP) data. However, such approaches have never been investigated in realistic online learning scenarios, where WP software changes arrive continuously over time and can be used to update the classifiers. It is unknown to what extent CP data can be helpful in such situation. In particular, it is unknown whether CP data are only useful during the very initial phase of the project when there is little WP data, or whether they could be helpful for extended periods of time. This work thus provides the first investigation of when and to what extent CP data are useful for JIT-SDP in a realistic online learning scenario. For that, we develop three different CP JIT-SDP approaches that can operate in online mode and be updated with both incoming CP and WP training examples over time. We also collect 2048 commits from three software repositories being developed by a software company over the course of 9 to 10 months, and use 19,8468 commits from 10 active open source GitHub projects being developed over the course of 6 to 14 years. The study shows that training classifiers with incoming CP+WP data can lead to improvements in G-mean of up to 53.90% compared to classifiers using only WP data at the initial stage of the projects. For the open source projects, which have been running for longer periods of time, using CP data to supplement WP data also helped the classifiers to reduce or prevent large drops in predictive performance that may occur over time, leading to up to around 40% better G-Mean during such periods. Such use of CP data was shown to be beneficial even after a large number of WP data were received, leading to overall G-means up to 18.5% better than those of WP classifiers.","online learning, cross-project learning, verification latency, software defect prediction, concept drift, transfer learning, class imbalance",554–565,12,Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering
320,@inproceedings: 10.1145/3377811.3380339,Structure-Invariant Testing for Machine Translation,"He, Pinjia and Meister, Clara and Su, Zhendong",2020,Association for Computing Machinery,,https://doi.org/10.1145/3377811.3380339,10.1145/3377811.3380339,"In recent years, machine translation software has increasingly been integrated into our daily lives. People routinely use machine translation for various applications, such as describing symptoms to a foreign doctor and reading political news in a foreign language. However, the complexity and intractability of neural machine translation (NMT) models that power modern machine translation make the robustness of these systems difficult to even assess, much less guarantee. Machine translation systems can return inferior results that lead to misunderstanding, medical misdiagnoses, threats to personal safety, or political conflicts. Despite its apparent importance, validating the robustness of machine translation systems is very difficult and has, therefore, been much under-explored.To tackle this challenge, we introduce structure-invariant testing (SIT), a novel metamorphic testing approach for validating machine translation software. Our key insight is that the translation results of ""similar"" source sentences should typically exhibit similar sentence structures. Specifically, SIT (1) generates similar source sentences by substituting one word in a given sentence with semantically similar, syntactically equivalent words; (2) represents sentence structure by syntax parse trees (obtained via constituency or dependency parsing); (3) reports sentence pairs whose structures differ quantitatively by more than some threshold. To evaluate SIT, we use it to test Google Translate and Bing Microsoft Translator with 200 source sentences as input, which led to 64 and 70 buggy issues with 69.5% and 70% top-1 accuracy, respectively. The translation errors are diverse, including under-translation, over-translation, incorrect modification, word/phrase mistranslation, and unclear logic.","structural invariance, machine translation, metamorphic testing",961–973,13,Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering
321,@inproceedings: 10.1145/3377811.3380340,HeteroRefactor: Refactoring for Heterogeneous Computing with FPGA,"Lau, Jason and Sivaraman, Aishwarya and Zhang, Qian and Gulzar, Muhammad Ali and Cong, Jason and Kim, Miryung",2020,Association for Computing Machinery,,https://doi.org/10.1145/3377811.3380340,10.1145/3377811.3380340,"Heterogeneous computing with field-programmable gate-arrays (FPGAs) has demonstrated orders of magnitude improvement in computing efficiency for many applications. However, the use of such platforms so far is limited to a small subset of programmers with specialized hardware knowledge. High-level synthesis (HLS) tools made significant progress in raising the level of programming abstraction from hardware programming languages to C/C++, but they usually cannot compile and generate accelerators for kernel programs with pointers, memory management, and recursion, and require manual refactoring to make them HLS-compatible. Besides, experts also need to provide heavily handcrafted optimizations to improve resource efficiency, which affects the maximum operating frequency, parallelization, and power efficiency.We propose a new dynamic invariant analysis and automated refactoring technique, called HeteroRefactor. First, HeteroRefactor monitors FPGA-specific dynamic invariants---the required bitwidth of integer and floating-point variables, and the size of recursive data structures and stacks. Second, using this knowledge of dynamic invariants, it refactors the kernel to make traditionally HLS-incompatible programs synthesizable and to optimize the accelerator's resource usage and frequency further. Third, to guarantee correctness, it selectively offloads the computation from CPU to FPGA, only if an input falls within the dynamic invariant. On average, for a recursive program of size 175 LOC, an expert FPGA programmer would need to write 185 more LOC to implement an HLS compatible version, while HeteroRefactor automates such transformation. Our results on Xilinx FPGA show that HeteroRefactor minimizes BRAM by 83% and increases frequency by 42% for recursive programs; reduces BRAM by 41% through integer bitwidth reduction; and reduces DSP by 50% through floating-point precision tuning.","FPGA, dynamic analysis, automated refactoring, high-level synthesis, heterogeneous computing",493–505,13,Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering
322,@inproceedings: 10.1145/3377811.3380383,Retrieval-Based Neural Source Code Summarization,"Zhang, Jian and Wang, Xu and Zhang, Hongyu and Sun, Hailong and Liu, Xudong",2020,Association for Computing Machinery,,https://doi.org/10.1145/3377811.3380383,10.1145/3377811.3380383,"Source code summarization aims to automatically generate concise summaries of source code in natural language texts, in order to help developers better understand and maintain source code. Traditional work generates a source code summary by utilizing information retrieval techniques, which select terms from original source code or adapt summaries of similar code snippets. Recent studies adopt Neural Machine Translation techniques and generate summaries from code snippets using encoder-decoder neural networks. The neural-based approaches prefer the high-frequency words in the corpus and have trouble with the low-frequency ones. In this paper, we propose a retrieval-based neural source code summarization approach where we enhance the neural model with the most similar code snippets retrieved from the training set. Our approach can take advantages of both neural and retrieval-based techniques. Specifically, we first train an attentional encoder-decoder model based on the code snippets and the summaries in the training set; Second, given one input code snippet for testing, we retrieve its two most similar code snippets in the training set from the aspects of syntax and semantics, respectively; Third, we encode the input and two retrieved code snippets, and predict the summary by fusing them during decoding. We conduct extensive experiments to evaluate our approach and the experimental results show that our proposed approach can improve the state-of-the-art methods.","information retrieval, deep neural network, source code summarization",1385–1397,13,Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering
323,@inproceedings: 10.1145/3377811.3380327,Unblind Your Apps: Predicting Natural-Language Labels for Mobile GUI Components by Deep Learning,"Chen, Jieshan and Chen, Chunyang and Xing, Zhenchang and Xu, Xiwei and Zhu, Liming and Li, Guoqiang and Wang, Jinshui",2020,Association for Computing Machinery,,https://doi.org/10.1145/3377811.3380327,10.1145/3377811.3380327,"According to the World Health Organization(WHO), it is estimated that approximately 1.3 billion people live with some forms of vision impairment globally, of whom 36 million are blind. Due to their disability, engaging these minority into the society is a challenging problem. The recent rise of smart mobile phones provides a new solution by enabling blind users' convenient access to the information and service for understanding the world. Users with vision impairment can adopt the screen reader embedded in the mobile operating systems to read the content of each screen within the app, and use gestures to interact with the phone. However, the prerequisite of using screen readers is that developers have to add natural-language labels to the image-based components when they are developing the app. Unfortunately, more than 77% apps have issues of missing labels, according to our analysis of 10,408 Android apps. Most of these issues are caused by developers' lack of awareness and knowledge in considering the minority. And even if developers want to add the labels to UI components, they may not come up with concise and clear description as most of them are of no visual issues. To overcome these challenges, we develop a deep-learning based model, called LabelDroid, to automatically predict the labels of image-based buttons by learning from large-scale commercial apps in Google Play. The experimental results show that our model can make accurate predictions and the generated labels are of higher quality than that from real Android developers.","user interface, neural networks, content description, image-based buttons, accessibility",322–334,13,Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering
324,@inproceedings: 10.1145/3377811.3380353,Misbehaviour Prediction for Autonomous Driving Systems,"Stocco, Andrea and Weiss, Michael and Calzana, Marco and Tonella, Paolo",2020,Association for Computing Machinery,,https://doi.org/10.1145/3377811.3380353,10.1145/3377811.3380353,"Deep Neural Networks (DNNs) are the core component of modern autonomous driving systems. To date, it is still unrealistic that a DNN will generalize correctly to all driving conditions. Current testing techniques consist of offline solutions that identify adversarial or corner cases for improving the training phase.In this paper, we address the problem of estimating the confidence of DNNs in response to unexpected execution contexts with the purpose of predicting potential safety-critical misbehaviours and enabling online healing of DNN-based vehicles. Our approach SelfOracle is based on a novel concept of self-assessment oracle, which monitors the DNN confidence at runtime, to predict unsupported driving scenarios in advance. SelfOracle uses autoencoder-and time series-based anomaly detection to reconstruct the driving scenarios seen by the car, and to determine the confidence boundary between normal and unsupported conditions.In our empirical assessment, we evaluated the effectiveness of different variants of SelfOracle at predicting injected anomalous driving contexts, using DNN models and simulation environment from Udacity. Results show that, overall, SelfOracle can predict 77% misbehaviours, up to six seconds in advance, outperforming the online input validation approach of DeepRoad.","testing, anomaly detection, misbehaviour prediction, deep learning",359–371,13,Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering
325,@inproceedings: 10.1145/3377811.3380404,Interpreting Cloud Computer Vision Pain-Points: A Mining Study of Stack Overflow,"Cummaudo, Alex and Vasa, Rajesh and Barnett, Scott and Grundy, John and Abdelrazek, Mohamed",2020,Association for Computing Machinery,,https://doi.org/10.1145/3377811.3380404,10.1145/3377811.3380404,"Intelligent services are becoming increasingly more pervasive; application developers want to leverage the latest advances in areas such as computer vision to provide new services and products to users, and large technology firms enable this via RESTful APIs. While such APIs promise an easy-to-integrate on-demand machine intelligence, their current design, documentation and developer interface hides much of the underlying machine learning techniques that power them. Such APIs look and feel like conventional APIs but abstract away data-driven probabilistic behaviour---the implications of a developer treating these APIs in the same way as other, traditional cloud services, such as cloud storage, is of concern. The objective of this study is to determine the various pain-points developers face when implementing systems that rely on the most mature of these intelligent services, specifically those that provide computer vision. We use Stack Overflow to mine indications of the frustrations that developers appear to face when using computer vision services, classifying their questions against two recent classification taxonomies (documentation-related and general questions). We find that, unlike mature fields like mobile development, there is a contrast in the types of questions asked by developers. These indicate a shallow understanding of the underlying technology that empower such systems. We discuss several implications of these findings via the lens of learning taxonomies to suggest how the software engineering community can improve these services and comment on the nature by which developers use them.","computer vision, intelligent services, documentation, stack overflow, pain points, empirical study",1584–1596,13,Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering
326,@inproceedings: 10.1145/3377811.3380357,How Android Developers Handle Evolution-Induced API Compatibility Issues: A Large-Scale Study,"Xia, Hao and Zhang, Yuan and Zhou, Yingtian and Chen, Xiaoting and Wang, Yang and Zhang, Xiangyu and Cui, Shuaishuai and Hong, Geng and Zhang, Xiaohan and Yang, Min and Yang, Zhemin",2020,Association for Computing Machinery,,https://doi.org/10.1145/3377811.3380357,10.1145/3377811.3380357,"As Android platform evolves in a fast pace, API-related compatibility issues become a significant challenge for developers. To handle an incompatible API invocation, developers mainly have two choices: merely performing sufficient checks to avoid invoking incompatible APIs on platforms that do not support them, or gracefully providing replacement implementations on those incompatible platforms. As providing more consistent app behaviors, the latter one is more recommended and more challenging to adopt. However, it is still unknown how these issues are handled in the real world, do developers meet difficulties and what can we do to help them.In light of this, this paper performs the first large-scale study on the current practice of handling evolution-induced API compatibility issues in about 300,000 Android market apps, and more importantly, their solutions (if exist). Actually, it is in general very challenging to determine if developers have put in counter-measure for a compatibility issue, as different APIs have diverse behaviors, rendering various repair. To facilitate a large-scale study, this paper proposes RAPID, an automated tool to determine whether a compatibility issue has been addressed or not, by incorporating both static analysis and machine learning techniques. Results show that our trained classifier is quite effective by achieving a F1-score of 95.21% and 91.96% in the training stage and the validation stage respectively. With the help of RAPID, our study yields many interesting findings, e.g. developers are not willing to provide alternative implementations when handling incompatible API invocations (only 38.4%); for those incompatible APIs that Google gives replacement recommendations, the ratio of providing alternative implementations is significantly higher than those without recommendations; developers find more ways to repair compatibility issues than Google's recommendations and the knowledge acquired from these experienced developers would be extremely useful to novice developers and may significantly improve the current status of compatibility issue handling.","compatibility issues, Android app analysis, API evolution",886–898,13,Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering
327,@inproceedings: 10.1145/3379177.3388901,Onboarding Bot for Newcomers to Software Engineering,"Dominic, James and Ritter, Charles and Rodeghero, Paige",2020,Association for Computing Machinery,,https://doi.org/10.1145/3379177.3388901,10.1145/3379177.3388901,"Software development teams dedicate considerable resources to training newcomers. Newcomers are new developers to a software project. The software onboarding process is more complicated than onboarding into other organizations. It is much more challenging and time-consuming. The role of a mentor in onboarding newcomers in software engineering is well understood. However, the disruptions to the work of an experienced developer can reduce the quality of their work and job satisfaction. We propose a conversational bot that can help onboard newcomers to a software project instead of an experienced programmer. The bot will act as a mentor for the newcomer, thus putting less stress on experienced programmers. The bot will also be able to scan outside sources, such as stack overflow, for solutions to issues a newcomer may face. The newcomer will be able to interact with the bot using natural language. We will use this bot to assess improvements to code quality in future studies.","bot, open source software, onboarding, newcomer",91–94,4,Proceedings of the International Conference on Software and System Processes
328,@inproceedings: 10.1145/3372787.3390435,TasRec: A Framework for Task Recommendation in Crowdsourcing,"Abhinav, Kumar and Bhatia, Gurpriya Kaur and Dubey, Alpana and Jain, Sakshi and Bhardwaj, Nitish",2020,Association for Computing Machinery,,https://doi.org/10.1145/3372787.3390435,10.1145/3372787.3390435,"Lately, crowdsourcing has emerged as a viable option of getting work done by leveraging the collective intelligence of the crowd. With many tasks posted every day, the size of crowdsourcing platforms is growing exponentially. Hence, workers face an important challenge of selecting the right task. Despite the task filtering criteria available on the platform to select the right task, crowd workers find it difficult to choose the most relevant task and must glean through the filtered tasks to find the relevant tasks. In this paper, we propose a framework for recommending tasks to workers. The proposed framework evaluates the worker's fitment over the tasks based on worker's preference, past tasks (s)he has performed, and tasks done by similar workers. We evaluated our approach on the datasets collected from popular crowdsourcing platform. Our experimental results based on 5,000 tasks and 3,000 workers show that the recommendation made by our framework is significantly better as compared to the baseline approach.","recommendation, personalization, crowdsourcing, task selection",86–95,10,Proceedings of the 15th International Conference on Global Software Engineering
329,@inproceedings: 10.1145/3396802.3396805,Towards the Automation of Grading Textual Student Submissions to Open-Ended Questions,"Bernius, Jan Philip and Kovaleva, Anna and Krusche, Stephan and Bruegge, Bernd",2020,Association for Computing Machinery,,https://doi.org/10.1145/3396802.3396805,10.1145/3396802.3396805,"Growing student numbers at universities worldwide pose new challenges for instructors. Providing feedback to textual exercises is a challenge in large courses while being important for student's learning success. Exercise submissions and their grading are a primary and individual communication channel between instructors and students. The pure amount of submissions makes it impossible for a single instructor to provide regular feedback to large student bodies. Employing tutors in the process introduces new challenges. Feedback should be consistent and fair for all students. Additionally, interactive teaching models strive for real-time feedback and multiple submissions.We propose a support system for grading textual exercises using an automatic segment-based assessment concept. The system aims at providing suggestions to instructors by reusing previous comments as well as scores. The goal is to reduce the workload for instructors, while at the same time creating timely and consistent feedback to the students. We present the design and a prototypical implementation of an algorithm using topic modeling for segmenting the submissions into smaller blocks. Thereby, the system derives smaller units for assessment and allowing the creation of reusable and structured feedback.We have evaluated the algorithm qualitatively by comparing automatically produced segments with manually produced segments created by humans. The results show that the system can produce topically coherent segments. The segmentation algorithm based on topic modeling is superior to approaches purely based on syntax and punctuation.","Software Engineering Education, Textual Exercise, Assessment Support Systems, Automatic Assessment",61–70,10,Proceedings of the 4th European Conference on Software Engineering Education
330,@inproceedings: 10.1145/3394450.3397469,Learned Garbage Collection,"Cen, Lujing and Marcus, Ryan and Mao, Hongzi and Gottschlich, Justin and Alizadeh, Mohammad and Kraska, Tim",2020,Association for Computing Machinery,,https://doi.org/10.1145/3394450.3397469,10.1145/3394450.3397469,"Several programming languages use garbage collectors (GCs) to automatically manage memory for the programmer. Such collectors must decide when to look for unreachable objects to free, which can have a large performance impact on some applications. In this preliminary work, we propose a design for a learned garbage collector that autonomously learns over time when to perform collections. By using reinforcement learning, our design can incorporate user-defined reward functions, allowing an autonomous garbage collector to learn to optimize the exact metric the user desires (e.g., request latency or queries per second). We conduct an initial experimental study on a prototype, demonstrating that an approach based on tabular Q learning may be promising.","Garbage collection, reinforcement learning",38–44,7,Proceedings of the 4th ACM SIGPLAN International Workshop on Machine Learning and Programming Languages
331,@inproceedings: 10.1145/3385412.3385995,PMEvo: Portable Inference of Port Mappings for out-of-Order Processors by Evolutionary Optimization,"Ritter, Fabian and Hack, Sebastian",2020,Association for Computing Machinery,,https://doi.org/10.1145/3385412.3385995,10.1145/3385412.3385995,"Achieving peak performance in a computer system requires optimizations in every layer of the system, be it hardware or software. A detailed understanding of the underlying hardware, and especially the processor, is crucial to optimize software. One key criterion for the performance of a processor is its ability to exploit instruction-level parallelism. This ability is determined by the port mapping of the processor, which describes the execution units of the processor for each instruction.  Processor manufacturers usually do not share the port mappings of their microarchitectures. While approaches to automatically infer port mappings from experiments exist, they are based on processor-specific hardware performance counters that are not available on every platform.  We present PMEvo, a framework to automatically infer port mappings solely based on the measurement of the execution time of short instruction sequences. PMEvo uses an evolutionary algorithm that evaluates the fitness of candidate mappings with an analytical throughput model formulated as a linear program. Our prototype implementation infers a port mapping for Intel's Skylake architecture that predicts measured instruction throughput with an accuracy that is competitive to existing work. Furthermore, it finds port mappings for AMD's Zen+ architecture and the ARM Cortex-A72 architecture, which are out of scope of existing techniques.","port mapping, evolutionary algorithm, processor reverse engineering",608–622,15,Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation
332,@inproceedings: 10.1145/3385412.3386016,Learning Fast and Precise Numerical Analysis,"He, Jingxuan and Singh, Gagandeep and P\""{u}schel, Markus and Vechev, Martin",2020,Association for Computing Machinery,,https://doi.org/10.1145/3385412.3386016,10.1145/3385412.3386016,"Numerical abstract domains are a key component of modern static analyzers. Despite recent advances, precise analysis with highly expressive domains remains too costly for many real-world programs. To address this challenge, we introduce a new data-driven method, called LAIT, that produces a faster and more scalable numerical analysis without significant loss of precision. Our approach is based on the key insight that sequences of abstract elements produced by the analyzer contain redundancy which can be exploited to increase performance without compromising precision significantly. Concretely, we present an iterative learning algorithm that learns a neural policy that identifies and removes redundant constraints at various points in the sequence. We believe that our method is generic and can be applied to various numerical domains.  We instantiate LAIT for the widely used Polyhedra and Octagon domains. Our evaluation of LAIT on a range of real-world applications with both domains shows that while the approach is designed to be generic, it is orders of magnitude faster on the most costly benchmarks than a state-of-the-art numerical library while maintaining close-to-original analysis precision. Further, LAIT outperforms hand-crafted heuristics and a domain-specific learning approach in terms of both precision and speed.","Performance optimization, Numerical domains, Machine learning, Abstract interpretation",1112–1127,16,Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation
333,@inproceedings: 10.1145/3385412.3386005,Type Error Feedback via Analytic Program Repair,"Sakkas, Georgios and Endres, Madeline and Cosman, Benjamin and Weimer, Westley and Jhala, Ranjit",2020,Association for Computing Machinery,,https://doi.org/10.1145/3385412.3386005,10.1145/3385412.3386005,"We introduce Analytic Program Repair, a data-driven strategy for providing feedback for type-errors via repairs for the erroneous program. Our strategy is based on insight that similar errors have similar repairs. Thus, we show how to use a training dataset of pairs of ill-typed programs and their fixed versions to: (1)&nbsp;learn a collection of candidate repair templates by abstracting and partitioning the edits made in the training set into a representative set of templates; (2)&nbsp;predict the appropriate template from a given error, by training multi-class classifiers on the repair templates used in the training set; (3)&nbsp;synthesize a concrete repair from the template by enumerating and ranking correct (e.g. well-typed) terms matching the predicted template. We have implemented our approach in Rite: a type error reporting tool for OCaml programs. We present an evaluation of the accuracy and efficiency of Rite on a corpus of 4,500 ill-typed Ocaml programs drawn from two instances of an introductory programming course, and a user-study of the quality of the generated error messages that shows the locations and final repair quality to be better than the state-of-the-art tool in a statistically-significant manner.","Program Synthesis, Machine Learning, Type Error Feedback, Program Repair",16–30,15,Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation
334,@inproceedings: 10.1145/3378678.3391882,Reviewing Inference Performance of State-of-the-Art Deep Learning Frameworks,"Ulker, Berk and Stuijk, Sander and Corporaal, Henk and Wijnhoven, Rob",2020,Association for Computing Machinery,,https://doi.org/10.1145/3378678.3391882,10.1145/3378678.3391882,"Deep learning models have replaced conventional methods for machine learning tasks. Efficient inference on edge devices with limited resources is key for broader deployment. In this work, we focus on the tool selection challenge for inference deployment. We present an extensive evaluation of the inference performance of deep learning software tools using state-of-the-art CNN architectures for multiple hardware platforms. We benchmark these hardware-software pairs for a broad range of network architectures, inference batch sizes, and floating-point precision, focusing on latency and throughput. Our results reveal interesting combinations for optimal tool selection, resulting in different optima when considering minimum latency and maximum throughput.",,48–53,6,Proceedings of the 23th International Workshop on Software and Compilers for Embedded Systems
335,@inproceedings: 10.1145/3403746.3403905,The Identification of Corn Leaf Diseases Based on Transfer Learning and Data Augmentation,"Hu, Rongjie and Zhang, Shan and Wang, Peng and Xu, Guoming and Wang, Daoyong and Qian, Yuqi",2020,Association for Computing Machinery,,https://doi.org/10.1145/3403746.3403905,10.1145/3403746.3403905,"Corn is one of the most important food crops in the world, but there are many kinds of corn diseases, and it is difficult to diagnose by planting personnel based on experience. However, the misdiagnosis reduces production efficiency. With the development of computer technology, the use of deep learning and image recognition technology for plant disease detection has become an important research direction. We propose a convolutional neural network based on data augmentation combined with transfer learning to identify corn leaf disease models. The algorithm first increases the data by means of data augmentation to improve the generalization and accuracy of the model, and builds a convolutional neural network model based on transfer learning. Then, it uses the model for training, accelerates the training process of the convolutional neural network, and uses test dataset feedback network training results. In this study, the corn leaf images in PlantVillage were used as the dataset of our experiment to classify the four categories which consists of Corn Gray leaf spot, Corn Common rust, Corn Northern Leaf Blight and healthy leaves. We first obtained our optimization model by fine-tuning the GoogLeNet pre-training network, adjusting parameters such as optimizer and learning rate. Then we trained the optimization model, like the original GoogLeNet network, ResNet18, Vgg16, and Vgg19 networks based on transfer learning and compare the results. The results show that by using our optimized model, the average recognition accuracy of corn disease which consists of corn common leaf rust, Corn Common rust, Corn Northern Leaf Blight and healthy leaves reached 97.6%, and the recognition accuracy of each category was greater than 95%. Compared with the original GoogLeNet model, the highest accuracy rate is improved by 5.9%. Also, the effect is better when compared with other networks based on transfer learning. Our model provides new ideas for the identification of diseases, insect pests of corn and other crops.","Corn disease recognition, Convolutional neural network, Transfer learning",58–65,8,Proceedings of the 2020 3rd International Conference on Computer Science and Software Engineering
336,@inproceedings: 10.1145/3403746.3403907,Asymmetric U-Shape Network for Contour Detection on Nonwoven Images,"Hou, Jue and Wang, Rongwu and Zhang, Xing",2020,Association for Computing Machinery,,https://doi.org/10.1145/3403746.3403907,10.1145/3403746.3403907,"COVID-19 becomes a pandemic all over the world in 2020. It is important to supply high-quality protective equipments to medical staff. Nonwoven is a widely used material for medical protective clothing. In this paper, we present an asymmetric U-shape network (AUN) for high quality contour detections of nonwoven images. AUN utilizes multiscale features through an encoder path and rebuilds high resolution side-outputs through a decoder path, feature extraction path introduced the gradient features into the CNN. The performance of the proposed network was evaluated on two datasets: the proposed nonwoven image dataset and PASCAL VOC2012. It was found that AUN can obtain high accuracy object contours on both datasets.","Nonwoven materials, Convolutional neural network, Contour detection",71–76,6,Proceedings of the 2020 3rd International Conference on Computer Science and Software Engineering
337,@inproceedings: 10.1145/3403746.3403912,ROI-Based Deep Learning Method for Variable-Length License Plate Character Segmentation,"Wang, Dan and Wang, Bingshu and Chen, Yang",2020,Association for Computing Machinery,,https://doi.org/10.1145/3403746.3403912,10.1145/3403746.3403912,"License plate character segmentation plays link function between license plate detection and recognition. It is based on the results of license plate detection and produces segmented characters for subsequent recognition module. Character is the region of interest (ROI) in the license plate. Previous works focus on fixed-length license plates and face challenging on the slant plates. To solve the problem, we propose an ROI-based deep learning method for variable-length license plate character segmentation. It treats all the characters as objects and converts character segmentation as object detection. This paper exploits Faster R-CNN to detect the possible character regions. Region Proposal Network is designed to provide sufficient proposals for character detection of license plate and full connected network can modify the candidate boxes and predict the character class simultaneously. Moreover, we create a dataset for character segmentation of license plate. Experimental results demonstrate that our method can achieve a high accuracy in three scenes compared with some state-of-the-art approaches.","Faster R-CNN, ROI, Character segmentation, License plate recognition",102–106,5,Proceedings of the 2020 3rd International Conference on Computer Science and Software Engineering
338,@inproceedings: 10.1145/3399871.3399888,A Variant Model of TGAN for Music Generation,"Cheng, Ping-Sung and Lai, Chieh-Ying and Chang, Chun-Chieh and Chiou, Shu-Fen and Yang, Yu-Chieh",2020,Association for Computing Machinery,,https://doi.org/10.1145/3399871.3399888,10.1145/3399871.3399888,"In the past five years, we have seen an increase in generative adversarial networks (GANs) and their applications for image generation. Due to the randomness and unpredictability of the structure of music, music generation is well suited to the use of GANs. Numerous studies have been published on music generation by using temporal GANs. However, few studies have focused on the relationships between melodies and chords, and the effects of latent space on time sequence.We also propose a new method to implement latent structure on GANs for music generation. The main innovation of the proposed model is the use of new discriminator to recognize the time sequence of music and use of a pretrained beat generator to improve the quality of patterned melodies and chords. Results indicated that the pretrained model improved the quality of generated music.","Artificial Intelligence, Music Generation, Generative Adversarial Networks",40–45,6,Proceedings of the 2020 Asia Service Sciences and Software Engineering Conference
339,@inproceedings: 10.1145/3399871.3399889,Cross-Sectional Stock Price Prediction Using Deep Learning for Actual Investment Management,"Abe, Masaya and Nakagawa, Kei",2020,Association for Computing Machinery,,https://doi.org/10.1145/3399871.3399889,10.1145/3399871.3399889,"Stock price prediction has been an important research theme both academically and practically. Various methods to predict stock prices have been studied until now. The feature that explains the stock price by a cross-section analysis is called a ""factor"" in the field of finance. Many empirical studies in finance have identified which stocks having features in the cross-section relatively increase and which decrease in terms of price. Recently, stock price prediction methods using machine learning, especially deep learning, have been proposed since the relationship between these factors and stock prices is complex and non-linear. However, there are no practical examples for actual investment management. In this paper, therefore, we present a cross-sectional daily stock price prediction framework using deep learning for actual investment management. For example, we build a portfolio with information available at the time of market closing and invest at the time of market opening the next day. We perform empirical analysis in the Japanese stock market and confirm the profitability of our framework.","Stock Return Prediction, Multi-factor Model, Deep Learning, Cross-Section",9–15,7,Proceedings of the 2020 Asia Service Sciences and Software Engineering Conference
340,@inproceedings: 10.1145/3383219.3383281,Cross-Project Software Fault Prediction Using Data Leveraging Technique to Improve Software Quality,"Khan, Bilal and Iqbal, Danish and Badshah, Sher",2020,Association for Computing Machinery,,https://doi.org/10.1145/3383219.3383281,10.1145/3383219.3383281,"Software fault prediction is a process to detect bugs in software projects. Fault prediction in software engineering has attracted much attention from the last decade. The early prognostication of faults in software minimize the cost and effort of errors that come at later stages. Different machine learning techniques have been utilized for fault prediction, that is proven to be utilizable. Despite, the significance of fault prediction most of the companies do not consider fault prediction in practice and do not build useful models due to lack of data or lack of enough data to strengthen the power of fault predictors. However, models trained and tested on less amount of data are difficult to generalize, because they do not consider project size, project differences, and features selection. To overcome these issues, we proposed an instance-based transfer learning through data leveraging using logistic linear regression as a base proposed statistical methodology. In our study, we considered three software projects within the same domain. Finally, we performed a comparative analysis of three different experiments for building models (targeted project). The experimental results of the proposed approach show promising improvements in (SFP).","Software Quality, Instance-based learning, Software fault prediction, Cross-project, data leveraging, Machine learning",434–438,5,Proceedings of the Evaluation and Assessment in Software Engineering
341,@inproceedings: 10.1145/3383219.3383288,Automated Identification of Security Requirements: A Machine Learning Approach,"Kobilica, Armin and Ayub, Mohammed and Hassine, Jameleddine",2020,Association for Computing Machinery,,https://doi.org/10.1145/3383219.3383288,10.1145/3383219.3383288,"Early characterization of security requirements supports system designers to integrate security aspects into early architectural design. However, distinguishing security related requirements from other functional and non-functional requirements can be tedious and error prone. To address this issue, machine learning techniques have proven to be successful in the identification of security requirements. In this paper, we have conducted an empirical study to evaluate the performance of 22 supervised machine learning classification algorithms and two deep learning approaches, in classifying security requirements, using the publicly availble SecReq dataset. More specifically, we focused on the robustness of these techniques with respect to the overhead of the pre-processing step. Results show that Long short-term memory (LSTM) network achieved the best accuracy (84%) among non-supervised algorithms, while Boosted Ensemble achieved the highest accuracy (80%), among supervised algorithms.","Security Requirements, Machine Learning, Fast Pre-processing Techniques",475–480,6,Proceedings of the Evaluation and Assessment in Software Engineering
342,@inproceedings: 10.1145/3383219.3383229,Feature Terms Prediction: A Feasible Way to Indicate the Notion of Features in Software Product Line,"Li, Yang and Schulze, Sandro and Xu, Jiahua",2020,Association for Computing Machinery,,https://doi.org/10.1145/3383219.3383229,10.1145/3383219.3383229,"In Software Product Lines (SPL), feature extraction from software requirements specifications has been subject to intense research in order to assist domain analysis in a time-saving way. Although various approaches are proposed to extract features, there still exists a gap to achieve the complete view of features, that is, how to figure out the intention of a feature. Feature terms as the smallest units in a feature can be regarded as vital indicators for describing a feature. Automated feature term extraction can provide key information regarding the intention of a feature, which improves the efficiency of domain analysis. In this paper, we propose an approach to train prediction models by using machine learning techniques to identify feature terms. To this end, we extract candidate terms from requirement specifications in one domain and take six attributes of each term into account to create a labeled dataset. Subsequently, we apply seven commonly used machine algorithms to train prediction models on the labeled dataset. We then use these prediction models to predict feature terms from the requirements belonging to the other two different domains. Our results show that (1) feature terms can be predicted with high accuracy of ≈ 90% within a domain (2) prediction across domains leads to a decreased but still good accuracy (≈ 80%), and (3) machine learning algorithms perform differently.","Feature Terms Identification, Software Product Lines, Feature Extraction, Requirement Documents",90–99,10,Proceedings of the Evaluation and Assessment in Software Engineering
343,@inbook: 10.1145/3373376.3378522,Shredder: Learning Noise Distributions to Protect Inference Privacy,"Mireshghallah, Fatemehsadat and Taram, Mohammadkazem and Ramrakhyani, Prakash and Jalali, Ali and Tullsen, Dean and Esmaeilzadeh, Hadi",2020,Association for Computing Machinery,,https://doi.org/10.1145/3373376.3378522,,"A wide variety of deep neural applications increasingly rely on the cloud to perform their compute-heavy inference. This common practice requires sending private and privileged data over the network to remote servers, exposing it to the service provider and potentially compromising its privacy. Even if the provider is trusted, the data can still be vulnerable over communication channels or via side-channel attacks in the cloud. To that end, this paper aims to reduce the information content of the communicated data with as little as possible compromise on the inference accuracy by making the sent data noisy. An undisciplined addition of noise can significantly reduce the accuracy of inference, rendering the service unusable. To address this challenge, this paper devises Shredder, an end-to-end framework, that, without altering the topology or the weights of a pre-trained network, learns additive noise distributions that significantly reduce the information content of communicated data while maintaining the inference accuracy. The key idea is finding the additive noise distributions by casting it as a disjoint offline learning process with a loss function that strikes a balance between accuracy and information degradation. The loss function also exposes a knob for a disciplined and controlled asymmetric trade-off between privacy and accuracy. While keeping the DNN intact, Shredder divides inference between the cloud and the edge device, striking a balance between computation and communication. In the separate phase of inference, the edge device takes samples from the Laplace distributions that were collected during the proposed offline learning phase and populates a noise tensor with these sampled elements. Then, the edge device merely adds this populated noise tensor to the intermediate results to be sent to the cloud. As such, Shredder enables accurate inference on noisy intermediate data without the need to update the model or the cloud, or any training process during inference. We also formally show that Shredder maximizes privacy with minimal impact on DNN accuracy while the tradeoff between privacy and accuracy is controlled through a mathematical knob. Experimentation with six real-world DNNs from text processing and image classification shows that Shredder reduces the mutual information between the input and the communicated data to the cloud by 74.70% compared to the original execution while only sacrificing 1.58% loss in accuracy. On average, Shredder also offers a speedup of 1.79x over Wi-Fi and 2.17x over LTE compared to cloud-only execution when using an off-the-shelf mobile GPU (Tegra X2) on the edge.",,3–18,1,Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems
344,@inbook: 10.1145/3373376.3378534,PatDNN: Achieving Real-Time DNN Execution on Mobile Devices with Pattern-Based Weight Pruning,"Niu, Wei and Ma, Xiaolong and Lin, Sheng and Wang, Shihao and Qian, Xuehai and Lin, Xue and Wang, Yanzhi and Ren, Bin",2020,Association for Computing Machinery,,https://doi.org/10.1145/3373376.3378534,,"With the emergence of a spectrum of high-end mobile devices, many applications that formerly required desktop-level computation capability are being transferred to these devices. However, executing Deep Neural Networks (DNNs) inference is still challenging considering the high computation and storage demands, specifically, if real-time performance with high accuracy is needed. Weight pruning of DNNs is proposed, but existing schemes represent two extremes in the design space: non-structured pruning is fine-grained, accurate, but not hardware friendly; structured pruning is coarse-grained, hardware-efficient, but with higher accuracy loss.In this paper, we advance the state-of-the-art by introducing a new dimension, fine-grained pruning patterns inside the coarse-grained structures, revealing a previously unknown point in the design space. With the higher accuracy enabled by fine-grained pruning patterns, the unique insight is to use the compiler to re-gain and guarantee high hardware efficiency. In other words, our method achieves the best of both worlds, and is desirable across theory/algorithm, compiler, and hardware levels. The proposed PatDNN is an end-to-end framework to efficiently execute DNN on mobile devices with the help of a novel model compression technique---pattern-based pruning based on an extended ADMM solution framework---and a set of thorough architecture-aware compiler/code generation-based optimizations, i.e., filter kernel reordering, compressed weight storage, register load redundancy elimination, and parameter auto-tuning. Evaluation results demonstrate that PatDNN outperforms three state-of-the-art end-to-end DNN frameworks, TensorFlow Lite, TVM, and Alibaba Mobile Neural Network with speedup up to 44.5X, 11.4X, and 7.1X, respectively, with no accuracy compromise. Real-time inference of representative large-scale DNNs (e.g., VGG-16, ResNet-50) can be achieved using mobile devices.",,907–922,1,Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems
345,@inbook: 10.1145/3373376.3378462,Game of Threads: Enabling Asynchronous Poisoning Attacks,"Sanchez Vicarte, Jose Rodrigo and Schreiber, Benjamin and Paccagnella, Riccardo and Fletcher, Christopher W.",2020,Association for Computing Machinery,,https://doi.org/10.1145/3373376.3378462,,"As data sizes continue to grow at an unprecedented rate, machine learning training is being forced to adopt asynchronous algorithms to maintain performance and scalability. In asynchronous training, many threads share and update the model in a racy fashion to avoid costly inter-thread synchronization.This paper studies the security implications of these codes by introducing asynchronous poisoning attacks. Our attack influences training outcome---e.g., degrades model accuracy or biases the model towards an adversary-specified label---purely by scheduling asynchronous training threads in a malicious fashion. Since thread scheduling is outside the protections of modern trusted execution environments (TEEs), e.g., Intel SGX, our attack bypasses these protections even when the training set can be verified as correct. To the best of our knowledge, this represents the first example where a class of applications loses integrity guarantees, despite being protected by enclave-based TEEs such as SGX.We demonstrate both accuracy degradation and model biasing attacks on the CIFAR-10 image recognition task, trained on Resnet-style DNNs using an asynchronous training code published by Pytorch. We also perform proof-of-concept experiments to validate our assumptions on an SGX-enabled machine. Our accuracy degradation attacks are capable of returning a converged model to pre-trained accuracy or to some accuracy in between. Our model biasing attack can force the model to predict an adversary-specified label up to ~40% of the time on the CIFAR-10 validation set (whereas the un-attacked model's prediction rate towards any label is ~10%).",,35–52,1,Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems
346,@inproceedings: 10.1145/3385032.3385039,Clustering Glossary Terms Extracted from Large-Sized Software Requirements Using FastText,"Bhatia, Kushagra and Mishra, Siba and Sharma, Arpit",2020,Association for Computing Machinery,,https://doi.org/10.1145/3385032.3385039,10.1145/3385032.3385039,"Specialized terms used in the requirements document should be defined in a glossary. We propose a technique for automated extraction and clustering of glossary terms from large-sized requirements documents. We use text chunking combined with WordNet removal to extract candidate glossary terms. Next, we apply a state-of-the art neural word embeddings model for clustering glossary terms based on semantic similarity measures. Word embeddings are capable of capturing the context of a word and compute its semantic similarity relation with other words used in a document. Its use for clustering ensures that terms that are used in similar ways belong to the same cluster. We apply our technique to the CrowdRE dataset, which is a large-sized dataset with around 3000 crowd-generated requirements for smart home applications. To measure the effectiveness of our extraction and clustering technique we manually extract and cluster the glossary terms from CrowdRE dataset and use it for computing precision, recall and coverage. Results indicate that our approach can be very useful for extracting and clustering of glossary terms from a large body of requirements.","FastText, Clustering, Requirements Engineering, Glossary, Word Embeddings, Natural Language Processing",,11,Proceedings of the 13th Innovations in Software Engineering Conference on Formerly Known as India Software Engineering Conference
347,@inproceedings: 10.1145/3373087.3375301,When Massive GPU Parallelism Ain't Enough: A Novel Hardware Architecture of 2D-LSTM Neural Network,"Rybalkin, Vladimir and Wehn, Norbert",2020,Association for Computing Machinery,,https://doi.org/10.1145/3373087.3375301,10.1145/3373087.3375301,"Multidimensional Long Short-Term Memory (MD-LSTM) neural network is an extension of one-dimensional LSTM for data with more than one dimension that allows MD-LSTM to show state-of-the-art results in various applications including handwritten text recognition, medical imaging, and many more. However, efficient implementation suffers from very sequential execution that tremendously slows down both training and inference compared to other neural networks. This is the primary reason that prevents intensive research involving MD-LSTM in the recent years, despite large progress in microelectronics and architectures. The main goal of the current research is to provide acceleration for inference of MD-LSTM, so to open a door for efficient training that can boost application of MD-LSTM. By this research we advocate that FPGA is an alternative platform for deep learning that can offer a solution in cases when a massive parallelism of GPUs does not provide the necessary performance required by the application. In this paper, we present the first hardware architecture for MD-LSTM. We conduct a systematic exploration of precision vs. accuracy trade-off using challenging dataset for historical document image binarization from DIBCO 2017 contest, and well known MNIST dataset for handwritten digits recognition. Based on our new architecture we implement FPGA-based accelerator that outperforms NVIDIA K80 GPU implementation in terms of runtime by up to 50x and energy efficiency by up to 746x. At the same time, our accelerator demonstrates higher accuracy and comparable throughput in comparison with state-of-the-art FPGA-based implementations of multilayer perceptron for MNIST dataset.","dibco, mnist, 2d-lstm, long short-term memory, fpga, deep learning, zynq, lstm, hardware architecture, image binarization, md-lstm",111–121,11,Proceedings of the 2020 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays
348,@inbook: 10.1145/3368826.3377928,NeuroVectorizer: End-to-End Vectorization with Deep Reinforcement Learning,"Haj-Ali, Ameer and Ahmed, Nesreen K. and Willke, Ted and Shao, Yakun Sophia and Asanovic, Krste and Stoica, Ion",2020,Association for Computing Machinery,,https://doi.org/10.1145/3368826.3377928,,"One of the key challenges arising when compilers vectorize loops for today’s SIMD-compatible architectures is to decide if vectorization or interleaving is beneficial. Then, the compiler has to determine the number of instructions to pack together and the interleaving level (stride). Compilers are designed today to use fixed-cost models that are based on heuristics to make vectorization decisions on loops. However, these models are unable to capture the data dependency, the computation graph, or the organization of instructions. Alternatively, software engineers often hand-write the vectorization factors of every loop. This, however, places a huge burden on them, since it requires prior experience and significantly increases the development time. In this work, we explore a novel approach for handling loop vectorization and propose an end-to-end solution using deep reinforcement learning (RL). We conjecture that deep RL can capture different instructions, dependencies, and data structures to enable learning a sophisticated model that can better predict the actual performance cost and determine the optimal vectorization factors. We develop an end-to-end framework, from code to vectorization, that integrates deep RL in the LLVM compiler. Our proposed framework takes benchmark codes as input and extracts the loop codes. These loop codes are then fed to a loop embedding generator that learns an embedding for these loops. Finally, the learned embeddings are used as input to a Deep RL agent, which dynamically determines the vectorization factors for all the loops. We further extend our framework to support random search, decision trees, supervised neural networks, and nearest-neighbor search. We evaluate our approaches against the currently used LLVM vectorizer and loop polyhedral optimization techniques. Our experiments show 1.29\texttimes{}−4.73\texttimes{} performance speedup compared to baseline and only 3% worse than the brute-force search on a wide range of benchmarks.",,242–255,1,Proceedings of the 18th ACM/IEEE International Symposium on Code Generation and Optimization
349,@inproceedings: 10.1145/3384544.3384568,Masked Face Detection Based on Locally Nonlinear Feature Fusion,"Peng, Xin-Yi and Cao, Jun and Zhang, Fu-Yuan",2020,Association for Computing Machinery,,https://doi.org/10.1145/3384544.3384568,10.1145/3384544.3384568,"Realizing that features with strong discrimination are both needed for the generation and discrimination of candidate regions in the masked face detection, LNFF-Net (Locally Nonlinear Feature Fusion-based Network) is proposed. To highlight the feature from the face region and suppress the background region, this method nonlinearly fuses the visual saliency map and the heat map, which is extracted from a light fully convolutional network (FCN). On the other hand, through transferring the Fast R-CNN based multi-objective detection to single masked face detection, the structure of candidate region discrimination using convolutional network is optimized. Experimental results show that the proposed algorithm has better detection accuracy than other method.","Masked face detection, FCN, The visual saliency map, Candidate region, Feature fusion",114–118,5,Proceedings of the 2020 9th International Conference on Software and Computer Applications
350,@inproceedings: 10.1145/3384544.3384567,Real Time Face Recognition on an Edge Computing Device,"Gupta, Samarth",2020,Association for Computing Machinery,,https://doi.org/10.1145/3384544.3384567,10.1145/3384544.3384567,"Face recognition systems have vast applications in surveillance systems and human-computer interactions. Different approaches such as Principal Component Analysis, Fisher linear discriminant analysis, Convolutional Neural Networks (CNN) have been commonly used for face recognition. However, in the recent times, CNN's have shown quite promising results in various face recognition systems. But, deep learning based CNNs have many limitations such as they require extensive training data, have excessively high computational and cooling requirements, and lack flexibility in deployment. Fields such as robotics and embedded systems that deploy face recognition systems have significantly less power on board and limited heat dissipation capacity. Therefore, it becomes difficult to deploy deep learning models on them but edge computing based devices like the Intel Neural Stick bridge this gap as they have certain advantages. In this paper, we review different applications of face recognition systems and various algorithms used for face recognition. We then elaborate the limitations of deep learning based face recognition systems and examine how edge-computing devices can solve these problems. We then present a flowchart to deploy a CNN based face recognition model on an edge-computing device.","Neural Stick based Edge Computing, Convolutional Neural Networks, Transfer Learning, Face Recognition System, Artificial Intelligence",110–113,4,Proceedings of the 2020 9th International Conference on Software and Computer Applications
351,@inproceedings: 10.1145/3384544.3384606,Deep Learning in Semantic Segmentation of Rust in Images,"Duy, Le Dinh and Anh, Ngo Tuan and Son, Ngo Tung and Tung, Nguyen Viet and Duong, Nguyen Ba and Khan, Muhammad Hassan Raza",2020,Association for Computing Machinery,,https://doi.org/10.1145/3384544.3384606,10.1145/3384544.3384606,"Rust detection is an essential topic in many areas, especially in telecommunication, which needs effective systems to segment and recognize rust on power electric towers, antenna. Our exclusive architecture use is based on a fully convolutional neural network for semantic segmentation and composed of Densenet encoder PSP intermediate layers and two skip connections upsample layers. The code written in Python used Pytorch libraries to compute and categorize the images. Comparing between models such as E-Net, U-Net, FCN, we have received our highest FCN (Fully Convolutional Neural) model for the most stable ratio of IoU (Intersection over Union) in 3 models stated with mean scores are 58.1 for origin images and 61.8 for background removal. With the results, we will contribute to detect rust on electric poles in time to avoid rust-causing serious consequences.","CNN, FCN, Corrosion detection, Rust detection, PSP, Densenet, Encoder",129–132,4,Proceedings of the 2020 9th International Conference on Software and Computer Applications
352,@inproceedings: 10.1145/3384544.3384565,Sub-Word Embedding Auxiliary Encoding in Mongolian-Chinese Neural Machine Translation,"Bai, Tiangang and Hou, Hongxu and Ji, Yatu",2020,Association for Computing Machinery,,https://doi.org/10.1145/3384544.3384565,10.1145/3384544.3384565,"For low-resource Mongolian-Chinese neural machine translation (NMT), the common pre-processing methods such as byte pair encoding (BPE) and tokenization, are unable to recognize Mongolian special character, which leads to the loss of complete sentence information. The translation quality of low-frequency words is undesirable due to the problem of data sparsity. In this paper, we firstly propose a process method for Mongolian special character, which can transform the Mongolian special characters into explicit form to decrease the pre-processing error. Secondly, according to the morphological knowledge of Mongolian, we generate the sub-word embedding with large scale monolingual corpus to enhance the contextual information of the representation of low-frequency words. The experiments show that 1) Mongolian special character processing can minimize the semantic loss, 2) systems with sub-word embedding from large scale monolingual corpus can capture the semantic information of low-frequency words effectively 3) the proposed approaches can improve 1-2 BLEU points above the baselines.","Special character processing, Sub-word embedding, Neural machine translation, Mongolian-Chinese machine translation",292–296,5,Proceedings of the 2020 9th International Conference on Software and Computer Applications
353,@inproceedings: 10.1145/3384544.3384607,Exploiting Deep Neural Networks for Intention Mining,"Habib, Anam and Jelani, Nosheen and Khattak, Asad Masood and Akbar, Saima and Asghar, Muhammad Zubair",2020,Association for Computing Machinery,,https://doi.org/10.1145/3384544.3384607,10.1145/3384544.3384607,"In the current era of digital media, people are greatly interested to express themselves on online interaction which produces a huge amount of data. The user generated content may contain user's emotions, opinions, daily events and specially their intent or motive behind their communication. Intention identification/mining of user's reviews, that is whether a user review contains intent or not, from social media network, is an emerging area and is in great demand in various fields like online advertising, improving customer services and decision making. Until now, a lot of work has been performed by researchers on user intention identification using machine learning approaches. However, it is demanded to focus on deep neural network methods. In this research work, we have conducted experimentation on intention dataset using a deep learning method namely CNN+BILSTM. The results exhibit that the proposed model efficiently performed identification of intention sentences in user generated text with a 90% accuracy.","Deep Learning, Intention Identification, Bidirectional LSTM, Convolutional Neural Network",26–30,5,Proceedings of the 2020 9th International Conference on Software and Computer Applications
354,@inproceedings: 10.1145/3384544.3384566,Training Strategies for CNN-Based Models to Parse Complex Floor Plans,"Zhu, Ruiyun and Shen, Jingcheng and Deng, Xiangtian and Walld\'{e}n, Marcus and Ino, Fumihiko",2020,Association for Computing Machinery,,https://doi.org/10.1145/3384544.3384566,10.1145/3384544.3384566,"A floor plan is one of the most fundamental diagrams for architectural design. Considering a large proportion of floor plans are rasterized images, we believe that parsing the rasterized images is a crucial procedure to automate architectural design. In this study, we evaluate the use of convolutional neural network (CNN) based image segmentation methods to handle floor plan parsing, instead of traditional measures such as template matching. Especially, we analyzed samples whose features are difficult for CNN-based models to learn; thus, we propose two training strategies, separate training and the use of a weighted loss function, to improve the learning accuracy for such complex samples. Experimental results demonstrate that the proposed strategies performed well for the complex samples, generating more favorable parsing output.","Machine learning, computer-aided design, image segmentation",11–16,6,Proceedings of the 2020 9th International Conference on Software and Computer Applications
355,@inproceedings: 10.1145/3377024.3377040,Generating Attributed Variability Models for Transfer Learning,"Dorn, Johannes and Apel, Sven and Siegmund, Norbert",2020,Association for Computing Machinery,,https://doi.org/10.1145/3377024.3377040,10.1145/3377024.3377040,"Modern software systems often provide configuration options for customizing of the system's functional and non-functional properties, such as response time and energy consumption. The valid configurations of a software system are commonly documented in a variability model. Supporting the optimization of a system's non-functional properties, variability models have been extended with attributes that represent the influence of one or multiple options on a property. The concrete values of attributes are typically determined only in a single environment (e.g., for a specific software version, a certain workload, and a specific hardware setup) and are applicable only for this context. Changing the environment, attribute values need to be updated. Instead of determining all attributes from scratch with new measurements, recent approaches rely on transfer learning to reduce the effort of obtaining new attribute values. However, the development and evaluation of new transfer-learning techniques requires extensive measurements by themselves, which often is prohibitively costly. To support research in this area, we propose an approach to synthesize realistic attributed variability models from a base model. This way, we can support research and validation of novel transfer-learning techniques for configurable software systems. We use a genetic algorithm to vary attribute values. Combined with a declarative objective function, we search a changed attributed variability model that keeps some key characteristics while mimicking realistic changes of individual attribute values. We demonstrate the applicability of our approach by replicating the evaluation of an existing transfer-learning technique.","transfer learning, attributed variability models, Loki, variability modelling",,8,Proceedings of the 14th International Working Conference on Variability Modelling of Software-Intensive Systems
356,@inproceedings: 10.1145/3372885.3373827,Exploration of Neural Machine Translation in Autoformalization of Mathematics in Mizar,"Wang, Qingxiang and Brown, Chad and Kaliszyk, Cezary and Urban, Josef",2020,Association for Computing Machinery,,https://doi.org/10.1145/3372885.3373827,10.1145/3372885.3373827,"In this paper we share several experiments trying to automatically translate informal mathematics into formal mathematics. In our context informal mathematics refers to human-written mathematical sentences in the LaTeX format; and formal mathematics refers to statements in the Mizar language. We conducted our experiments against three established neural network-based machine translation models that are known to deliver competitive results on translating between natural languages. To train these models we also prepared four informal-to-formal datasets. We compare and analyze our results according to whether the model is supervised or unsupervised. In order to augment the data available for auto-formalization and improve the results, we develop a custom type-elaboration mechanism and integrate it in the supervised translation.","Mizar, Neural Machine Translation, Automating Formalization, Proof Assistants, Machine Learning",85–98,14,Proceedings of the 9th ACM SIGPLAN International Conference on Certified Programs and Proofs
357,@inproceedings: 10.1145/3380625.3380647,Hybrid Self-Interactive Attentive Siamese Network for Medical Textual Semantic Similarity,"An, Hongda and Wu, Di and Li, Zhengguang",2020,Association for Computing Machinery,,https://doi.org/10.1145/3380625.3380647,10.1145/3380625.3380647,"The rapid development of medicine produces a large number of medical texts, but it is difficult to process these texts due to many similar sentences. Therefore, estimating the similarity of medical texts has become a key technology, filtering out medical texts quickly. Nowadays, many methods for estimation similarity between medical sentences extract semantic features mainly via Siamese network. However; these methods don't achieve the best results due to the large amount of noise in the texts. To improve the performance of the Siamese network, a hybrid self-interactive attention model is proposed in this paper. The aim is to reduce the noise of the text and strengthen the token with high correlation between the two texts. In addition, this proposed model also uses BERT as the embedding layer to carry out a preliminary pre-training of text. Then, two datasets are employed to verify the effectiveness of our method and our method achieves better results on Pearson correlation coefficient, compared with the other existing methods. The experimental results still indicate that the results of pre-trained BERT are better than that of Word2Vec, and the hybrid self-interactive attention model obtains better results due to the effect of interactive attention.","Interactive attention, Siamese, Self-attention, BERT, Textual Similarity",52–56,5,"Proceedings of the 2020 4th International Conference on Management Engineering, Software Engineering and Service Sciences"
358,@inproceedings: 10.1145/3380625.3380669,Research on Feature Extraction and Multimodal Fusion of Video Caption Based on Deep Learning,"Chen, Hongjun and Li, Hengyi and Wu, Xueqin",2020,Association for Computing Machinery,,https://doi.org/10.1145/3380625.3380669,10.1145/3380625.3380669,"Video Caption shows the objects, attributes and their relationship in natural language. It has been a very challenging research topic in the field of computer and multimedia. In this paper, the method of deep learning is used to extract the video frame feature, motion information, video sequence feature. And the multi-modal feature fusion method: feature cascade, model weighted average fusion are studied, and then the valuation is also studied. The experimental results show that the score of each evaluation in the model of weighted average fusion method is higher than that of the feature cascade method. The feature extraction methods and multimodal fusion methods in this paper have certain value for the application of video caption.","Video caption, deep learning, feature cascade, feature extraction, multimodal fusion",73–76,4,"Proceedings of the 2020 4th International Conference on Management Engineering, Software Engineering and Service Sciences"
359,@inproceedings: 10.1145/3378936.3378968,Performance Comparison of Deep Learning Models for Black Lung Detection on Chest X-Ray Radiographs,"Devnath, Liton and Luo, Suhuai and Summons, Peter and Wang, Dadong",2020,Association for Computing Machinery,,https://doi.org/10.1145/3378936.3378968,10.1145/3378936.3378968,"Black Lung (BL) is an incurable respiratory disease caused by long term inhalation of respirable coal dust. Confidentiality restrictions and disease incidence limit the availability of BL datasets, which presents significant challenges in the training of deep learning (DL) models. This paper presents the implementations and detailed performance comparison of seven DL models for BL detection with small datasets. The models include VGG16, VGG19, InceptionV3, Xception, ResNet50, DenseNet121 and CheXNet. A small BL dataset of real and synthetic images was used to train the seven deep learning models. Segmented lung X-ray images, with and without BL, were used as training images to establish a benchmark. To increase the number of images required for training a deep learning system the training data set was augmented, using a Cycle-Consistent Adversarial Networks (CycleGAN) and the Keras Image Data Generator, to generate additional augmented and synthetic radiographs. The effects of different dropout nodes as a blocking factor was also investigated on all seven models. The best sensitivity (Normal Prediction Rate), specificity (BL prediction Rate), error rate (ERR or incorrect prediction rate), accuracy (1-ERR), as well as total execution time for binary classification for each model, with and without augmentation, was compared for optimal BL detection. On average, the CheXNet model gave the best performance of all seven DL models.","Keras, Computer-Aided Diagnosis, CycleGAN, Black Lung, Pneumoconiosis, Coal Workers' Pneumoconiosis, Deep Learning",150–154,5,Proceedings of the 3rd International Conference on Software Engineering and Information Management
360,@inproceedings: 10.1145/3378936.3378970,A Transfer Learning Approach for Handwritten Numeral Digit Recognition,"Zhang, Le",2020,Association for Computing Machinery,,https://doi.org/10.1145/3378936.3378970,10.1145/3378936.3378970,"Handwritten numeral digit recognition is a classical problem in the field of computer vision, which has a wide range of applications in various fields including financial and post services. The accuracy of handwritten numeral digit recognition has been greatly improved by using deep learning in the past few years. However, deep learning relies on a large amount of training data and time-consuming calculation. In this paper, we adopt a transfer learning approach for handwritten numeral digit recognition and use both the multi-layer perceptron and convolutional neural network models to share the feature extraction process among five handwritten numerical datasets, namely, Tibetan, Arabic, Bangla, Devanagari, and Telugu. We compare the transfer learning scheme with the model based on a single dataset. We find that using the transfer learning method can significantly reduce the training time of the deep learning models, and slightly reduces the recognition accuracy.","Multi-layer Perceptron, Deep Learning, Convolutional Neural Network, Transfer Learning, Handwritten Numeral Digit Recognition",140–145,6,Proceedings of the 3rd International Conference on Software Engineering and Information Management
361,@inproceedings: 10.1145/3378936.3378978,Massively Scalable Image Processing on the HPCC Systems Big Data Platform,"Hukkeri, Tanmay Sanjay and G, Shobha and Phal, Shubham Milind and Shetty, Jyothi and R, Yatish H and Mohammed, Naweed",2020,Association for Computing Machinery,,https://doi.org/10.1145/3378936.3378978,10.1145/3378936.3378978,"Today's fast-moving world sees an abundance of image data in everyday life. From messages to insurance claims to even judicial systems, image data plays a pivotal role in facilitating several critical Big Data applications. Some of these applications such as automatic license plate recognition (ALPR) use CCTV cameras to capture snapshots of traffic from real-time video, inadvertently resulting in the generation vast amounts of image data on a daily basis. This brings with it the herculean task of processing these images to extract the essential information as efficiently as possible. The conventional method of processing images in a sequential manner can be very time consuming on account of the vast multitude of images and the intensive computation involved in order to process these. Distributed image processing seeks to provide a solution to this problem by splitting the computations involved across multiple nodes. This paper presents a novel framework to implement distributed image processing via OpenCV on HPCC Systems distributed node architecture*, a set of high-performance computing clusters. The proposed approach when tested on the Indian License Plates Dataset was found to be 85 percent accurate. Additionally, a 30 percent decrease in computation time was observed when executed on a multi-node setup without any impact to accuracy.","Big Data, Image Processing, Automatic License Plate Recognition, HPCC Systems, OpenCV, Distributed Computing, Machine Learning",26–31,6,Proceedings of the 3rd International Conference on Software Engineering and Information Management
362,@inproceedings: 10.1145/3378936.3378941,Efficient Semantic Segmentation through Dense Upscaling Convolutions,"Schoenhoff, Kurt and Holdsworth, Jason and Lee, Ickjai",2020,Association for Computing Machinery,,https://doi.org/10.1145/3378936.3378941,10.1145/3378936.3378941,"Semantic segmentation is the classification of each pixel in an image to an object, the resultant pixel map has significant usage in many fields. Some fields where this technology is being actively researched is in medicine, agriculture and robotics. For uses where the resources or power requirements are restricted such as robotics or where large amounts of images are required to process, efficiency can be key to the feasibility of a technique. Other applications that require real-time processing have a need for fast and efficient methods, especially where collision avoidance or safety may be involved. We take a combination of existing semantic segmentation methods and improve upon the efficiency by the replacement of the decoder network in ERFNet with a method based upon Dense Upscaling Convolutions, we then add a novel layer that allows the fine tuning of the decoder channel depth and therefore the efficiency of the network. Our proposed modification achieves 20-30% improvement in efficiency on moderate hardware (Nvidia GTX 960) over the original ERFNET and an additional 10% efficiency over the original Dense Upscaling Convolution. We perform a series of experiments to determine viable hyperparameters for the modification and measure the efficiency and accuracy over a range of image sizes, proving the viability of our approach.","Classification, Efficiency, CNN, Deep Learning, Semantic Segmentation, Computer Vision, Image Processing",244–248,5,Proceedings of the 3rd International Conference on Software Engineering and Information Management
363,@inproceedings: 10.1145/3371158.3371194,Weakly-Supervised Deep Learning for Domain Invariant Sentiment Classification,"Kayal, Pratik and Singh, Mayank and Goyal, Pawan",2020,Association for Computing Machinery,,https://doi.org/10.1145/3371158.3371194,10.1145/3371158.3371194,"The task of learning a sentiment classification model that adapts well to any target domain, different from the source domain, is a challenging problem. Majority of the existing approaches focus on learning a common representation by leveraging both source and target data during training. In this paper, we introduce a two-stage training procedure that leverages weakly supervised datasets for developing simple lift-and-shift-based predictive models without being exposed to the target domain during the training phase. Experimental results show that transfer with weak supervision from a source domain to various target domains provides performance very close to that obtained via supervised training on the target domain itself.","Domain Transfer, Sentiment Analysis, Weakly labeled datasets",239–243,5,Proceedings of the 7th ACM IKDD CoDS and 25th COMAD
364,@inproceedings: 10.1145/3371158.3371196,PlantDoc: A Dataset for Visual Plant Disease Detection,"Singh, Davinder and Jain, Naman and Jain, Pranjali and Kayal, Pratik and Kumawat, Sudhakar and Batra, Nipun",2020,Association for Computing Machinery,,https://doi.org/10.1145/3371158.3371196,10.1145/3371158.3371196,"India loses 35% of the annual crop yield due to plant diseases. Early detection of plant diseases remains difficult due to the lack of lab infrastructure and expertise. In this paper, we explore the possibility of computer vision approaches for scalable and early plant disease detection. The lack of availability of sufficiently large-scale non-lab data set remains a major challenge for enabling vision based plant disease detection. Against this background, we present PlantDoc: a dataset for visual plant disease detection. Our dataset contains 2,598 data points in total across 13 plant species and up to 17 classes of diseases, involving approximately 300 human hours of effort in annotating internet scraped images. To show the efficacy of our dataset, we learn 3 models for the task of plant disease classification. Our results show that modelling using our dataset can increase the classification accuracy by up to 31%. We believe that our dataset can help reduce the entry barrier of computer vision techniques in plant disease detection.","Object Detection, Deep Learning, Image Classification",249–253,5,Proceedings of the 7th ACM IKDD CoDS and 25th COMAD
365,@inproceedings: 10.1145/3371158.3371162,Meta-Learning for Few-Shot Time Series Classification,"Narwariya, Jyoti and Malhotra, Pankaj and Vig, Lovekesh and Shroff, Gautam and Vishnu, T. V.",2020,Association for Computing Machinery,,https://doi.org/10.1145/3371158.3371162,10.1145/3371158.3371162,"Deep neural networks (DNNs) have achieved state-of-the-art results on time series classification (TSC) tasks. In this work, we focus on leveraging DNNs in the often-encountered practical scenario where access to labeled training data is difficult, and where DNNs would be prone to overfitting. We leverage recent advancements in gradient-based meta-learning, and propose an approach to train a residual neural network with convolutional layers as a meta-learning agent for few-shot TSC. The network is trained on a diverse set of few-shot tasks sampled from various domains (e.g. healthcare, activity recognition, etc.) such that it can solve a target task from another domain using only a small number of training samples from the target task. Most existing meta-learning approaches are limited in practice as they assume a fixed number of target classes across tasks. We overcome this limitation in order to train a common agent across domains with each domain having different number of target classes, we utilize a triplet-loss based learning procedure that does not require any constraints to be enforced on the number of classes for the few-shot TSC tasks. To the best of our knowledge, we are the first to use meta-learning based pre-training for TSC. Our approach sets a new benchmark for few-shot TSC, outperforming several strong baselines on few-shot tasks sampled from 41 datasets in UCR TSC Archive. We observe that pre-training under the meta-learning paradigm allows the network to quickly adapt to new unseen tasks with small number of labeled instances.","Convolutional Neural Networks, Time Series Classification, Few-Shot Learning, Meta-Learning",28–36,9,Proceedings of the 7th ACM IKDD CoDS and 25th COMAD
366,@inproceedings: 10.1145/3371158.3371165,A Unified System for Aggression Identification in English Code-Mixed and Uni-Lingual Texts,"Khandelwal, Anant and Kumar, Niraj",2020,Association for Computing Machinery,,https://doi.org/10.1145/3371158.3371165,10.1145/3371158.3371165,"Wide usage of social media platforms has increased the risk of aggression, which results in mental stress and affects the lives of people negatively like psychological agony, fighting behavior, and disrespect to others. Majority of such conversations contains code-mixed languages[28]. Additionally, the way used to express thought or communication style also changes from one social media platform to another platform (e.g., communication styles are different in twitter and Facebook). These all have increased the complexity of the problem. To solve these problems, we have introduced a unified and robust multi-modal deep learning architecture which works for English code-mixed dataset and uni-lingual English dataset both. The devised system, uses psycho-linguistic features and very basic linguistic features. Our multi-modal deep learning architecture contains, Deep Pyramid CNN, Pooled BiLSTM, and Disconnected RNN(with Glove and FastText embedding, both). Finally, the system takes the decision based on model averaging. We evaluated our system on English Code-Mixed TRAC1 2018 dataset and uni-lingual English dataset obtained from Kaggle2. Experimental results show that our proposed system outperforms all the previous approaches on English code-mixed dataset and uni-lingual English dataset.","Disconnected RNN, Pooled BiLSTM, Model Averaging, English code-mixed Text, Deep Pyramid CNN",55–64,10,Proceedings of the 7th ACM IKDD CoDS and 25th COMAD
367,@inproceedings: 10.1145/3371158.3371202,Knowledge Graph Based Automated Generation of Test Cases in Software Engineering,"Nayak, Anmol and Kesri, Vaibhav and Dubey, Rahul Kumar",2020,Association for Computing Machinery,,https://doi.org/10.1145/3371158.3371202,10.1145/3371158.3371202,"Knowledge Graph (KG) is extremely efficient in storing and retrieving information from data that contains complex relationships between entities. Such a representation is relevant in software engineering projects, which contain large amounts of inter-dependencies between classes, modules, functions etc. In this paper, we propose a methodology to create a KG from software engineering documents that will be used for automated generation of test cases from natural (domain) language requirement statements. We propose a KG creation tool that includes a novel Constituency Parse Tree (CPT) based path finding algorithm for test intent extraction, Conditional Random field (CRF) based Named Entity Recognition (NER) model with automatic feature engineering and a Sentence vector embedding based signal extraction. This paper demonstrates the contributions on an automotive domain software project.","Named Entity Recognition (NER), Constituency Parse Tree (CPT), Knowledge Graph (KG), Requirement to Test Case generation",289–295,7,Proceedings of the 7th ACM IKDD CoDS and 25th COMAD
368,@inproceedings: 10.1145/3371158.3371167,Temporal Prediction of Socio-Economic Indicators Using Satellite Imagery,"Bansal, Chahat and Jain, Arpit and Barwaria, Phaneesh and Choudhary, Anuj and Singh, Anupam and Gupta, Ayush and Seth, Aaditeshwar",2020,Association for Computing Machinery,,https://doi.org/10.1145/3371158.3371167,10.1145/3371158.3371167,"Machine learning models based on satellite data have been actively researched to serve as a proxy for the prediction of socio-economic development indicators. Such models have however rarely been tested for transferability over time, i.e. whether models learned on data for a certain year are able to make accurate predictions on data for another year. Using a dataset from the Indian census at two time points, for the years 2001 and 2011, we evaluate the temporal transferability of a simple machine learning model at sub-national scales of districts and propose a generic method to improve its performance. This method can be especially relevant when training datasets are small to train a robust prediction model. Then, we go further to build an aggregate development index at the district-level, on the lines of the Human Development Index (HDI) and demonstrate high accuracy in predicting the index based on satellite data for different years. This can be used to build applications to guide data-driven policy making at fine spatial and temporal scales, without the need to conduct frequent expensive censuses and surveys on the ground.","census, poverty mapping, temporal prediction, satellite imagery, Landsat, socio-economic development",73–81,9,Proceedings of the 7th ACM IKDD CoDS and 25th COMAD
369,@inproceedings: 10.1145/3371158.3371200,On-Device Information Extraction from Screenshots in Form of Tags,"Kumar, Sumit and Ramena, Gopi and Goyal, Manoj and Mohanty, Debi and Agarwal, Ankur and Changmai, Benu and Moharana, Sukumar",2020,Association for Computing Machinery,,https://doi.org/10.1145/3371158.3371200,10.1145/3371158.3371200,"We propose a method to make mobile Screenshots easily searchable. In this paper, we present the workflow in which we: 1) pre-processed a collection of screenshots, 2) identified script present in image, 3) extracted unstructured text from images, 4) identified language of the extracted text, 5) extracted keywords from the text, 6) identified tags based on image features, 7) expanded tag set by identifying related keywords, 8) inserted image tags with relevant images after ranking and indexed them to make it searchable on device. We made the pipeline which supports multiple languages and executed it on-device, which addressed privacy concerns. We developed novel architectures for components in the pipeline, optimized performance and memory for on-device computation. We observed from experimentation that the solution developed can reduce overall user effort and improve end user experience while searching, whose results are published.","tag expansion, on-device tag extraction, tag recommendation, on-device search",275–281,7,Proceedings of the 7th ACM IKDD CoDS and 25th COMAD
370,@inproceedings: 10.1145/3371158.3371168,Deep Reinforcement Learning for Single-Shot Diagnosis and Adaptation in Damaged Robots,"Verma, Shresth and Nair, Haritha S. and Agarwal, Gaurav and Dhar, Joydip and Shukla, Anupam",2020,Association for Computing Machinery,,https://doi.org/10.1145/3371158.3371168,10.1145/3371158.3371168,"Robotics has proved to be an indispensable tool in many industrial as well as social applications, such as warehouse automation, manufacturing, disaster robotics, etc. In most of these scenarios, damage to the agent while accomplishing mission-critical tasks can result in failure. To enable robotic adaptation in such situations, the agent needs to adopt policies which are robust to a diverse set of damages and must do so with minimum computational complexity. We thus propose a damage aware control architecture which diagnoses the damage prior to gait selection while also incorporating domain randomization in the damage space for learning a robust policy. To implement damage awareness, we have used a Long Short Term Memory based supervised learning network which diagnoses the damage and predicts the type of damage. The main novelty of this approach is that only a single policy is trained to adapt against a wide variety of damages and the diagnosis is done in a single trial at the time of damage.","Gait Selection, LSTM, Reinforcement Learning, Domain Adaptation, Damage recovery",82–89,8,Proceedings of the 7th ACM IKDD CoDS and 25th COMAD
371,@inproceedings: 10.1145/3371158.3371187,Computational Fact Validation from Knowledge Graph Using Structured and Unstructured Information,"Khandelwal, Saransh and Kumar, Dhananjay",2020,Association for Computing Machinery,,https://doi.org/10.1145/3371158.3371187,10.1145/3371158.3371187,"In today's world, data or information is increasing at an exponential rate, and so is the fake news. Traditional fact-checking methods like fake news detection by experts, analysts, or some organizations do not match with the volume of information available. This is where the problem of computational fact-checking or validation becomes relevant. Given a Knowledge Graph, a knowledge corpus, and a fact (triple statement), the goal of fact-checking is to decide whether the fact or knowledge is correct or not. Existing approaches extensively used several structural features of the input Knowledge Graph to address the mentioned problem. In this work, our primary focus would be to leverage the unstructured information along with the structured ones. Our approach considers finding evidence from Wikipedia and structured information from Wikidata, which helps in determining the validity of the input facts. As features from the structured domain, we have used TransE embedding considering components of the input fact. The similarity of input fact with elements of relevant Wikipedia pages has been used as unstructured features. The experiments with a dataset consisting of nine relations of Wikidata has established the advantage of combining unstructured features with structured features for the given task.","RDF triples, Supervised Learning, Unstructured information, Knowledge representation, Fact validation, Data Mining, Knowledge Graph",204–208,5,Proceedings of the 7th ACM IKDD CoDS and 25th COMAD
372,@inproceedings: 10.1145/3371158.3371195,A Hybrid Distributed Model for Learning Representation of Short Texts with Attribute Labels,"Kumar, Shashi and Roy, Suman and Pathak, Vishal",2020,Association for Computing Machinery,,https://doi.org/10.1145/3371158.3371195,10.1145/3371158.3371195,"Short text documents in real-world applications, such as incident tickets, bug tickets, feedback texts etc. contain fixed field entries in the form of certain attribute instances as well as free text entries capturing the summaries of them. We propose an approach based on the Paragraph Vector (due to Le and Mikolov) to learn fixed-length feature representation from these short texts of varying lengths appended with attribute instances. Our method contributes to the existing approach by learning representation from summary of tickets as well as their attribute contents captured using fixed field entries. Further we show such representation of short texts produce better performance on a few learning tasks compared to the other popular representations.",,244–248,5,Proceedings of the 7th ACM IKDD CoDS and 25th COMAD
373,@inproceedings: 10.1145/3371158.3371160,Human-Machine Collaboration for Face Recognition,"Ravindranath, Saurabh and Baburaj, Rahul and Balasubramanian, Vineeth N. and Namburu, NageswaraRao and Gujar, Sujit and Jawahar, C. V.",2020,Association for Computing Machinery,,https://doi.org/10.1145/3371158.3371160,10.1145/3371158.3371160,"Despite advances in deep learning and facial recognition techniques, the problem of fault-intolerant facial recognition remains challenging. With the current state of progress in the field of automatic face recognition and the in-feasibility of fully manual recognition, the situation calls for human-machine collaborative methods. We design a system that uses machine predictions for a given face to generate queries that are answered by human experts to provide the system with the information required to predict the identity of the face correctly. We use a Markov Decision Process for which we devise an appropriate query structure and a reward structure to generate these queries in a budget or accuracy-constrained setting. Finally, as we do not know the capabilities of the human experts involved, we model each human as a bandit and adopt a multi-armed bandit approach with consensus queries to efficiently estimate their individual accuracies, enabling us to maximize the accuracy of our system. Through careful analysis and experimentation on real-world data-sets using humans, we show that our system outperforms methods that exploit only machine intelligence, simultaneously being highly cost-efficient as compared to fully manual methods. In summary, our system uses human-machine collaboration for face recognition problem more intelligently and efficiently.","multi-armed bandits, crowdsourcing, Markov Decision Process, facial recognition techniques",10–18,9,Proceedings of the 7th ACM IKDD CoDS and 25th COMAD
374,@inproceedings: 10.1145/3371158.3371183,Co-Clustering Triples from Open Information Extraction,"Pal, Koninika and Ho, Vinh Thinh and Weikum, Gerhard",2020,Association for Computing Machinery,,https://doi.org/10.1145/3371158.3371183,10.1145/3371158.3371183,"Similar facts are often expressed in different ways in natural language text, which introduces the redundancy and ambiguity of Subject-Predicate-Object (SPO) triples in Open Information Extraction (Open IE). This work focuses on canonicalizing such SPO triples. We propose a clustering framework using non-negative matrix tri-factorization that jointly clusters predicate phrases and subject-object pairs, and aligns them in a meaningful manner. The evaluation shows that our co-clustering method outperforms significantly over rule mining and Knowledge-Base-embedding approaches for two existing datasets.","Co-clustering, Matrix Factorization, Knowledge Bases",190–194,5,Proceedings of the 7th ACM IKDD CoDS and 25th COMAD
375,@inproceedings: 10.1145/3371158.3371182,IIITM Face: A Database for Facial Attribute Detection in Constrained and Simulated Unconstrained Environments,"Arya, K. V. and Verma, Shresth and Gupta, Raj Kuwar and Agarwal, Soumya and Gupta, Prince",2020,Association for Computing Machinery,,https://doi.org/10.1145/3371158.3371182,10.1145/3371158.3371182,"This paper addresses the challenges of face attribute detection specifically in the Indian context. While there are numerous face datasets in unconstrained environments, none of them captures emotions in different facial orientations. Moreover, there is an under-representation of people of Indian ethnicity in these datasets since they have been scraped from popular search engines. As a result, the performance of state-of-the-art techniques can't be evaluated on Indian faces. In this work, we introduce a new dataset IIITM Face for scientific community to address these challenges. Our dataset includes 107 participants who exhibit 6 emotions in 3 different face orientations. Each of theses images are further labelled on attributes like gender, presence of moustache, beard or eyeglasses, clothes worn by the subjects and the density of their hair. Moreover, the images are captured in high resolution with specific background colors which can be easily replaced by cluttered backgrounds to simulate 'in the Wild' behavior. We demonstrate the same by constructing IIITM Face-SUE. Both IIITM Face and IIITM Face-SUE have been benchmarked across key multi-label metrics for the research community to compare their results.","facial attribute classification, multi-label classification, emotion recognition, multi-task learning",185–189,5,Proceedings of the 7th ACM IKDD CoDS and 25th COMAD
376,@inproceedings: 10.1145/3371158.3371176,Deep Neural Learning for Automated Diagnostic Code Group Prediction Using Unstructured Nursing Notes,"Jayasimha, Aditya and Gangavarapu, Tushaar and Kamath, S. Sowmya and Krishnan, Gokul S.",2020,Association for Computing Machinery,,https://doi.org/10.1145/3371158.3371176,10.1145/3371158.3371176,"Disease prediction, a central problem in clinical care and management, has gained much significance over the last decade. Nursing notes documented by caregivers contain valuable information concerning a patient's state, which can aid in the development of intelligent clinical prediction systems. Moreover, due to the limited adaptation of structured electronic health records in developing countries, the need for disease prediction from such clinical text has garnered substantial interest from the research community. The availability of large, publicly available databases such as MIMIC-III, and advancements in machine and deep learning models with high predictive capabilities have further facilitated research in this direction. In this work, we model the latent knowledge embedded in the unstructured clinical nursing notes, to address the clinical task of disease prediction as a multi-label classification of ICD-9 code groups. We present EnTAGS, which facilitates aggregation of the data in the clinical nursing notes of a patient, by modeling them independent of one another. To handle the sparsity and high dimensionality of clinical nursing notes effectively, our proposed EnTAGS is built on the topics extracted using Non-negative matrix factorization. Furthermore, we explore the applicability of deep learning models for the clinical task of disease prediction, and assess the reliability of the proposed models using standard evaluation metrics. Our experimental evaluation revealed that the proposed approach consistently exceeded the state-of-the-art prediction model by 1.87% in accuracy, 12.68% in AUPRC, and 11.64% in MCC score.","Deep Learning, Natural Language Processing, Clinical Decision Support Systems, Disease Prediction, Healthcare Analytics, Multi-label Classification",152–160,9,Proceedings of the 7th ACM IKDD CoDS and 25th COMAD
377,@article: 10.1145/3368305,DNNTune: Automatic Benchmarking DNN Models for Mobile-Cloud Computing,"Xia, Chunwei and Zhao, Jiacheng and Cui, Huimin and Feng, Xiaobing and Xue, Jingling",2019,Association for Computing Machinery,1544-3566,https://doi.org/10.1145/3368305,10.1145/3368305,"Deep Neural Networks (DNNs) are now increasingly adopted in a variety of Artificial Intelligence (AI) applications. Meantime, more and more DNNs are moving from cloud to the mobile devices, as emerging AI chips are integrated into mobiles. Therefore, the DNN models can be deployed in the cloud, on the mobile devices, or even mobile-cloud coordinate processing, making it a big challenge to select an optimal deployment strategy under specific objectives.This article proposes a DNN tuning framework, i.e., DNNTune, that can provide layer-wise behavior analysis across a number of platforms. Using DNNTune, this article further selects 13 representative DNN models, including CNN, LSTM, and MLP, and three mobile devices ranging from low-end to high-end, and two AI accelerator chips to characterize the DNN models on these devices to further assist users finding opportunities for mobile-cloud coordinate computing. Our experimental results demonstrate that DNNTune can find a coordinated deployment achieving up to 1.66\texttimes{} speedup and 15\texttimes{} energy saving comparing with mobile-only and cloud-only deployment.","mobile-cloud computing, heterogeneous computing, DN",,26,ACM Trans. Archit. Code Optim.
378,@article: 10.1145/3356773.3356806,"SST'19 - Software and Systems Traceability: Summary of the 10th International Workshop at the 41st International Conference on Software Engineering (ICSE), May 27, 2019","Stegh\""{o}fer, Jan-Philipp and Niu, Nan and Guo, Jin L.C. and Mahmoud, Anas",2019,Association for Computing Machinery,0163-5948,https://doi.org/10.1145/3356773.3356806,10.1145/3356773.3356806,"Traceability is the ability to relate di erent artifacts during the development and operation of a system to each other. It enables program comprehension, change impact analysis, and facilitates the cooperation of engineers from di erent disciplines. The 10th International Workshop on Software and Systems Traceability (former International Workshop on Traceability in Emerging Forms of Software Engineering, TEFSE), explored the role and impact of traceability in modern software and systems development. The event brought together researchers and practitioners to examine the challenges of recovering, maintaining, and utilizing traceability for the myriad forms of software and systems engineering artifacts. SST'19 was a highly interactive working event focused on discussing the main problems related to software traceability in particular in the context of opportunities and challenges posed by the recent progress in Arti cial Intelligence techniques and proposing possible solutions for such problems.","artificial intelligence, traceability, requirement",43–47,5,SIGSOFT Softw. Eng. Notes
379,@inproceedings: 10.1145/3359996.3364239,CompRate: Power Efficient Heart Rate and Heart Rate Variability Monitoring on Smart Wearables,"Dissanayake, Vipula and Elvitigala, Don Samitha and Zhang, Haimo and Weerasinghe, Chamod and Nanayakkara, Suranga",2019,Association for Computing Machinery,,https://doi.org/10.1145/3359996.3364239,10.1145/3359996.3364239,"Currently, smartwatches are equipped with Photoplethysmography (PPG) sensors to measure Heart Rate (HR) and Heart Rate Variability (HRV). However, PPG sensors consume considerably high energy, making it impractical to monitor HR &amp; HRV continuously for an extended period. Utilising low power accelerometers to estimate HR has been broadly discussed in previous decades. Inspired by prior work, we introduce CompRate, an alternative method to measure HR continuously for an extended period in low-intensity physical activities. CompRate model calibrated for individual users only has an average performance of Root Mean Squared Error (RMSE) 1.58 Beats Per Minute (BPM). Further, CompRate used 3.75 times less energy compared to the built-in PPG sensor. We also demonstrate that CompRate model can be extended to predict HRV. We will demonstrate CompRate in several application scenarios: self-awareness of fatigue and just-in-time interruption while driving; enabling teachers to be aware of students’ mental effort during a learning activity; and the broadcasting of the location of live victims in a disaster situation.","Heart Rate, Heart Rate Variability, Accelerometer, Photoplethysmography, Low Power, Inferring Stress",,8,25th ACM Symposium on Virtual Reality Software and Technology
380,@inproceedings: 10.1145/3359996.3364260,Floating-Point Precision and Deformation Awareness for Scalable and Robust 3D Face Alignment,"Morton, Jacob and Lee, Seungyong",2019,Association for Computing Machinery,,https://doi.org/10.1145/3359996.3364260,10.1145/3359996.3364260,"This paper improves the accuracy of heatmap-based 3D face alignment neural networks. Many current approaches in face alignment are limited by two major problems, quantization and the lack of regularization of heatmaps. The first limitation is caused by the non-differentiable argmax function, which extracts landmark coordinates from heatmaps as integer indices. Heatmaps are generated at low-resolution to reduce the memory and computational costs, which results in heatmaps far lower than the input image’s resolution. We propose a heatmap generator network producing floating-point precision heatmaps that are scalable to higher-resolutions. To resolve the second limitation, we propose a novel deformation constraint on heatmaps. The constraint is based on graph-Laplacian and enables a heatmap generator to regularize overall shape of the output face landmarks using the global face structure. By eliminating quantization and including regularization, our method can vastly improve landmark localization accuracy, and achieves the state-of-the-art performance without adding complex network structures.","neural networks, heatmap, soft-argmax, graph Laplacian, face alignment",,10,25th ACM Symposium on Virtual Reality Software and Technology
381,@inproceedings: 10.1109/ASE.2019.00031,Emotions Extracted from Text vs. True Emotions: An Empirical Evaluation in SE Context,"Wang, Yi",2019,IEEE Press,,https://doi.org/10.1109/ASE.2019.00031,10.1109/ASE.2019.00031,"Emotion awareness research in SE context has been growing in recent years. Currently, researchers often rely on textual communication records to extract emotion states using natural language processing techniques. However, how well these extracted emotion states reflect people's real emotions has not been thoroughly investigated. In this paper, we report a multi-level, longitudinal empirical study with 82 individual members in 27 project teams. We collected their self-reported retrospective emotion states on a weekly basis during their year-long projects and also extracted corresponding emotions from the textual communication records. We then model and compare the dynamics of these two types of emotions using multiple statistical and time series analysis methods. Our analyses yield a rich set of findings. The most important one is that the dynamics of emotions extracted using text-based algorithms often do not well reflect the dynamics of self-reported retrospective emotions. Besides, the extracted emotions match self-reported retrospective emotions better at the team-level. Our results also suggest that individual personalities and the team's emotion display norms significantly impact the match/mismatch. Our results should warn the research community about the limitations and challenges of applying text-based emotion recognition tools in SE research.",,230–242,13,Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering
382,@inproceedings: 10.1109/ASE.2019.00108,CocoQa: Question Answering for Coding Conventions over Knowledge Graphs,"Du, Tianjiao and Cao, Junming and Wu, Qinyue and Li, Wei and Shen, Beijun and Chen, Yuting",2019,IEEE Press,,https://doi.org/10.1109/ASE.2019.00108,10.1109/ASE.2019.00108,"Coding convention plays an important role in guaranteeing software quality. However, coding conventions are usually informally presented and inconvenient for programmers to use. In this paper, we present CocoQa, a system that answers programmer's questions about coding conventions. CocoQa answers questions by querying a knowledge graph for coding conventions. It employs 1) a subgraph matching algorithm that parses the question into a SPARQL query, and 2) a machine comprehension algorithm that uses an end-to-end neural network to detect answers from searched paragraphs. We have implemented CocoQa, and evaluated it on a coding convention QA dataset. The results show that CocoQa can answer questions about coding conventions precisely. In particular, CocoQa can achieve a precision of 82.92% and a recall of 91.10%.","knowledge graph, question answering, coding convention",1086–1089,4,Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering
383,@inproceedings: 10.1109/ASE.2019.00137,Lancer: Your Code Tell Me What You Need,"Zhou, Shufan and Shen, Beijun and Zhong, Hao",2019,IEEE Press,,https://doi.org/10.1109/ASE.2019.00137,10.1109/ASE.2019.00137,"Programming is typically a difficult and repetitive task. Programmers encounter endless problems during programming, and they often need to write similar code over and over again. To prevent programmers from reinventing wheels thus increase their productivity, we propose a context-aware code-to-code recommendation tool named Lancer. With the support of a Library-Sensitive Language Model (LSLM) and the BERT model, Lancer is able to automatically analyze the intention of the incomplete code and recommend relevant and reusable code samples in real-time. A video demonstration of Lancer can be found at https://youtu.be/tO9nhqZY35g. Lancer is open source and the code is available at https://github.com/sfzhou5678/Lancer.","code recommendation, code reuse, language model",1202–1205,4,Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering
384,@inproceedings: 10.1109/ASE.2019.00099,CLCDSA: Cross Language Code Clone Detection Using Syntactical Features and API Documentation,"Nafi, Kawser Wazed and Kar, Tonny Shekha and Roy, Banani and Roy, Chanchal K. and Schneider, Kevin A.",2019,IEEE Press,,https://doi.org/10.1109/ASE.2019.00099,10.1109/ASE.2019.00099,"Software clones are detrimental to software maintenance and evolution and as a result many clone detectors have been proposed. These tools target clone detection in software applications written in a single programming language. However, a software application may be written in different languages for different platforms to improve the application's platform compatibility and adoption by users of different platforms. Cross language clones (CLCs) introduce additional challenges when maintaining multi-platform applications and would likely go undetected using existing tools. In this paper, we propose CLCDSA, a cross language clone detector which can detect CLCs without extensive processing of the source code and without the need to generate an intermediate representation. The proposed CLCDSA model analyzes different syntactic features of source code across different programming languages to detect CLCs. To support large scale clone detection, the CLCDSA model uses an action filter based on cross language API call similarity to discard non-potential clones. The design methodology of CLCDSA is twofold: (a) it detects CLCs on the fly by comparing the similarity of features, and (b) it uses a deep neural network based feature vector learning model to learn the features and detect CLCs. Early evaluation of the model observed an average precision, recall and F-measure score of 0.55, 0.86, and 0.64 respectively for the first phase and 0.61, 0.93, and 0.71 respectively for the second phase which indicates that CLCDSA outperforms all available models in detecting cross language clones.","source code syntax, Word2Vector, code clone, API documentation",1026–1037,12,Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering
385,@inproceedings: 10.1109/ASE.2019.00042,Continuous Incident Triage for Large-Scale Online Service Systems,"Chen, Junjie and He, Xiaoting and Lin, Qingwei and Zhang, Hongyu and Hao, Dan and Gao, Feng and Xu, Zhangwei and Dang, Yingnong and Zhang, Dongmei",2019,IEEE Press,,https://doi.org/10.1109/ASE.2019.00042,10.1109/ASE.2019.00042,"In recent years, online service systems have become increasingly popular. Incidents of these systems could cause significant economic loss and customer dissatisfaction. Incident triage, which is the process of assigning a new incident to the responsible team, is vitally important for quick recovery of the affected service. Our industry experience shows that in practice, incident triage is not conducted only once in the beginning, but is a continuous process, in which engineers from different teams have to discuss intensively among themselves about an incident, and continuously refine the incident-triage result until the correct assignment is reached. In particular, our empirical study on 8 real online service systems shows that the percentage of incidents that were reassigned ranges from 5.43% to 68.26% and the number of discussion items before achieving the correct assignment is up to 11.32 on average. To improve the existing incident triage process, in this paper, we propose DeepCT, a Deep learning based approach to automated Continuous incident Triage. DeepCT incorporates a novel GRU-based (Gated Recurrent Unit) model with an attention-based mask strategy and a revised loss function, which can incrementally learn knowledge from discussions and update incident-triage results. Using DeepCT, the correct incident assignment can be achieved with fewer discussions. We conducted an extensive evaluation of DeepCT on 14 large-scale online service systems in Microsoft. The results show that DeepCT is able to achieve more accurate and efficient incident triage, e.g., the average accuracy identifying the responsible team precisely is 0.641~0.729 with the number of discussion items increasing from 1 to 5. Also, DeepCT statistically significantly outperforms the state-of-the-art bug triage approach.","online service systems, incident triage, deep learning",364–375,12,Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering
386,@inproceedings: 10.1109/ASE.2019.00011,Assessing the Generalizability of Code2vec Token Embeddings,"Kang, Hong Jin and Bissyand\'{e}, Tegawend\'{e} F. and Lo, David",2019,IEEE Press,,https://doi.org/10.1109/ASE.2019.00011,10.1109/ASE.2019.00011,"Many Natural Language Processing (NLP) tasks, such as sentiment analysis or syntactic parsing, have benefited from the development of word embedding models. In particular, regardless of the training algorithms, the learned embeddings have often been shown to be generalizable to different NLP tasks. In contrast, despite recent momentum on word embeddings for source code, the literature lacks evidence of their generalizability beyond the example task they have been trained for.In this experience paper, we identify 3 potential downstream tasks, namely code comments generation, code authorship identification, and code clones detection, that source code token embedding models can be applied to. We empirically assess a recently proposed code token embedding model, namely code2vec's token embeddings. Code2vec was trained on the task of predicting method names, and while there is potential for using the vectors it learns on other tasks, it has not been explored in literature. Therefore, we fill this gap by focusing on its generalizability for the tasks we have identified. Eventually, we show that source code token embeddings cannot be readily leveraged for the downstream tasks. Our experiments even show that our attempts to use them do not result in any improvements over less sophisticated methods. We call for more research into effective and general use of code embeddings.","distributed representations, big code, code embeddings",1–12,12,Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering
387,@inproceedings: 10.1109/ASE.2019.00014,AutoFocus: Interpreting Attention-Based Neural Networks by Code Perturbation,"Bui, Nghi D. Q. and Yu, Yijun and Jiang, Lingxiao",2019,IEEE Press,,https://doi.org/10.1109/ASE.2019.00014,10.1109/ASE.2019.00014,"Despite being adopted in software engineering tasks, deep neural networks are treated mostly as a black box due to the difficulty in interpreting how the networks infer the outputs from the inputs. To address this problem, we propose AutoFocus, an automated approach for rating and visualizing the importance of input elements based on their effects on the outputs of the networks. The approach is built on our hypotheses that (1) attention mechanisms incorporated into neural networks can generate discriminative scores for various input elements and (2) the discriminative scores reflect the effects of input elements on the outputs of the networks. This paper verifies the hypotheses by applying AutoFocus on the task of algorithm classification (i.e., given a program source code as input, determine the algorithm implemented by the program). AutoFocus identifies and perturbs code elements in a program systematically, and quantifies the effects of the perturbed elements on the network's classification results. Based on evaluation on more than 1000 programs for 10 different sorting algorithms, we observe that the attention scores are highly correlated to the effects of the perturbed code elements. Such a correlation provides a strong basis for the uses of attention scores to interpret the relations between code elements and the algorithm classification results of a neural network, and we believe that visualizing code elements in an input program ranked according to their attention scores can facilitate faster program comprehension with reduced code.","code perturbation, program comprehension, neural networks, explainability, interpretability, algorithm classification, attention mechanisms",38–41,4,Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering
388,@inproceedings: 10.1109/ASE.2019.00092,Improving the Decision-Making Process of Self-Adaptive Systems by Accounting for Tactic Volatility,"Palmerino, Jeffrey and Yu, Qi and Desell, Travis and Krutz, Daniel E.",2019,IEEE Press,,https://doi.org/10.1109/ASE.2019.00092,10.1109/ASE.2019.00092,"When self-adaptive systems encounter changes within their surrounding environments, they enact tactics to perform necessary adaptations. For example, a self-adaptive cloud-based system may have a tactic that initiates additional computing resources when response time thresholds are surpassed, or there may be a tactic to activate a specific security measure when an intrusion is detected. In real-world environments, these tactics frequently experience tactic volatility which is variable behavior during the execution of the tactic.Unfortunately, current self-adaptive approaches do not account for tactic volatility in their decision-making processes, and merely assume that tactics do not experience volatility. This limitation creates uncertainty in the decision-making process and may adversely impact the system's ability to effectively and efficiently adapt. Additionally, many processes do not properly account for volatility that may effect the system's Service Level Agreement (SLA). This can limit the system's ability to act proactively, especially when utilizing tactics that contain latency.To address the challenge of sufficiently accounting for tactic volatility, we propose a Tactic Volatility Aware (TVA) solution. Using Multiple Regression Analysis (MRA), TVA enables self-adaptive systems to accurately estimate the cost and time required to execute tactics. TVA also utilizes Autoregressive Integrated Moving Average (ARIMA) for time series forecasting, allowing the system to proactively maintain specifications.","machine learning, artificial intelligence, self-adaptation",949–961,13,Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering
389,@article: 10.1145/3291055,AUKE: Automatic Kernel Code Generation for an Analogue SIMD Focal-Plane Sensor-Processor Array,"Debrunner, Thomas and Saeedi, Sajad and Kelly, Paul H. J.",2019,Association for Computing Machinery,1544-3566,https://doi.org/10.1145/3291055,10.1145/3291055,"Focal-plane Sensor-Processor Arrays (FPSPs) are new imaging devices with parallel Single Instruction Multiple Data (SIMD) computational capabilities built into every pixel. Compared to traditional imaging devices, FPSPs allow for massive pixel-parallel execution of image processing algorithms. This enables the application of certain algorithms at extreme frame rates (&gt;10,000 frames per second). By performing some early-stage processing in-situ, systems incorporating FPSPs can consume less power compared to conventional approaches using standard digital cameras. In this article, we explore code generation for an FPSP whose 256 \texttimes{} 256 processors operate on analogue signal data, leading to further opportunities for power reduction—and additional code synthesis challenges.While rudimentary image processing algorithms have been demonstrated on FPSPs before, progress with higher-level computer vision algorithms has been sparse due to the unique architecture and limits of the devices. This article presents a code generator for convolution filters for the SCAMP-5 FPSP, with applications in many high-level tasks such as convolutional neural networks, pose estimation, and so on. The SCAMP-5 FPSP has no effective multiply operator. Convolutions have to be implemented through sequences of more primitive operations such as additions, subtractions, and multiplications/divisions by two. We present a code generation algorithm to optimise convolutions by identifying common factors in the different weights and by determining an optimised pattern of pixel-to-pixel data movements to exploit them. We present evaluation in terms of both speed and energy consumption for a suite of well-known convolution filters. Furthermore, an application of the method is shown by the implementation of a Viola-Jones face detection algorithm.","automatic code generation, Focal-plane sensor-processor arrays, kernel filterin",,26,ACM Trans. Archit. Code Optim.
390,@article: 10.1145/3291054,A System-Level Simulator for RRAM-Based Neuromorphic Computing Chips,"Lee, Matthew Kay Fei and Cui, Yingnan and Somu, Thannirmalai and Luo, Tao and Zhou, Jun and Tang, Wai Teng and Wong, Weng-Fai and Goh, Rick Siow Mong",2019,Association for Computing Machinery,1544-3566,https://doi.org/10.1145/3291054,10.1145/3291054,"Advances in non-volatile resistive switching random access memory (RRAM) have made it a promising memory technology with potential applications in low-power and embedded in-memory computing devices owing to a number of advantages such as low-energy consumption, low area cost and good scaling. There have been proposals to employ RRAM in architecting chips for neuromorphic computing and artificial neural networks where matrix-vector multiplication can be computed in the analog domain in a single timestep. However, it is challenging to employ RRAM devices in neuromorphic chips owing to the non-ideal behavior of RRAM. In this article, we propose a cycle-accurate and scalable system-level simulator that can be used to study the effects of using RRAM devices in neuromorphic computing chips. The simulator models a spatial neuromorphic chip architecture containing many neural cores with RRAM crossbars connected via a Network-on-Chip (NoC). We focus on system-level simulation and demonstrate the effectiveness of our simulator in understanding how non-linear RRAM effects such as stuck-at-faults (SAFs), write variability, and random telegraph noise (RTN) can impact an application’s behavior. By using our simulator, we show that RTN and write variability can have adverse effects on an application. Nevertheless, we show that these effects can be mitigated through proper design choices and the implementation of a write-verify scheme.","simulator, RRAM, Neuromorphic computin",,24,ACM Trans. Archit. Code Optim.
391,@article: 10.1145/3291053,The Art of Getting Deep Neural Networks in Shape,"Mammadli, Rahim and Wolf, Felix and Jannesari, Ali",2019,Association for Computing Machinery,1544-3566,https://doi.org/10.1145/3291053,10.1145/3291053,"Training a deep neural network (DNN) involves selecting a set of hyperparameters that define the network topology and influence the accuracy of the resulting network. Often, the goal is to maximize prediction accuracy on a given dataset. However, non-functional requirements of the trained network -- such as inference speed, size, and energy consumption -- can be very important as well. In this article, we aim to automate the process of selecting an appropriate DNN topology that fulfills both functional and non-functional requirements of the application. Specifically, we focus on tuning two important hyperparameters, depth and width, which together define the shape of the resulting network and directly affect its accuracy, speed, size, and energy consumption. To reduce the time needed to search the design space, we train a fraction of DNNs and build a model to predict the performances of the remaining ones. We are able to produce tuned ResNets, which are up to 4.22 times faster than original depth-scaled ResNets on a batch of 128 images while matching their accuracy.","Deep neural networks, parallel processing, computer visio",,21,ACM Trans. Archit. Code Optim.
392,@article: 10.1145/3290350,Bayesian Synthesis of Probabilistic Programs for Automatic Data Modeling,"Saad, Feras A. and Cusumano-Towner, Marco F. and Schaechtle, Ulrich and Rinard, Martin C. and Mansinghka, Vikash K.",2019,Association for Computing Machinery,,https://doi.org/10.1145/3290350,10.1145/3290350,"We present new techniques for automatically constructing probabilistic programs for data analysis, interpretation, and prediction. These techniques work with probabilistic domain-specific data modeling languages that capture key properties of a broad class of data generating processes, using Bayesian inference to synthesize probabilistic programs in these modeling languages given observed data. We provide a precise formulation of Bayesian synthesis for automatic data modeling that identifies sufficient conditions for the resulting synthesis procedure to be sound. We also derive a general class of synthesis algorithms for domain-specific languages specified by probabilistic context-free grammars and establish the soundness of our approach for these languages. We apply the techniques to automatically synthesize probabilistic programs for time series data and multivariate tabular data. We show how to analyze the structure of the synthesized programs to compute, for key qualitative properties of interest, the probability that the underlying data generating process exhibits each of these properties. Second, we translate probabilistic programs in the domain-specific language into probabilistic programs in Venture, a general-purpose probabilistic programming system. The translated Venture programs are then executed to obtain predictions of new time series data and new multivariate data records. Experimental results show that our techniques can accurately infer qualitative structure in multiple real-world data sets and outperform standard data analysis methods in forecasting and predicting new data.","model discovery, probabilistic programming, synthesis, Bayesian inferenc",,32,Proc. ACM Program. Lang.
393,@inproceedings: 10.1145/3301761.3301773,Sentiment Analysis of Thai Financial News,"Esichaikul, Vatcharaporn and Phumdontree, Chawisa",2018,Association for Computing Machinery,,https://doi.org/10.1145/3301761.3301773,10.1145/3301761.3301773,"Due to the big data and FinTech influencers, the novel SentiFine framework was developed to facilitate the financial analysts or specialists who need to understand the financial and economic circumstances from the daily news. The objective of this framework is to analyze the sentiment of Thai financial daily news by integrating the fine-grained sentiment analysis technique with the deep neural network. Based on the proposed SentiFine framework, the prototype of the SentiFine web-based system was developed. With the main feature ""View Daily News"", the users can view the daily Thai financial news detail, the date and time of the news, the source of news, and its sentiment which is categorized into three tones (positive, neutral, and negative).","Sentiment analysis, FinTech, thai natural language processing, deep neural network",39–43,5,Proceedings of the 2018 2nd International Conference on Software and E-Business
394,@inproceedings: 10.5555/3291291.3291305,Evaluating Music Mastering Quality Using Machine Learning,"Shtern, Mark and Casas, Pedro and Tzerpos, Vassilios",2018,IBM Corp.,,,,"Machine learning has been applied in a vast array of applications in the recent years, including several qualitative problems in the arts. However, in the world of music production, including mixing and mastering, most tasks are still performed by music professionals with decades of experience. Aspiring mastering engineers typically have to apprentice with professionals to learn their craft. Access to professionals is a scarce resource though, as they are typically very busy.In this paper, we present a method to evaluate the mastering quality of a piece of music automatically. We delegate the task of determining what we deem to be a subjectively well mastered song to professional mastering engineers. Using professionally mastered music, we derive datasets with varying degrees of deviation from the original music and train models to recognize the changes that have been made. This allows us to provide novice mastering engineers with an automatic rating of their work based on the magnitude of the deviation from the gold standard. We present experiments that demonstrate the accuracy of our approach, as well as a user study that shows how the results of our approach correlate to assessments made by human evaluators.","machine learning, computer music, computational creativity",126–135,10,Proceedings of the 28th Annual International Conference on Computer Science and Software Engineering
395,@inproceedings: 10.1145/3236024.3236074,Learning to Sample: Exploiting Similarities across Environments to Learn Performance Models for Configurable Systems,"Jamshidi, Pooyan and Velez, Miguel and K\""{a}stner, Christian and Siegmund, Norbert",2018,Association for Computing Machinery,,https://doi.org/10.1145/3236024.3236074,10.1145/3236024.3236074,"Most software systems provide options that allow users to tailor the system in terms of functionality and qualities. The increased flexibility raises challenges for understanding the configuration space and the effects of options and their interactions on performance and other non-functional properties. To identify how options and interactions affect the performance of a system, several sampling and learning strategies have been recently proposed. However, existing approaches usually assume a fixed environment (hardware, workload, software release) such that learning has to be repeated once the environment changes. Repeating learning and measurement for each environment is expensive and often practically infeasible. Instead, we pursue a strategy that transfers knowledge across environments but sidesteps heavyweight and expensive transfer-learning strategies. Based on empirical insights about common relationships regarding (i) influential options, (ii) their interactions, and (iii) their performance distributions, our approach, L2S (Learning to Sample), selects better samples in the target environment based on information from the source environment. It progressively shrinks and adaptively concentrates on interesting regions of the configuration space. With both synthetic benchmarks and several real systems, we demonstrate that L2S outperforms state of the art performance learning and transfer-learning approaches in terms of measurement effort and learning accuracy.","configurable systems, transfer learning, Software performance",71–82,12,Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
396,@inproceedings: 10.1145/3236024.3236085,Code Vectors: Understanding Programs through Embedded Abstracted Symbolic Traces,"Henkel, Jordan and Lahiri, Shuvendu K. and Liblit, Ben and Reps, Thomas",2018,Association for Computing Machinery,,https://doi.org/10.1145/3236024.3236085,10.1145/3236024.3236085,"With the rise of machine learning, there is a great deal of interest in treating programs as data to be fed to learning algorithms. However, programs do not start off in a form that is immediately amenable to most off-the-shelf learning techniques. Instead, it is necessary to transform the program to a suitable representation before a learning technique can be applied.  In this paper, we use abstractions of traces obtained from symbolic execution of a program as a representation for learning word embeddings. We trained a variety of word embeddings under hundreds of parameterizations, and evaluated each learned embedding on a suite of different tasks. In our evaluation, we obtain 93% top-1 accuracy on a benchmark consisting of over 19,000 API-usage analogies extracted from the Linux kernel. In addition, we show that embeddings learned from (mainly) semantic abstractions provide nearly triple the accuracy of those learned from (mainly) syntactic abstractions.","Program Understanding, Analogical Reasoning, Word Embeddings, Linux",163–174,12,Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
397,@inproceedings: 10.1145/3236024.3264835,DLFuzz: Differential Fuzzing Testing of Deep Learning Systems,"Guo, Jianmin and Jiang, Yu and Zhao, Yue and Chen, Quan and Sun, Jiaguang",2018,Association for Computing Machinery,,https://doi.org/10.1145/3236024.3264835,10.1145/3236024.3264835,"Deep learning (DL) systems are increasingly applied to safety-critical domains such as autonomous driving cars. It is of significant importance to ensure the reliability and robustness of DL systems. Existing testing methodologies always fail to include rare inputs in the testing dataset and exhibit low neuron coverage. In this paper, we propose DLFuzz, the first differential fuzzing testing framework to guide DL systems exposing incorrect behaviors. DLFuzz keeps minutely mutating the input to maximize the neuron coverage and the prediction difference between the original input and the mutated input, without manual labeling effort or cross-referencing oracles from other DL systems with the same functionality. We present empirical evaluations on two well-known datasets to demonstrate its efficiency. Compared with DeepXplore, the state-of-the-art DL whitebox testing framework, DLFuzz does not require extra efforts to find similar functional DL systems for cross-referencing check, but could generate 338.59% more adversarial inputs with 89.82% smaller perturbations, averagely obtain 2.86% higher neuron coverage, and save 20.11% time consumption.","Deep Learning, Neuron Coverage, Fuzzing Testing",739–743,5,Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
398,@inproceedings: 10.1145/3236024.3236082,MODE: Automated Neural Network Model Debugging via State Differential Analysis and Input Selection,"Ma, Shiqing and Liu, Yingqi and Lee, Wen-Chuan and Zhang, Xiangyu and Grama, Ananth",2018,Association for Computing Machinery,,https://doi.org/10.1145/3236024.3236082,10.1145/3236024.3236082,"Artificial intelligence models are becoming an integral part of modern computing systems. Just like software inevitably has bugs, models have bugs too, leading to poor classification/prediction accuracy. Unlike software bugs, model bugs cannot be easily fixed by directly modifying models. Existing solutions work by providing additional training inputs. However, they have limited effectiveness due to the lack of understanding of model misbehaviors and hence the incapability of selecting proper inputs. Inspired by software debugging, we propose a novel model debugging technique that works by first conducting model state differential analysis to identify the internal features of the model that are responsible for model bugs and then performing training input selection that is similar to program input selection in regression testing. Our evaluation results on 29 different models for 6 different applications show that our technique can fix model bugs effectively and efficiently without introducing new bugs. For simple applications (e.g., digit recognition), MODE improves the test accuracy from 75% to 93% on average whereas the state-of-the-art can only improve to 85% with 11 times more training time. For complex applications and models (e.g., object recognition), MODE is able to improve the accuracy from 75% to over 91% in minutes to a few hours, whereas state-of-the-art fails to fix the bug or even degrades the test accuracy.","Debugging, Deep Neural Network, Differential Analysis",175–186,12,Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
399,@inproceedings: 10.1145/3236024.3236068,DeepSim: Deep Learning Code Functional Similarity,"Zhao, Gang and Huang, Jeff",2018,Association for Computing Machinery,,https://doi.org/10.1145/3236024.3236068,10.1145/3236024.3236068,"Measuring code similarity is fundamental for many software engineering tasks, e.g., code search, refactoring and reuse. However, most existing techniques focus on code syntactical similarity only, while measuring code functional similarity remains a challenging problem. In this paper, we propose a novel approach that encodes code control flow and data flow into a semantic matrix in which each element is a high dimensional sparse binary feature vector, and we design a new deep learning model that measures code functional similarity based on this representation. By concatenating hidden representations learned from a code pair, this new model transforms the problem of detecting functionally similar code to binary classification, which can effectively learn patterns between functionally similar code with very different syntactics.  We have implemented our approach, DeepSim, for Java programs and evaluated its recall, precision and time performance on two large datasets of functionally similar code. The experimental results show that DeepSim significantly outperforms existing state-of-the-art techniques, such as DECKARD, RtvNN, CDLH, and two baseline deep neural networks models.","Classification, Deep Learning, Code functional similarity, Control/Data flow",141–151,11,Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
400,@inproceedings: 10.1145/3236024.3275524,VulSeeker-pro: Enhanced Semantic Learning Based Binary Vulnerability Seeker with Emulation,"Gao, Jian and Yang, Xin and Fu, Ying and Jiang, Yu and Shi, Heyuan and Sun, Jiaguang",2018,Association for Computing Machinery,,https://doi.org/10.1145/3236024.3275524,10.1145/3236024.3275524,"Learning-based clone detection is widely exploited for binary vulnerability search. Although they solve the problem of high time overhead of traditional dynamic and static search approaches to some extent, their accuracy is limited, and need to manually identify the true positive cases among the top-M search results during the industrial practice. This paper presents VulSeeker-Pro, an enhanced binary vulnerability seeker that integrates function semantic emulation at the back end of semantic learning, to release the engineers from the manual identification work. It first uses the semantic learning based predictor to quickly predict the top-M candidate functions which are the most similar to the vulnerability from the target binary. Then the top-M candidates are fed to the emulation engine to resort, and more accurate top-N candidate functions are obtained. With fast filtering of semantic learning and dynamic trace generation of function semantic emulation, VulSeeker-Pro can achieve higher search accuracy with little time overhead. The experimental results on 15 known CVE vulnerabilities involving 6 industry widely used programs show that VulSeeker-Pro significantly outperforms the state-of-the-art approaches in terms of accuracy. In a total of 45 searches, VulSeeker-Pro finds 40 and 43 real vulnerabilities in the top-1 and top-5 candidate functions, which are 12.33\texttimes{} and 2.58\texttimes{} more than the most recent and related work Gemini. In terms of efficiency, it takes 0.22 seconds on average to determine whether the target binary function contains a known vulnerability or not.","semantic learning, function emulation, vulnerability search",803–808,6,Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
401,@inproceedings: 10.1145/3290621.3290630,Applying Topic Segmentation to Document-Level Information Retrieval,"Shtekh, Gennady and Kazakova, Polina and Nikitinsky, Nikita and Skachkov, Nikolay",2018,Association for Computing Machinery,,https://doi.org/10.1145/3290621.3290630,10.1145/3290621.3290630,"In the present paper we discuss how text segmentation could be applied in the information retrieval domain. We assume that topic text segmentation allows one to better model text structure and therefore language itself, which influences the quality of text representation. We test the initial hypothesis by conducting experiments with several baseline models on the arXiv dataset comparing their quality on whole texts and on segmented texts. The experiments demonstrated that, indeed, the quality of retrieval is generally slightly improved.","Information Retrieval, Topic Segmentation, Topic Modeling",,6,Proceedings of the 14th Central and Eastern European Software Engineering Conference Russia
402,@inproceedings: 10.1145/3242587.3242652,Unimanual Pen+Touch Input Using Variations of Precision Grip Postures,"Cami, Drini and Matulic, Fabrice and Calland, Richard G. and Vogel, Brian and Vogel, Daniel",2018,Association for Computing Machinery,,https://doi.org/10.1145/3242587.3242652,10.1145/3242587.3242652,"We introduce a new pen input space by forming postures with the same hand that also grips the pen while writing, drawing, or selecting. The postures contact the multitouch surface around the pen to enable detection without special sensors. A formative study investigates the effectiveness, accuracy, and comfort of 33 candidate postures in controlled tasks. The results indicate a useful subset of postures. Using raw capacitive sensor data captured in the study, a convolutional neural network is trained to recognize 10 postures in real time. This recognizer is used to create application demonstrations for pen-based document annotation and vector drawing. A small usability study shows the approach is feasible.","interaction techniques, touch input, pen input",825–837,13,Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
403,@inproceedings: 10.1145/3239235.3240503,Prediction of Relatedness in Stack Overflow: Deep Learning vs. SVM: A Reproducibility Study,"Xu, Bowen and Shirani, Amirreza and Lo, David and Alipour, Mohammad Amin",2018,Association for Computing Machinery,,https://doi.org/10.1145/3239235.3240503,10.1145/3239235.3240503,"Background Xu et al. used a deep neural network (DNN) technique to classify the degree of relatedness between two knowledge units (question-answer threads) on Stack Overflow. More recently, extending Xu et al.'s work, Fu and Menzies proposed a simpler classification technique based on a fine-tuned support vector machine (SVM) that achieves similar performance but in a much shorter time. Thus, they suggested that researchers need to compare their sophisticated methods against simpler alternatives.Aim The aim of this work is to replicate the previous studies and further investigate the validity of Fu and Menzies' claim by evaluating the DNN- and SVM-based approaches on a larger dataset. We also compare the effectiveness of these two approaches against SimBow, a lightweight SVM-based method that was previously used for general community question-answering.Method We (1) collect a large dataset containing knowledge units from Stack Overflow, (2) show the value of the new dataset addressing shortcomings of the original one, (3) re-evaluate both the DNN-and SVM-based approaches on the new dataset, and (4) compare the performance of the two approaches against that of SimBow.Results We find that: (1) there are several limitations in the original dataset used in the previous studies, (2) effectiveness of both Xu et al.'s and Fu and Menzies' approaches (as measured using F1-score) drop sharply on the new dataset, (3) similar to the previous finding, performance of SVM-based approaches (Fu and Menzies' approach and SimBow) are slightly better than the DNN-based approach, (4) contrary to the previous findings, Fu and Menzies' approach runs much slower than DNN-based approach on the larger dataset - its runtime grows sharply with increase in dataset size, and (5) SimBow outperforms both Xu et al. and Fu and Menzies' approaches in terms of runtime.Conclusion We conclude that, for this task, simpler approaches based on SVM performs adequately well. We also illustrate the challenges brought by the increased size of the dataset and show the benefit of a lightweight SVM-based approach for this task.","deep learning, support vector machine, relatedness prediction",,10,Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement
404,@inproceedings: 10.1145/3242587.3242609,Ubicoustics: Plug-and-Play Acoustic Activity Recognition,"Laput, Gierad and Ahuja, Karan and Goel, Mayank and Harrison, Chris",2018,Association for Computing Machinery,,https://doi.org/10.1145/3242587.3242609,10.1145/3242587.3242609,"Despite sound being a rich source of information, computing devices with microphones do not leverage audio to glean useful insights about their physical and social context. For example, a smart speaker sitting on a kitchen countertop cannot figure out if it is in a kitchen, let alone know what a user is doing in a kitchen - a missed opportunity. In this work, we describe a novel, real-time, sound-based activity recognition system. We start by taking an existing, state-of-the-art sound labeling model, which we then tune to classes of interest by drawing data from professional sound effect libraries traditionally used in the entertainment industry. These well-labeled and high-quality sounds are the perfect atomic unit for data augmentation, including amplitude, reverb, and mixing, allowing us to exponentially grow our tuning data in realistic ways. We quantify the performance of our approach across a range of environments and device categories and show that microphone-equipped computing devices already have the requisite capability to unlock real-time activity recognition comparable to human accuracy.","internet-of-things, iot, microphones, smart environments, acoustics, sound sensing, ubiquitous sensing",213–224,12,Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
405,@inproceedings: 10.1145/3242587.3242599,Lip-Interact: Improving Mobile Device Interaction with Silent Speech Commands,"Sun, Ke and Yu, Chun and Shi, Weinan and Liu, Lan and Shi, Yuanchun",2018,Association for Computing Machinery,,https://doi.org/10.1145/3242587.3242599,10.1145/3242587.3242599,"We present Lip-Interact, an interaction technique that allows users to issue commands on their smartphone through silent speech. Lip-Interact repurposes the front camera to capture the user's mouth movements and recognize the issued commands with an end-to-end deep learning model. Our system supports 44 commands for accessing both system-level functionalities (launching apps, changing system settings, and handling pop-up windows) and application-level functionalities (integrated operations for two apps). We verify the feasibility of Lip-Interact with three user experiments: evaluating the recognition accuracy, comparing with touch on input efficiency, and comparing with voiced commands with regards to personal privacy and social norms. We demonstrate that Lip-Interact can help users access functionality efficiently in one step, enable one-handed input when the other hand is occupied, and assist touch to make interactions more fluent.","touch-free, lip interaction, semantic gesture, vision-based recognition, mobile interaction, silent speech",581–593,13,Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
406,@inproceedings: 10.1145/3242587.3242617,Facilitating Document Reading by Linking Text and Tables,"Kim, Dae Hyun and Hoque, Enamul and Kim, Juho and Agrawala, Maneesh",2018,Association for Computing Machinery,,https://doi.org/10.1145/3242587.3242617,10.1145/3242587.3242617,"Document authors commonly use tables to support arguments presented in the text. But, because tables are usually separate from the main body text, readers must split their attention between different parts of the document. We present an interactive document reader that automatically links document text with corresponding table cells. Readers can select a sentence (or tables cells) and our reader highlights the relevant table cells (or sentences). We provide an automatic pipeline for extracting such references between sentence text and table cells for existing PDF documents that combines structural analysis of tables with natural language processing and rule-based matching. On a test corpus of 330 (sentence, table) pairs, our pipeline correctly extracts 48.8% of the references. An additional 30.5% contain only false negatives (FN) errors -- the reference is missing table cells. The remaining 20.7% contain false positives (FP) errors -- the reference includes extraneous table cells and could therefore mislead readers. A user study finds that despite such errors, our interactive document reader helps readers match sentences with corresponding table cells more accurately and quickly than a baseline document reader.","text analysis, interactive documents, visualization",423–434,12,Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
407,@inproceedings: 10.1145/3242587.3242598,Sprout: Crowd-Powered Task Design for Crowdsourcing,"Bragg, Jonathan and Mausam and Weld, Daniel S.",2018,Association for Computing Machinery,,https://doi.org/10.1145/3242587.3242598,10.1145/3242587.3242598,"While crowdsourcing enables data collection at scale, ensuring high-quality data remains a challenge. In particular, effective task design underlies nearly every reported crowdsourcing success, yet remains difficult to accomplish. Task design is hard because it involves a costly iterative process: identifying the kind of work output one wants, conveying this information to workers, observing worker performance, understanding what remains ambiguous, revising the instructions, and repeating the process until the resulting output is satisfactory. To facilitate this process, we propose a novel meta-workflow that helps requesters optimize crowdsourcing task designs and Sprout, our open-source tool, which implements this workflow. Sprout improves task designs by (1) eliciting points of confusion from crowd workers, (2) enabling requesters to quickly understand these misconceptions and the overall space of questions, and (3) guiding requesters to improve the task design in response. We report the results of a user study with two labeling tasks demonstrating that requesters strongly prefer Sprout and produce higher-rated instructions compared to current best practices for creating gated instructions (instructions plus a workflow for training and testing workers). We also offer a set of design recommendations for future tools that support crowdsourcing task design.",,165–176,12,Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
408,@inproceedings: 10.1145/3242587.3242596,The Exploratory Labeling Assistant: Mixed-Initiative Label Curation with Large Document Collections,"Felix, Cristian and Dasgupta, Aritra and Bertini, Enrico",2018,Association for Computing Machinery,,https://doi.org/10.1145/3242587.3242596,10.1145/3242587.3242596,"In this paper, we define the concept of exploratory labeling: the use of computational and interactive methods to help analysts categorize groups of documents into a set of unknown and evolving labels. While many computational methods exist to analyze data and build models once the data is organized around a set of predefined categories or labels, few methods address the problem of reliably discovering and curating such labels in the first place. In order to move first steps towards bridging this gap, we propose an interactive visual data analysis method that integrates human-driven label ideation, specification and refinement with machine-driven recommendations. The proposed method enables the user to progressively discover and ideate labels in an exploratory fashion and specify rules that can be used to automatically match sets of documents to labels. To support this process of ideation, specification, as well as evaluation of the labels, we use unsupervised machine learning methods that provide suggestions and data summaries. We evaluate our method by applying it to a real-world labeling problem as well as through controlled user studies to identify and reflect on patterns of interaction emerging from exploratory labeling activities.","text analysis, document labeling, visualization, exploratory labeling",153–164,12,Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
409,@inproceedings: 10.1145/3242587.3242608,Vibrosight: Long-Range Vibrometry for Smart Environment Sensing,"Zhang, Yang and Laput, Gierad and Harrison, Chris",2018,Association for Computing Machinery,,https://doi.org/10.1145/3242587.3242608,10.1145/3242587.3242608,"Smart and responsive environments rely on the ability to detect physical events, such as appliance use and human activities. Currently, to sense these types of events, one must either upgrade to ""smart"" appliances, or attach aftermarket sensors to existing objects. These approaches can be expensive, intrusive and inflexible. In this work, we present Vibrosight, a new approach to sense activities across entire rooms using long-range laser vibrometry. Unlike a microphone, our approach can sense physical vibrations at one specific point, making it robust to interference from other activities and noisy environments. This property enables detection of simultaneous activities, which has proven challenging in prior work. Through a series of evaluations, we show that Vibrosight can offer high accuracies at long range, allowing our sensor to be placed in an inconspicuous location. We also explore a range of additional uses, including data transmission, sensing user input and modes of appliance operation, and detecting human movement and activities on work surfaces.","appliance monitoring, internet-of-things, activity detection, context-aware sensing, laser vibrometry.",225–236,12,Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology
410,@inproceedings: 10.1145/3233027.3233033,Reverse Engineering Variability from Requirement Documents Based on Probabilistic Relevance and Word Embedding,"Li, Yang and Schulze, Sandro and Saake, Gunter",2018,Association for Computing Machinery,,https://doi.org/10.1145/3233027.3233033,10.1145/3233027.3233033,"Feature and variability extraction from different artifacts is an indispensable activity to support systematic integration of single software systems and Software Product Line (SPL). Beyond manually extracting variability, a variety of approaches, such as feature location in source code and feature extraction in requirements, has been proposed to provide an automatic identification of features and their variation points. Compared with source code, requirements contain more complete variability information and provide traceability links to other artifacts from early development phases. In this paper, we propose a method to automatically extract features and relationships based on a probabilistic relevance and word embedding. In particular, our technique consists of three steps: First, we apply word2vec to obtain a prediction model, which we use to determine the word level similarity of requirements. Second, based on word level similarity and the significance of a word in a domain, we compute the requirements level similarity using probabilistic relevance. Third, we adopt hierarchical clustering to group features and we define four criteria to detect variation points between identified features. We perform a case study to evaluate the usability and robustness of our method and to compare it with the results of other related approaches. Initial results reveal that our approach identifies the majority of features correctly and also extracts variability information with reasonable accuracy.","reverse engineering, software product lines, requirement documents, variability extraction, feature identification",121–131,11,Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1
411,@article: 10.1145/3233300,Low Complexity Multiply-Accumulate Units for Convolutional Neural Networks with Weight-Sharing,"Garland, James and Gregg, David",2018,Association for Computing Machinery,1544-3566,https://doi.org/10.1145/3233300,10.1145/3233300,"Convolutional neural networks (CNNs) are one of the most successful machine-learning techniques for image, voice, and video processing. CNNs require large amounts of processing capacity and memory bandwidth. Hardware accelerators have been proposed for CNNs that typically contain large numbers of multiply-accumulate (MAC) units, the multipliers of which are large in integrated circuit (IC) gate count and power consumption. “Weight-sharing” accelerators have been proposed where the full range of weight values in a trained CNN are compressed and put into bins, and the bin index is used to access the weight-shared value. We reduce power and area of the CNN by implementing parallel accumulate shared MAC (PASM) in a weight-shared CNN. PASM re-architects the MAC to instead count the frequency of each weight and place it in a bin. The accumulated value is computed in a subsequent multiply phase, significantly reducing gate count and power consumption of the CNN. In this article, we implement PASM in a weight-shared CNN convolution hardware accelerator and analyze its effectiveness. Experiments show that for a clock speed 1GHz implemented on a 45nm ASIC process our approach results in fewer gates, smaller logic, and reduced power with only a slight increase in latency. We also show that the same weight-shared-with-PASM CNN accelerator can be implemented in resource-constrained FPGAs, where the FPGA has limited numbers of digital signal processor (DSP) units to accelerate the MAC operations.","FPGA, CNN, multiply accumulate, arithmetic hardware circuits, power efficiency, ASI",,24,ACM Trans. Archit. Code Optim.
412,@inbook: 10.1145/3238147.3238206,Improving Automatic Source Code Summarization via Deep Reinforcement Learning,"Wan, Yao and Zhao, Zhou and Yang, Min and Xu, Guandong and Ying, Haochao and Wu, Jian and Yu, Philip S.",2018,Association for Computing Machinery,,https://doi.org/10.1145/3238147.3238206,,"Code summarization provides a high level natural language description of the function performed by code, as it can benefit the software maintenance, code categorization and retrieval. To the best of our knowledge, most state-of-the-art approaches follow an encoder-decoder framework which encodes the code into a hidden space and then decode it into natural language space, suffering from two major drawbacks: a) Their encoders only consider the sequential content of code, ignoring the tree structure which is also critical for the task of code summarization; b) Their decoders are typically trained to predict the next word by maximizing the likelihood of next ground-truth word with previous ground-truth word given. However, it is expected to generate the entire sequence from scratch at test time. This discrepancy can cause an exposure bias issue, making the learnt decoder suboptimal. In this paper, we incorporate an abstract syntax tree structure as well as sequential content of code snippets into a deep reinforcement learning framework (i.e., actor-critic network). The actor network provides the confidence of predicting the next word according to current state. On the other hand, the critic network evaluates the reward value of all possible extensions of the current state and can provide global guidance for explorations. We employ an advantage reward composed of BLEU metric to train both networks. Comprehensive experiments on a real-world dataset show the effectiveness of our proposed model when compared with some state-of-the-art methods.",,397–407,1,Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering
413,@inbook: 10.1145/3238147.3238202,DeepGauge: Multi-Granularity Testing Criteria for Deep Learning Systems,"Ma, Lei and Juefei-Xu, Felix and Zhang, Fuyuan and Sun, Jiyuan and Xue, Minhui and Li, Bo and Chen, Chunyang and Su, Ting and Li, Li and Liu, Yang and Zhao, Jianjun and Wang, Yadong",2018,Association for Computing Machinery,,https://doi.org/10.1145/3238147.3238202,,"Deep learning (DL) defines a new data-driven programming paradigm that constructs the internal system logic of a crafted neuron network through a set of training data. We have seen wide adoption of DL in many safety-critical scenarios. However, a plethora of studies have shown that the state-of-the-art DL systems suffer from various vulnerabilities which can lead to severe consequences when applied to real-world applications. Currently, the testing adequacy of a DL system is usually measured by the accuracy of test data. Considering the limitation of accessible high quality test data, good accuracy performance on test data can hardly provide confidence to the testing adequacy and generality of DL systems. Unlike traditional software systems that have clear and controllable logic and functionality, the lack of interpretability in a DL system makes system analysis and defect detection difficult, which could potentially hinder its real-world deployment. In this paper, we propose DeepGauge, a set of multi-granularity testing criteria for DL systems, which aims at rendering a multi-faceted portrayal of the testbed. The in-depth evaluation of our proposed testing criteria is demonstrated on two well-known datasets, five DL systems, and with four state-of-the-art adversarial attack techniques against DL. The potential usefulness of DeepGauge sheds light on the construction of more generic and robust DL systems.",,120–131,1,Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering
414,@inbook: 10.1145/3238147.3238222,Expandable Group Identification in Spreadsheets,"Dou, Wensheng and Han, Shi and Xu, Liang and Zhang, Dongmei and Wei, Jun",2018,Association for Computing Machinery,,https://doi.org/10.1145/3238147.3238222,,"Spreadsheets are widely used in various business tasks. Spreadsheet users may put similar data and computations by repeating a block of cells (a unit) in their spreadsheets. We name the unit and all its expanding ones as an expandable group. All units in an expandable group share the same or similar formats and semantics. As a data storage and management tool, expandable groups represent the fundamental structure in spreadsheets. However, existing spreadsheet systems do not recognize any expandable groups. Therefore, other spreadsheet analysis tools, e.g., data integration and fault detection, cannot utilize this structure of expandable groups to perform precise analysis. In this paper, we propose ExpCheck to automatically extract expandable groups in spreadsheets. We observe that continuous units that share the similar formats and semantics are likely to be an expandable group. Inspired by this, we inspect the format of each cell and its corresponding semantics, and further classify them into expandable groups according to their similarity. We evaluate ExpCheck on 120 spreadsheets randomly sampled from the EUSES and VEnron corpora. The experimental results show that ExpCheck is effective. ExpCheck successfully detect expandable groups with F1-measure of 73.1%, significantly outperforming the state-of-the-art techniques (F1-measure of 13.3%).",,498–508,1,Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering
415,@inbook: 10.1145/3238147.3238187,DeepRoad: GAN-Based Metamorphic Testing and Input Validation Framework for Autonomous Driving Systems,"Zhang, Mengshi and Zhang, Yuqun and Zhang, Lingming and Liu, Cong and Khurshid, Sarfraz",2018,Association for Computing Machinery,,https://doi.org/10.1145/3238147.3238187,,"While Deep Neural Networks (DNNs) have established the fundamentals of image-based autonomous driving systems, they may exhibit erroneous behaviors and cause fatal accidents. To address the safety issues in autonomous driving systems, a recent set of testing techniques have been designed to automatically generate artificial driving scenes to enrich test suite, e.g., generating new input images transformed from the original ones. However, these techniques are insufficient due to two limitations: first, many such synthetic images often lack diversity of driving scenes, and hence compromise the resulting efficacy and reliability. Second, for machine-learning-based systems, a mismatch between training and application domain can dramatically degrade system accuracy, such that it is necessary to validate inputs for improving system robustness.  In this paper, we propose DeepRoad, an unsupervised DNN-based framework for automatically testing the consistency of DNN-based autonomous driving systems and online validation. First, DeepRoad automatically synthesizes large amounts of diverse driving scenes without using image transformation rules (e.g. scale, shear and rotation). In particular, DeepRoad is able to produce driving scenes with various weather conditions (including those with rather extreme conditions) by applying Generative Adversarial Networks (GANs) along with the corresponding real-world weather scenes. Second, DeepRoad utilizes metamorphic testing techniques to check the consistency of such systems using synthetic images. Third, DeepRoad validates input images for DNN-based systems by measuring the distance of the input and training images using their VGGNet features. We implement DeepRoad to test three well-recognized DNN-based autonomous driving systems in Udacity self-driving car challenge. The experimental results demonstrate that DeepRoad can detect thousands of inconsistent behaviors for these systems, and effectively validate input images to potentially enhance the system robustness as well.",,132–142,1,Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering
416,@inproceedings: 10.1145/3243127.3243132,A Language-Agnostic Model for Semantic Source Code Labeling,"Gelman, Ben and Hoyle, Bryan and Moore, Jessica and Saxe, Joshua and Slater, David",2018,Association for Computing Machinery,,https://doi.org/10.1145/3243127.3243132,10.1145/3243127.3243132,"Code search and comprehension have become more difficult in recent years due to the rapid expansion of available source code. Current tools lack a way to label arbitrary code at scale while maintaining up-to-date representations of new programming languages, libraries, and functionalities. Comprehensive labeling of source code enables users to search for documents of interest and obtain a high-level understanding of their contents. We use Stack Overflow code snippets and their tags to train a language-agnostic, deep convolutional neural network to automatically predict semantic labels for source code documents. On Stack Overflow code snippets, we demonstrate a mean area under ROC of 0.957 over a long-tailed list of 4,508 tags. We also manually validate the model outputs on a diverse set of unlabeled source code documents retrieved from Github, and obtain a top-1 accuracy of 86.6%. This strongly indicates that the model successfully transfers its knowledge from Stack Overflow snippets to arbitrary source code documents.","source code, multilabel classification, crowdsourcing, deep learning, semantic labeling, natural language processing",36–44,9,Proceedings of the 1st International Workshop on Machine Learning and Software Engineering in Symbiosis
417,@inbook: 10.1145/3238147.3238175,AutoConfig: Automatic Configuration Tuning for Distributed Message Systems,"Bao, Liang and Liu, Xin and Xu, Ziheng and Fang, Baoyin",2018,Association for Computing Machinery,,https://doi.org/10.1145/3238147.3238175,,"Distributed message systems (DMSs) serve as the communication backbone for many real-time streaming data processing applications. To support the vast diversity of such applications, DMSs provide a large number of parameters to configure. However, It overwhelms for most users to configure these parameters well for better performance. Although many automatic configuration approaches have been proposed to address this issue, critical challenges still remain: 1) to train a better and robust performance prediction model using a limited number of samples, and 2) to search for a high-dimensional parameter space efficiently within a time constraint. In this paper, we propose AutoConfig -- an automatic configuration system that can optimize producer-side throughput on DMSs. AutoConfig constructs a novel comparison-based model (CBM) that is more robust that the prediction-based model (PBM) used by previous learning-based approaches. Furthermore, AutoConfig uses a weighted Latin hypercube sampling (wLHS) approach to select a set of samples that can provide a better coverage over the high-dimensional parameter space. wLHS allows AutoConfig to search for more promising configurations using the trained CBM. We have implemented AutoConfig on the Kafka platform, and evaluated it using eight different testing scenarios deployed on a public cloud. Experimental results show that our CBM can obtain better results than that of PBM under the same random forests based model. Furthermore, AutoConfig outperforms default configurations by 215.40% on average, and five state-of-the-art configuration algorithms by 7.21%-64.56%.",,29–40,1,Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering
418,@inbook: 10.1145/3238147.3238172,Concolic Testing for Deep Neural Networks,"Sun, Youcheng and Wu, Min and Ruan, Wenjie and Huang, Xiaowei and Kwiatkowska, Marta and Kroening, Daniel",2018,Association for Computing Machinery,,https://doi.org/10.1145/3238147.3238172,,"Concolic testing combines program execution and symbolic analysis to explore the execution paths of a software program. In this paper, we develop the first concolic testing approach for Deep Neural Networks (DNNs). More specifically, we utilise quantified linear arithmetic over rationals to express test requirements that have been studied in the literature, and then develop a coherent method to perform concolic testing with the aim of better coverage. Our experimental results show the effectiveness of the concolic testing approach in both achieving high coverage and finding adversarial examples.",,109–119,1,Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering
419,@inbook: 10.1145/3238147.3238191,API Method Recommendation without Worrying about the Task-API Knowledge Gap,"Huang, Qiao and Xia, Xin and Xing, Zhenchang and Lo, David and Wang, Xinyu",2018,Association for Computing Machinery,,https://doi.org/10.1145/3238147.3238191,,"Developers often need to search for appropriate APIs for their programming tasks. Although most libraries have API reference documentation, it is not easy to find appropriate APIs due to the lexical gap and knowledge gap between the natural language description of the programming task and the API description in API documentation. Here, the lexical gap refers to the fact that the same semantic meaning can be expressed by different words, and the knowledge gap refers to the fact that API documentation mainly describes API functionality and structure but lacks other types of information like concepts and purposes, which are usually the key information in the task description. In this paper, we propose an API recommendation approach named BIKER (Bi-Information source based KnowledgE Recommendation) to tackle these two gaps. To bridge the lexical gap, BIKER uses word embedding technique to calculate the similarity score between two text descriptions. Inspired by our survey findings that developers incorporate Stack Overflow posts and API documentation for bridging the knowledge gap, BIKER leverages Stack Overflow posts to extract candidate APIs for a program task, and ranks candidate APIs by considering the query’s similarity with both Stack Overflow posts and API documentation. It also summarizes supplementary information (e.g., API description, code examples in Stack Overflow posts) for each API to help developers select the APIs that are most relevant to their tasks. Our evaluation with 413 API-related questions confirms the effectiveness of BIKER for both class- and method-level API recommendation, compared with state-of-the-art baselines. Our user study with 28 Java developers further demonstrates the practicality of BIKER for API search.",,293–304,1,Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering
420,@inbook: 10.1145/3238147.3238204,PerfLearner: Learning from Bug Reports to Understand and Generate Performance Test Frames,"Han, Xue and Yu, Tingting and Lo, David",2018,Association for Computing Machinery,,https://doi.org/10.1145/3238147.3238204,,"Software performance is important for ensuring the quality of software products. Performance bugs, defined as programming errors that cause significant performance degradation, can lead to slow systems and poor user experience. While there has been some research on automated performance testing such as test case generation, the main idea is to select workload values to increase the program execution times. These techniques often assume the initial test cases have the right combination of input parameters and focus on evolving values of certain input parameters. However, such an assumption may not hold for highly configurable real-word applications, in which the combinations of input parameters can be very large. In this paper, we manually analyze 300 bug reports from three large open source projects - Apache HTTP Server, MySQL, and Mozilla Firefox. We found that 1) exposing performance bugs often requires combinations of multiple input parameters, and 2) certain input parameters are frequently involved in exposing performance bugs. Guided by these findings, we designed and evaluated an automated approach, PerfLearner, to extract execution commands and input parameters from descriptions of performance bug reports and use them to generate test frames for guiding actual performance test case generation.",,17–28,1,Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering
421,@inproceedings: 10.1145/3213846.3213866,An Empirical Study on TensorFlow Program Bugs,"Zhang, Yuhao and Chen, Yifan and Cheung, Shing-Chi and Xiong, Yingfei and Zhang, Lu",2018,Association for Computing Machinery,,https://doi.org/10.1145/3213846.3213866,10.1145/3213846.3213866,"Deep learning applications become increasingly popular in important domains such as self-driving systems and facial identity systems. Defective deep learning applications may lead to catastrophic consequences. Although recent research efforts were made on testing and debugging deep learning applications, the characteristics of deep learning defects have never been studied. To fill this gap, we studied deep learning applications built on top of TensorFlow and collected program bugs related to TensorFlow from StackOverflow QA pages and Github projects. We extracted information from QA pages, commit messages, pull request messages, and issue discussions to examine the root causes and symptoms of these bugs. We also studied the strategies deployed by TensorFlow users for bug detection and localization. These findings help researchers and TensorFlow users to gain a better understanding of coding defects in TensorFlow programs and point out a new direction for future research.","TensorFlow Program Bug, Deep Learning, Empirical Study",129–140,12,Proceedings of the 27th ACM SIGSOFT International Symposium on Software Testing and Analysis
422,@article: 10.1145/3204459,FEMOSAA: Feature-Guided and Knee-Driven Multi-Objective Optimization for Self-Adaptive Software,"Chen, Tao and Li, Ke and Bahsoon, Rami and Yao, Xin",2018,Association for Computing Machinery,1049-331X,https://doi.org/10.1145/3204459,10.1145/3204459,"Self-Adaptive Software (SAS) can reconfigure itself to adapt to the changing environment at runtime, aiming to continually optimize conflicted nonfunctional objectives (e.g., response time, energy consumption, throughput, cost, etc.). In this article, we present Feature-guided and knEe-driven Multi-Objective optimization for Self-Adaptive softwAre (FEMOSAA), a novel framework that automatically synergizes the feature model and Multi-Objective Evolutionary Algorithm (MOEA) to optimize SAS at runtime. FEMOSAA operates in two phases: at design time, FEMOSAA automatically transposes the engineers’ design of SAS, expressed as a feature model, to fit the MOEA, creating new chromosome representation and reproduction operators. At runtime, FEMOSAA utilizes the feature model as domain knowledge to guide the search and further extend the MOEA, providing a larger chance for finding better solutions. In addition, we have designed a new method to search for the knee solutions, which can achieve a balanced tradeoff. We comprehensively evaluated FEMOSAA on two running SAS: One is a highly complex SAS with various adaptable real-world software under the realistic workload trace; another is a service-oriented SAS that can be dynamically composed from services. In particular, we compared the effectiveness and overhead of FEMOSAA against four of its variants and three other search-based frameworks for SAS under various scenarios, including three commonly applied MOEAs, two workload patterns, and diverse conflicting quality objectives. The results reveal the effectiveness of FEMOSAA and its superiority over the others with high statistical significance and nontrivial effect sizes.","multi-objective optimization, search-based software engineering, performance engineering, self-adaptive system, Feature model, multi-objective evolutionary algorith",,50,ACM Trans. Softw. Eng. Methodol.
423,@inproceedings: 10.1145/3210459.3210469,Bug Localization with Semantic and Structural Features Using Convolutional Neural Network and Cascade Forest,"Xiao, Yan and Keung, Jacky and Mi, Qing and Bennin, Kwabena E.",2018,Association for Computing Machinery,,https://doi.org/10.1145/3210459.3210469,10.1145/3210459.3210469,"Background: Correctly localizing buggy files for bug reports together with their semantic and structural information is a crucial task, which would essentially improve the accuracy of bug localization techniques. Aims: To empirically evaluate and demonstrate the effects of both semantic and structural information in bug reports and source files on improving the performance of bug localization, we propose CNN_Forest involving convolutional neural network and ensemble of random forests that have excellent performance in the tasks of semantic parsing and structural information extraction. Method: We first employ convolutional neural network with multiple filters and an ensemble of random forests with multi-grained scanning to extract semantic and structural features from the word vectors derived from bug reports and source files. And a subsequent cascade forest (a cascade of ensembles of random forests) is used to further extract deeper features and observe the correlated relationships between bug reports and source files. CNNLForest is then empirically evaluated over 10,754 bug reports extracted from AspectJ, Eclipse UI, JDT, SWT, and Tomcat projects. Results: The experiments empirically demonstrate the significance of including semantic and structural information in bug localization, and further show that the proposed CNN_Forest achieves higher Mean Average Precision and Mean Reciprocal Rank measures than the best results of the four current state-of-the-art approaches (NPCNN, LR+WE, DNNLOC, and BugLocator). Conclusion: CNNLForest is capable of defining the correlated relationships between bug reports and source files, and we empirically show that semantic and structural information in bug reports and source files are crucial in improving bug localization.","convolutional neural network, bug localization, structural information, semantic information, cascade forest, word embedding",101–111,11,Proceedings of the 22nd International Conference on Evaluation and Assessment in Software Engineering 2018
424,@inproceedings: 10.1145/3211346.3211352,Obfuscation Resilient Search through Executable Classification,"Su, Fang-Hsiang and Bell, Jonathan and Kaiser, Gail and Ray, Baishakhi",2018,Association for Computing Machinery,,https://doi.org/10.1145/3211346.3211352,10.1145/3211346.3211352,"Android applications are usually obfuscated before release, making it difficult to analyze them for malware presence or intellectual property violations. Obfuscators might hide the true intent of code by renaming variables and/or modifying program structures. It is challenging to search for executables relevant to an obfuscated application for developers to analyze efficiently. Prior approaches toward obfuscation resilient search have relied on certain structural parts of apps remaining as landmarks, un-touched by obfuscation. For instance, some prior approaches have assumed that the structural relationships between identifiers are not broken by obfuscators; others have assumed that control flow graphs maintain their structures. Both approaches can be easily defeated by a motivated obfuscator. We present a new approach, MACNETO, to search for programs relevant to obfuscated executables leveraging deep learning and principal components on instructions. MACNETO makes few assumptions about the kinds of modifications that an obfuscator might perform. We show that it has high search precision for executables obfuscated by a state-of-the-art obfuscator that changes control flow. Further, we also demonstrate the potential of MACNETO to help developers understand executables, where MACNETO infers keywords (which are from relevant un-obfuscated programs) for obfuscated executables.","bytecode analysis, bytecode search, deep learning, executable search, obfuscation resilience",20–30,11,Proceedings of the 2nd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages
425,@inproceedings: 10.1145/3211346.3211355,The Three Pillars of Machine Programming,"Gottschlich, Justin and Solar-Lezama, Armando and Tatbul, Nesime and Carbin, Michael and Rinard, Martin and Barzilay, Regina and Amarasinghe, Saman and Tenenbaum, Joshua B. and Mattson, Tim",2018,Association for Computing Machinery,,https://doi.org/10.1145/3211346.3211355,10.1145/3211346.3211355,"In this position paper, we describe our vision of the future of machine programming through a categorical examination of three pillars of research. Those pillars are: (i) intention, (ii) invention, and (iii) adaptation. Intention emphasizes advancements in the human-to-computer and computer-to-machine-learning interfaces. Invention emphasizes the creation or refinement of algorithms or core hardware and software building blocks through machine learning (ML). Adaptation emphasizes advances in the use of ML-based constructs to autonomously evolve software.","software development, machine programming, program synthesis, invention, adaptation, intention, software maintenance",69–80,12,Proceedings of the 2nd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages
426,@inproceedings: 10.1145/3192366.3192410,Accelerating Search-Based Program Synthesis Using Learned Probabilistic Models,"Lee, Woosuk and Heo, Kihong and Alur, Rajeev and Naik, Mayur",2018,Association for Computing Machinery,,https://doi.org/10.1145/3192366.3192410,10.1145/3192366.3192410,"A key challenge in program synthesis concerns how to efficiently search for the desired program in the space of possible programs. We propose a general approach to accelerate search-based program synthesis by biasing the search towards likely programs. Our approach targets a standard formulation, syntax-guided synthesis (SyGuS), by extending the grammar of possible programs with a probabilistic model dictating the likelihood of each program. We develop a weighted search algorithm to efficiently enumerate programs in order of their likelihood. We also propose a method based on transfer learning that enables to effectively learn a powerful model, called probabilistic higher-order grammar, from known solutions in a domain. We have implemented our approach in a tool called Euphony and evaluate it on SyGuS benchmark problems from a variety of domains. We show that Euphony can learn good models using easily obtainable solutions, and achieves significant performance gains over existing general-purpose as well as domain-specific synthesizers.","Synthesis, Domain-specific languages, Statistical methods, Transfer learning",436–449,14,Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation
427,@article: 10.1145/3296979.3192410,Accelerating Search-Based Program Synthesis Using Learned Probabilistic Models,"Lee, Woosuk and Heo, Kihong and Alur, Rajeev and Naik, Mayur",2018,Association for Computing Machinery,0362-1340,https://doi.org/10.1145/3296979.3192410,10.1145/3296979.3192410,"A key challenge in program synthesis concerns how to efficiently search for the desired program in the space of possible programs. We propose a general approach to accelerate search-based program synthesis by biasing the search towards likely programs. Our approach targets a standard formulation, syntax-guided synthesis (SyGuS), by extending the grammar of possible programs with a probabilistic model dictating the likelihood of each program. We develop a weighted search algorithm to efficiently enumerate programs in order of their likelihood. We also propose a method based on transfer learning that enables to effectively learn a powerful model, called probabilistic higher-order grammar, from known solutions in a domain. We have implemented our approach in a tool called Euphony and evaluate it on SyGuS benchmark problems from a variety of domains. We show that Euphony can learn good models using easily obtainable solutions, and achieves significant performance gains over existing general-purpose as well as domain-specific synthesizers.","Statistical methods, Synthesis, Transfer learning, Domain-specific language",436–449,14,SIGPLAN Not.
428,@inproceedings: 10.1145/3194085.3194093,Towards a Methodology for Training with Synthetic Data on the Example of Pedestrian Detection in a Frame-by-Frame Semantic Segmentation Task,"Poibrenski, Atanas and Sprenger, Janis and M\""{u}ller, Christian",2018,Association for Computing Machinery,,https://doi.org/10.1145/3194085.3194093,10.1145/3194085.3194093,"In order to make highly/fully automated driving safe, synthetic training and validation data will be required, because critical road situations are too divers and too rare. A few studies on using synthetic data have been published, reporting a general increase in accuracy. In this paper, we propose a novel method to gain more in-depth insights in the quality, performance, and influence of synthetic data during training phase in a bounded setting. We demonstrate this method for the example of pedestrian detection in a frame-by-frame semantic segmentation class.","semantic segmentation, synthetic data, automated driving",31–34,4,Proceedings of the 1st International Workshop on Software Engineering for AI in Autonomous Systems
429,@inproceedings: 10.1145/3194085.3194087,Deep Learning for Self-Driving Cars: Chances and Challenges,"Rao, Qing and Frtunikj, Jelena",2018,Association for Computing Machinery,,https://doi.org/10.1145/3194085.3194087,10.1145/3194085.3194087,"Artificial Intelligence (AI) is revolutionizing the modern society. In the automotive industry, researchers and developers are actively pushing deep learning based approaches for autonomous driving. However, before a neural network finds its way into series production cars, it has to first undergo strict assessment concerning functional safety. The chances and challenges of incorporating deep learning for self-driving cars are presented in this paper.","deep learning, automotive, functional safety",35–38,4,Proceedings of the 1st International Workshop on Software Engineering for AI in Autonomous Systems
430,@inproceedings: 10.1145/3196321.3196351,Automatic Tag Recommendation for Software Development Video Tutorials,"Parra, Esteban and Escobar-Avila, Javier and Haiduc, Sonia",2018,Association for Computing Machinery,,https://doi.org/10.1145/3196321.3196351,10.1145/3196321.3196351,"Software development video tutorials are emerging as a new resource for developers to support their information needs. However, when trying to find the right video to watch for a task at hand, developers have little information at their disposal to quickly decide if they found the right video or not. This can lead to missing the best tutorials or wasting time watching irrelevant ones.Other external sources of information for developers, such as StackOverflow, have benefited from the existence of informative tags, which help developers to quickly gauge the relevance of posts and find related ones. We argue that the same is valid also for videos and propose the first set of approaches to automatically generate tags describing the contents of software development video tutorials. We investigate seven tagging approaches for this purpose, some using information retrieval techniques and leveraging only the information in the videos, others relying on external sources of information, such as StackOverflow, as well as two out-of-the-box commercial video tagging approaches. We evaluated 19 different configurations of these tagging approaches and the results of a user study showed that some of the information retrieval-based approaches performed the best and were able to recommend tags that developers consider relevant for describing programming videos.","information retrieval, software engineering, automatic tagging, video tutorials",222–232,11,Proceedings of the 26th Conference on Program Comprehension
431,@inproceedings: 10.1145/3194085.3194088,Distributed Deep Reinforcement Learning on the Cloud for Autonomous Driving,"Spryn, Mitchell and Sharma, Aditya and Parkar, Dhawal and Shrimal, Madhur",2018,Association for Computing Machinery,,https://doi.org/10.1145/3194085.3194088,10.1145/3194085.3194088,"This paper proposes an architecture for leveraging cloud computing technology to reduce training time for deep reinforcement learning models for autonomous driving by distributing the training process across a pool of virtual machines. By parallelizing the training process, careful design of the reward function and use of techniques like transfer learning, we demonstrate a decrease in training time for our example autonomous driving problem from 140 hours to less than 1 hour. We go over our network architecture, job distribution paradigm, reward function design and report results from experiments on small sized cluster (1--6 training nodes) of machines. We also discuss the limitations of our approach when trying to scale up to massive clusters.","deep reinforcement learning, simulation, autonomous driving, distributed machine learning, cloud computing",16–22,7,Proceedings of the 1st International Workshop on Software Engineering for AI in Autonomous Systems
432,@inproceedings: 10.1145/3194133.3194147,A Learning Approach to Enhance Assurances for Real-Time Self-Adaptive Systems,"Rodrigues, Arthur and Caldas, Ricardo Diniz and Rodrigues, Gena\'{\i}na Nunes and Vogel, Thomas and Pelliccione, Patrizio",2018,Association for Computing Machinery,,https://doi.org/10.1145/3194133.3194147,10.1145/3194133.3194147,"The assurance of real-time properties is prone to context variability. Providing such assurance at design time would require to check all the possible context and system variations or to predict which one will be actually used. Both cases are not viable in practice since there are too many possibilities to foresee. Moreover, the knowledge required to fully provide the assurance for self-adaptive systems is only available at runtime and therefore difficult to predict at early development stages. Despite all the efforts on assurances for self-adaptive systems at design or runtime, there is still a gap on verifying and validating real-time constraints accounting for context variability. To fill this gap, we propose a method to provide assurance of self-adaptive systems, at design- and runtime, with special focus on real-time constraints. We combine off-line requirements elicitation and model checking with on-line data collection and data mining to guarantee the system's goals, both functional and non-functional, with fine tuning of the adaptation policies towards the optimization of quality attributes. We experimentally evaluate our method on a simulated prototype of a Body Sensor Network system (BSN) implemented in OpenDaVINCI. The results of the validation are promising and show that our method is effective in providing evidence that support the provision of assurance.","self-adaptive systems, goal-oriented, learning approach, assurance evidence, real-time systems, data mining",206–216,11,Proceedings of the 13th International Conference on Software Engineering for Adaptive and Self-Managing Systems
433,@inproceedings: 10.1145/3194085.3194092,How Machine Perception Relates to Human Perception: Visual Saliency and Distance in a Frame-by-Frame Semantic Segmentation Task for Highly/Fully Automated Driving,"Herbig, Nico and Wiehr, Frederik and Poibrenski, Atanas and Sprenger, Janis and M\""{u}ller, Christian",2018,Association for Computing Machinery,,https://doi.org/10.1145/3194085.3194092,10.1145/3194085.3194092,"In this paper, we investigate the link between machine perception and human perception for highly/fully automated driving. We compare the classification results of a camera-based frame-by-frame semantic segmentation model (Machine) with a well-established visual saliency model (Human) on the Cityscapes dataset. The results show that Machine classifies foreground objects better if they are more salient, indicating a similarity with the human visual system. For background objects, the accuracy drops when the saliency increases, giving evidence for the assumption that Machine has an implicit concept of saliency.","saliency, semantic segmentation, automated driving",6–10,5,Proceedings of the 1st International Workshop on Software Engineering for AI in Autonomous Systems
434,@inproceedings: 10.1145/3194133.3194152,Adapting a System with Noisy Outputs with Statistical Guarantees,"Gerostathopoulos, Ilias and Prehofer, Christian and Bures, Tomas",2018,Association for Computing Machinery,,https://doi.org/10.1145/3194133.3194152,10.1145/3194133.3194152,"Many complex systems are intrinsically stochastic in their behavior which complicates their control and optimization. Current self-adaptation and self-optimization approaches are not tailored to systems that have (i) complex internal behavior that is unrealistic to model explicitly, (ii) noisy outputs, (iii) high cost of bad adaptation decisions, i.e. systems that are both hard and risky to adapt at runtime. In response, we propose to model the system to be adapted as black box and apply state-of-the-art optimization techniques combined with statistical guarantees. Our main contribution is a framework that combines runtime optimization with guarantees obtained from statistical testing and with a method for handling cost of bad adaptation decisions. We evaluate the feasibility of our approach by applying it on an existing traffic navigation self-adaptation exemplar.","statistical guarantees, experimentation cost, self-adaptation",58–68,11,Proceedings of the 13th International Conference on Software Engineering for Adaptive and Self-Managing Systems
435,@inproceedings: 10.1145/3196398.3196442,Data-Driven Search-Based Software Engineering,"Nair, Vivek and Agrawal, Amritanshu and Chen, Jianfeng and Fu, Wei and Mathew, George and Menzies, Tim and Minku, Leandro and Wagner, Markus and Yu, Zhe",2018,Association for Computing Machinery,,https://doi.org/10.1145/3196398.3196442,10.1145/3196398.3196442,"This paper introduces Data-Driven Search-based Software Engineering (DSE), which combines insights from Mining Software Repositories (MSR) and Search-based Software Engineering (SBSE). While MSR formulates software engineering problems as data mining problems, SBSE reformulate Software Engineering (SE) problems as optimization problems and use meta-heuristic algorithms to solve them. Both MSR and SBSE share the common goal of providing insights to improve software engineering. The algorithms used in these two areas also have intrinsic relationships. We, therefore, argue that combining these two fields is useful for situations (a) which require learning from a large data source or (b) when optimizers need to know the lay of the land to find better solutions, faster.This paper aims to answer the following three questions: (1) What are the various topics addressed by DSE?, (2) What types of data are used by the researchers in this area?, and (3) What research approaches do researchers use? The paper briefly sets out to act as a practical guide to develop new DSE techniques and also to serve as a teaching resource.This paper also presents a resource (tiny.cc/data-se) for exploring DSE. The resource contains 89 artifacts which are related to DSE, divided into 13 groups such as requirements engineering, software product lines, software processes. All the materials in this repository have been used in recent software engineering papers; i.e., for all this material, there exist baseline results against which researchers can comparatively assess their new ideas.",,341–352,12,Proceedings of the 15th International Conference on Mining Software Repositories
436,@inproceedings: 10.1145/3196398.3196408,Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow,"Yin, Pengcheng and Deng, Bowen and Chen, Edgar and Vasilescu, Bogdan and Neubig, Graham",2018,Association for Computing Machinery,,https://doi.org/10.1145/3196398.3196408,10.1145/3196398.3196408,"For tasks like code synthesis from natural language, code retrieval, and code summarization, data-driven models have shown great promise. However, creating these models require parallel data between natural language (NL) and code with fine-grained alignments. Stack Overflow (SO) is a promising source to create such a data set: the questions are diverse and most of them have corresponding answers with high quality code snippets. However, existing heuristic methods (e.g., pairing the title of a post with the code in the accepted answer) are limited both in their coverage and the correctness of the NL-code pairs obtained. In this paper, we propose a novel method to mine high-quality aligned data from SO using two sets of features: hand-crafted features considering the structure of the extracted snippets, and correspondence features obtained by training a probabilistic model to capture the correlation between NL and code using neural networks. These features are fed into a classifier that determines the quality of mined NL-code pairs. Experiments using Python and Java as test beds show that the proposed method greatly expands coverage and accuracy over existing mining methods, even when using only a small number of labeled examples. Further, we find that reasonable results are achieved even when training the classifier on one language and testing on another, showing promise for scaling NL-code mining to a wide variety of programming languages beyond those for which we are able to annotate data.",,476–486,11,Proceedings of the 15th International Conference on Mining Software Repositories
437,@inproceedings: 10.1145/3196398.3196431,Deep Learning Similarities from Different Representations of Source Code,"Tufano, Michele and Watson, Cody and Bavota, Gabriele and Di Penta, Massimiliano and White, Martin and Poshyvanyk, Denys",2018,Association for Computing Machinery,,https://doi.org/10.1145/3196398.3196431,10.1145/3196398.3196431,"Assessing the similarity between code components plays a pivotal role in a number of Software Engineering (SE) tasks, such as clone detection, impact analysis, refactoring, etc. Code similarity is generally measured by relying on manually defined or hand-crafted features, e.g., by analyzing the overlap among identifiers or comparing the Abstract Syntax Trees of two code components. These features represent a best guess at what SE researchers can utilize to exploit and reliably assess code similarity for a given task. Recent work has shown, when using a stream of identifiers to represent the code, that Deep Learning (DL) can effectively replace manual feature engineering for the task of clone detection. However, source code can be represented at different levels of abstraction: identifiers, Abstract Syntax Trees, Control Flow Graphs, and Bytecode. We conjecture that each code representation can provide a different, yet orthogonal view of the same code fragment, thus, enabling a more reliable detection of similarities in code. In this paper, we demonstrate how SE tasks can benefit from a DL-based approach, which can automatically learn code similarities from different representations.","deep learning, neural networks, code similarities",542–553,12,Proceedings of the 15th International Conference on Mining Software Repositories
438,@inproceedings: 10.1145/3180155.3180220,DeepTest: Automated Testing of Deep-Neural-Network-Driven Autonomous Cars,"Tian, Yuchi and Pei, Kexin and Jana, Suman and Ray, Baishakhi",2018,Association for Computing Machinery,,https://doi.org/10.1145/3180155.3180220,10.1145/3180155.3180220,"Recent advances in Deep Neural Networks (DNNs) have led to the development of DNN-driven autonomous cars that, using sensors like camera, LiDAR, etc., can drive without any human intervention. Most major manufacturers including Tesla, GM, Ford, BMW, and Waymo/Google are working on building and testing different types of autonomous vehicles. The lawmakers of several US states including California, Texas, and New York have passed new legislation to fast-track the process of testing and deployment of autonomous vehicles on their roads.However, despite their spectacular progress, DNNs, just like traditional software, often demonstrate incorrect or unexpected corner-case behaviors that can lead to potentially fatal collisions. Several such real-world accidents involving autonomous cars have already happened including one which resulted in a fatality. Most existing testing techniques for DNN-driven vehicles are heavily dependent on the manual collection of test data under different driving conditions which become prohibitively expensive as the number of test conditions increases.In this paper, we design, implement, and evaluate DeepTest, a systematic testing tool for automatically detecting erroneous behaviors of DNN-driven vehicles that can potentially lead to fatal crashes. First, our tool is designed to automatically generated test cases leveraging real-world changes in driving conditions like rain, fog, lighting conditions, etc. DeepTest systematically explore different parts of the DNN logic by generating test inputs that maximize the numbers of activated neurons. DeepTest found thousands of erroneous behaviors under different realistic driving conditions (e.g., blurring, rain, fog, etc.) many of which lead to potentially fatal crashes in three top performing DNNs in the Udacity self-driving car challenge.","deep neural networks, autonomous vehicle, deep learning, neuron coverage, self-driving cars, testing",303–314,12,Proceedings of the 40th International Conference on Software Engineering
439,@inbook: 10.1145/3173162.3173184,CALOREE: Learning Control for Predictable Latency and Low Energy,"Mishra, Nikita and Imes, Connor and Lafferty, John D. and Hoffmann, Henry",2018,Association for Computing Machinery,,https://doi.org/10.1145/3173162.3173184,,"Many modern computing systems must provide reliable latency with minimal energy. Two central challenges arise when allocating system resources to meet these conflicting goals: (1) complexity modern hardware exposes diverse resources with complicated interactions and (2) dynamics latency must be maintained despite unpredictable changes in operating environment or input. Machine learning accurately models the latency of complex, interacting resources, but does not address system dynamics; control theory adjusts to dynamic changes, but struggles with complex resource interaction. We therefore propose CALOREE, a resource manager that learns key control parameters to meet latency requirements with minimal energy in complex, dynamic en- vironments. CALOREE breaks resource allocation into two sub-tasks: learning how interacting resources affect speedup, and controlling speedup to meet latency requirements with minimal energy. CALOREE deines a general control system whose parameters are customized by a learning framework while maintaining control-theoretic formal guarantees that the latency goal will be met. We test CALOREE's ability to deliver reliable latency on heterogeneous ARM big.LITTLE architectures in both single and multi-application scenarios. Compared to the best prior learning and control solutions, CALOREE reduces deadline misses by 60% and energy consumption by 13%.",,184–198,1,Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems
440,@article: 10.1145/3296957.3173184,CALOREE: Learning Control for Predictable Latency and Low Energy,"Mishra, Nikita and Imes, Connor and Lafferty, John D. and Hoffmann, Henry",2018,Association for Computing Machinery,0362-1340,https://doi.org/10.1145/3296957.3173184,10.1145/3296957.3173184,"Many modern computing systems must provide reliable latency with minimal energy. Two central challenges arise when allocating system resources to meet these conflicting goals: (1) complexity modern hardware exposes diverse resources with complicated interactions and (2) dynamics latency must be maintained despite unpredictable changes in operating environment or input. Machine learning accurately models the latency of complex, interacting resources, but does not address system dynamics; control theory adjusts to dynamic changes, but struggles with complex resource interaction. We therefore propose CALOREE, a resource manager that learns key control parameters to meet latency requirements with minimal energy in complex, dynamic en- vironments. CALOREE breaks resource allocation into two sub-tasks: learning how interacting resources affect speedup, and controlling speedup to meet latency requirements with minimal energy. CALOREE deines a general control system whose parameters are customized by a learning framework while maintaining control-theoretic formal guarantees that the latency goal will be met. We test CALOREE's ability to deliver reliable latency on heterogeneous ARM big.LITTLE architectures in both single and multi-application scenarios. Compared to the best prior learning and control solutions, CALOREE reduces deadline misses by 60% and energy consumption by 13%.","machine learning, real-time systems, energy, resource allocation, heterogeneous architectures, control theor",184–198,15,SIGPLAN Not.
441,@inbook: 10.1145/3173162.3173185,Potluck: Cross-Application Approximate Deduplication for Computation-Intensive Mobile Applications,"Guo, Peizhen and Hu, Wenjun",2018,Association for Computing Machinery,,https://doi.org/10.1145/3173162.3173185,,"Emerging mobile applications, such as cognitive assistance and augmented reality (AR) based gaming, are increasingly computation-intensive and latency-sensitive, while running on resource-constrained devices. The standard approaches to addressing these involve either offloading to a cloud(let) or local system optimizations to speed up the computation, often trading off computation quality for low latency. Instead, we observe that these applications often operate on similar input data from the camera feed and share common processing components, both within the same (type of) applications and across different ones. Therefore, deduplicating processing across applications could deliver the best of both worlds. In this paper, we present Potluck, to achieve approximate deduplication. At the core of the system is a cache service that stores and shares processing results between applications and a set of algorithms to process the input data to maximize deduplication opportunities. This is implemented as a background service on Android. Extensive evaluation shows that Potluck can reduce the processing latency for our AR and vision workloads by a factor of 2.5 to 10.",,271–284,1,Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems
442,@article: 10.1145/3296957.3173185,Potluck: Cross-Application Approximate Deduplication for Computation-Intensive Mobile Applications,"Guo, Peizhen and Hu, Wenjun",2018,Association for Computing Machinery,0362-1340,https://doi.org/10.1145/3296957.3173185,10.1145/3296957.3173185,"Emerging mobile applications, such as cognitive assistance and augmented reality (AR) based gaming, are increasingly computation-intensive and latency-sensitive, while running on resource-constrained devices. The standard approaches to addressing these involve either offloading to a cloud(let) or local system optimizations to speed up the computation, often trading off computation quality for low latency. Instead, we observe that these applications often operate on similar input data from the camera feed and share common processing components, both within the same (type of) applications and across different ones. Therefore, deduplicating processing across applications could deliver the best of both worlds. In this paper, we present Potluck, to achieve approximate deduplication. At the core of the system is a cache service that stores and shares processing results between applications and a set of algorithms to process the input data to maximize deduplication opportunities. This is implemented as a background service on Android. Extensive evaluation shows that Potluck can reduce the processing latency for our AR and vision workloads by a factor of 2.5 to 10.","approximate deduplication, caching, mobile computin",271–284,14,SIGPLAN Not.
443,@inbook: 10.1145/3173162.3173177,Google Workloads for Consumer Devices: Mitigating Data Movement Bottlenecks,"Boroumand, Amirali and Ghose, Saugata and Kim, Youngsok and Ausavarungnirun, Rachata and Shiu, Eric and Thakur, Rahul and Kim, Daehyun and Kuusela, Aki and Knies, Allan and Ranganathan, Parthasarathy and Mutlu, Onur",2018,Association for Computing Machinery,,https://doi.org/10.1145/3173162.3173177,,"We are experiencing an explosive growth in the number of consumer devices, including smartphones, tablets, web-based computers such as Chromebooks, and wearable devices. For this class of devices, energy efficiency is a first-class concern due to the limited battery capacity and thermal power budget. We find that data movement is a major contributor to the total system energy and execution time in consumer devices. The energy and performance costs of moving data between the memory system and the compute units are significantly higher than the costs of computation. As a result, addressing data movement is crucial for consumer devices. In this work, we comprehensively analyze the energy and performance impact of data movement for several widely-used Google consumer workloads: (1) the Chrome web browser; (2) TensorFlow Mobile, Google's machine learning framework; (3) video playback, and (4) video capture, both of which are used in many video services such as YouTube and Google Hangouts. We find that processing-in-memory (PIM) can significantly reduce data movement for all of these workloads, by performing part of the computation close to memory. Each workload contains simple primitives and functions that contribute to a significant amount of the overall data movement. We investigate whether these primitives and functions are feasible to implement using PIM, given the limited area and power constraints of consumer devices. Our analysis shows that offloading these primitives to PIM logic, consisting of either simple cores or specialized accelerators, eliminates a large amount of data movement, and significantly reduces total system energy (by an average of 55.4% across the workloads) and execution time (by an average of 54.2%).",,316–331,1,Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems
444,@article: 10.1145/3296957.3173177,Google Workloads for Consumer Devices: Mitigating Data Movement Bottlenecks,"Boroumand, Amirali and Ghose, Saugata and Kim, Youngsok and Ausavarungnirun, Rachata and Shiu, Eric and Thakur, Rahul and Kim, Daehyun and Kuusela, Aki and Knies, Allan and Ranganathan, Parthasarathy and Mutlu, Onur",2018,Association for Computing Machinery,0362-1340,https://doi.org/10.1145/3296957.3173177,10.1145/3296957.3173177,"We are experiencing an explosive growth in the number of consumer devices, including smartphones, tablets, web-based computers such as Chromebooks, and wearable devices. For this class of devices, energy efficiency is a first-class concern due to the limited battery capacity and thermal power budget. We find that data movement is a major contributor to the total system energy and execution time in consumer devices. The energy and performance costs of moving data between the memory system and the compute units are significantly higher than the costs of computation. As a result, addressing data movement is crucial for consumer devices. In this work, we comprehensively analyze the energy and performance impact of data movement for several widely-used Google consumer workloads: (1) the Chrome web browser; (2) TensorFlow Mobile, Google's machine learning framework; (3) video playback, and (4) video capture, both of which are used in many video services such as YouTube and Google Hangouts. We find that processing-in-memory (PIM) can significantly reduce data movement for all of these workloads, by performing part of the computation close to memory. Each workload contains simple primitives and functions that contribute to a significant amount of the overall data movement. We investigate whether these primitives and functions are feasible to implement using PIM, given the limited area and power constraints of consumer devices. Our analysis shows that offloading these primitives to PIM logic, consisting of either simple cores or specialized accelerators, eliminates a large amount of data movement, and significantly reduces total system energy (by an average of 55.4% across the workloads) and execution time (by an average of 54.2%).","consumer workloads, memory systems, processing-in-memory, data movement, energy efficienc",316–331,16,SIGPLAN Not.
445,@inproceedings: 10.1145/3174243.3174266,A Lightweight YOLOv2: A Binarized CNN with A Parallel Support Vector Regression for an FPGA,"Nakahara, Hiroki and Yonekawa, Haruyoshi and Fujii, Tomoya and Sato, Shimpei",2018,Association for Computing Machinery,,https://doi.org/10.1145/3174243.3174266,10.1145/3174243.3174266,"A frame object detection problem consists of two problems: one is a regression problem to spatially separated bounding boxes, the second is the associated classification of the objects within realtime frame rate. It is widely used in the embedded systems, such as robotics, autonomous driving, security, and drones - all of which require high-performance and low-power consumption. This paper implements the YOLO (You only look once) object detector on an FPGA, which is faster and has a higher accuracy. It is based on the convolutional deep neural network (CNN), and it is a dominant part both the performance and the area. However, the object detector based on the CNN consists of a bounding box prediction (regression) and a class estimation (classification). Thus, the conventional all binarized CNN fails to recognize in most cases. In the paper, we propose a lightweight YOLOv2, which consists of the binarized CNN for a feature extraction and the parallel support vector regression (SVR) for both a classification and a localization. To our knowledge, this is the first time binarized CNN»s have been successfully used in object detection. We implement a pipelined based architecture for the lightweight YOLOv2 on the Xilinx Inc. zcu102 board, which has the Xilinx Inc. Zynq Ultrascale+ MPSoC. The implemented object detector archived 40.81 frames per second (FPS). Compared with the ARM Cortex-A57, it was 177.4 times faster, it dissipated 1.1 times more power, and its performance per power efficiency was 158.9 times better. Also, compared with the nVidia Pascall embedded GPU, it was 27.5 times faster, it dissipated 1.5 times lower power, and its performance per power efficiency was 42.9 times better. Thus, our method is suitable for the frame object detector for an embedded vision system.","binarized deep neural network, object detection, convolutional deep neural network",31–40,10,Proceedings of the 2018 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays
446,@inproceedings: 10.1145/3174243.3174261,DeltaRNN: A Power-Efficient Recurrent Neural Network Accelerator,"Gao, Chang and Neil, Daniel and Ceolini, Enea and Liu, Shih-Chii and Delbruck, Tobi",2018,Association for Computing Machinery,,https://doi.org/10.1145/3174243.3174261,10.1145/3174243.3174261,"Recurrent Neural Networks (RNNs) are widely used in speech recognition and natural language processing applications because of their capability to process temporal sequences. Because RNNs are fully connected, they require a large number of weight memory accesses, leading to high power consumption. Recent theory has shown that an RNN delta network update approach can reduce memory access and computes with negligible accuracy loss. This paper describes the implementation of this theoretical approach in a hardware accelerator called ""DeltaRNN"" (DRNN). The DRNN updates the output of a neuron only when the neuron»s activation changes by more than a delta threshold. It was implemented on a Xilinx Zynq-7100 FPGA. FPGA measurement results from a single-layer RNN of 256 Gated Recurrent Unit (GRU) neurons show that the DRNN achieves 1.2 TOp/s effective throughput and 164 GOp/s/W power efficiency. The delta update leads to a 5.7x speedup compared to a conventional RNN update because of the sparsity created by the DN algorithm and the zero-skipping ability of DRNN.","hardware accelerator, recurrent neural network, fpga, gated recurrent unit, deep learning, delta network",21–30,10,Proceedings of the 2018 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays
447,@inproceedings: 10.1145/3178487.3178495,Bridging the Gap between Deep Learning and Sparse Matrix Format Selection,"Zhao, Yue and Li, Jiajia and Liao, Chunhua and Shen, Xipeng",2018,Association for Computing Machinery,,https://doi.org/10.1145/3178487.3178495,10.1145/3178487.3178495,"This work presents a systematic exploration on the promise and special challenges of deep learning for sparse matrix format selection---a problem of determining the best storage format for a matrix to maximize the performance of Sparse Matrix Vector Multiplication (SpMV). It describes how to effectively bridge the gap between deep learning and the special needs of the pillar HPC problem through a set of techniques on matrix representations, deep learning structure, and cross-architecture model migrations. The new solution cuts format selection errors by two thirds, and improves SpMV performance by 1.73X on average over the state of the art.","deep learning, format selection, convolutional neural network, SpMV, sparse matrix",94–108,15,Proceedings of the 23rd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming
448,@article: 10.1145/3200691.3178495,Bridging the Gap between Deep Learning and Sparse Matrix Format Selection,"Zhao, Yue and Li, Jiajia and Liao, Chunhua and Shen, Xipeng",2018,Association for Computing Machinery,0362-1340,https://doi.org/10.1145/3200691.3178495,10.1145/3200691.3178495,"This work presents a systematic exploration on the promise and special challenges of deep learning for sparse matrix format selection---a problem of determining the best storage format for a matrix to maximize the performance of Sparse Matrix Vector Multiplication (SpMV). It describes how to effectively bridge the gap between deep learning and the special needs of the pillar HPC problem through a set of techniques on matrix representations, deep learning structure, and cross-architecture model migrations. The new solution cuts format selection errors by two thirds, and improves SpMV performance by 1.73X on average over the state of the art.","convolutional neural network, deep learning, format selection, sparse matrix, SpM",94–108,15,SIGPLAN Not.
449,@inproceedings: 10.1145/3172871.3172872,Feature Selection Techniques to Counter Class Imbalance Problem for Aging Related Bug Prediction: Aging Related Bug Prediction,"Kumar, Lov and Sureka, Ashish",2018,Association for Computing Machinery,,https://doi.org/10.1145/3172871.3172872,10.1145/3172871.3172872,"Aging-Related Bugs (ARBs) occur in long running systems due to error conditions caused because of accumulation of problems such as memory leakage or unreleased files and locks. Aging-Related Bugs are hard to discover during software testing and also challenging to replicate. Automatic identification and prediction of aging related fault-prone files and classes in an object oriented system can help the software quality assurance team to optimize their testing efforts. In this paper, we present a study on the application of static source code metrics and machine learning techniques to predict aging related bugs. We conduct a series of experiments on publicly available dataset from two large open-source software systems: Linux and MySQL. Class imbalance and high dimensionality are the two main technical challenges in building effective predictors for aging related bugs.We investigate the application of five different feature selection techniques (OneR, Information Gain, Gain Ratio, RELEIF and Symmetric Uncertainty) for dimensionality reduction and five different strategies (Random Under-sampling, Random Oversampling, SMOTE, SMOTEBoost and RUSBoost) to counter the effect of class imbalance in our proposed machine learning based solution approach. Experimental results reveal that the random under-sampling approach performs best followed by RUSBoost in-terms of the mean AUC metric. Statistical significance test demonstrates that there is a significant difference between the performance of the various feature selection techniques. Experimental results shows that Gain Ratio and RELEIF performs best in comparison to other strategies to address the class imbalance problem. We infer from the statistical significance test that there is no difference between the performances of the five different learning algorithms.","Machine Learning, Source Code Metrics, Software Maintenance, Imbalance Learning, Predictive Modeling, Feature Selection Techniques, Empirical Software Engineering, Aging Related Bugs",,11,Proceedings of the 11th Innovations in Software Engineering Conference
450,@inproceedings: 10.1145/3268935.3268937,VirtSense: Virtualize Sensing through ARM TrustZone on Internet-of-Things,"Liu, Renju and Srivastava, Mani",2018,Association for Computing Machinery,,https://doi.org/10.1145/3268935.3268937,10.1145/3268935.3268937,"Internet-of-Things (IoTs) are becoming more and more popular in our life. IoT devices are generally designed for sensing or actuation purposes. However, the current sensing system on IoT devices lacks the understanding of sensing needs, which diminishes the sensing flexibility, isolation, and security when multiple sensing applications need to use sensor resources. In this work, we propose VirtSense, an ARM TrustZone based virtual sensing system, to provide each sensing application a virtual sensor instance, which further enables a safe, flexible and isolated sensing environment on the IoT devices. Our preliminary results show that VirtSense: 1) can provide virtual sensor instance for each sensing application so that the sensing needs of each application will be satisfied without affecting others; 2) is able to enforce access control policy even under an untrusted environment.","arm trustzone, iot security, sensor network, sensor virtualization",2–7,6,Proceedings of the 3rd Workshop on System Software for Trusted Execution
451,@inproceedings: 10.1145/3178212.3178230,Combining Word Order and CNN-LSTM for Sentence Sentiment Classification,"Shuang, Kai and Ren, Xintao and Chen, Jian and Shan, Xiaohan and Xu, Peng",2017,Association for Computing Machinery,,https://doi.org/10.1145/3178212.3178230,10.1145/3178212.3178230,"Neural network models have been demonstrated to be capable of achieving state-of-the-art performance in sentence sentiment classification. Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) are two widely used neural network models for NLP. However, since sentences consist of the same words in different order may represent different meaning in sentiment, it cannot be neglected that the word embedding training model ignores the factor of word order in sentence to quicken the training process. In this work, we mainly consider that word order is important for sentence sentiment classification, designing an encode-decode model called CNN-LSTM combined the strength of CNN and LSTM to demonstrate that word order of sentence plays an important role in sentiment analysis based on the word embedding which is designed as an order_w2v model taking in word order during word2vec training process. We evaluate the CNN-LSTM and order_w2v in sentiment classification both on Chinese and English datasets. The experimental results verify that the model considering the word order can achieve better results in sentiment analysis.","CNN-LSTM, word order, sentiment classification, Natural language processing",17–21,5,Proceedings of the 2017 International Conference on Software and E-Business
452,@inproceedings: 10.1145/3139131.3139144,A Study on Improving Performance in Gesture Training through Visual Guidance Based on Learners' Errors,"Jeanne, Florian and Thouvenin, Indira and Lenglet, Alban",2017,Association for Computing Machinery,,https://doi.org/10.1145/3139131.3139144,10.1145/3139131.3139144,"Gesture training, especially for technical gestures, requires supervisors to point out errors made by trainees. Virtual reality (VR) makes it possible to reduce reliance on supervisors (fewer interventions and of shorter duration) and to reduce the length of training, using extrinsic feedback that provides training or learning assistance using different modalities (visual, auditory, and haptic). Visual feedback has received much attention in recent decades. Users can be guided by a metaphor in a virtual environment. This metaphor may be a 3D trace of canonical movements, a visual cue pointing in the right direction, or gestures by an avatar that the trainee must mimic. However, with many kinds of feedback, trainees are not aware of their errors while performing gestures. Our hypothesis is that guiding users with a dynamic metaphor based on the visualization of errors will reduce these errors and improve performance. To this end, in a previous work we designed and implemented a new 3D metaphor called EBAGG to guide users in real time.In the present paper we evaluate EBAGG in relation to two other visual cues: first, a feedforward technique that displays the trace of a reference movement, and, second, a concurrent orientation feedback. The results of the user study show that EBAGG outperformed the others in improving users' performances over a training session. Moreover, the information assimilated during training with this dynamic feedback had a persistent effect when the metaphor was no longer displayed.","gestures, guidance, user study, performance, visual feedback, virtual reality",,10,Proceedings of the 23rd ACM Symposium on Virtual Reality Software and Technology
453,@inproceedings: 10.1145/3139131.3139137,Measurement of Exceptional Motion in VR Video Contents for VR Sickness Assessment Using Deep Convolutional Autoencoder,"Kim, Hak Gu and Baddar, Wissam J. and Lim, Heoun-taek and Jeong, Hyunwook and Ro, Yong Man",2017,Association for Computing Machinery,,https://doi.org/10.1145/3139131.3139137,10.1145/3139131.3139137,"This paper proposes a new objective metric of exceptional motion in VR video contents for VR sickness assessment. In VR environment, VR sickness can be caused by several factors which are mismatched motion, field of view, motion parallax, viewing angle, etc. Similar to motion sickness, VR sickness can induce a lot of physical symptoms such as general discomfort, headache, stomach awareness, nausea, vomiting, fatigue, and disorientation. To address the viewing safety issues in virtual environment, it is of great importance to develop an objective VR sickness assessment method that predicts and analyses the degree of VR sickness induced by the VR content. The proposed method takes into account motion information that is one of the most important factors in determining the overall degree of VR sickness. In this paper, we detect the exceptional motion that is likely to induce VR sickness. Spatio-temporal features of the exceptional motion in the VR video content are encoded using a convolutional autoencoder. For objectively assessing the VR sickness, the level of exceptional motion in VR video content is measured by using the convolutional autoencoder as well. The effectiveness of the proposed method has been successfully evaluated by subjective assessment experiment using simulator sickness questionnaires (SSQ) in VR environment.","virtual reality, cybersickness, machine learning",,7,Proceedings of the 23rd ACM Symposium on Virtual Reality Software and Technology
454,@inproceedings: 10.1145/3139131.3139139,Auris: Creating Affective Virtual Spaces from Music,"Sra, Misha and Maes, Pattie and Vijayaraghavan, Prashanth and Roy, Deb",2017,Association for Computing Machinery,,https://doi.org/10.1145/3139131.3139139,10.1145/3139131.3139139,"Affective virtual spaces are of interest in many virtual reality applications such as education, wellbeing, rehabilitation, and entertainment. In this paper we present Auris, a system that attempts to generate affective virtual environments from music. We use music as input because it inherently encodes emotions that listeners readily recognize and respond to. Creating virtual environments is a time consuming and labor-intensive task involving various skills like design, 3D modeling, texturing, animation, and coding. Auris helps make this easier by automating the virtual world generation task using mood and content extracted from song audio and lyrics data respectively. Our user study results indicate virtual spaces created by Auris successfully convey the mood of the songs used to create them and achieve high presence scores with the potential to provide novel experiences of listening to music.","music, deep neural networks, virtual reality, generative models",,11,Proceedings of the 23rd ACM Symposium on Virtual Reality Software and Technology
455,@inproceedings: 10.1145/3127005.3127007,Clustering Dycom: An Online Cross-Company Software Effort Estimation Study,"Minku, Leandro L. and Hou, Siqing",2017,Association for Computing Machinery,,https://doi.org/10.1145/3127005.3127007,10.1145/3127005.3127007,"Background: Software Effort Estimation (SEE) can be formulated as an online learning problem, where new projects are completed over time and may become available for training. In this scenario, a Cross-Company (CC) SEE approach called Dycom can drastically reduce the number of Within-Company (WC) projects needed for training, saving the high cost of collecting such training projects. However, Dycom relies on splitting CC projects into different subsets in order to create its CC models. Such splitting can have a significant impact on Dycom's predictive performance. Aims: This paper investigates whether clustering methods can be used to help finding good CC splits for Dycom. Method: Dycom is extended to use clustering methods for creating the CC subsets. Three different clustering methods are investigated, namely Hierarchical Clustering, K-Means, and Expectation-Maximisation. Clustering Dycom is compared against the original Dycom with CC subsets of different sizes, based on four SEE databases. A baseline WC model is also included in the analysis. Results: Clustering Dycom with K-Means can potentially help to split the CC projects, managing to achieve similar or better predictive performance than Dycom. However, K-Means still requires the number of CC subsets to be pre-defined, and a poor choice can negatively affect predictive performance. EM enables Dycom to automatically set the number of CC subsets while still maintaining or improving predictive performance with respect to the baseline WC model. Clustering Dycom with Hierarchical Clustering did not offer significant advantage in terms of predictive performance. Conclusion: Clustering methods can be an effective way to automatically generate Dycom's CC subsets.","Software effort estimation, online learning, concept drift, ensembles, cross-company learning",12–21,10,Proceedings of the 13th International Conference on Predictive Models and Data Analytics in Software Engineering
456,@inproceedings: 10.5555/3172795.3172818,Transfer Learning in Neural Networks: An Experience Report,"Shtern, Mark and Ejaz, Rabia and Tzerpos, Vassilios",2017,IBM Corp.,,,,"Perhaps the most important characteristic of deep neural networks is their ability to discover and extract the necessary features for a particular machine learning task from a raw input representation. This requires a significant time commitment, both in terms of assembling the training dataset, and training the neural network. Reusing the knowledge inherent in a trained neural network for a machine learning task in a related domain can provide significant improvements in terms of the time required to complete the task.In this paper, we present our experience with such a transfer learning situation. We reuse a neural network that was trained on a real world image dataset, for the task of classifying music in terms of genre, instrumentation, composer etc. (audio files are converted to spectrograms for this purpose). Even though the image and music domains are not directly related, our experiments show that features extracted to recognize images allow for high accuracy in many music classification tasks.","music classification, transfer learning, deep learning",201–210,10,Proceedings of the 27th Annual International Conference on Computer Science and Software Engineering
457,@inproceedings: 10.1145/3141235.3141236,An Initial Investigation of Protocol Customization,"Hong, David Ke and Chen, Qi Alfred and Mao, Z. Morley",2017,Association for Computing Machinery,,https://doi.org/10.1145/3141235.3141236,10.1145/3141235.3141236,"Attacks exploiting design or implementation flaws of particular features in popular protocols are becoming prevalent and have led to severe security impacts on a majority of software systems. Protocol customization as a general approach to specialize a standard protocol holds significant promise in reducing such attack surfaces in common protocols. In this work, we perform an initial investigation of applying protocol customization practices to reduce the attack surface of standard protocols. Our characterization study on 20 medium or high-impact common vulnerability exposures (CVEs) published in recent years indicates that some forms of customization have been supported in existing protocol software, but were implemented with huge manual effort and in an ad-hoc manner. More systematic and automated ways of protocol customization are awaited to generalize common customization practices across protocols. To work towards this goal, we identify key research challenges for the support of systematic and sufficiently automated protocol customization through real-world case study on popular protocol software, and propose an access control framework as a principled solution to unify existing protocol customization practices. We also present a preliminary design of a protocol customization system based on this design principle. Preliminary evaluation results demonstrate that our proposed system supports common customization practices for a majority of real-world protocol vulnerabilities in a systematic way.","attack surface, protocol feature, protocol customization",57–64,8,Proceedings of the 2017 Workshop on Forming an Ecosystem Around Software Transformation
458,@inproceedings: 10.1145/3126594.3126640,More than a Feeling: The MiFace Framework for Defining Facial Communication Mappings,"Butler, Crystal and Michalowicz, Stephanie and Subramanian, Lakshmi and Burleson, Winslow",2017,Association for Computing Machinery,,https://doi.org/10.1145/3126594.3126640,10.1145/3126594.3126640,"Facial expressions transmit a variety of social, grammatical, and affective signals. For technology to leverage this rich source of communication, tools that better model the breadth of information they convey are required. MiFace is a novel framework for creating expression lexicons that map signal values to parameterized facial muscle movements. In traditional mapping paradigms using posed photographs, na\""{\i}ve judges select from predetermined label sets and movements are inferred by trained experts. The set of generally accepted expressions established in this way is limited to six basic displays of affect. In contrast, our approach generatively simulates muscle movements on a 3D avatar. By applying natural language processing techniques to crowdsourced free-response labels for the resulting images, we efficiently converge on an expression's value across signal categories. Two studies returned 218 discriminable facial expressions with 51 unique labels. The six basic emotions are included, but we additionally define such nuanced expressions as embarrassed, curious, and hopeful.","natural language processing, virtual humans, 3D modeling, avatars, facial expression recognition, social signal processing, affective computing",773–786,14,Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology
459,@inproceedings: 10.1145/3126594.3126634,CircuitSense: Automatic Sensing of Physical Circuits and Generation of Virtual Circuits to Support Software Tools.,"Wu, Te-Yen and Wang, Bryan and Lee, Jiun-Yu and Shen, Hao-Ping and Wu, Yu-Chian and Chen, Yu-An and Ku, Pin-Sung and Hsu, Ming-Wei and Lin, Yu-Chih and Chen, Mike Y.",2017,Association for Computing Machinery,,https://doi.org/10.1145/3126594.3126634,10.1145/3126594.3126634,"The rise of Maker communities and open-source electronic prototyping platforms have made electronic circuit projects increasingly popular around the world. Although there are software tools that support the debugging and sharing of circuits, they require users to manually create the virtual circuits in software, which can be time-consuming and error-prone. We present CircuitSense, a system that automatically recognizes the wires and electronic components placed on breadboards. It uses a combination of passive sensing and active probing to detect and generate the corresponding circuit representation in software in real-time. CircuitSense bridges the gap between the physical and virtual representations of circuits. It enables users to interactively construct and experiment with physical circuits while gaining the benefits of using software tools. It also dramatically simplifies the sharing of circuit designs with online communities.","component recognition, circuit virtualization, electric circuits",311–319,9,Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology
460,@inproceedings: 10.1145/3126594.3126614,Everyday Eye Contact Detection Using Unsupervised Gaze Target Discovery,"Zhang, Xucong and Sugano, Yusuke and Bulling, Andreas",2017,Association for Computing Machinery,,https://doi.org/10.1145/3126594.3126614,10.1145/3126594.3126614,"Eye contact is an important non-verbal cue in social signal processing and promising as a measure of overt attention in human-object interactions and attentive user interfaces. However, robust detection of eye contact across different users, gaze targets, camera positions, and illumination conditions is notoriously challenging. We present a novel method for eye contact detection that combines a state-of-the-art appearance-based gaze estimator with a novel approach for unsupervised gaze target discovery, i.e. without the need for tedious and time-consuming manual data annotation. We evaluate our method in two real-world scenarios: detecting eye contact at the workplace, including on the main work display, from cameras mounted to target objects, as well as during everyday social interactions with the wearer of a head-mounted egocentric camera. We empirically evaluate the performance of our method in both scenarios and demonstrate its effectiveness for detecting eye contact independent of target object type and size, camera position, and user and recording environment.","social signal processing, eye contact, attentive user interfaces, appearance-based gaze estimation",193–203,11,Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology
461,@inproceedings: 10.1145/3126594.3126653,Learning Visual Importance for Graphic Designs and Data Visualizations,"Bylinskii, Zoya and Kim, Nam Wook and O'Donovan, Peter and Alsheikh, Sami and Madan, Spandan and Pfister, Hanspeter and Durand, Fredo and Russell, Bryan and Hertzmann, Aaron",2017,Association for Computing Machinery,,https://doi.org/10.1145/3126594.3126653,10.1145/3126594.3126653,"Knowing where people look and click on visual designs can provide clues about how the designs are perceived, and where the most important or relevant content lies. The most important content of a visual design can be used for effective summarization or to facilitate retrieval from a database. We present automated models that predict the relative importance of different elements in data visualizations and graphic designs. Our models are neural networks trained on human clicks and importance annotations on hundreds of designs. We collected a new dataset of crowdsourced importance, and analyzed the predictions of our models with respect to ground truth importance and human eye movements. We demonstrate how such predictions of importance can be used for automatic design retargeting and thumbnailing. User studies with hundreds of MTurk participants validate that, with limited post-processing, our importance-driven applications are on par with, or outperform, current state-of-the-art methods, including natural image saliency. We also provide a demonstration of how our importance predictions can be built into interactive design tools to offer immediate feedback during the design process.","eye tracking, visualization, deep learning, retargeting, graphic design, saliency, computer vision, machine learning",57–69,13,Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology
462,@inproceedings: 10.1145/3125503.3125625,Snapshot-Based Offloading for Machine Learning Web App: Work-in-Progress,"Jeong, InChang and Jeong, Hyuk-Jin and Moon, Soo-Mook",2017,Association for Computing Machinery,,https://doi.org/10.1145/3125503.3125625,10.1145/3125503.3125625,"We propose a new approach to running machine learning (ML) web app on resource-constrained embedded devices by offloading ML computations to servers. We can dynamically offload computations depending on the problem size and network status. The execution state is saved in the form of another web app called snapshot which simplifies the state migration. Some issues related to ML such as how to handle the Canvas object, the ML model, and the privacy of user data are addressed. The proposed offloading works for real web apps with a performance comparable to running the app entirely on the server.","web application, machine learning, computation offloading",,2,Proceedings of the Thirteenth ACM International Conference on Embedded Software 2017 Companion
463,@inproceedings: 10.1145/3125502.3125606,Small Neural Nets Are Beautiful: Enabling Embedded Systems with Small Deep-Neural-Network Architectures,"Iandola, Forrest and Keutzer, Kurt",2017,Association for Computing Machinery,,https://doi.org/10.1145/3125502.3125606,10.1145/3125502.3125606,"Over the last five years Deep Neural Nets have offered more accurate solutions to many problems in speech recognition, and computer vision, and these solutions have surpassed a threshold of acceptability for many applications. As a result, Deep Neural Networks have supplanted other approaches to solving problems in these areas, and enabled many new applications. While the design of Deep Neural Nets is still something of an art form, in our work we have found basic principles of design space exploration used to develop embedded microprocessor architectures to be highly applicable to the design of Deep Neural Net architectures. In particular, we have used these design principles to create a novel Deep Neural Net called SqueezeNet that requires only 480KB of storage for its model parameters. We have further integrated all these experiences to develop something of a playbook for creating small Deep Neural Nets for embedded systems.","deep neural nets, embedded computer vision, convolutional neural nets, deep learning",,10,Proceedings of the Twelfth IEEE/ACM/IFIP International Conference on Hardware/Software Codesign and System Synthesis Companion
464,@inproceedings: 10.1145/3125502.3125542,Driving Behavior Modeling and Estimation for Battery Optimization in Electric Vehicles: Work-in-Progress,"Vatanpavar, Korosh and Faezi, Sina and Burago, Igor and Levorato, Marco and Faruque, Mohammad Abdullah Al",2017,Association for Computing Machinery,,https://doi.org/10.1145/3125502.3125542,10.1145/3125502.3125542,"Battery and energy management methodologies such as automotive climate controls have been proposed to address the design challenges of driving range and battery lifetime in Electric Vehicles (EV). However, driving behavior estimation is a major factor neglected in these methodologies. In this paper, we propose a novel context-aware methodology for estimating the driving behavior in terms of future vehicle speeds that will be integrated into the EV battery optimization. We implement a driving behavior model using a variation of Artificial Neural Networks (ANN) called Nonlinear AutoRegressive model with eXogenous inputs (NARX). We train our novel context-aware NARX model based on historical behavior of real drivers, their recent driving reactions, and the route average speed retrieved from Google Maps in order to enable driver-specific and self-adaptive driving behavior modeling and long-term estimation. Our methodology shows only 12% error for up to 30-second speed prediction which is improved by 27% compared to the state-of-the-art. Hence, it can achieve up to 82% of the maximum energy saving and battery lifetime improvement possible by the ideal methodology where the future vehicle speed is known.","statistical modeling, model predictive control, HVAC, battery, neural network, electric vehicle, power optimization, CPS",,2,Proceedings of the Twelfth IEEE/ACM/IFIP International Conference on Hardware/Software Codesign and System Synthesis Companion
465,@article: 10.1145/3133887,SQLizer: Query Synthesis from Natural Language,"Yaghmazadeh, Navid and Wang, Yuepeng and Dillig, Isil and Dillig, Thomas",2017,Association for Computing Machinery,,https://doi.org/10.1145/3133887,10.1145/3133887," This paper presents a new technique for automatically synthesizing SQL queries from natural language (NL). At the core of our technique is a new NL-based program synthesis methodology that combines semantic parsing techniques from the NLP community with type-directed program synthesis and automated program repair. Starting with a program sketch obtained using standard parsing techniques, our approach involves an iterative refinement loop that alternates between probabilistic type inhabitation and automated sketch repair. We use the proposed idea to build an end-to-end system called SQLIZER that can synthesize SQL queries from natural language. Our method is fully automated, works for any database without requiring additional customization, and does not require users to know the underlying database schema. We evaluate our approach on over 450 natural language queries concerning three different databases, namely MAS, IMDB, and YELP. Our experiments show that the desired query is ranked within the top 5 candidates in close to 90% of the cases and that SQLIZER outperforms NALIR, a state-of-the-art tool that won a best paper award at VLDB'14. ","Program Synthesis, Programming by Natural Languages, Relational Database",,26,Proc. ACM Program. Lang.
466,@article: 10.1145/3127360.3127368,Transfer Learning for Cross-Project Change-Proneness Prediction in Object-Oriented Software Systems: A Feasibility Analysis,"Kumar, Lov and Behera, Ranjan Kumar and Rath, Santanu and Sureka, Ashish",2017,Association for Computing Machinery,0163-5948,https://doi.org/10.1145/3127360.3127368,10.1145/3127360.3127368,"Change-prone classes or modules are defined as regions of the source code which are more likely to change as a result of a software development of maintenance activity. Automatic identification of change-prone classes are useful for the software development team as they can focus their testing efforts on areas within the source code which are more likely to change. Several machine learning techniques have been proposed for predicting change-prone classes based on the application of source code metrics as indicators. However, most of the work has focused on within-project training and model building. There are several real word scenario in which sufficient training dataset is not available for model building such as in the case of a new project. Cross-project prediction is an approach which consists of training a model from dataset belonging to one project and testing it on dataset belonging to a different project. Cross-project change-proneness prediction is relatively unexplored.We propose a machine learning based approach for cross-project change-proneness prediction. We conduct experiments on 10 open-source Eclipse plug-ins and demonstrate the effectiveness of our approach. We frame several research questions comparing the performance of within project and cross project prediction and also propose a Genetic Algorithm (GA) based approach for identifying the best set of source code metrics. We conclude that for within project experimental setting, Random Forest (RF) technique results in the best precision. In case of cross-project change-proneness prediction, our analysis reveals that the NDTF ensemble method performs higher than other individual classifiers (such as decision tree and logistic regression) and ensemble methods in the experimental dataset. We conduct a comparison of within-project, cross-project without GA and cross-project with GA and our analysis reveals that cross-project with GA performs best followed by within-project and then cross-project without GA.",,1–11,1,SIGSOFT Softw. Eng. Notes
467,@inproceedings: 10.1145/3106237.3117776,Applying Deep Learning Based Automatic Bug Triager to Industrial Projects,"Lee, Sun-Ro and Heo, Min-Jae and Lee, Chan-Gun and Kim, Milhan and Jeong, Gaeul",2017,Association for Computing Machinery,,https://doi.org/10.1145/3106237.3117776,10.1145/3106237.3117776," Finding the appropriate developer for a bug report, so called `Bug Triage', is one of the bottlenecks in the bug resolution process. To address this problem, many approaches have proposed various automatic bug triage techniques in recent studies. We argue that most previous studies focused on open source projects only and did not consider deep learning techniques. In this paper, we propose to use Convolutional Neural Network and word embedding to build an automatic bug triager. The results of the experiments applied to both industrial and open source projects reveal benefits of the automatic approach and suggest co-operation of human and automatic triagers. Our experience in integrating and operating the proposed system in an industrial development environment is also reported. ","text classification, convolutional neural network, industrial project, automatic bug triage",926–931,6,Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering
468,@inproceedings: 10.1145/3092703.3092709,Reinforcement Learning for Automatic Test Case Prioritization and Selection in Continuous Integration,"Spieker, Helge and Gotlieb, Arnaud and Marijan, Dusica and Mossige, Morten",2017,Association for Computing Machinery,,https://doi.org/10.1145/3092703.3092709,10.1145/3092703.3092709," Testing in Continuous Integration (CI) involves test case prioritization, selection, and execution at each cycle. Selecting the most promising test cases to detect bugs is hard if there are uncertainties on the impact of committed code changes or, if traceability links between code and tests are not available. This paper introduces Retecs, a new method for automatically learning test case selection and prioritization in CI with the goal to minimize the round-trip time between code commits and developer feedback on failed test cases. The Retecs method uses reinforcement learning to select and prioritize test cases according to their duration, previous last execution and failure history. In a constantly changing environment, where new test cases are created and obsolete test cases are deleted, the Retecs method learns to prioritize error-prone test cases higher under guidance of a reward function and by observing previous CI cycles. By applying Retecs on data extracted from three industrial case studies, we show for the first time that reinforcement learning enables fruitful automatic adaptive test case selection and prioritization in CI and regression testing. ","Test case prioritization, Machine Learning, Continuous Integration, Regression testing, Test case selection, Reinforcement Learning",12–22,11,Proceedings of the 26th ACM SIGSOFT International Symposium on Software Testing and Analysis
469,@inproceedings: 10.1145/3084226.3084249,Effort and Cost in Software Engineering: A Comparison of Two Industrial Data Sets,"Huijgens, Hennie and van Deursen, Arie and Minku, Leandro L. and Lokan, Chris",2017,Association for Computing Machinery,,https://doi.org/10.1145/3084226.3084249,10.1145/3084226.3084249,"Context The research literature on software development projects usually assumes that effort is a good proxy for cost. Practice, however, suggests that there are circumstances in which costs and effort should be distinguished. Objectives: We determine similarities and differences between size, effort, cost, duration, and number of defects of software projects. Method: We compare two established repositories (ISBSG and EBSPM) comprising almost 700 projects from industry. Results: We demonstrate a (log)-linear relation between cost on the one hand, and size, duration and number of defects on the other. This justifies conducting linear regression for cost. We establish that ISBSG is substantially different from EBSPM, in terms of cost (cheaper) and duration (faster), and the relation between cost and effort. We show that while in ISBSG effort is the most important cost factor, this is not the case in other repositories, such as EBSPM in which size is the dominant factor. Conclusion: Practitioners and researchers alike should be cautious when drawing conclusions from a single repository.","EBSPM, Benchmarking, Cost Prediction, Software Economics, ISBSG, Evidence-Based Software Portfolio Management",51–60,10,Proceedings of the 21st International Conference on Evaluation and Assessment in Software Engineering
470,@inproceedings: 10.1109/ICPC.2017.24,Bug Localization with Combination of Deep Learning and Information Retrieval,"Lam, An Ngoc and Nguyen, Anh Tuan and Nguyen, Hoan Anh and Nguyen, Tien N.",2017,IEEE Press,,https://doi.org/10.1109/ICPC.2017.24,10.1109/ICPC.2017.24,"The automated task of locating the potential buggy files in a software project given a bug report is called bug localization. Bug localization helps developers focus on crucial files. However, the existing automated bug localization approaches face a key challenge, called lexical mismatch. Specifically, the terms used in bug reports to describe a bug are different from the terms and code tokens used in source files. To address that, we present a novel approach that uses deep neural network (DNN) in combination with rVSM, an information retrieval (IR) technique. rVSM collects the feature on the textual similarity between bug reports and source files. DNN is used to learn to relate the terms in bug reports to potentially different code tokens and terms in source files. Our empirical evaluation on real-world bug reports in the open-source projects shows that DNN and IR complement well to each other to achieve higher bug localization accuracy than individual models. Importantly, our new model, DnnLoc, with a combination of the features built from DNN, rVSM, and project's bug-fixing history, achieves higher accuracy than the state-of-the-art IR and machine learning techniques. In half of the cases, it is correct with just a single suggested file. In 66% of the time, a correct buggy file is in the list of three suggested files. With 5 suggested files, it is correct in almost 70% of the cases.","bug localization, code retrieval, deep learning",218–229,12,Proceedings of the 25th International Conference on Program Comprehension
471,@inproceedings: 10.1109/ICSE-NIER.2017.13,"DARVIZ: Deep Abstract Representation, Visualization, and Verification of Deep Learning Models","Sankaran, Anush and Aralikatte, Rahul and Mani, Senthil and Khare, Shreya and Panwar, Naveen and Gantayat, Neelamadhav",2017,IEEE Press,,https://doi.org/10.1109/ICSE-NIER.2017.13,10.1109/ICSE-NIER.2017.13,"Traditional software engineering programming paradigms are mostly object or procedure oriented, driven by deterministic algorithms. With the advent of deep learning and cognitive sciences there is an emerging trend for data-driven programming, creating a shift in the programming paradigm among the software engineering communities. Visualizing and interpreting the execution of a current large scale data-driven software development is challenging. Further, for deep learning development there are many libraries in multiple programming languages such as TensorFlow (Python), CAFFE (C++), Theano (Python), Torch (Lua), and Deeplearning4j (Java), driving a huge need for interoperability across libraries.We propose a model driven development based solution frame-work, that facilitates intuitive designing of deep learning models in a platform agnostic fashion. This framework could potentially generate library specific code, perform program translation across languages, and debug the training process of a deep learning model from a fault localization and repair perspective. Further we identify open research problems in this emerging domain, and discuss some new software tooling requirements to serve this new age data-driven programming paradigm.","software tools, model validation, model driven development, deep learning, visualization",47–50,4,Proceedings of the 39th International Conference on Software Engineering: New Ideas and Emerging Results Track
472,@inproceedings: 10.1109/SEAMS.2017.11,Transfer Learning for Improving Model Predictions in Highly Configurable Software,"Jamshidi, Pooyan and Velez, Miguel and K\""{a}stner, Christian and Siegmund, Norbert and Kawthekar, Prasad",2017,IEEE Press,,https://doi.org/10.1109/SEAMS.2017.11,10.1109/SEAMS.2017.11,"Modern software systems are built to be used in dynamic environments using configuration capabilities to adapt to changes and external uncertainties. In a self-adaptation context, we are often interested in reasoning about the performance of the systems under different configurations. Usually, we learn a black-box model based on real measurements to predict the performance of the system given a specific configuration. However, as modern systems become more complex, there are many configuration parameters that may interact and we end up learning an exponentially large configuration space. Naturally, this does not scale when relying on real measurements in the actual changing environment. We propose a different solution: Instead of taking the measurements from the real system, we learn the model using samples from other sources, such as simulators that approximate performance of the real system at low cost. We define a cost model that transform the traditional view of model learning into a multi-objective problem that not only takes into account model accuracy but also measurements effort as well. We evaluate our cost-aware transfer learning solution using real-world configurable software including (i) a robotic system, (ii) 3 different stream processing applications, and (iii) a NoSQL database system. The experimental results demonstrate that our approach can achieve (a) a high prediction accuracy, as well as (b) a high model reliability.","machine learning, model prediction, highly configurable software, model learning, transfer learning",31–41,11,Proceedings of the 12th International Symposium on Software Engineering for Adaptive and Self-Managing Systems
473,@inproceedings: 10.1109/ICSE-C.2017.5,SURF: Summarizer of User Reviews Feedback,"Di Sorbo, Andrea and Panichella, Sebastiano and Alexandru, Carol V. and Visaggio, Corrado A. and Canfora, Gerardo",2017,IEEE Press,,https://doi.org/10.1109/ICSE-C.2017.5,10.1109/ICSE-C.2017.5,"Continuous Delivery (CD) enables mobile developers to release small, high quality chunks of working software in a rapid manner. However, faster delivery and a higher software quality do neither guarantee user satisfaction nor positive business outcomes. Previous work demonstrates that app reviews may contain crucial information that can guide developer's software maintenance efforts to obtain higher customer satisfaction. However, previous work also proves the difficulties encountered by developers in manually analyzing this rich source of data, namely (i) the huge amount of reviews an app may receive on a daily basis and (ii) the unstructured nature of their content. In this paper, we propose SURF (Summarizer of User Reviews Feedback), a tool able to (i) analyze and classify the information contained in app reviews and (ii) distill actionable change tasks for improving mobile applications. Specifically, SURF performs a systematic summarization of thousands of user reviews through the generation of an interactive, structured and condensed agenda of recommended software changes. An end-to-end evaluation of SURF, involving 2622 reviews related to 12 different mobile applications, demonstrates the high accuracy of SURF in summarizing user reviews content. In evaluating our approach we also involve the original developers of some apps, who confirm the practical usefulness of the software change recommendations made by SURF.Demo URL: https://youtu.be/Yf-U5ylJXvoDemo webpage: http://www.ifi.uzh.ch/en/seal/people/panichella/tools/SURFTool.html","mobile applications, summarization, natural language processing, software maintenance",55–58,4,Proceedings of the 39th International Conference on Software Engineering Companion
474,@inproceedings: 10.1109/SER-IP.2017..8,Data-Driven Application Maintenance: Experience from the Trenches,"Misra, Janardan and Sengupta, Shubhashis and Rawat, Divya and Savagaonkar, Milind and Podder, Sanjay",2017,IEEE Press,,https://doi.org/10.1109/SER-IP.2017..8,10.1109/SER-IP.2017..8,"In this paper we present our experience during design, development, and pilot deployments of a data-driven machine learning based application maintenance solution. We implemented a proof of concept to address a spectrum of interrelated problems encountered in application maintenance projects including duplicate incident ticket identification, assignee recommendation, theme mining, and mapping of incidents to business processes. In the context of IT services, these problems are frequently encountered, yet there is a gap in bringing automation and optimization. Despite long-standing research around mining and analysis of software repositories, such research outputs are not adopted well in practice due to the constraints these solutions impose on the users. We discuss need for designing pragmatic solutions with low barriers to adoption and addressing right level of complexity of problems with respect to underlying business constraints and nature of data.","incident management, assignee recommendation, duplicate bug identification, theme mining, machine learning, business process mapping, text analysis, application maintenance",48–54,7,Proceedings of the 4th International Workshop on Software Engineering Research and Industrial Practice
475,@inproceedings: 10.1109/ICSE-NIER.2017.12,Building Usage Profiles Using Deep Neural Nets,"Curro, Domenic and Derpanis, Konstantinos G. and Miranskyy, Andriy V.",2017,IEEE Press,,https://doi.org/10.1109/ICSE-NIER.2017.12,10.1109/ICSE-NIER.2017.12,"To improve software quality, one needs to build test scenarios resembling the usage of a software product in the field. This task is rendered challenging when a product's customer base is large and diverse. In this scenario, existing profiling approaches, such as operational profiling, are difficult to apply. In this work, we consider publicly available video tutorials of a product to profile usage. Our goal is to construct an automatic approach to extract information about user actions from instructional videos. To achieve this goal, we use a Deep Convolutional Neural Network (DCNN) to recognize user actions. Our pilot study shows that a DCNN trained to recognize user actions in video can classify five different actions in a collection of 236 publicly available Microsoft Word tutorial videos (published on YouTube). In our empirical evaluation we report a mean average precision of 94.42% across all actions. This study demonstrates the efficacy of DCNN-based methods for extracting software usage information from videos. Moreover, this approach may aid in other software engineering activities that require information about customer usage of a product.",,43–46,4,Proceedings of the 39th International Conference on Software Engineering: New Ideas and Emerging Results Track
476,@inproceedings: 10.1145/3020078.3021698,Improving the Performance of OpenCL-Based FPGA Accelerator for Convolutional Neural Network,"Zhang, Jialiang and Li, Jing",2017,Association for Computing Machinery,,https://doi.org/10.1145/3020078.3021698,10.1145/3020078.3021698,"OpenCL FPGA has recently gained great popularity with emerging needs for workload acceleration such as Convolutional Neural Network (CNN), which is the most popular deep learning architecture in the domain of computer vision. While OpenCL enhances the code portability and programmability of FPGA, it comes at the expense of performance. The key challenge is to optimize the OpenCL kernels to efficiently utilize the flexible hardware resources in FPGA. Simply optimizing the OpenCL kernel code through various compiler options turns out insufficient to achieve desirable performance for both compute-intensive and data-intensive workloads such as convolutional neural networks.In this paper, we first propose an analytical performance model and apply it to perform an in-depth analysis on the resource requirement of CNN classifier kernels and available resources on modern FPGAs. We identify that the key performance bottleneck is the on-chip memory bandwidth. We propose a new kernel design to effectively address such bandwidth limitation and to provide an optimal balance between computation, on-chip, and off-chip memory access. As a case study, we further apply these techniques to design a CNN accelerator based on the VGG model. Finally, we evaluate the performance of our CNN accelerator using an Altera Arria 10 GX1150 board. We achieve 866 Gop/s floating point performance at 370MHz working frequency and 1.79 Top/s 16-bit fixed-point performance at 385MHz. To the best of our knowledge, our implementation achieves the best power efficiency and performance density compared to existing work.","convolutional neural networks, hardware accelerator, opencl, fpga",25–34,10,Proceedings of the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays
477,@inproceedings: 10.1145/3019943.3020000,Using Multi-Stage Features in Fast R-CNN for Pedestrian Detection,"Farrajota, M. and Rodrigues, J. M.F. and du Buf, J. M.H.",2016,Association for Computing Machinery,,https://doi.org/10.1145/3019943.3020000,10.1145/3019943.3020000,"Pedestrian detection and tracking remains popular issue in computer vision, with many applications in robotics, surveillance, security and telecare systems, especially when connected with Smart Cities and Smart Destinations. As a particular case of object detection, pedestrian detection in general is a difficult task due to a large variability of features due to different scales, views and occlusion. Typically, smaller and occluded pedestrians are hard to detect due to fewer discriminative features if compared to large-size, visible pedestrians. In order to overcome this, we use convolutional features from different stages in a deep Convolutional Neural Network (CNN), with the idea of combining more global features with finer details. In this paper we present an object detection framework based on multi-stage convolutional features for pedestrian detection. This framework extends the Fast R-CNN framework for the combination of several convolutional features from different stages of the used CNN to improve the network's detection accuracy. The Caltech Pedestrian dataset was used to train and evaluate the proposed method.","Multistage feature, Object Detection, Deep learning, Pedestrian detection",400–407,8,Proceedings of the 7th International Conference on Software Development and Technologies for Enhancing Accessibility and Fighting Info-Exclusion
478,@inproceedings: 10.1145/2950290.2983938,ARdoc: App Reviews Development Oriented Classifier,"Panichella, Sebastiano and Di Sorbo, Andrea and Guzman, Emitza and Visaggio, Corrado A. and Canfora, Gerardo and Gall, Harald C.",2016,Association for Computing Machinery,,https://doi.org/10.1145/2950290.2983938,10.1145/2950290.2983938," Google Play, Apple App Store and Windows Phone Store are well known distribution platforms where users can download mobile apps, rate them and write review comments about the apps they are using. Previous research studies demonstrated that these reviews contain important information to help developers improve their apps. However, analyzing reviews is challenging due to the large amount of reviews posted every day, the unstructured nature of reviews and its varying quality.  In this demo we present ARdoc, a tool which combines three techniques: (1) Natural Language Parsing, (2) Text Analysis and (3) Sentiment Analysis to automatically classify useful feedback contained in app reviews important for performing software maintenance and evolution tasks. Our quantitative and qualitative analysis (involving mobile professional developers) demonstrates that ARdoc correctly classifies feedback useful for maintenance perspectives in user reviews with high precision (ranging between 84% and 89%), recall (ranging between 84% and 89%), and F-Measure (ranging between 84% and 89%). While evaluating our tool developers of our study confirmed the usefulness of ARdoc in extracting important maintenance tasks for their mobile applications.  Demo URL: https://youtu.be/Baf18V6sN8E  Demo Web Page: http://www.ifi.uzh.ch/seal/people/panichella/tools/ARdoc.html ","Natural Language Processing, User Reviews, Sentiment Analysis, Text classification, Mobile Applications",1023–1027,5,Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering
479,@inproceedings: 10.1145/2950290.2950319,Parameter-Free Probabilistic API Mining across GitHub,"Fowkes, Jaroslav and Sutton, Charles",2016,Association for Computing Machinery,,https://doi.org/10.1145/2950290.2950319,10.1145/2950290.2950319,"Existing API mining algorithms can be difficult to use as they require expensive parameter tuning and the returned set of API calls can be large, highly redundant and difficult to understand. To address this, we present PAM (Probabilistic API Miner), a near parameter-free probabilistic algorithm for mining the most interesting API call patterns. We show that PAM significantly outperforms both MAPO and UPMiner, achieving 69% test-set precision, at retrieving relevant API call sequences from GitHub. Moreover, we focus on libraries for which the developers have explicitly provided code examples, yielding over 300,000 LOC of hand-written API example code from the 967 client projects in the data set. This evaluation suggests that the hand-written examples actually have limited coverage of real API usages.","sequential pattern mining, API mining",254–265,12,Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering
480,@inproceedings: 10.1145/2950290.2950363,Semi-Supervised Verified Feedback Generation,"Kaleeswaran, Shalini and Santhiar, Anirudh and Kanade, Aditya and Gulwani, Sumit",2016,Association for Computing Machinery,,https://doi.org/10.1145/2950290.2950363,10.1145/2950290.2950363,"Students have enthusiastically taken to online programming lessons and contests. Unfortunately, they tend to struggle due to lack of personalized feedback. There is an urgent need of program analysis and repair techniques capable of handling both the scale and variations in student submissions, while ensuring quality of feedback. Towards this goal, we present a novel methodology called semi-supervised verified feedback generation. We cluster submissions by solution strategy and ask the instructor to identify or add a correct submission in each cluster. We then verify every submission in a cluster against the instructor-validated submission in the same cluster. If faults are detected in the submission then feedback suggesting fixes to them is generated. Clustering reduces the burden on the instructor and also the variations that have to be handled during feedback generation. The verified feedback generation ensures that only correct feedback is generated. We implemented a tool, named CoderAssist, based on this approach and evaluated it on dynamic programming assignments. We have designed a novel counter-example guided feedback generation algorithm capable of suggesting fixes to all faults in a submission. In an evaluation on 2226 submissions to 4 problems, CoderAssist could generate verified feedback for 1911 (85%) submissions in 1.6s each on an average. It does a good job of reducing the burden on the instructor. Only one submission had to be manually validated or added for every 16 submissions.","Feedback generation, Clustering, Verification, MOOCs",739–750,12,Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering
481,@inproceedings: 10.5555/3049877.3049906,Word Representation Using a Deep Neural Network,"Li, Yunpeng and Lyons, Kelly",2016,IBM Corp.,,,,"A growth in the number of applications that make use of cognitive computing has increased the need for algorithms that can parse and understand natural language. Most modern systems rely on machine learning algorithms that take a large corpus of text as input and identify features of the language in the input data. Deep learning is a recently-developed type of machine learning that uses a network with several layers (a deep network) to identify and process features in a given data set automatically. In this paper, we introduce a deep learning neural network model called the Character-Morphology-Word network (CMW), to solve the word embedding problem. We implemented our CMW model and trained it on the Wikipedia corpus. We compared our model against two past approaches in a number of word-embedding tasks and found that, while the average performance of our model is not as good as the past approaches, our model achieves comparable precision and, in one task, our model outperforms these approaches. We also used our model to generate random text and found that our model produces words in the vocabulary and can produce lexical-correct text. The results of our research indicate that a deeper neural network architecture can be used to represent words and ultimately, help solve related Natural Language Processing (NLP) tasks.","recursive neural networks, recurrent neural networks, word embedding, natural language processing",268–279,12,Proceedings of the 26th Annual International Conference on Computer Science and Software Engineering
482,@inproceedings: 10.1145/2984511.2984536,AggreGaze: Collective Estimation of Audience Attention on Public Displays,"Sugano, Yusuke and Zhang, Xucong and Bulling, Andreas",2016,Association for Computing Machinery,,https://doi.org/10.1145/2984511.2984536,10.1145/2984511.2984536,"Gaze is frequently explored in public display research given its importance for monitoring and analysing audience attention. However, current gaze-enabled public display interfaces require either special-purpose eye tracking equipment or explicit personal calibration for each individual user. We present AggreGaze, a novel method for estimating spatio-temporal audience attention on public displays. Our method requires only a single off-the-shelf camera attached to the display, does not require any personal calibration, and provides visual attention estimates across the full display. We achieve this by 1) compensating for errors of state-of-the-art appearance-based gaze estimation methods through on-site training data collection, and by 2) aggregating uncalibrated and thus inaccurate gaze estimates of multiple users into joint attention estimates. We propose different visual stimuli for this compensation: a standard 9-point calibration, moving targets, text and visual stimuli embedded into the display content, as well as normal video content. Based on a two-week deployment in a public space, we demonstrate the effectiveness of our method for estimating attention maps that closely resemble ground-truth audience gaze distributions.","gaze estimation, visual attention, eye tracking, public displays",821–831,11,Proceedings of the 29th Annual Symposium on User Interface Software and Technology
483,@inproceedings: 10.1145/2984511.2984565,Interacting with Soli: Exploring Fine-Grained Dynamic Gesture Recognition in the Radio-Frequency Spectrum,"Wang, Saiwen and Song, Jie and Lien, Jaime and Poupyrev, Ivan and Hilliges, Otmar",2016,Association for Computing Machinery,,https://doi.org/10.1145/2984511.2984565,10.1145/2984511.2984565,"This paper proposes a novel machine learning architecture, specifically designed for radio-frequency based gesture recognition. We focus on high-frequency (60]GHz), short-range radar based sensing, in particular Google's Soli sensor. The signal has unique properties such as resolving motion at a very fine level and allowing for segmentation in range and velocity spaces rather than image space. This enables recognition of new types of inputs but poses significant difficulties for the design of input recognition algorithms. The proposed algorithm is capable of detecting a rich set of dynamic gestures and can resolve small motions of fingers in fine detail. Our technique is based on an end-to-end trained combination of deep convolutional and recurrent neural networks. The algorithm achieves high recognition rates (avg 87%) on a challenging set of 11 dynamic gestures and generalizes well across 10 users. The proposed model runs on commodity hardware at 140 Hz (CPU only).","wearables, gesture recognition, radar sensing, deep learning",851–860,10,Proceedings of the 29th Annual Symposium on User Interface Software and Technology
484,@inproceedings: 10.1145/2968456.2968457,Efficient Design Space Exploration by Knowledge Transfer,"Li, Dandan and Wang, Senzhang and Yao, Shuzhen and Liu, Yu-Hang and Cheng, Yuanqi and Sun, Xian-He",2016,Association for Computing Machinery,,https://doi.org/10.1145/2968456.2968457,10.1145/2968456.2968457,"Due to the exponentially increasing size of design space of microprocessors and time-consuming simulations, predictive models have been widely employed in design space exploration (DSE). Traditional approaches mostly build a program-specific predictor that needs a large number of program-specific samples. Thus considerable simulation cost is required for each program. In this paper, we study the novel problem of transferring knowledge from the labeled samples of previous programs to help predict the responses of the new target program whose labeled samples are very sparse. Inspired by the recent advances of transfer learning, we propose a transfer learning based DSE framework TrDSE to build a more efficient and effective predictive model for the target program with only a few simulations by borrowing knowledge from previous programs. Specifically, TrDSE includes two phases: 1) clustering the programs based on the proposed orthogonal array sampling and the distribution related features, and 2) with the guidance of clustering results, predicting the responses of configurations in design space of the target program by a transfer learning based regression algorithm. We evaluate the proposed TrDSE on the benchmarks of SPEC CPU 2006 suite. The results demonstrate that the proposed framework is more efficient and effective than state-of-art DSE techniques.","knowledge transfer, processor design, design space exploration",,10,Proceedings of the Eleventh IEEE/ACM/IFIP International Conference on Hardware/Software Codesign and System Synthesis
485,@inproceedings: 10.1145/2961111.2962584,Towards Effectively Test Report Classification to Assist Crowdsourced Testing,"Wang, Junjie and Cui, Qiang and Wang, Qing and Wang, Song",2016,Association for Computing Machinery,,https://doi.org/10.1145/2961111.2962584,10.1145/2961111.2962584,"Context: Automatic classification of crowdsourced test reports is important due to their tremendous sizes and large proportion of noises. Most existing approaches towards this problem focus on examining the performance of different machine learning or information retrieval techniques, and most are evaluated on open source dataset. However, our observation reveals that these approaches generate poor and unstable performances on real industrial crowdsourced testing data. We further analyze the deep reason and find that industrial data have significant local bias, which degrades existing approaches.Goal: We aim at designing an approach to overcome the local bias in industrial data and automatically classifying true fault from the large amounts of crowdsourced reports.Method: We propose a cluster-based classification approach, which first clusters similar reports together and then builds classifiers based on most similar clusters with ensemble method.Results: Evaluation is conducted on 15,095 test reports of 35 industrial projects from Chinese largest crowdsourced testing platform and results are promising, with 0.89 precision and 0.97 recall on average. In addition, our approach improves the existing baselines by 17% - 63% in average precision and 15% - 61% in average recall.Conclusions: Results imply that our approach can effectively discriminate true fault from large amounts of crowdsourced reports, which can reduce the effort required for manual inspection and facilitate project management in crowdsourced testing. To the best of our knowledge, this is the first work to address the test report classification problem in real industrial crowdsourced testing practice.","Report classification, Cluster, Crowdsourced testing",,10,Proceedings of the 10th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement
486,@inproceedings: 10.1145/2970276.2970339,Too Much Automation? The Bellwether Effect and Its Implications for Transfer Learning,"Krishna, Rahul and Menzies, Tim and Fu, Wei",2016,Association for Computing Machinery,,https://doi.org/10.1145/2970276.2970339,10.1145/2970276.2970339," Transfer learning: is the process of translating quality predictors learned in one data set to another. Transfer learning has been the subject of much recent research. In practice, that research means changing models all the time as transfer learners continually exchange new models to the current project. This paper offers a very simple bellwether transfer learner. Given N data sets, we find which one produce the best predictions on all the others. This bellwether data set is then used for all subsequent predictions (or, until such time as its predictions start failing-- at which point it is wise to seek another bellwether). Bellwethers are interesting since they are very simple to find (just wrap a for-loop around standard data miners). Also, they simplify the task of making general policies in SE since as long as one bellwether remains useful, stable conclusions for N data sets can be achieved just by reasoning over that bellwether. From this, we conclude (1) this bellwether method is a useful (and very simple) transfer learning method; (2) bellwethers are a baseline method against which future transfer learners should be compared; (3) sometimes, when building increasingly complex automatic methods, researchers should pause and compare their supposedly more sophisticated method against simpler alternatives. ","Defect Prediction, Data Mining, Transfer learning",122–131,10,Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering
487,@inproceedings: 10.1145/2970276.2970317,Learning a Dual-Language Vector Space for Domain-Specific Cross-Lingual Question Retrieval,"Chen, Guibin and Chen, Chunyang and Xing, Zhenchang and Xu, Bowen",2016,Association for Computing Machinery,,https://doi.org/10.1145/2970276.2970317,10.1145/2970276.2970317," The lingual barrier limits the ability of millions of non-English speaking developers to make effective use of the tremendous knowledge in Stack Overflow, which is archived in English. For cross-lingual question retrieval, one may use translation-based methods that first translate the non-English queries into English and then perform monolingual question retrieval in English. However, translation-based methods suffer from semantic deviation due to inappropriate translation, especially for domain-specific terms, and lexical gap between queries and questions that share few words in common. To overcome the above issues, we propose a novel cross-lingual question retrieval based on word embeddings and convolutional neural network (CNN) which are the state-of-the-art deep learning techniques to capture word- and sentence-level semantics. The CNN model is trained with large amounts of examples from Stack Overflow duplicate questions and their corresponding translation by machine, which guides the CNN to learn to capture informative word and sentence features to recognize and quantify semantic similarity in the presence of semantic deviations and lexical gaps. A uniqueness of our approach is that the trained CNN can map documents in two languages (e.g., Chinese queries and English questions) in a dual-language vector space, and thus reduce the cross-lingual question retrieval problem to a simple k-nearest neighbors search problem in the dual-language vector space, where no query or question translation is required. Our evaluation shows that our approach significantly outperforms the translation-based method, and can be extended to dual-language documents retrieval from different sources. ","Word embeddings, Dual-Language Vector Space, Convolutional Neural Network, Cross-lingual question retrieval",744–755,12,Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering
488,@inproceedings: 10.1145/2889160.2889171,TASSAL: Autofolding for Source Code Summarization,"Fowkes, Jaroslav and Chanthirasegaran, Pankajan and Ranca, Razvan and Allamanis, Miltiadis and Lapata, Mirella and Sutton, Charles",2016,Association for Computing Machinery,,https://doi.org/10.1145/2889160.2889171,10.1145/2889160.2889171,"We present a novel tool, TASSAL, that automatically creates a summary of each source file in a project by folding its least salient code regions. The intended use-case for our tool is the first-look problem: to help developers who are unfamiliar with a new codebase and are attempting to understand it. TASSAL is intended to aid developers in this task by folding away less informative regions of code and allowing them to focus their efforts on the most informative ones. While modern code editors do provide code folding to selectively hide blocks of code, it is impractical to use as folding decisions must be made manually or based on simple rules. We find through a case study that TASSAL is strongly preferred by experienced developers over simple folding baselines, demonstrating its usefulness. In short, we strongly believe TASSAL can aid program comprehension by turning code folding into a usable and valuable tool. A video highlighting the main features of TASSAL can be found at https://youtu.be/_yu7JZgiBA4.",,649–652,4,Proceedings of the 38th International Conference on Software Engineering Companion
489,@inproceedings: 10.1145/2872362.2872368,Baymax: QoS Awareness and Increased Utilization for Non-Preemptive Accelerators in Warehouse Scale Computers,"Chen, Quan and Yang, Hailong and Mars, Jason and Tang, Lingjia",2016,Association for Computing Machinery,,https://doi.org/10.1145/2872362.2872368,10.1145/2872362.2872368,"Modern warehouse-scale computers (WSCs) are being outfitted with accelerators to provide the significant compute required by emerging intelligent personal assistant (IPA) workloads such as voice recognition, image classification, and natural language processing. It is well known that the diurnal user access pattern of user-facing services provides a strong incentive to co-locate applications for better accelerator utilization and efficiency, and prior work has focused on enabling co-location on multicore processors. However, interference when co-locating applications on non-preemptive accelerators is fundamentally different than contention on multi-core CPUs and introduces a new set of challenges to reduce QoS violation. To address this open problem, we first identify the underlying causes for QoS violation in accelerator-outfitted servers. Our experiments show that queuing delay for the compute resources and PCI-e bandwidth contention for data transfer are the main two factors that contribute to the long tails of user-facing applications. We then present Baymax, a runtime system that orchestrates the execution of compute tasks from different applications and mitigates PCI-e bandwidth contention to deliver the required QoS for user-facing applications and increase the accelerator utilization. Using DjiNN, a deep neural network service, Sirius, an end-to-end IPA workload, and traditional applications on a Nvidia K40 GPU, our evaluation shows that Baymax improves the accelerator utilization by 91.3% while achieving the desired 99%-ile latency target for for user-facing applications. In fact, Baymax reduces the 99%-ile latency of user-facing applications by up to 195x over default execution.","warehouse scale computers, non-preemptive accelerators, quality of service, scheduling",681–696,16,Proceedings of the Twenty-First International Conference on Architectural Support for Programming Languages and Operating Systems
490,@article: 10.1145/2954679.2872368,Baymax: QoS Awareness and Increased Utilization for Non-Preemptive Accelerators in Warehouse Scale Computers,"Chen, Quan and Yang, Hailong and Mars, Jason and Tang, Lingjia",2016,Association for Computing Machinery,0362-1340,https://doi.org/10.1145/2954679.2872368,10.1145/2954679.2872368,"Modern warehouse-scale computers (WSCs) are being outfitted with accelerators to provide the significant compute required by emerging intelligent personal assistant (IPA) workloads such as voice recognition, image classification, and natural language processing. It is well known that the diurnal user access pattern of user-facing services provides a strong incentive to co-locate applications for better accelerator utilization and efficiency, and prior work has focused on enabling co-location on multicore processors. However, interference when co-locating applications on non-preemptive accelerators is fundamentally different than contention on multi-core CPUs and introduces a new set of challenges to reduce QoS violation. To address this open problem, we first identify the underlying causes for QoS violation in accelerator-outfitted servers. Our experiments show that queuing delay for the compute resources and PCI-e bandwidth contention for data transfer are the main two factors that contribute to the long tails of user-facing applications. We then present Baymax, a runtime system that orchestrates the execution of compute tasks from different applications and mitigates PCI-e bandwidth contention to deliver the required QoS for user-facing applications and increase the accelerator utilization. Using DjiNN, a deep neural network service, Sirius, an end-to-end IPA workload, and traditional applications on a Nvidia K40 GPU, our evaluation shows that Baymax improves the accelerator utilization by 91.3% while achieving the desired 99%-ile latency target for for user-facing applications. In fact, Baymax reduces the 99%-ile latency of user-facing applications by up to 195x over default execution.","quality of service, non-preemptive accelerators, scheduling, warehouse scale computer",681–696,16,SIGPLAN Not.
491,@article: 10.1145/2980024.2872368,Baymax: QoS Awareness and Increased Utilization for Non-Preemptive Accelerators in Warehouse Scale Computers,"Chen, Quan and Yang, Hailong and Mars, Jason and Tang, Lingjia",2016,Association for Computing Machinery,0163-5964,https://doi.org/10.1145/2980024.2872368,10.1145/2980024.2872368,"Modern warehouse-scale computers (WSCs) are being outfitted with accelerators to provide the significant compute required by emerging intelligent personal assistant (IPA) workloads such as voice recognition, image classification, and natural language processing. It is well known that the diurnal user access pattern of user-facing services provides a strong incentive to co-locate applications for better accelerator utilization and efficiency, and prior work has focused on enabling co-location on multicore processors. However, interference when co-locating applications on non-preemptive accelerators is fundamentally different than contention on multi-core CPUs and introduces a new set of challenges to reduce QoS violation. To address this open problem, we first identify the underlying causes for QoS violation in accelerator-outfitted servers. Our experiments show that queuing delay for the compute resources and PCI-e bandwidth contention for data transfer are the main two factors that contribute to the long tails of user-facing applications. We then present Baymax, a runtime system that orchestrates the execution of compute tasks from different applications and mitigates PCI-e bandwidth contention to deliver the required QoS for user-facing applications and increase the accelerator utilization. Using DjiNN, a deep neural network service, Sirius, an end-to-end IPA workload, and traditional applications on a Nvidia K40 GPU, our evaluation shows that Baymax improves the accelerator utilization by 91.3% while achieving the desired 99%-ile latency target for for user-facing applications. In fact, Baymax reduces the 99%-ile latency of user-facing applications by up to 195x over default execution.","non-preemptive accelerators, quality of service, scheduling, warehouse scale computer",681–696,16,SIGARCH Comput. Archit. News
492,@inproceedings: 10.1145/2847263.2847276,Throughput-Optimized OpenCL-Based FPGA Accelerator for Large-Scale Convolutional Neural Networks,"Suda, Naveen and Chandra, Vikas and Dasika, Ganesh and Mohanty, Abinash and Ma, Yufei and Vrudhula, Sarma and Seo, Jae-sun and Cao, Yu",2016,Association for Computing Machinery,,https://doi.org/10.1145/2847263.2847276,10.1145/2847263.2847276,"Convolutional Neural Networks (CNNs) have gained popularity in many computer vision applications such as image classification, face detection, and video analysis, because of their ability to train and classify with high accuracy. Due to multiple convolution and fully-connected layers that are compute-/memory-intensive, it is difficult to perform real-time classification with low power consumption on today?s computing systems. FPGAs have been widely explored as hardware accelerators for CNNs because of their reconfigurability and energy efficiency, as well as fast turn-around-time, especially with high-level synthesis methodologies. Previous FPGA-based CNN accelerators, however, typically implemented generic accelerators agnostic to the CNN configuration, where the reconfigurable capabilities of FPGAs are not fully leveraged to maximize the overall system throughput. In this work, we present a systematic design space exploration methodology to maximize the throughput of an OpenCL-based FPGA accelerator for a given CNN model, considering the FPGA resource constraints such as on-chip memory, registers, computational resources and external memory bandwidth. The proposed methodology is demonstrated by optimizing two representative large-scale CNNs, AlexNet and VGG, on two Altera Stratix-V FPGA platforms, DE5-Net and P395-D8 boards, which have different hardware resources. We achieve a peak performance of 136.5 GOPS for convolution operation, and 117.8 GOPS for the entire VGG network that performs ImageNet classification on P395-D8 board.","fpga, optimization, convolutional neural networks, opencl",16–25,10,Proceedings of the 2016 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays
493,@inproceedings: 10.1145/2847263.2847265,Going Deeper with Embedded FPGA Platform for Convolutional Neural Network,"Qiu, Jiantao and Wang, Jie and Yao, Song and Guo, Kaiyuan and Li, Boxun and Zhou, Erjin and Yu, Jincheng and Tang, Tianqi and Xu, Ningyi and Song, Sen and Wang, Yu and Yang, Huazhong",2016,Association for Computing Machinery,,https://doi.org/10.1145/2847263.2847265,10.1145/2847263.2847265,"In recent years, convolutional neural network (CNN) based methods have achieved great success in a large number of applications and have been among the most powerful and widely used techniques in computer vision. However, CNN-based methods are com-putational-intensive and resource-consuming, and thus are hard to be integrated into embedded systems such as smart phones, smart glasses, and robots. FPGA is one of the most promising platforms for accelerating CNN, but the limited bandwidth and on-chip memory size limit the performance of FPGA accelerator for CNN.In this paper, we go deeper with the embedded FPGA platform on accelerating CNNs and propose a CNN accelerator design on embedded FPGA for Image-Net large-scale image classification. We first present an in-depth analysis of state-of-the-art CNN models and show that Convolutional layers are computational-centric and Fully-Connected layers are memory-centric.Then the dynamic-precision data quantization method and a convolver design that is efficient for all layer types in CNN are proposed to improve the bandwidth and resource utilization. Results show that only 0.4% accuracy loss is introduced by our data quantization flow for the very deep VGG16 model when 8/4-bit quantization is used. A data arrangement method is proposed to further ensure a high utilization of the external memory bandwidth. Finally, a state-of-the-art CNN, VGG16-SVD, is implemented on an embedded FPGA platform as a case study. VGG16-SVD is the largest and most accurate network that has been implemented on FPGA end-to-end so far. The system on Xilinx Zynq ZC706 board achieves a frame rate at 4.45 fps with the top-5 accuracy of 86.66% using 16-bit quantization. The average performance of convolutional layers and the full CNN is 187.8 GOP/s and 137.0 GOP/s under 150MHz working frequency, which outperform previous approaches significantly.","dynamic-precision data quantization, bandwidth utilization, embedded fpga, convolutional neural network (cnn)",26–35,10,Proceedings of the 2016 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays
494,@inproceedings: 10.1109/ASE.2015.107,Pseudogen: A Tool to Automatically Generate Pseudo-Code from Source Code,"Fudaba, Hiroyuki and Oda, Yusuke and Akabe, Koichi and Neubig, Graham and Hata, Hideaki and Sakti, Sakriani and Toda, Tomoki and Nakamura, Satoshi",2015,IEEE Press,,https://doi.org/10.1109/ASE.2015.107,10.1109/ASE.2015.107,"Understanding the behavior of source code written in an unfamiliar programming language is difficult. One way to aid understanding of difficult code is to add corresponding pseudo-code, which describes in detail the workings of the code in a natural language such as English. In spite of its usefulness, most source code does not have corresponding pseudo-code because it is tedious to create. This paper demonstrates a tool Pseudogen that makes it possible to automatically generate pseudo-code from source code using statistical machine translation (SMT).1 Pseudogen currently supports generation of English or Japanese pseudo-code from Python source code, and the SMT framework makes it easy for users to create new generators for their preferred source code/pseudo-code pairs.",,824–829,6,Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering
495,@inproceedings: 10.1109/ASE.2015.56,CLAMI: Defect Prediction on Unlabeled Datasets,"Nam, Jaechang and Kim, Sunghun",2015,IEEE Press,,https://doi.org/10.1109/ASE.2015.56,10.1109/ASE.2015.56,"Defect prediction on new projects or projects with limited historical data is an interesting problem in software engineering. This is largely because it is difficult to collect defect information to label a dataset for training a prediction model. Cross-project defect prediction (CPDP) has tried to address this problem by reusing prediction models built by other projects that have enough historical data. However, CPDP does not always build a strong prediction model because of the different distributions among datasets. Approaches for defect prediction on unlabeled datasets have also tried to address the problem by adopting unsupervised learning but it has one major limitation, the necessity for manual effort.In this study, we propose novel approaches, CLA and CLAMI, that show the potential for defect prediction on unlabeled datasets in an automated manner without need for manual effort. The key idea of the CLA and CLAMI approaches is to label an unlabeled dataset by using the magnitude of metric values. In our empirical study on seven open-source projects, the CLAMI approach led to the promising prediction performances, 0.636 and 0.723 in average f-measure and AUC, that are comparable to those of defect prediction based on supervised learning.",,452–463,12,Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering
496,@inproceedings: 10.1145/2807442.2807455,CLuster: Smart Clustering of Free-Hand Sketches on Large Interactive Surfaces,"Perteneder, Florian and Bresler, Martin and Grossauer, Eva-Maria and Leong, Joanne and Haller, Michael",2015,Association for Computing Machinery,,https://doi.org/10.1145/2807442.2807455,10.1145/2807442.2807455,"Structuring and rearranging free-hand sketches on large interactive surfaces typically requires making multiple stroke selections. This can be both time-consuming and fatiguing in the absence of well-designed selection tools. Investigating the concept of automated clustering, we conducted a background study which highlighted the fact that people have varying perspectives on how elements in sketches can and should be grouped. In response to these diverse user expectations, we present cLuster, a flexible, domain-independent clustering approach for free-hand sketches. Our approach is designed to accept an initial user selection, which is then used to calculate a linear combination of pre-trained perspectives in real-time. The remaining elements are then clustered. An initial evaluation revealed that in many cases, only a few corrections were necessary to achieve the desired clustering results. Finally, we demonstrate the utility of our approach in a variety of application scenarios.","segmentation, cluster analysis, free-hand sketching",37–46,10,Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp; Technology
497,@inproceedings: 10.1145/2807442.2807489,Gunslinger: Subtle Arms-down Mid-Air Interaction,"Liu, Mingyu and Nancel, Mathieu and Vogel, Daniel",2015,Association for Computing Machinery,,https://doi.org/10.1145/2807442.2807489,10.1145/2807442.2807489,"We describe Gunslinger, a mid-air interaction technique using barehand postures and gestures. Unlike past work, we explore a relaxed arms-down position with both hands interacting at the sides of the body. It features ""hand-cursor"" feedback to communicate recognized hand posture, command mode and tracking quality; and a simple, but flexible hand posture recognizer. Although Gunslinger is suitable for many usage contexts, we focus on integrating mid-air gestures with large display touch input. We show how the Gunslinger form factor enables an interaction language that is equivalent, coherent, and compatible with large display touch input. A four-part study evaluates Midas Touch, posture recognition feedback, pointing and clicking, and general usability.","large displays, wearable, barehand, touch, gestures",63–71,9,Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp; Technology
498,@inproceedings: 10.1145/2814204.2814205,The Next-Generation in-Stadium Experience (Keynote),"Narasimhan, Priya and Drolia, Utsav and Tan, Jiaqi and Mickulicz, Nathan D. and Gandhi, Rajeev",2015,Association for Computing Machinery,,https://doi.org/10.1145/2814204.2814205,10.1145/2814204.2814205," YinzCam is a cloud-hosted service that provides sports fans with real-time scores, news, photos, statistics, live radio, streaming video, etc., on their mobile devices. YinzCam’s infrastructure is currently hosted on Amazon Web Services (AWS) and supports over 30 million installs of the official mobile apps of 140+ NHL/NFL/NBA/NRL/NCAA sports teams and venues. YinzCam’s workload is necessarily multi-modal (e.g., pre-game, in-game, post-game, game-day, non-gameday), with normal game-time traffic being twenty-fold of that on non-game days. This paper describes the evolution of YinzCam’s production architecture and distributed infrastructure, from its beginnings in 2009, when it was used to support thousands of concurrent users, to today’s system that supports millions of concurrent users on any game day. We also discuss key new opportunities to improve the fan experience inside the stadium of the future, without impacting the available bandwidth, by crowd-sourcing the thousands of mobile devices that are in fans’ hands inside these venues. We present Krowd, a novel distributed key-value store for promoting efficient content sharing, discovery and retrieval across the mobile devices inside a stadium. We present CHIPS, a system that ensures that users’ privacy is maintained while their devices participate in the crowdsourced infrastructure. ","mobile, streaming, sports, wireless, stadium, video, crowdsourcing, cloud computing, replays",1–10,10,Proceedings of the 2015 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences
499,@article: 10.1145/2936314.2814205,The Next-Generation in-Stadium Experience (Keynote),"Narasimhan, Priya and Drolia, Utsav and Tan, Jiaqi and Mickulicz, Nathan D. and Gandhi, Rajeev",2015,Association for Computing Machinery,0362-1340,https://doi.org/10.1145/2936314.2814205,10.1145/2936314.2814205," YinzCam is a cloud-hosted service that provides sports fans with real-time scores, news, photos, statistics, live radio, streaming video, etc., on their mobile devices. YinzCam’s infrastructure is currently hosted on Amazon Web Services (AWS) and supports over 30 million installs of the official mobile apps of 140+ NHL/NFL/NBA/NRL/NCAA sports teams and venues. YinzCam’s workload is necessarily multi-modal (e.g., pre-game, in-game, post-game, game-day, non-gameday), with normal game-time traffic being twenty-fold of that on non-game days. This paper describes the evolution of YinzCam’s production architecture and distributed infrastructure, from its beginnings in 2009, when it was used to support thousands of concurrent users, to today’s system that supports millions of concurrent users on any game day. We also discuss key new opportunities to improve the fan experience inside the stadium of the future, without impacting the available bandwidth, by crowd-sourcing the thousands of mobile devices that are in fans’ hands inside these venues. We present Krowd, a novel distributed key-value store for promoting efficient content sharing, discovery and retrieval across the mobile devices inside a stadium. We present CHIPS, a system that ensures that users’ privacy is maintained while their devices participate in the crowdsourced infrastructure. ","video, wireless, stadium, replays, crowdsourcing, streaming, sports, cloud computing, mobil",1–10,10,SIGPLAN Not.
500,@inproceedings: 10.5555/2830840.2830854,Big/Little Deep Neural Network for Ultra Low Power Inference,"Park, Eunhyeok and Kim, Dongyoung and Kim, Soobeom and Kim, Yong-Deok and Kim, Gunhee and Yoon, Sungroh and Yoo, Sungjoo",2015,IEEE Press,,,,"Deep neural networks (DNNs) have recently proved their effectiveness in complex data analyses such as object/speech recognition. As their applications are being expanded to mobile devices, their energy efficiencies are becoming critical. In this paper, we propose a novel concept called big/LITTLE DNN (BL-DNN) which significantly reduces energy consumption required for DNN execution at a negligible loss of inference accuracy. The BL-DNN consists of a little DNN (consuming low energy) and a full-fledged big DNN. In order to reduce energy consumption, the BL-DNN aims at avoiding the execution of the big DNN whenever possible. The key idea for this goal is to execute the little DNN first for inference (without big DNN execution) and simply use its result as the final inference result as long as the result is estimated to be accurate. On the other hand, if the result from the little DNN is not considered to be accurate, the big DNN is executed to give the final inference result. This approach reduces the total energy consumption by obtaining the inference result only with the little, energy-efficient DNN in most cases, while maintaining the similar level of inference accuracy through selectively utilizing the big DNN execution. We present design-time and runtime methods to control the execution of big DNN under a trade-off between energy consumption and inference accuracy. Experiments with state-of-the-art DNNs for ImageNet and MNIST show that our proposed BL-DNN can offer up to 53.7% (ImageNet) and 94.1% (MNIST) reductions in energy consumption at a loss of 0.90% (ImageNet) and 0.12% (MNIST) in inference accuracy, respectively.","low power, deep neural network",124–132,9,Proceedings of the 10th International Conference on Hardware/Software Codesign and System Synthesis
501,@inproceedings: 10.1145/2786805.2786813,Heterogeneous Cross-Company Defect Prediction by Unified Metric Representation and CCA-Based Transfer Learning,"Jing, Xiaoyuan and Wu, Fei and Dong, Xiwei and Qi, Fumin and Xu, Baowen",2015,Association for Computing Machinery,,https://doi.org/10.1145/2786805.2786813,10.1145/2786805.2786813," Cross-company defect prediction (CCDP) learns a prediction model by using training data from one or multiple projects of a source company and then applies the model to the target company data. Existing CCDP methods are based on the assumption that the data of source and target companies should have the same software metrics. However, for CCDP, the source and target company data is usually heterogeneous, namely the metrics used and the size of metric set are different in the data of two companies. We call CCDP in this scenario as heterogeneous CCDP (HCCDP) task. In this paper, we aim to provide an effective solution for HCCDP. We propose a unified metric representation (UMR) for the data of source and target companies. The UMR consists of three types of metrics, i.e., the common metrics of the source and target companies, source-company specific metrics and target-company specific metrics. To construct UMR for source company data, the target-company specific metrics are set as zeros, while for UMR of the target company data, the source-company specific metrics are set as zeros. Based on the unified metric representation, we for the first time introduce canonical correlation analysis (CCA), an effective transfer learning method, into CCDP to make the data distributions of source and target companies similar. Experiments on 14 public heterogeneous datasets from four companies indicate that: 1) for HCCDP with partially different metrics, our approach significantly outperforms state-of-the-art CCDP methods; 2) for HCCDP with totally different metrics, our approach obtains comparable prediction performances in contrast with within-project prediction results. The proposed approach is effective for HCCDP. ","Heterogeneous cross-company defect prediction (HCCDP), unified metric representation, company-specific metrics, common metrics, canonical correlation analysis (CCA)",496–507,12,Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering
502,@inproceedings: 10.1145/2786805.2786814,Heterogeneous Defect Prediction,"Nam, Jaechang and Kim, Sunghun",2015,Association for Computing Machinery,,https://doi.org/10.1145/2786805.2786814,10.1145/2786805.2786814," Software defect prediction is one of the most active research areas in software engineering. We can build a prediction model with defect data collected from a software project and predict defects in the same project, i.e. within-project defect prediction (WPDP). Researchers also proposed cross-project defect prediction (CPDP) to predict defects for new projects lacking in defect data by using prediction models built by other projects. In recent studies, CPDP is proved to be feasible. However, CPDP requires projects that have the same metric set, meaning the metric sets should be identical between projects. As a result, current techniques for CPDP are difficult to apply across projects with heterogeneous metric sets. To address the limitation, we propose heterogeneous defect prediction (HDP) to predict defects across projects with heterogeneous metric sets. Our HDP approach conducts metric selection and metric matching to build a prediction model between projects with heterogeneous metric sets. Our empirical study on 28 subjects shows that about 68% of predictions using our approach outperform or are comparable to WPDP with statistical significance. ","Defect prediction, heterogeneous metrics, quality assurance",508–519,12,Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering
503,@inproceedings: 10.5555/2818754.2818807,Learning to Log: Helping Developers Make Informed Logging Decisions,"Zhu, Jieming and He, Pinjia and Fu, Qiang and Zhang, Hongyu and Lyu, Michael R. and Zhang, Dongmei",2015,IEEE Press,,,,"Logging is a common programming practice of practical importance to collect system runtime information for postmortem analysis. Strategic logging placement is desired to cover necessary runtime information without incurring unintended consequences (e.g., performance overhead, trivial logs). However, in current practice, there is a lack of rigorous specifications for developers to govern their logging behaviours. Logging has become an important yet tough decision which mostly depends on the domain knowledge of developers. To reduce the effort on making logging decisions, in this paper, we propose a ""learning to log"" framework, which aims to provide informative guidance on logging during development. As a proof of concept, we provide the design and implementation of a logging suggestion tool, LogAdvisor, which automatically learns the common logging practices on where to log from existing logging instances and further leverages them for actionable suggestions to developers. Specifically, we identify the important factors for determining where to log and extract them as structural features, textual features, and syntactic features. Then, by applying machine learning techniques (e.g., feature selection and classifier learning) and noise handling techniques, we achieve high accuracy of logging suggestions. We evaluate LogAdvisor on two industrial software systems from Microsoft and two open-source software systems from GitHub (totally 19.1M LOC and 100.6K logging statements). The encouraging experimental results, as well as a user study, demonstrate the feasibility and effectiveness of our logging suggestion tool. We believe our work can serve as an important first step towards the goal of ""learning to log"".",,415–425,11,Proceedings of the 37th International Conference on Software Engineering - Volume 1
504,@inproceedings: 10.1145/2694344.2694358,PuDianNao: A Polyvalent Machine Learning Accelerator,"Liu, Daofu and Chen, Tianshi and Liu, Shaoli and Zhou, Jinhong and Zhou, Shengyuan and Teman, Olivier and Feng, Xiaobing and Zhou, Xuehai and Chen, Yunji",2015,Association for Computing Machinery,,https://doi.org/10.1145/2694344.2694358,10.1145/2694344.2694358,"Machine Learning (ML) techniques are pervasive tools in various emerging commercial applications, but have to be accommodated by powerful computer systems to process very large data. Although general-purpose CPUs and GPUs have provided straightforward solutions, their energy-efficiencies are limited due to their excessive supports for flexibility. Hardware accelerators may achieve better energy-efficiencies, but each accelerator often accommodates only a single ML technique (family). According to the famous No-Free-Lunch theorem in the ML domain, however, an ML technique performs well on a dataset may perform poorly on another dataset, which implies that such accelerator may sometimes lead to poor learning accuracy. Even if regardless of the learning accuracy, such accelerator can still become inapplicable simply because the concrete ML task is altered, or the user chooses another ML technique.In this study, we present an ML accelerator called PuDianNao, which accommodates seven representative ML techniques, including k-means, k-nearest neighbors, naive bayes, support vector machine, linear regression, classification tree, and deep neural network. Benefited from our thorough analysis on computational primitives and locality properties of different ML techniques, PuDianNao can perform up to 1056 GOP/s (e.g., additions and multiplications) in an area of 3.51 mm^2, and consumes 596 mW only. Compared with the NVIDIA K20M GPU (28nm process), PuDianNao (65nm process) is 1.20x faster, and can reduce the energy by 128.41x.","computer architecture, machine learning, accelerator",369–381,13,Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems
505,@article: 10.1145/2786763.2694358,PuDianNao: A Polyvalent Machine Learning Accelerator,"Liu, Daofu and Chen, Tianshi and Liu, Shaoli and Zhou, Jinhong and Zhou, Shengyuan and Teman, Olivier and Feng, Xiaobing and Zhou, Xuehai and Chen, Yunji",2015,Association for Computing Machinery,0163-5964,https://doi.org/10.1145/2786763.2694358,10.1145/2786763.2694358,"Machine Learning (ML) techniques are pervasive tools in various emerging commercial applications, but have to be accommodated by powerful computer systems to process very large data. Although general-purpose CPUs and GPUs have provided straightforward solutions, their energy-efficiencies are limited due to their excessive supports for flexibility. Hardware accelerators may achieve better energy-efficiencies, but each accelerator often accommodates only a single ML technique (family). According to the famous No-Free-Lunch theorem in the ML domain, however, an ML technique performs well on a dataset may perform poorly on another dataset, which implies that such accelerator may sometimes lead to poor learning accuracy. Even if regardless of the learning accuracy, such accelerator can still become inapplicable simply because the concrete ML task is altered, or the user chooses another ML technique.In this study, we present an ML accelerator called PuDianNao, which accommodates seven representative ML techniques, including k-means, k-nearest neighbors, naive bayes, support vector machine, linear regression, classification tree, and deep neural network. Benefited from our thorough analysis on computational primitives and locality properties of different ML techniques, PuDianNao can perform up to 1056 GOP/s (e.g., additions and multiplications) in an area of 3.51 mm^2, and consumes 596 mW only. Compared with the NVIDIA K20M GPU (28nm process), PuDianNao (65nm process) is 1.20x faster, and can reduce the energy by 128.41x.","computer architecture, machine learning, accelerato",369–381,13,SIGARCH Comput. Archit. News
506,@article: 10.1145/2775054.2694358,PuDianNao: A Polyvalent Machine Learning Accelerator,"Liu, Daofu and Chen, Tianshi and Liu, Shaoli and Zhou, Jinhong and Zhou, Shengyuan and Teman, Olivier and Feng, Xiaobing and Zhou, Xuehai and Chen, Yunji",2015,Association for Computing Machinery,0362-1340,https://doi.org/10.1145/2775054.2694358,10.1145/2775054.2694358,"Machine Learning (ML) techniques are pervasive tools in various emerging commercial applications, but have to be accommodated by powerful computer systems to process very large data. Although general-purpose CPUs and GPUs have provided straightforward solutions, their energy-efficiencies are limited due to their excessive supports for flexibility. Hardware accelerators may achieve better energy-efficiencies, but each accelerator often accommodates only a single ML technique (family). According to the famous No-Free-Lunch theorem in the ML domain, however, an ML technique performs well on a dataset may perform poorly on another dataset, which implies that such accelerator may sometimes lead to poor learning accuracy. Even if regardless of the learning accuracy, such accelerator can still become inapplicable simply because the concrete ML task is altered, or the user chooses another ML technique.In this study, we present an ML accelerator called PuDianNao, which accommodates seven representative ML techniques, including k-means, k-nearest neighbors, naive bayes, support vector machine, linear regression, classification tree, and deep neural network. Benefited from our thorough analysis on computational primitives and locality properties of different ML techniques, PuDianNao can perform up to 1056 GOP/s (e.g., additions and multiplications) in an area of 3.51 mm^2, and consumes 596 mW only. Compared with the NVIDIA K20M GPU (28nm process), PuDianNao (65nm process) is 1.20x faster, and can reduce the energy by 128.41x.","computer architecture, accelerator, machine learnin",369–381,13,SIGPLAN Not.
507,@article: 10.1145/2717311,Accelerating Divergent Applications on SIMD Architectures Using Neural Networks,"Grigorian, Beayna and Reinman, Glenn",2015,Association for Computing Machinery,1544-3566,https://doi.org/10.1145/2717311,10.1145/2717311,"The purpose of this research is to find a neural-network-based solution to the well-known problem of branch divergence in Single Instruction Multiple Data (SIMD) architectures. Our approach differs from existing techniques that handle branch (or control-flow) divergence, which use costly hardware modifications, low-utilization masking techniques, or static prediction methods. As we examine divergent applications, we characterize the degree of data-dependent control flow seen in each and isolate the code regions (or “kernels”) that cause the most performance degradation due to branch divergence. We then train neural networks (NNs) offline to approximate these kernels and inject the NN computations directly into the applications as substitutes for the kernels they approximate. This essentially translates control flow into nondivergent computation, trading off precision for performance. As our methodology manipulates application source code directly, it is inherently platform agnostic and can be adopted as a general means for accelerating divergent applications on data-parallel architectures. In this article, we present the Neuralizer, an automated software flow for kernel identification, NN training, and NN integration, as well as supplementary user-controlled optimization techniques. Evaluating our approach on a variety of divergent applications run on a Graphics Processing Unit (GPU), we on average achieve performance gains of 13.6 \texttimes{} and energy savings of 14.8 \texttimes{} with 96% accuracy.","approximate computing, Branch divergence, hardware acceleration, neural network",,23,ACM Trans. Archit. Code Optim.
508,@article: 10.1145/2685394,Efficient Data Encoding for Convolutional Neural Network Application,"Trinh, Hong-Phuc and Duranton, Marc and Paindavoine, Michel",2015,Association for Computing Machinery,1544-3566,https://doi.org/10.1145/2685394,10.1145/2685394,"This article presents an approximate data encoding scheme called Significant Position Encoding (SPE). The encoding allows efficient implementation of the recall phase (forward propagation pass) of Convolutional Neural Networks (CNN)—a typical Feed-Forward Neural Network. This implementation uses only 7 bits data representation and achieves almost the same classification performance compared with the initial network: on MNIST handwriting recognition task, using this data encoding scheme losses only 0.03% in terms of recognition rate (99.27% vs. 99.3%). In terms of storage, we achieve a 12.5% gain compared with an 8 bits fixed-point implementation of the same CNN. Moreover, this data encoding allows efficient implementation of processing unit thanks to the simplicity of scalar product operation—the principal operation in a Feed-Forward Neural Network.","convolutional neural network, canonical signed digit, Significant position encodin",,21,ACM Trans. Archit. Code Optim.
509,@inproceedings: 10.1145/2635868.2661671,"Archie: A Tool for Detecting, Monitoring, and Preserving Architecturally Significant Code","Mirakhorli, Mehdi and Fakhry, Ahmed and Grechko, Artem and Wieloch, Matteusz and Cleland-Huang, Jane",2014,Association for Computing Machinery,,https://doi.org/10.1145/2635868.2661671,10.1145/2635868.2661671," The quality of a software architecture is largely dependent upon the underlying architectural decisions at the framework, tactic, and pattern levels. Decisions to adopt certain solutions determine the extent to which desired qualities such as security, availability, and performance are achieved in the delivered system. In this tool demo, we present our Eclipse plug-in named Archie as a solution for maintaining architectural qualities in the design and code despite long-term maintenance and evolution activities. Archie detects architectural tactics such as heartbeat, resource pooling, and role-based access control (RBAC) in the source code of a project; constructs traceability links between the tactics, design models, rationales and source code; and then uses these to monitor the environment for architecturally significant changes and to keep developers informed of underlying design decisions and their associated rationales. ","Tactics, Patterns, Architecture, Degradation",739–742,4,Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering
510,@inproceedings: 10.1145/2568225.2568263,AR-Miner: Mining Informative Reviews for Developers from Mobile App Marketplace,"Chen, Ning and Lin, Jialiu and Hoi, Steven C. H. and Xiao, Xiaokui and Zhang, Boshen",2014,Association for Computing Machinery,,https://doi.org/10.1145/2568225.2568263,10.1145/2568225.2568263," With the popularity of smartphones and mobile devices, mobile application (a.k.a. “app”) markets have been growing exponentially in terms of number of users and downloads. App developers spend considerable effort on collecting and exploiting user feedback to improve user satisfaction, but suffer from the absence of effective user review analytics tools. To facilitate mobile app developers discover the most “informative” user reviews from a large and rapidly increasing pool of user reviews, we present “AR-Miner” — a novel computational framework for App Review Mining, which performs comprehensive analytics from raw user reviews by (i) first extracting informative user reviews by filtering noisy and irrelevant ones, (ii) then grouping the informative reviews automatically using topic modeling, (iii) further prioritizing the informative reviews by an effective review ranking scheme, (iv) and finally presenting the groups of most “informative” reviews via an intuitive visualization approach. We conduct extensive experiments and case studies on four popular Android apps to evaluate AR-Miner, from which the encouraging results indicate that AR-Miner is effective, efficient and promising for app developers. ","data mining, user reviews, mobile application, User feedback",767–778,12,Proceedings of the 36th International Conference on Software Engineering
511,@inproceedings: 10.1145/2568225.2568228,How to Make Best Use of Cross-Company Data in Software Effort Estimation?,"Minku, Leandro L. and Yao, Xin",2014,Association for Computing Machinery,,https://doi.org/10.1145/2568225.2568228,10.1145/2568225.2568228," Previous works using Cross-Company (CC) data for making Within-Company (WC) Software Effort Estimation (SEE) try to use CC data or models directly to provide predictions in the WC context. So, these data or models are only helpful when they match the WC context well. When they do not, a fair amount of WC training data, which are usually expensive to acquire, are still necessary to achieve good performance. We investigate how to make best use of CC data, so that we can reduce the amount of WC data while maintaining or improving performance in comparison to WC SEE models. This is done by proposing a new framework to learn the relationship between CC and WC projects explicitly, allowing CC models to be mapped to the WC context. Such mapped models can be useful even when the CC models themselves do not match the WC context directly. Our study shows that a new approach instantiating this framework is able not only to use substantially less WC data than a corresponding WC model, but also to achieve similar/better performance. This approach can also be used to provide insight into the behaviour of a company in comparison to others. ","transfer learning, cross-company learning, online learning, Software effort estimation, ensembles of learning machines",446–456,11,Proceedings of the 36th International Conference on Software Engineering
512,@inproceedings: 10.1145/2499393.2499400,Building a Second Opinion: Learning Cross-Company Data,"Kocaguneli, Ekrem and Cukic, Bojan and Menzies, Tim and Lu, Huihua",2013,Association for Computing Machinery,,https://doi.org/10.1145/2499393.2499400,10.1145/2499393.2499400,"Background: Developing and maintaining a software effort estimation (SEE) data set within a company (within data) is costly. Often times parts of data may be missing or too difficult to collect, e.g. effort values. However, information about the past projects-although incomplete- may be helpful, when incorporated with the SEE data sets from other companies (cross data).Aim: Utilizing cross data to aid within company estimates and local experts; Proposing a synergy between semi-supervised, active and cross company learning for software effort estimation.Method: The proposed method: 1) Summarizes existing unlabeled within data; 2) Uses cross data to provide pseudo-labels for the summarized within data; 3) Uses steps 1 and 2 to provide an estimate for the within test data as an input for the local company experts. We use 21 data sets and compare the proposed method to existing state-of-the-art within and cross company effort estimation methods subject to evaluation by 7 different error measures.Results: In 132 out of 147 settings (21 data sets X 7 error measures = 147 settings), the proposed method performs as well as the state-of-the-art methods. Also, the proposed method summarizes the past within data down to at most 15% of the original data.Conclusion: It is important to look for synergies amongst cross company and within-company effort estimation data, even when the latter is imperfect or sparse. In this research, we provide the experts with a method that: 1) is competent (performs as well as prior within and cross data estimation methods) 2) reflects on local data (estimates come from the within data); 3) is succinct (summarizes within data down to 15% or less); 4) cheap (easy to build).","popularity, transfer learning, k-NN, analogy-based estimation",,10,Proceedings of the 9th International Conference on Predictive Models in Software Engineering
513,@inproceedings: 10.1145/2499393.2499401,"Beyond Data Mining; towards ""Idea Engineering""","Menzies, Tim",2013,Association for Computing Machinery,,https://doi.org/10.1145/2499393.2499401,10.1145/2499393.2499401,"Pablo Picasso said ""computers are stupid- they only give you answers"". I seek to build reasoners that are not stupid- that know predictions and decisions are important, but so too are the questions and insights generated on the way to those conclusions. Within a society of carbon and/or silicon-based agents, discussion systems let us share, reflect, and try to improve each other's insights.","landscape mining, decision mining, AI, idea engineering, discussion mining",,6,Proceedings of the 9th International Conference on Predictive Models in Software Engineering
514,@inproceedings: 10.1145/2501988.2502012,Imaginary Reality Gaming: Ball Games without a Ball,"Baudisch, Patrick and Pohl, Henning and Reinicke, Stefanie and Wittmers, Emilia and L\""{u}hne, Patrick and Knaust, Marius and K\""{o}hler, Sven and Schmidt, Patrick and Holz, Christian",2013,Association for Computing Machinery,,https://doi.org/10.1145/2501988.2502012,10.1145/2501988.2502012,"We present imaginary reality games, i.e., games that mimic the respective real world sport, such as basketball or soccer, except that there is no visible ball. The ball is virtual and players learn about its position only from watching each other act and a small amount of occasional auditory feed-back, e.g., when a person is receiving the ball. Imaginary reality games maintain many of the properties of physical sports, such as unencumbered play, physical exertion, and immediate social interaction between players. At the same time, they allow introducing game elements from video games, such as power-ups, non-realistic physics, and player balancing. Most importantly, they create a new game dynamic around the notion of the invisible ball. To allow players to successfully interact with the invisible ball, we have created a physics engine that evaluates all plausible ball trajectories in parallel, allowing the game engine to select the trajectory that leads to the most enjoyable game play while still favoring skillful play.","motion capture, probabilistic, augmented reality gaming, physical gaming, imaginary interfaces",405–410,6,Proceedings of the 26th Annual ACM Symposium on User Interface Software and Technology
515,@inproceedings: 10.1145/2501988.2502031,Mirage: Exploring Interaction Modalities Using off-Body Static Electric Field Sensing,"Mujibiya, Adiyan and Rekimoto, Jun",2013,Association for Computing Machinery,,https://doi.org/10.1145/2501988.2502031,10.1145/2501988.2502031,"Mirage proposes an effective non body contact technique to infer the amount and type of body motion, gesture, and activity. This approach involves passive measurement of static electric field of the environment flowing through sense electrode. This sensing method leverages electric field distortion by the presence of an intruder (e.g. human body). Mirage sensor has simple analog circuitry and supports ultra-low power operation. It requires no instrumentation to the user, and can be configured as environmental, mobile, and peripheral-attached sensor. We report on a series of experiments with 10 participants showing robust activity and gesture recognition, as well as promising results for robust location classification and multiple user differentiation. To further illustrate the utility of our approach, we demonstrate real-time interactive applications including activity monitoring, and two games which allow the users to interact with a computer using body motion and gestures.","activity recognition, motion detection, electric field sensing, non body contact sensing, gesture recognition",211–220,10,Proceedings of the 26th Annual ACM Symposium on User Interface Software and Technology
516,@inproceedings: 10.5555/2486788.2486839,Transfer Defect Learning,"Nam, Jaechang and Pan, Sinno Jialin and Kim, Sunghun",2013,IEEE Press,,,," Many software defect prediction approaches have been proposed and most are effective in within-project prediction settings. However, for new projects or projects with limited training data, it is desirable to learn a prediction model by using sufficient training data from existing source projects and then apply the model to some target projects (cross-project defect prediction). Unfortunately, the performance of cross-project defect prediction is generally poor, largely because of feature distribution differences between the source and target projects. In this paper, we apply a state-of-the-art transfer learning approach, TCA, to make feature distributions in source and target projects similar. In addition, we propose a novel transfer defect learning approach, TCA+, by extending TCA. Our experimental results for eight open-source projects show that TCA+ significantly improves cross-project prediction performance. ",,382–391,10,Proceedings of the 2013 International Conference on Software Engineering
517,@inbook: 10.1145/2380116.2380154,"Sketch-Editing Games: Human-Machine Communication, Game Theory and Applications","Ribeiro, Andre and Igarashi, Takeo",2012,Association for Computing Machinery,,https://doi.org/10.1145/2380116.2380154,,"We study uncertainty in graphical-based interaction (with special attention to sketches). We argue that a comprehensive model for the problem must include the interaction participants (and their current beliefs), their possible actions and their past sketches. It's yet unclear how to frame and solve the former problem, considering all the latter elements. We suggest framing the problem as a game and solving it with a game-theoretical solution, which leads to a framework for the design of new two-way, sketch-based user interfaces. In special, we use the framework to design a game that can progressively learn visual models of objects from user sketches, and use the models in real-world interactions. Instead of an abstract visual criterion, players in this game learn models to optimize interaction (the game's duration). This two-way sketching game addresses problems essential in emerging interfaces (such as learning and how to deal with interpretation errors). We review possible applications in robotic sketch-to-command, hand gesture recognition, media authoring and visual search, and evaluate two. Evaluations demonstrate how players improve performance with repeated play, and the influence of interaction aspects on learning.",,287–298,1,Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology
518,@inproceedings: 10.1145/2047196.2047233,Imaginary Phone: Learning Imaginary Interfaces by Transferring Spatial Memory from a Familiar Device,"Gustafson, Sean and Holz, Christian and Baudisch, Patrick",2011,Association for Computing Machinery,,https://doi.org/10.1145/2047196.2047233,10.1145/2047196.2047233,"We propose a method for learning how to use an imaginary interface (i.e., a spatial non-visual interface) that we call ""transfer learning"". By using a physical device (e.g. an iPhone) a user inadvertently learns the interface and can then transfer that knowledge to an imaginary interface. We illustrate this concept with our Imaginary Phone prototype. With it users interact by mimicking the use of a physical iPhone by tapping and sliding on their empty non-dominant hand without visual feedback. Pointing on the hand is tracked using a depth camera and touch events are sent wirelessly to an actual iPhone, where they invoke the corresponding actions. Our prototype allows the user to perform everyday task such as picking up a phone call or launching the timer app and setting an alarm. Imaginary Phone thereby serves as a shortcut that frees users from the necessity of retrieving the actual physical device. We present two user studies that validate the three assumptions underlying the transfer learning method. (1) Users build up spatial memory automatically while using a physical device: participants knew the correct location of 68% of their own iPhone home screen apps by heart. (2) Spatial memory transfers from a physical to an imaginary inter-face: participants recalled 61% of their home screen apps when recalling app location on the palm of their hand. (3) Palm interaction is precise enough to operate a typical mobile phone: Participants could reliably acquire 0.95cm wide iPhone targets on their palm-sufficiently large to operate any iPhone standard widget.","mobile, imaginary interface, wearable, spatial memory, touch, non-visual, screen-less, memory",283–292,10,Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology
519,@inproceedings: 10.1145/2047196.2047212,ShowMeHow: Translating User Interface Instructions between Applications,"Ramesh, Vidya and Hsu, Charlie and Agrawala, Maneesh and Hartmann, Bj\""{o}rn",2011,Association for Computing Machinery,,https://doi.org/10.1145/2047196.2047212,10.1145/2047196.2047212,"Many people learn how to use complex authoring applications through tutorials. However, user interfaces for authoring tools differ between versions, platforms, and competing products, limiting the utility of tutorials. Our goal is to make tutorials more useful by enabling users to repurpose tutorials between similar applications. We introduce UI translation interfaces which enable users to locate commands in one application using the interface language of another application. Our end-user tool, ShowMeHow, demonstrates two interaction techniques to accomplish translations: 1) direct manipulation of interface facades and 2) text search for commands using the vocabulary of another application. We discuss tools needed to construct the translation maps that enable these techniques. An initial study (n=12) shows that users can locate unfamiliar commands twice as fast with interface facades. A second study showed that users can work through tutorials written for one application in another application.","tutorials, instructions, mapping, translation",127–134,8,Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology
520,@inproceedings: 10.1145/1622176.1622193,User Guided Audio Selection from Complex Sound Mixtures,"Smaragdis, Paris",2009,Association for Computing Machinery,,https://doi.org/10.1145/1622176.1622193,10.1145/1622176.1622193,"In this paper we present a novel interface for selecting sounds in audio mixtures. Traditional interfaces in audio editors provide a graphical representation of sounds which is either a waveform, or some variation of a time/frequency transform. Although with these representations a user might be able to visually identify elements of sounds in a mixture, they do not facilitate object-specific editing (e.g. selecting only the voice of a singer in a song). This interface uses audio guidance from a user in order to select a target sound within a mixture. The user is asked to vocalize (or otherwise sonically represent) the desired target sound, and an automatic process identifies and isolates the elements of the mixture that best relate to the user's input. This way of pointing to specific parts of an audio stream allows a user to perform audio selections which would have been infeasible otherwise.",audio interfaces,89–92,4,Proceedings of the 22nd Annual ACM Symposium on User Interface Software and Technology
521,@inproceedings: 10.5555/1667872.1667883,Combining POMDPs Trained with User Simulations and Rule-Based Dialogue Management in a Spoken Dialogue System,"Varges, Sebastian and Quarteroni, Silvia and Riccardi, Giuseppe and Ivanov, Alexei V. and Roberti, Pierluigi",2009,Association for Computational Linguistics,,,,"Over several years, we have developed an approach to spoken dialogue systems that includes rule-based and trainable dialogue managers, spoken language understanding and generation modules, and a comprehensive dialogue system architecture. We present a Reinforcement Learning-based dialogue system that goes beyond standard rule-based models and computes on-line decisions of the best dialogue moves. The key concept of this work is that we bridge the gap between manually written dialog models (e.g. rule-based) and adaptive computational models such as Partially Observable Markov Decision Processes (POMDP) based dialogue managers.",,41–44,4,Proceedings of the ACL-IJCNLP 2009 Software Demonstrations
522,@inproceedings: 10.1145/1370750.1370772,An Extension of Fault-Prone Filtering Using Precise Training and a Dynamic Threshold,"Hata, Hideaki and Mizuno, Osamu and Kikuno, Tohru",2008,Association for Computing Machinery,,https://doi.org/10.1145/1370750.1370772,10.1145/1370750.1370772,"Fault-prone module detection in source code is important for assurance of software quality. Most previous fault-prone detection approaches have been based on software metrics. Such approaches, however, have difficulties in collecting the metrics and in constructing mathematical models based on the metrics. To mitigate such difficulties, we have proposed a novel approach for detecting fault-prone modules using a spam-filtering technique, named Fault-Prone Filtering. In our approach, fault-prone modules are detected in such a way that the source code modules are considered as text files and are applied to the spam filter directly. In practice, we use the training only errors procedure and apply this procedure to fault-prone. Since no pre-training is required, this procedure can be applied to an actual development field immediately. This paper describes an extension of the training only errors procedures. We introduce a precise unit of training, ""modified lines of code,"" instead of methods. In addition, we introduce the dynamic threshold for classification. The result of the experiment shows that our extension leads to twice the precision with about the same recall, and improves 15% on the best F1 measurement.","text mining, fault-prone modules, spam filter",89–98,10,Proceedings of the 2008 International Working Conference on Mining Software Repositories
523,@inproceedings: 10.1109/RAISE.2019.00011,Automatic Detection of Latent Software Component Relationships from Online Q&amp;A Sites,"Karthik, Suhrid and Medvidovic, Nenad",2019,IEEE Press,,https://doi.org/10.1109/RAISE.2019.00011,10.1109/RAISE.2019.00011,"Modern software system stacks are composed of large numbers of software components. These components may include a broad range of entities such as services, libraries, and frameworks, all intended to address specific requirements. It is not only necessary that these components satisfy respective functional and non-functional concerns, but also that the combinations of selected components work well together. The space of component combinations to explore is huge. Together with the almost universal lack of formal documentation suggesting desirable combinations and cautioning against undesirable ones, this renders the proper selection of combinations very challenging. For this reason, software engineers often solicit advice and document their experience on online forums such as community Q&amp;A sites. In this paper, we show that these Q&amp;A sites contain valuable knowledge about inter-component relations. We develop an approach using information extraction techniques to automatically identify three different types of compatibility relations from unstructured text on Q&amp;A site postings. Our work demonstrates that identifying such relations is valuable for the design of component-based systems and that automatic relation extraction is a promising technique to systematically harness such community knowledge.",,15–21,7,Proceedings of the 7th International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering
524,@inproceedings: 10.1109/ICSE-NIER.2019.00016,Leveraging Small Software Engineering Data Sets with Pre-Trained Neural Networks,"Robbes, Romain and Janes, Andrea",2019,IEEE Press,,https://doi.org/10.1109/ICSE-NIER.2019.00016,10.1109/ICSE-NIER.2019.00016,"Many software engineering data sets, particularly those that demand manual labelling for classification, are necessarily small. As a consequence, several recent software engineering papers have cast doubt on the effectiveness of deep neural networks for classification tasks, when applied to these data sets. We provide initial evidence that recent advances in Natural Language Processing, that allow neural networks to leverage large amount of unlabelled data in a pre-training phase, can significantly improve performance.","transfer learning, data sets, deep learning",29–32,4,Proceedings of the 41st International Conference on Software Engineering: New Ideas and Emerging Results
525,@inproceedings: 10.1109/ICSE-SEIP.2019.00042,Software Engineering for Machine Learning: A Case Study,"Amershi, Saleema and Begel, Andrew and Bird, Christian and DeLine, Robert and Gall, Harald and Kamar, Ece and Nagappan, Nachiappan and Nushi, Besmira and Zimmermann, Thomas",2019,IEEE Press,,https://doi.org/10.1109/ICSE-SEIP.2019.00042,10.1109/ICSE-SEIP.2019.00042,"Recent advances in machine learning have stimulated widespread interest within the Information Technology sector on integrating AI capabilities into software and services. This goal has forced organizations to evolve their development processes. We report on a study that we conducted on observing software teams at Microsoft as they develop AI-based applications. We consider a nine-stage workflow process informed by prior experiences developing AI applications (e.g., search and NLP) and data science tools (e.g. application diagnostics and bug reporting). We found that various Microsoft teams have united this workflow into preexisting, well-evolved, Agile-like software engineering processes, providing insights about several essential engineering challenges that organizations may face in creating large-scale AI solutions for the marketplace. We collected some best practices from Microsoft teams to address these challenges. In addition, we have identified three aspects of the AI domain that make it fundamentally different from prior software application domains: 1) discovering, managing, and versioning the data needed for machine learning applications is much more complex and difficult than other types of software engineering, 2) model customization and model reuse require very different skills than are typically found in software teams, and 3) AI components are more difficult to handle as distinct modules than traditional software components --- models may be ""entangled"" in complex ways and experience non-monotonic error behavior. We believe that the lessons learned by Microsoft teams will be valuable to other organizations.","data, software engineering, AI, process",291–300,10,Proceedings of the 41st International Conference on Software Engineering: Software Engineering in Practice
526,@inproceedings: 10.1109/SBST.2019.000-1,One-Size-Fits-None? Improving Test Generation Using Context-Optimized Fitness Functions,"Gay, Gregory",2019,IEEE Press,,https://doi.org/10.1109/SBST.2019.000-1,10.1109/SBST.2019.000-1,"Current approaches to search-based test case generation have yielded limited results in terms of human-competitiveness. However, effective search-based test generation relies on the selection of the correct fitness functions---feedback mechanisms---for a chosen goal. We propose that the key to overcoming these limitations lies in infusing domain knowledge and context into the fitness functions used to guide the search and the ability to automatically optimize the fitness functions used when generating tests for a given class, goal, and algorithm.","search-based software engineering, automated test generation, search-based software testing",3–4,2,Proceedings of the 12th International Workshop on Search-Based Software Testing
527,@inproceedings: 10.1109/ICSE-NIER.2019.00031,Structural Coverage Criteria for Neural Networks Could Be Misleading,"Li, Zenan and Ma, Xiaoxing and Xu, Chang and Cao, Chun",2019,IEEE Press,,https://doi.org/10.1109/ICSE-NIER.2019.00031,10.1109/ICSE-NIER.2019.00031,"There is a dramatically increasing interest in the quality assurance for DNN-based systems in the software engineering community. An emerging hot topic in this direction is structural coverage criteria for testing neural networks, which are inspired by coverage metrics used in conventional software testing. In this short paper, we argue that these criteria could be misleading because of the fundamental differences between neural networks and human written programs. Our preliminary exploration shows that (1) adversarial examples are pervasively distributed in the finely divided space defined by such coverage criteria, while available natural samples are very sparse, and as a consequence, (2) previously reported fault-detection ""capabilities"" conjectured from high coverage testing are more likely due to the adversary-oriented search but not the real ""high"" coverage.","software testing, neural networks, coverage",89–92,4,Proceedings of the 41st International Conference on Software Engineering: New Ideas and Emerging Results
528,@inproceedings: 10.1109/BotSE.2019.00020,Building Sankie: An AI Platform for DevOps,"Kumar, Rahul and Bansal, Chetan and Maddila, Chandra and Sharma, Nitin and Martelock, Shawn and Bhargava, Ravi",2019,IEEE Press,,https://doi.org/10.1109/BotSE.2019.00020,10.1109/BotSE.2019.00020,"There has been a fundamental shift amongst software developers and engineers in the past few years. The software development life cycle (SDLC) for a developer has increased in complexity and scale. Changes that were developed and deployed over a matter of days or weeks are now deployed in a matter of hours. Due to greater availability of compute, storage, better tooling, and the necessity to react, developers are constantly looking to increase their velocity and throughput of developing and deploying changes. Consequently, there is a great need for more intelligent and context sensitive DevOps tools and services that help developers increase their efficiency while developing and debugging. Given the vast amounts of heterogeneous data available from the SDLC, such intelligent tools and services can now be built and deployed at a large scale to help developers achieve their goals and be more productive. In this paper, we present Sankie, a scalable and general service that has been developed to assist and impact all stages of the modern SDLC. Sankie provides all the necessary infrastructure (back-end and front-end bots) to ingest data from repositories and services, train models based on the data, and eventually perform decorations or provide information to engineers to help increase the velocity and throughput of changes, bug fixes etc. This paper discusses the architecture as well as some of the key observations we have made from wide scale deployment of Sankie within Microsoft.","empirical software engineering, scale, Azure, bot, machine learning, pull request, DevOps, infrastructure, software development life cycle",48–53,6,Proceedings of the 1st International Workshop on Bots in Software Engineering
529,@inproceedings: 10.1109/SST.2019.00013,Ideas on Improving Software Artifact Reuse via Traceability and Self-Awareness,"Tinnes, Christof and Biesdorf, Andreas and Hohenstein, Uwe and Matthes, Florian",2019,IEEE Press,,https://doi.org/10.1109/SST.2019.00013,10.1109/SST.2019.00013,We describe our vision towards automatic software and system development and argue that reusing knowledge from existing projects as well as traceability between corresponding artifacts are important steps towards this vision. We furthermore list barriers that are currently experienced with software artifact reuse and traceability in industry and suggest some ideas to overcome these barriers.,,13–16,4,Proceedings of the 10th International Workshop on Software and Systems Traceability
530,@inproceedings: 10.1109/MSR.2019.00078,Cross-Language Clone Detection by Learning over Abstract Syntax Trees,"Perez, Daniel and Chiba, Shigeru",2019,IEEE Press,,https://doi.org/10.1109/MSR.2019.00078,10.1109/MSR.2019.00078,"Clone detection across programs written in the same programming language has been studied extensively in the literature. On the contrary, the task of detecting clones across multiple programming languages has not been studied as much, and approaches based on comparison cannot be directly applied. In this paper, we present a clone detection method based on semi-supervised machine learning designed to detect clones across programming languages with similar syntax. Our method uses an unsupervised learning approach to learn token-level vector representations and an LSTM-based neural network to predict whether two code fragments are clones. To train our network, we present a cross-language code clone dataset --- which is to the best of our knowledge the first of its kind --- containing around 45,000 code fragments written in Java and Python. We evaluate our approach on the dataset we created and show that our method gives promising results when detecting similarities between code fragments written in Java and Python.","source code representation, clone detection, machine learning",518–528,11,Proceedings of the 16th International Conference on Mining Software Repositories
531,@inproceedings: 10.1109/MSR.2019.00015,Semantic Source Code Models Using Identifier Embeddings,"Efstathiou, Vasiliki and Spinellis, Diomidis",2019,IEEE Press,,https://doi.org/10.1109/MSR.2019.00015,10.1109/MSR.2019.00015,"The emergence of online open source repositories in the recent years has led to an explosion in the volume of openly available source code, coupled with metadata that relate to a variety of software development activities. As an effect, in line with recent advances in machine learning research, software maintenance activities are switching from symbolic formal methods to data-driven methods. In this context, the rich semantics hidden in source code identifiers provide opportunities for building semantic representations of code which can assist tasks of code search and reuse. To this end, we deliver in the form of pretrained vector space models, distributed code representations for six popular programming languages, namely, Java, Python, PHP, C, C++, and C#. The models are produced using fastText, a state-of-the-art library for learning word representations. Each model is trained on data from a single programming language; the code mined for producing all models amounts to over 13.000 repositories. We indicate dissimilarities between natural language and source code, as well as variations in coding conventions in between the different programming languages we processed. We describe how these heterogeneities guided the data preprocessing decisions we took and the selection of the training parameters in the released models. Finally, we propose potential applications of the models and discuss limitations of the models.","fastText, semantic similarity, code semantics, vector space models",29–33,5,Proceedings of the 16th International Conference on Mining Software Repositories
532,@inproceedings: 10.1109/MSR.2019.00020,Exploring Word Embedding Techniques to Improve Sentiment Analysis of Software Engineering Texts,"Biswas, Eeshita and Vijay-Shanker, K. and Pollock, Lori",2019,IEEE Press,,https://doi.org/10.1109/MSR.2019.00020,10.1109/MSR.2019.00020,"Sentiment analysis (SA) of text-based software artifacts is increasingly used to extract information for various tasks including providing code suggestions, improving development team productivity, giving recommendations of software packages and libraries, and recommending comments on defects in source code, code quality, possibilities for improvement of applications. Studies of state-of-the-art sentiment analysis tools applied to software-related texts have shown varying results based on the techniques and training approaches. In this paper, we investigate the impact of two potential opportunities to improve the training for sentiment analysis of SE artifacts in the context of the use of neural networks customized using the Stack Overflow data developed by Lin et al.We customize the process of sentiment analysis to the software domain, using software domain-specific word embeddings learned from Stack Overflow (SO) posts, and study the impact of software domain-specific word embeddings on the performance of the sentiment analysis tool, as compared to generic word embeddings learned from Google News. We find that the word embeddings learned from the Google News data performs mostly similar and in some cases better than the word embeddings learned from SO posts. We also study the impact of two machine learning techniques, oversampling and undersampling of data, on the training of a sentiment classifier for handling small SE datasets with a skewed distribution. We find that oversampling alone, as well as the combination of oversampling and undersampling together, helps in improving the performance of a sentiment classifier.","software engineering, sentiment analysis, word embeddings",68–78,11,Proceedings of the 16th International Conference on Mining Software Repositories
533,@inproceedings: 10.1109/ICSE.2019.00107,"CRADLE: <u class=""uu"">Cr</u>oss-Backend V<u class=""uu"">a</u>lidation to <u class=""uu"">D</u>etect and <u class=""uu"">L</u>ocalize Bugs in D<u class=""uu"">e</u>ep Learning Libraries","Pham, Hung Viet and Lutellier, Thibaud and Qi, Weizhen and Tan, Lin",2019,IEEE Press,,https://doi.org/10.1109/ICSE.2019.00107,10.1109/ICSE.2019.00107,"Deep learning (DL) systems are widely used in domains including aircraft collision avoidance systems, Alzheimer's disease diagnosis, and autonomous driving cars. Despite the requirement for high reliability, DL systems are difficult to test.Existing DL testing work focuses on testing the DL models, not the implementations (e.g., DL software libraries) of the models. One key challenge of testing DL libraries is the difficulty of knowing the expected output of DL libraries given an input instance. Fortunately, there are multiple implementations of the same DL algorithms in different DL libraries.Thus, we propose CRADLE, a new approach that focuses on finding and localizing bugs in DL software libraries. CRADLE (1) performs cross-implementation inconsistency checking to detect bugs in DL libraries, and (2) leverages anomaly propagation tracking and analysis to localize faulty functions in DL libraries that cause the bugs. We evaluate CRADLE on three libraries (TensorFlow, CNTK, and Theano), 11 datasets (including ImageNet, MNIST, and KGS Go game), and 30 pre-trained models. CRADLE detects 12 bugs and 104 unique inconsistencies, and highlights functions relevant to the causes of inconsistencies for all 104 unique inconsistencies.","cross-implementation testing, software testing, bugs detection, deep learning software testing",1027–1038,12,Proceedings of the 41st International Conference on Software Engineering
534,@inproceedings: 10.1109/ICSE.2019.00027,Resource-Aware Program Analysis via Online Abstraction Coarsening,"Heo, Kihong and Oh, Hakjoo and Yang, Hongseok",2019,IEEE Press,,https://doi.org/10.1109/ICSE.2019.00027,10.1109/ICSE.2019.00027,"We present a new technique for developing a resource-aware program analysis. Such an analysis is aware of constraints on available physical resources, such as memory size, tracks its resource use, and adjusts its behaviors during fixpoint computation in order to meet the constraint and achieve high precision. Our resource-aware analysis adjusts behaviors by coarsening program abstraction, which usually makes the analysis consume less memory and time until completion. It does so multiple times during the analysis, under the direction of what we call a controller. The controller constantly intervenes in the fixpoint computation of the analysis and decides how much the analysis should coarsen the abstraction. We present an algorithm for learning a good controller automatically from benchmark programs. We applied our technique to a static analysis for C programs, where we control the degree of flow-sensitivity to meet a constraint on peak memory consumption. The experimental results with 18 real-world programs show that our algorithm can learn a good controller and the analysis with this controller meets the constraint and utilizes available memory effectively.","static analysis, resource constraint, learning",94–104,11,Proceedings of the 41st International Conference on Software Engineering
535,@inproceedings: 10.1109/SEAMS.2019.00027,Is Adaptivity a Core Property of Intelligent Systems? It Depends,"ElSaid, AbdElRahman and Desell, Travis and Krutz, Daniel E.",2019,IEEE Press,,https://doi.org/10.1109/SEAMS.2019.00027,10.1109/SEAMS.2019.00027,"Autonomous systems rely upon intelligence to enable them to function without the need for human intervention. In many cases, these systems must adapt to properly react to numerous scenarios. This adaptivity could be a simple action such as activating an additional server, or could be incredibly complex in creating a completely new adaptation strategy. In the following work, we address the question ""Is Adaptability a Core Property of Intelligent Systems?"" We determine that due to the ambiguity of the term `intelligent system' and the wide range of such systems, that there is no clear and definitive answer to this question. This paper details the deliberation of three AI/Self-Adaptive researchers in addressing this question. We use several scenarios of intelligent systems ranging from a simple motion detector to a complex `humanoid' futuristic artificial system to demonstrate the complexity of this question.",,153–154,2,Proceedings of the 14th International Symposium on Software Engineering for Adaptive and Self-Managing Systems
536,@inproceedings: 10.1109/SEAMS.2019.00029,All versus One: An Empirical Comparison on Retrained and Incremental Machine Learning for Modeling Performance of Adaptable Software,"Chen, Tao",2019,IEEE Press,,https://doi.org/10.1109/SEAMS.2019.00029,10.1109/SEAMS.2019.00029,"Given the ever-increasing complexity of adaptable software systems and their commonly hidden internal information (e.g., software runs in the public cloud), machine learning based performance modeling has gained momentum for evaluating, understanding and predicting software performance, which facilitates better informed self-adaptations. As performance data accumulates during the run of the software, updating the performance models becomes necessary. To this end, there are two conventional modeling methods: the retrained modeling that always discard the old model and retrain a new one using all available data; or the incremental modeling that retains the existing model and tunes it using one newly arrival data sample. Generally, literature on machine learning based performance modeling for adaptable software chooses either of those methods according to a general belief, but they provide insufficient evidences or references to justify their choice. This paper is the first to report on a comprehensive empirical study that examines both modeling methods under distinct domains of adaptable software, 5 performance indicators, 8 learning algorithms and settings, covering a total of 1,360 different conditions. Our findings challenge the general belief, which is shown to be only partially correct, and reveal some of the important, statistically significant factors that are often overlooked in existing work, providing evidence-based insights on the choice.","self-adaptive system, machine learning, performance modeling, software runtime",157–168,12,Proceedings of the 14th International Symposium on Software Engineering for Adaptive and Self-Managing Systems
537,@inproceedings: 10.1109/ICSE.2019.00108,Guiding Deep Learning System Testing Using Surprise Adequacy,"Kim, Jinhan and Feldt, Robert and Yoo, Shin",2019,IEEE Press,,https://doi.org/10.1109/ICSE.2019.00108,10.1109/ICSE.2019.00108,"Deep Learning (DL) systems are rapidly being adopted in safety and security critical domains, urgently calling for ways to test their correctness and robustness. Testing of DL systems has traditionally relied on manual collection and labelling of data. Recently, a number of coverage criteria based on neuron activation values have been proposed. These criteria essentially count the number of neurons whose activation during the execution of a DL system satisfied certain properties, such as being above predefined thresholds. However, existing coverage criteria are not sufficiently fine grained to capture subtle behaviours exhibited by DL systems. Moreover, evaluations have focused on showing correlation between adversarial examples and proposed criteria rather than evaluating and guiding their use for actual testing of DL systems. We propose a novel test adequacy criterion for testing of DL systems, called Surprise Adequacy for Deep Learning Systems (SADL), which is based on the behaviour of DL systems with respect to their training data. We measure the surprise of an input as the difference in DL system's behaviour between the input and the training data (i.e., what was learnt during training), and subsequently develop this as an adequacy criterion: a good test input should be sufficiently but not overtly surprising compared to training data. Empirical evaluation using a range of DL systems from simple image classifiers to autonomous driving car platforms shows that systematic sampling of inputs based on their surprise can improve classification accuracy of DL systems against adversarial examples by up to 77.5% via retraining.","deep learning systems, test adequacy",1039–1049,11,Proceedings of the 41st International Conference on Software Engineering
538,@inproceedings: 10.1109/ICSE.2019.00086,A Novel Neural Source Code Representation Based on Abstract Syntax Tree,"Zhang, Jian and Wang, Xu and Zhang, Hongyu and Sun, Hailong and Wang, Kaixuan and Liu, Xudong",2019,IEEE Press,,https://doi.org/10.1109/ICSE.2019.00086,10.1109/ICSE.2019.00086,"Exploiting machine learning techniques for analyzing programs has attracted much attention. One key problem is how to represent code fragments well for follow-up analysis. Traditional information retrieval based methods often treat programs as natural language texts, which could miss important semantic information of source code. Recently, state-of-the-art studies demonstrate that abstract syntax tree (AST) based neural models can better represent source code. However, the sizes of ASTs are usually large and the existing models are prone to the long-term dependency problem. In this paper, we propose a novel AST-based Neural Network (ASTNN) for source code representation. Unlike existing models that work on entire ASTs, ASTNN splits each large AST into a sequence of small statement trees, and encodes the statement trees to vectors by capturing the lexical and syntactical knowledge of statements. Based on the sequence of statement vectors, a bidirectional RNN model is used to leverage the naturalness of statements and finally produce the vector representation of a code fragment. We have applied our neural network based source code representation method to two common program comprehension tasks: source code classification and code clone detection. Experimental results on the two tasks indicate that our model is superior to state-of-the-art approaches.","abstract syntax tree, neural network, source code representation, code classification, code clone detection",783–794,12,Proceedings of the 41st International Conference on Software Engineering
539,@inproceedings: 10.1109/ICSE.2019.00045,NL2Type: Inferring JavaScript Function Types from Natural Language Information,"Malik, Rabee Sohail and Patra, Jibesh and Pradel, Michael",2019,IEEE Press,,https://doi.org/10.1109/ICSE.2019.00045,10.1109/ICSE.2019.00045,"JavaScript is dynamically typed and hence lacks the type safety of statically typed languages, leading to suboptimal IDE support, difficult to understand APIs, and unexpected runtime behavior. Several gradual type systems have been proposed, e.g., Flow and TypeScript, but they rely on developers to annotate code with types. This paper presents NL2Type, a learning-based approach for predicting likely type signatures of JavaScript functions. The key idea is to exploit natural language information in source code, such as comments, function names, and parameter names, a rich source of knowledge that is typically ignored by type inference algorithms. We formulate the problem of predicting types as a classification problem and train a recurrent, LSTM-based neural model that, after learning from an annotated code base, predicts function types for unannotated code. We evaluate the approach with a corpus of 162,673 JavaScript files from real-world projects. NL2Type predicts types with a precision of 84.1% and a recall of 78.9% when considering only the top-most suggestion, and with a precision of 95.5% and a recall of 89.6% when considering the top-5 suggestions. The approach outperforms both JSNice, a state-of-the-art approach that analyzes implementations of functions instead of natural language information, and DeepTyper, a recent type prediction approach that is also based on deep learning. Beyond predicting types, NL2Type serves as a consistency checker for existing type annotations. We show that it discovers 39 inconsistencies that deserve developer attention (from a manual analysis of 50 warnings), most of which are due to incorrect type annotations.","deep learning, type inference, comments, JavaScript, identifiers",304–315,12,Proceedings of the 41st International Conference on Software Engineering
540,@inproceedings: 10.1109/ICSE-Companion.2019.00051,DeepConcolic: Testing and Debugging Deep Neural Networks,"Sun, Youcheng and Huang, Xiaowei and Kroening, Daniel and Sharp, James and Hill, Matthew and Ashmore, Rob",2019,IEEE Press,,https://doi.org/10.1109/ICSE-Companion.2019.00051,10.1109/ICSE-Companion.2019.00051,"Deep neural networks (DNNs) have been deployed in a wide range of applications. We introduce a DNN testing and debugging tool, called DeepConcolic, which is able to detect errors with sufficient rigour so as to be applicable to the testing of DNNs in safety-related applications. DeepConcolic is the first tool that implements a concolic testing technique for DNNs, and the first testing tool that provides users with the functionality of investigating particular parts of a DNN. The tool has been made publicly available and a demo video can be found at https://youtu.be/rliynbhoNLM.",,111–114,4,Proceedings of the 41st International Conference on Software Engineering: Companion Proceedings
541,@inproceedings: 10.1109/ICSE-Companion.2019.00054,Towards Zero Knowledge Learning for Cross Language API Mappings,"Bui, Nghi D. Q.",2019,IEEE Press,,https://doi.org/10.1109/ICSE-Companion.2019.00054,10.1109/ICSE-Companion.2019.00054,"Programmers often need to migrate programs from one language or platform to another in order to implement functionality, instead of rewriting the code from scratch. However, most techniques proposed to identify API mappings across languages and facilitate automated program translation require manually curated parallel corpora that contain already mapped API seeds or functionally-equivalent code using the APIs in two different languages so that the techniques can have an anchor to map APIs. To alleviate the need of curating parallel data and to generalize the applicability of program translation techniques, we develop a new automated approach for identifying API mappings across languages based on the idea of unsupervised domain adaption via Generative Adversarial Network (GAN) and an additional refinement procedure that can transform two vector spaces to align the API vectors in the two spaces without the need of manually provided anchors. We show that our approach can identify API mappings more accurately than Api2Api [25] without the need of curated parallel seeds.",,123–125,3,Proceedings of the 41st International Conference on Software Engineering: Companion Proceedings
542,@inproceedings: 10.1109/SEAMS.2019.00013,Using Unstructured Data to Improve the Continuous Planning of Critical Processes Involving Humans,"Paterson, Colin and Calinescu, Radu and Wang, Di and Manandhar, Suresh",2019,IEEE Press,,https://doi.org/10.1109/SEAMS.2019.00013,10.1109/SEAMS.2019.00013,"The success of processes executed in uncertain and changing environments is reliant on the dependable use of relevant information to support continuous planning at runtime. At the core of this planning is a model which, if incorrect, can lead to failures and, in critical processes such as evacuation and disaster relief operations, to harm to humans. Obtaining reliable and timely estimations of model parameters is often difficult, and considerable research effort has been expended to derive methods for updating models at run-time. Typically, these methods use data sources such as system logs, run-time events and sensor readings, which are well structured. However, in many critical processes, the most relevant data are produced by human participants to, and observers of, the process and its environment (e.g., through social media) and is unstructured. For such scenarios we propose COPE, a work-in-progress method for the continuous planning of critical processes involving humans and carried out in uncertain, changing environments. COPE uses a combination of runtime natural-language processing (to update a stochastic model of the target process based on unstructured data) and stochastic model synthesis (to generate Pareto-optimal plans for the process). Preliminary experiments indicate that COPE can support continuous planning effectively for a simulated evacuation operation after a natural disaster.","probabilistic model checking, natural-language processing, stochastic model synthesis",25–31,7,Proceedings of the 14th International Symposium on Software Engineering for Adaptive and Self-Managing Systems
543,@inproceedings: 10.1109/ICSE.2019.00021,On Learning Meaningful Code Changes via Neural Machine Translation,"Tufano, Michele and Pantiuchina, Jevgenija and Watson, Cody and Bavota, Gabriele and Poshyvanyk, Denys",2019,IEEE Press,,https://doi.org/10.1109/ICSE.2019.00021,10.1109/ICSE.2019.00021,"Recent years have seen the rise of Deep Learning (DL) techniques applied to source code. Researchers have exploited DL to automate several development and maintenance tasks, such as writing commit messages, generating comments and detecting vulnerabilities among others. One of the long lasting dreams of applying DL to source code is the possibility to automate non-trivial coding activities. While some steps in this direction have been taken (e.g., learning how to fix bugs), there is still a glaring lack of empirical evidence on the types of code changes that can be learned and automatically applied by DL.Our goal is to make this first important step by quantitatively and qualitatively investigating the ability of a Neural Machine Translation (NMT) model to learn how to automatically apply code changes implemented by developers during pull requests. We train and experiment with the NMT model on a set of 236k pairs of code components before and after the implementation of the changes provided in the pull requests. We show that, when applied in a narrow enough context (i.e., small/medium-sized pairs of methods before/after the pull request changes), NMT can automatically replicate the changes implemented by developers during pull requests in up to 36% of the cases. Moreover, our qualitative analysis shows that the model is capable of learning and replicating a wide variety of meaningful code changes, especially refactorings and bug-fixing activities. Our results pave the way for novel research in the area of DL on code, such as the automatic learning and applications of refactoring.","empirical study, neural-machine translation",25–36,12,Proceedings of the 41st International Conference on Software Engineering
544,@inproceedings: 10.1109/ICSE.2019.00019,Learning to Spot and Refactor Inconsistent Method Names,"Liu, Kui and Kim, Dongsun and Bissyand\'{e}, Tegawend\'{e} F. and Kim, Taeyoung and Kim, Kisub and Koyuncu, Anil and Kim, Suntae and Traon, Yves Le",2019,IEEE Press,,https://doi.org/10.1109/ICSE.2019.00019,10.1109/ICSE.2019.00019,"To ensure code readability and facilitate software maintenance, program methods must be named properly. In particular, method names must be consistent with the corresponding method implementations. Debugging method names remains an important topic in the literature, where various approaches analyze commonalities among method names in a large dataset to detect inconsistent method names and suggest better ones. We note that the state-of-the-art does not analyze the implemented code itself to assess consistency. We thus propose a novel automated approach to debugging method names based on the analysis of consistency between method names and method code. The approach leverages deep feature representation techniques adapted to the nature of each artifact. Experimental results on over 2.1 million Java methods show that we can achieve up to 15 percentage points improvement over the state-of-the-art, establishing a record performance of 67.9% F1-measure in identifying inconsistent method names. We further demonstrate that our approach yields up to 25% accuracy in suggesting full names, while the state-of-the-art lags far behind at 1.1% accuracy. Finally, we report on our success in fixing 66 inconsistent method names in a live study on projects in the wild.",,1–12,12,Proceedings of the 41st International Conference on Software Engineering
545,@inproceedings: 10.1109/ICSE-Companion.2019.00085,An Artificial Intelligence-Based Model-Driven Approach for Exposing off-Nominal Behaviors,"Madala, Kaushik",2019,IEEE Press,,https://doi.org/10.1109/ICSE-Companion.2019.00085,10.1109/ICSE-Companion.2019.00085,"With an increase in the automation of cyber-physical systems (e.g., automated vehicles and robots), quality problems such as off-nominal behaviors (ONBs) have also increased. While there are techniques that can find ONBs at the requirements engineering stage as it reduces the cost of addressing defects early in development, they do not meet the current industrial needs and often ignore functional safety. These techniques suffer from limitations such as scalability, need for significant human effort and inability to detect overlooked or unknown ONBs. To address these limitations we need a technique that analyzes requirements with respect to functional safety, but with less human effort. To achieve this, we propose our artificial intelligence-based model-driven methodology that provides a means to find ONBs during requirements engineering with minimal human effort. Our methodology utilizes existing approaches such as causal component model (CCM) and systems theoretic process analysis (STPA). We describe the details of each step of our approach and how our approach would support finding ONBs. Using our research and the results of our studies, we intend to provide empirical evidence that considering ONBs during requirements engineering stage and analyzing requirements with respect to functional safety can help create more robust designs and higher-quality products.",,214–217,4,Proceedings of the 41st International Conference on Software Engineering: Companion Proceedings
546,@inproceedings: 10.1109/SEAMS.2019.00015,Machine Learning Meets Quantitative Planning: Enabling Self-Adaptation in Autonomous Robots,"Jamshidi, Pooyan and C\'{a}mara, Javier and Schmerl, Bradley and K\""{a}stner, Christian and Garlan, David",2019,IEEE Press,,https://doi.org/10.1109/SEAMS.2019.00015,10.1109/SEAMS.2019.00015,"Modern cyber-physical systems (e.g., robotics systems) are typically composed of physical and software components, the characteristics of which are likely to change over time. Assumptions about parts of the system made at design time may not hold at run time, especially when a system is deployed for long periods (e.g., over decades). Self-adaptation is designed to find reconfigurations of systems to handle such run-time inconsistencies. Planners can be used to find and enact optimal reconfigurations in such an evolving context. However, for systems that are highly configurable, such planning becomes intractable due to the size of the adaptation space. To overcome this challenge, in this paper we explore an approach that (a) uses machine learning to find Pareto-optimal configurations without needing to explore every configuration and (b) restricts the search space to such configurations to make planning tractable. We explore this in the context of robot missions that need to consider task timeliness and energy consumption. An independent evaluation shows that our approach results in high-quality adaptation plans in uncertain and adversarial environments.","robotics systems, self-adaptive systems, quantitative planning, machine learning, artificial intelligence",39–50,12,Proceedings of the 14th International Symposium on Software Engineering for Adaptive and Self-Managing Systems
547,@inproceedings: 10.1145/3339363.3339385,ADNet: Appearance and Depth Features Network for Visual Object Tracking,"Li, Jing and Hu, Weiming and Yang, Jinfeng",2019,Association for Computing Machinery,,https://doi.org/10.1145/3339363.3339385,10.1145/3339363.3339385,"End-to-end Correlation Filters based trackers have shown excellent performance in visual tracking both in speed and accuracy. Most existing approaches adopt the Convolutional Neural Network (CNN) features pre-trained from the large-scale datasets off-line. However, Convolutional Neural Network features only capture the appearance information and ignore the depth information which can supply the complementary information for visual object tracking. In this paper, we treat the depth information as an auxiliary feature and combine it with the appearance feature in an efficient manner to improve the tracking performance. Specially, we employ the DCFNet tracker as the basis for feature fusion. As for depth feature, we extract it by a monocular depth estimation network that was pre-trained by KITTI and Cityscapes datasets. Various experiments on OTB-2013 and OTB-2015 benchmarks verify that our method has the state-of-the-art performance while the speed is also at a high level---100 frames per second.","Visual Tracking, Depth Feature, Feature Fusion, Correlation Filters",84–88,5,Proceedings of the 2nd International Conference on Computer Science and Software Engineering
548,@inproceedings: 10.1145/3339363.3339389,Semantic BI-Embedded GRU for Fill-in-the-Blank Image Question Answering,"Hu, Jun and Shu, Xiangbo",2019,Association for Computing Machinery,,https://doi.org/10.1145/3339363.3339389,10.1145/3339363.3339389,"Image question answering is becoming a very attractive topic in the field of computer vision and natural language processing in recent years. In this work, we propose a novel Semantic Bi-Embedded Gated Recurrent Unit (SBE-GRU) method to answer fill-in-the-blank style multiple choice questions for images. Different from the single network, we use the SBE-GRU model to learn the high-level semantic information existing in images. To learn the semantic mapping of an image from the visual level to the language level, we feed the visual-language mappings into a stacked GRU. Moreover, to choose the right answer in the candidate options more simply and effectively, we regard the answer sentence as an answer list while training. In the extensive experiments, the proposed method can get better results compared with the state-of-the-art and CCA methods.","Fill-in-the-blank, Image question answering, Gated recurrent unit",108–113,6,Proceedings of the 2nd International Conference on Computer Science and Software Engineering
549,@inproceedings: 10.1145/3339363.3339394,Weakly Supervised Semantic Segmentation Based on Superpixel Sampling Clustering Networks,"Xiao, Jun-sheng and Xu, Hua-hu and Ma, Xiao-jin",2019,Association for Computing Machinery,,https://doi.org/10.1145/3339363.3339394,10.1145/3339363.3339394,"Semantic segmentation models based on deep neural networks depend highly on a large number of pixel-level annotations, while pixel-level annotations require a lot of manpower and resources. Therefore, we propose a weakly supervised image semantic segmentation algorithm based on deep convolutional neural network. This method only the quantity image-level annotations to perform image semantic segmentation. In this paper, the non-conductible superpixel sampling algorithm SLIC is transformed into derivable by relaxing restrictions, added to the deep convolutional neural network to achieve end-to-end training, and the center of the superpixel is dynamically obtained step by step. The saliency map with semantic information is used as the seed, and the seed is used as the cluster center. Then the superpixel block is used as the basic unit when seed grows, and the dynamically generated superpixel module is clustered using the K-Means algorithm. Set the number of categories for image segmentation to the number of clusters of the K-Means algorithm. Until a steady state is reached. Finally, the segmentation results optimized by CRF algorithm to generates fined graind contour. The algorithm is tested in the open challenge data set PASCAL VOC 2012, which is superior to the existing weakly supervised semantic segmentation algorithm, the convergence speed is significantly improved, and the segmentation result is more refined.","K-Means, Superpixel, Semantic segmentation, Weakely supervision",178–183,6,Proceedings of the 2nd International Conference on Computer Science and Software Engineering
550,@inproceedings: 10.1145/3328833.3328856,Computer Aided Detection System for Early Cancerous Pulmonary Nodules by Optimizing Deep Learning Features,"Elnakib, Ahmed and Amer, Hanan M. and Abou-Chadi, Fatma E. Z.",2019,Association for Computing Machinery,,https://doi.org/10.1145/3328833.3328856,10.1145/3328833.3328856,"In this paper, a deep learning technique for the early detection of pulmonary nodules from low dose CT (LDCT) images is proposed. The proposed technique is composed from four stages. Firstly, a preprocessing stage is applied to enhance image contrast of low dose images. Secondly, a transfer learning is utilized to extract deep learning features that describe the LDCT images. Thirdly, a genetic algorithm (GA) is learned on the extracted deep learning features using a training subset of the data to optimize the feature-set and select the most relevant features for cancerous nodules detection. Finally, a classification step of the selected features is performed using supported vector machines (SVM) to detect cancerous pulmonary nodules. Preliminary results on a number of 320 LDCT images acquired from 50 different subjects from the International Early Lung Cancer Action Project, I-ELCAP, online public lung image database has achieved a detection accuracy of 92.5%, sensitivity of 90%, and specificity of 95% Comparison results has shown the outstanding results of the proposed method. These preliminary results confirm the promising of our proposed method.","Lung Cancer, Image Processing, Deep Learning, Nodules, Transfer Learning, LDCT, Support Vector Machine, Detection",75–79,5,Proceedings of the 2019 8th International Conference on Software and Information Engineering
551,@inproceedings: 10.1145/3328833.3328867,Deep Learning Approach for Breast Cancer Diagnosis,"Rashed, Essam and El Seoud, M. Samir Abou",2019,Association for Computing Machinery,,https://doi.org/10.1145/3328833.3328867,10.1145/3328833.3328867,"Breast cancer is one of the leading fatal disease worldwide with high risk control if early discovered. Conventional method for breast screening is x-ray mammography, which is known to be challenging for early detection of cancer lesions. The dense breast structure produced due to the compression process during imaging lead to difficulties to recognize small size abnormalities. Also, inter- and intra-variations of breast tissues lead to significant difficulties to achieve high diagnosis accuracy using hand-crafted features. Deep learning is an emerging machine learning technology that requires a relatively high computation power. Yet, it proved to be very effective in several difficult tasks that requires decision making at the level of human intelligence. In this paper, we develop a new network architecture inspired by the U-net structure that can be used for effective and early detection of breast cancer. Results indicate a high rate of sensitivity and specificity that indicate potential usefulness of the proposed approach in clinical use.","Breast cancer diagnosis, Convolution neural networks, Deep learning",243–247,5,Proceedings of the 2019 8th International Conference on Software and Information Engineering
552,@inproceedings: 10.1145/3328833.3328845,Decoder-Encoder LSTM for Lip Reading,"Fenghour, Souheil and Chen, Daqing and Xiao, Perry",2019,Association for Computing Machinery,,https://doi.org/10.1145/3328833.3328845,10.1145/3328833.3328845,"The success of automated lip reading has been constrained by the inability to distinguish between homopheme words, which are words that have different characters and produce the same lip movements (e.g. ""time"" and ""some""), despite being intrinsically different. One word can often have different phonemes (units of sound) producing exactly the same viseme or visual equivalent of a phoneme for a unit of sound. Through the use of a Long-Short Term Memory Network with word embeddings, we can distinguish between homopheme words or words that produce identical lip movements. The neural network architecture achieved a character accuracy rate of 77.1% and a word accuracy rate of 72.2%.","Deep Learning, Long-Short Term Memory Networks, Lip Reading, Recurrent Neural Networks, Speech Recognition",162–166,5,Proceedings of the 2019 8th International Conference on Software and Information Engineering
553,@inproceedings: 10.1145/3328833.3328860,(Un)Discounted Usability: Evaluating Low-Budget Educational Technology Projects with Dual-Personae Evaluators,"Hassan, Muhammad Mustafa and Tukiainen, Markku and Qureshi, Adnan N.",2019,Association for Computing Machinery,,https://doi.org/10.1145/3328833.3328860,10.1145/3328833.3328860,"The discounted usability inspections typically employ usability-expert evaluators set to evaluate the target system situated out of the real field. Nonetheless, usability-experts work hard to disguise under an end-user persona, and imitate the context of the original domain and workplace to its maximum, but it hardly is an imitation of the original settings. Hence, these techniques are consistently reported to miss usability problems related to real task scenarios. The problem is especially evident in the particular case of Heuristic Evaluation, where the set of rules used for evaluations weighs domain and task orientation less than other attributes, contributing further to negligence of task and domain related problem. It is thus advised to supplement usability-expert based discounted inspections with expensive end-user based testing---a privilege otherwise unavailable to low-budget projects. The authors in this work, however, propose and test using dual-personae evaluators employed in discounted inspections. The results of the experimentation are promising. The evaluators identify a large number of task and domain related problems, as compared to the number of problems identified in other dimensions during the activity. The authors conclude that such evaluators are especially useful in the case of low-budget projects where expensive testing is not possible.","User Based inspection, Discounted Usability, Heuristic Evaluation, Dual-Personae Evaluators, Usability Inspection, Low-Budget Projects",253–258,6,Proceedings of the 2019 8th International Conference on Software and Information Engineering
554,@inproceedings: 10.1145/3297858.3304051,DeepSigns: An End-to-End Watermarking Framework for Ownership Protection of Deep Neural Networks,"Darvish Rouhani, Bita and Chen, Huili and Koushanfar, Farinaz",2019,Association for Computing Machinery,,https://doi.org/10.1145/3297858.3304051,10.1145/3297858.3304051,"Deep Learning (DL) models have created a paradigm shift in our ability to comprehend raw data in various important fields, ranging from intelligence warfare and healthcare to autonomous transportation and automated manufacturing. A practical concern, in the rush to adopt DL models as a service, is protecting the models against Intellectual Property (IP) infringement. DL models are commonly built by allocating substantial computational resources that process vast amounts of proprietary training data. The resulting models are therefore considered to be an IP of the model builder and need to be protected to preserve the owner's competitive advantage. We propose DeepSigns, the first end-to-end IP protection framework that enables developers to systematically insert digital watermarks in the target DL model before distributing the model. DeepSigns is encapsulated as a high-level wrapper that can be leveraged within common deep learning frameworks including TensorFlow and PyTorch. The libraries in DeepSigns work by dynamically learning the Probability Density Function (pdf) of activation maps obtained in different layers of a DL model. DeepSigns uses the low probabilistic regions within the model to gradually embed the owner's signature (watermark) during DL training while minimally affecting the overall accuracy and training overhead. DeepSigns can demonstrably withstand various removal and transformation attacks, including model pruning, model fine-tuning, and watermark overwriting. We evaluate DeepSigns performance on a wide variety of DL architectures including wide residual convolution neural networks, multi-layer perceptrons, and long short-term memory models. Our extensive evaluations corroborate DeepSigns' effectiveness and applicability. We further provide a highly-optimized accompanying API to facilitate training watermarked neural networks with a training overhead as low as 2.2%.","digital watermark, deep neural networks, intellectual property protection",485–497,13,Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems
555,@inproceedings: 10.1145/3297858.3304028,Packing Sparse Convolutional Neural Networks for Efficient Systolic Array Implementations: Column Combining Under Joint Optimization,"Kung, H.T. and McDanel, Bradley and Zhang, Sai Qian",2019,Association for Computing Machinery,,https://doi.org/10.1145/3297858.3304028,10.1145/3297858.3304028,"This paper describes a novel approach of packing sparse convolutional neural networks into a denser format for efficient implementations using systolic arrays. By combining multiple sparse columns of a convolutional filter matrix into a single dense column stored in the systolic array, the utilization efficiency of the systolic array can be substantially increased (e.g., 8x) due to the increased density of nonzero weights in the resulting packed filter matrix. In combining columns, for each row, all filter weights but the one with the largest magnitude are pruned. The remaining weights are retrained to preserve high accuracy. We study the effectiveness of this joint optimization for both high utilization efficiency and classification accuracy with ASIC and FPGA designs based on efficient bit-serial implementations of multiplier-accumulators. We demonstrate that in mitigating data privacy concerns the retraining can be accomplished with only fractions of the original dataset (e.g., 10% for CIFAR-10). We present analysis and empirical evidence on the superior performance of our column combining approach against prior arts under metrics such as energy efficiency (3x) and inference latency (12x).","sparsity, neural networks, joint optimization, data flow architectures, systolic arrays",821–834,14,Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems
556,@inproceedings: 10.1145/3297858.3304076,ADMM-NN: An Algorithm-Hardware Co-Design Framework of DNNs Using Alternating Direction Methods of Multipliers,"Ren, Ao and Zhang, Tianyun and Ye, Shaokai and Li, Jiayu and Xu, Wenyao and Qian, Xuehai and Lin, Xue and Wang, Yanzhi",2019,Association for Computing Machinery,,https://doi.org/10.1145/3297858.3304076,10.1145/3297858.3304076,"Model compression is an important technique to facilitate efficient embedded and hardware implementations of deep neural networks (DNNs), a number of prior works are dedicated to model compression techniques. The target is to simultaneously reduce the model storage size and accelerate the computation, with minor effect on accuracy. Two important categories of DNN model compression techniques are weight pruning and weight quantization. The former leverages the redundancy in the number of weights, whereas the latter leverages the redundancy in bit representation of weights. These two sources of redundancy can be combined, thereby leading to a higher degree of DNN model compression. However, a systematic framework of joint weight pruning and quantization of DNNs is lacking, thereby limiting the available model compression ratio. Moreover, the computation reduction, energy efficiency improvement, and hardware performance overhead need to be accounted besides simply model size reduction, and the hardware performance overhead resulted from weight pruning method needs to be taken into consideration. To address these limitations, we present ADMM-NN, the first algorithm-hardware co-optimization framework of DNNs using Alternating Direction Method of Multipliers (ADMM), a powerful technique to solve non-convex optimization problems with possibly combinatorial constraints. The first part of ADMM-NN is a systematic, joint framework of DNN weight pruning and quantization using ADMM. It can be understood as a smart regularization technique with regularization target dynamically updated in each ADMM iteration, thereby resulting in higher performance in model compression than the state-of-the-art. The second part is hardware-aware DNN optimizations to facilitate hardware-level implementations. We perform ADMM-based weight pruning and quantization considering (i) the computation reduction and energy efficiency improvement, and (ii) the hardware performance overhead due to irregular sparsity. The first requirement prioritizes the convolutional layer compression over fully-connected layers, while the latter requires a concept of the break-even pruning ratio, defined as the minimum pruning ratio of a specific layer that results in no hardware performance degradation. Without accuracy loss, ADMM-NN achieves 85\texttimes{} and 24\texttimes{} pruning on LeNet-5 and AlexNet models, respectively, --- significantly higher than the state-of-the-art. The improvements become more significant when focusing on computation reduction. Combining weight pruning and quantization, we achieve 1,910\texttimes{} and 231\texttimes{} reductions in overall model size on these two benchmarks, when focusing on data storage. Highly promising results are also observed on other representative DNNs such as VGGNet and ResNet-50. We release codes and models at https://github.com/yeshaokai/admm-nn.",,925–938,14,Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems
557,@inproceedings: 10.1145/3297858.3304058,FA3C: FPGA-Accelerated Deep Reinforcement Learning,"Cho, Hyungmin and Oh, Pyeongseok and Park, Jiyoung and Jung, Wookeun and Lee, Jaejin",2019,Association for Computing Machinery,,https://doi.org/10.1145/3297858.3304058,10.1145/3297858.3304058,"Deep Reinforcement Learning (Deep RL) is applied to many areas where an agent learns how to interact with the environment to achieve a certain goal, such as video game plays and robot controls. Deep RL exploits a DNN to eliminate the need for handcrafted feature engineering that requires prior domain knowledge. The Asynchronous Advantage Actor-Critic (A3C) is one of the state-of-the-art Deep RL methods. In this paper, we present an FPGA-based A3C Deep RL platform, called FA3C. Traditionally, FPGA-based DNN accelerators have mainly focused on inference only by exploiting fixed-point arithmetic. Our platform targets both inference and training using single-precision floating-point arithmetic. We demonstrate the performance and energy efficiency of FA3C using multiple A3C agents that learn the control policies of six Atari 2600 games. Its performance is better than a high-end GPU-based platform (NVIDIA Tesla P100). FA3C achieves 27.9% better performance than that of a state-of-the-art GPU-based implementation. Moreover, the energy efficiency of FA3C is 1.62x better than that of the GPU-based implementation.","reinforcement learning, FPGA, deep neural networks",499–513,15,Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems
558,@inproceedings: 10.1145/3297858.3304004,Seer: Leveraging Big Data to Navigate the Complexity of Performance Debugging in Cloud Microservices,"Gan, Yu and Zhang, Yanqi and Hu, Kelvin and Cheng, Dailun and He, Yuan and Pancholi, Meghna and Delimitrou, Christina",2019,Association for Computing Machinery,,https://doi.org/10.1145/3297858.3304004,10.1145/3297858.3304004,"Performance unpredictability is a major roadblock towards cloud adoption, and has performance, cost, and revenue ramifications. Predictable performance is even more critical as cloud services transition from monolithic designs to microservices. Detecting QoS violations after they occur in systems with microservices results in long recovery times, as hotspots propagate and amplify across dependent services. We present Seer, an online cloud performance debugging system that leverages deep learning and the massive amount of tracing data cloud systems collect to learn spatial and temporal patterns that translate to QoS violations. Seer combines lightweight distributed RPC-level tracing, with detailed low-level hardware monitoring to signal an upcoming QoS violation, and diagnose the source of unpredictable performance. Once an imminent QoS violation is detected, Seer notifies the cluster manager to take action to avoid performance degradation altogether. We evaluate Seer both in local clusters, and in large-scale deployments of end-to-end applications built with microservices with hundreds of users. We show that Seer correctly anticipates QoS violations 91% of the time, and avoids the QoS violation to begin with in 84% of cases. Finally, we show that Seer can identify application-level design bugs, and provide insights on how to better architect microservices to achieve predictable performance.","QoS, cloud computing, performance debugging, microservices, resource management, datacenter, data mining, monitoring, tracing, deep learning",19–33,15,Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems
559,@inproceedings: 10.1145/3289602.3293904,"REQ-YOLO: A Resource-Aware, Efficient Quantization Framework for Object Detection on FPGAs","Ding, Caiwen and Wang, Shuo and Liu, Ning and Xu, Kaidi and Wang, Yanzhi and Liang, Yun",2019,Association for Computing Machinery,,https://doi.org/10.1145/3289602.3293904,10.1145/3289602.3293904,"Deep neural networks (DNNs), as the basis of object detection, will play a key role in the development of future autonomous systems with full autonomy. The autonomous systems have special requirements of real-time, energy-e cient implementations of DNNs on a power-budgeted system. Two research thrusts are dedicated to per- formance and energy e ciency enhancement of the inference phase of DNNs. The first one is model compression techniques while the second is e cient hardware implementations. Recent researches on extremely-low-bit CNNs such as binary neural network (BNN) and XNOR-Net replace the traditional oating point operations with bi- nary bit operations, signi cantly reducing memory bandwidth and storage requirement, whereas suffering non-negligible accuracy loss and waste of digital signal processing (DSP) blocks on FPGAs. To overcome these limitations, this paper proposes REQ-YOLO, a resource aware, systematic weight quantization framework for object detection, considering both algorithm and hardware resource aspects in object detection. We adopt the block-circulant matrix method and propose a heterogeneous weight quantization using Alternative Direction Method of Multipliers (ADMM), an e ective optimization technique for general, non-convex optimization problems. To achieve real-time, highly efficient implementations on FPGA, we present the detailed hardware implementation of block circulant matrices on CONV layers and de- velop an e cient processing element (PE) structure supporting the heterogeneous weight quantization, CONV data ow and pipelining techniques, design optimization, and a template-based automatic synthesis framework to optimally exploit hardware resource. Experimental results show that our proposed REQ-YOLO framework can signi cantly compress the YOLO model while introducing very small accuracy degradation. The related codes are here: https://github.com/Anonymous788/heterogeneous_ADMM_YOLO.","object detection, yolo, compression, fpga, admm",33–42,10,Proceedings of the 2019 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays
560,@inproceedings: 10.1145/3289602.3293902,Synetgy: Algorithm-Hardware Co-Design for ConvNet Accelerators on Embedded FPGAs,"Yang, Yifan and Huang, Qijing and Wu, Bichen and Zhang, Tianjun and Ma, Liang and Gambardella, Giulio and Blott, Michaela and Lavagno, Luciano and Vissers, Kees and Wawrzynek, John and Keutzer, Kurt",2019,Association for Computing Machinery,,https://doi.org/10.1145/3289602.3293902,10.1145/3289602.3293902,"Using FPGAs to accelerate ConvNets has attracted significant attention in recent years. However, FPGA accelerator design has not leveraged the latest progress of ConvNets. As a result, the key application characteristics such as frames-per-second (FPS) are ignored in favor of simply counting GOPs, and results on accuracy, which is critical to application success, are often not even reported. In this work, we adopt an algorithm-hardware co-design approach to develop a ConvNet accelerator called Synetgy and a novel ConvNet model called DiracDeltaNet. Both the accelerator and ConvNet are tailored to FPGA requirements. DiracDeltaNet, as the name suggests, is a ConvNet with only $1times 1$ convolutions while spatial convolutions are replaced by more efficient shift operations. DiracDeltaNet achieves competitive accuracy on ImageNet (89.0% top-5), but with 48\texttimes{} fewer parameters and 65\texttimes{} fewer OPs than VGG16. We further quantize DiracDeltaNet's weights to 1-bit and activations to 4-bits, with less than 1% accuracy loss. These quantizations exploit well the nature of FPGA hardware. In short, DiracDeltaNet's small model size, low computational OP count, ultra-low precision and simplified operators allow us to co-design a highly customized computing unit for an FPGA. We implement the computing units for DiracDeltaNet on an Ultra96 SoC system through high-level synthesis. Our accelerator's final top-5 accuracy of 88.2% on ImageNet, is higher than all the previously reported embedded FPGA accelerators. In addition, the accelerator reaches an inference speed of 96.5 FPS on the ImageNet classification task, surpassing prior works with similar accuracy by at least 16.9\texttimes{}.","computer vision problems, neural networks, hardware accelerators, imagenet classification, fpga, embedded hardware, hardware-software codesign",23–32,10,Proceedings of the 2019 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays
561,@inproceedings: 10.1145/3289602.3293898,Efficient and Effective Sparse LSTM on FPGA with Bank-Balanced Sparsity,"Cao, Shijie and Zhang, Chen and Yao, Zhuliang and Xiao, Wencong and Nie, Lanshun and Zhan, Dechen and Liu, Yunxin and Wu, Ming and Zhang, Lintao",2019,Association for Computing Machinery,,https://doi.org/10.1145/3289602.3293898,10.1145/3289602.3293898,"Neural networks based on Long Short-Term Memory (LSTM) are widely deployed in latency-sensitive language and speech applications. To speed up LSTM inference, previous research proposes weight pruning techniques to reduce computational cost. Unfortunately, irregular computation and memory accesses in unrestricted sparse LSTM limit the realizable parallelism, especially when implemented on FPGA. To address this issue, some researchers propose block-based sparsity patterns to increase the regularity of sparse weight matrices, but these approaches suffer from deteriorated prediction accuracy. This work presents Bank-Balanced Sparsity (BBS), a novel sparsity pattern that can maintain model accuracy at a high sparsity level while still enable an efficient FPGA implementation. BBS partitions each weight matrix row into banks for parallel computing, while adopts fine-grained pruning inside each bank to maintain model accuracy. We develop a 3-step software-hardware co-optimization approach to apply BBS in real FPGA hardware. First, we propose a bank-balanced pruning method to induce the BBS pattern on weight matrices. Then we introduce a decoding-free sparse matrix format, Compressed Sparse Banks (CSB), that transparently exposes inter-bank parallelism in BBS to hardware. Finally, we design an FPGA accelerator that takes advantage of BBS to eliminate irregular computation and memory accesses. Implemented on Intel Arria-10 FPGA, the BBS accelerator can achieve 750.9 GOPs on sparse LSTM networks with a batch size of 1. Compared to state-of-the-art FPGA accelerators for LSTM with different compression techniques, the BBS accelerator achieves 2.3 ~ 3.7x improvement on energy efficiency and 7.0 ~ 34.4x reduction on latency with negligible loss of model accuracy.","bank-balanced sparsity, lstm, inference, weight pruning, deep neural networks, fpga",63–72,10,Proceedings of the 2019 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays
562,@inproceedings: 10.1145/3289602.3293910,HeteroCL: A Multi-Paradigm Programming Infrastructure for Software-Defined Reconfigurable Computing,"Lai, Yi-Hsiang and Chi, Yuze and Hu, Yuwei and Wang, Jie and Yu, Cody Hao and Zhou, Yuan and Cong, Jason and Zhang, Zhiru",2019,Association for Computing Machinery,,https://doi.org/10.1145/3289602.3293910,10.1145/3289602.3293910,"With the pursuit of improving compute performance under strict power constraints, there is an increasing need for deploying applications to heterogeneous hardware architectures with accelerators, such as GPUs and FPGAs. However, although these heterogeneous computing platforms are becoming widely available, they are very difficult to program especially with FPGAs. As a result, the use of such platforms has been limited to a small subset of programmers with specialized hardware knowledge. To tackle this challenge, we introduce HeteroCL, a programming infrastructure composed of a Python-based domain-specific language (DSL) and an FPGA-targeted compilation flow. The HeteroCL DSL provides a clean programming abstraction that decouples algorithm specification from three important types of hardware customization in compute, data types, and memory architectures. HeteroCL further captures the interdependence among these different customization techniques, allowing programmers to explore various performance/area/accuracy trade-offs in a systematic and productive manner. In addition, our framework produces highly efficient hardware implementations for a variety of popular workloads by targeting spatial architecture templates such as systolic arrays and stencil with dataflow architectures. Experimental results show that HeteroCL allows programmers to explore the design space efficiently in both performance and accuracy by combining different types of hardware customization and targeting spatial architectures, while keeping the algorithm code intact.","fpga, compiler, stencil, python, hardware accelerator, domain-specific language, high-level synthesis, reconfigurable computing, systolic array, spatial architecture, multi-paradigm programming",242–251,10,Proceedings of the 2019 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays
563,@inproceedings: 10.1145/3289602.3293915,Cloud-DNN: An Open Framework for Mapping DNN Models to Cloud FPGAs,"Chen, Yao and He, Jiong and Zhang, Xiaofan and Hao, Cong and Chen, Deming",2019,Association for Computing Machinery,,https://doi.org/10.1145/3289602.3293915,10.1145/3289602.3293915,"The efficacy and effectiveness of Convolutional Neural Networks (CNNs) have been proven in a wide range of machine learning applications. However, the high computational complexity of CNNs presents a critical challenge towards their broader adoption in real-time and power-efficient scenarios. FPGAs are poised to take a significant role for high-performance and energy-efficient computation of CNNs for both mobile (e.g., UAVs, self-driving cars, and IoT devices) and cloud computing domains. However, implementing an effective CNN system onto FPGAs efficiently remains problematic. The current cloud-based FPGAs with unique design constraints and architectural characteristics further increase the challenges. To address these challenges, we propose a novel open-source automated tool chain called Cloud-DNN. Our tool chain takes trained CNN models specified in Caffe as input, performs a set of transformations, and maps the model to a cloud-based FPGA. Cloud-DNN can significantly improve the overall design productivity of CNNs on FPGAs while satisfying the emergent computational requirements. Our design provides an alternative solution compared to other cloud-based options (e.g., GPUs or TPUs) while offering flexible, and high performance DNN inferences. The unique features of Cloud-DNN include the optimizations with cloud-platform characteristics and the support of easier and streamlined implementation. Experimental results demonstrate up to 104.55x performance improvement when compared to CPU implementation and comparable usability, flexibility, and strong quality compared to other state-of-the-art DNN inference implementations on standalone FPGAs.","cloud computing, dnn accelerator, high-level synthesis, neural network, reconfiguration, fpga",73–82,10,Proceedings of the 2019 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays
564,@inproceedings: 10.1145/3316615.3316675,Enhancing Simplified General Perturbations-4 Model for Orbit Propagation Using Deep Learning: A Review,"Salleh, Nor'asnilawati and Yuhaniz, Siti Sophiayati and Azmi, Nurulhuda Firdaus Mohd. and Sabri, Sharizal Fadlie",2019,Association for Computing Machinery,,https://doi.org/10.1145/3316615.3316675,10.1145/3316615.3316675,"This paper studies the method used in orbit propagation in order to enhance the Simplified General Perturbations-4 (SGP4) model which is the common orbit propagation model used by the satellite operator. The orbit propagation is used to determine and predict the position and velocity of a satellite. The capability of making an accurate orbital prediction is important to ensure satellite operation planning will not be disrupted and prevent any disrupted collisions or disasters. However, the accuracy of the SGP4 model is decreased once the propagation horizon increased. Therefore, a study is done to identify a technique that can be applied to enhance the SGP4 model. The model needs to be improved in term of minimizing the error and increase the accuracy even though the propagation span is increased. The method used in this study is by comparing the techniques that have been used by other researchers for the orbit propagation model. From the review that has been done, a deep learning technique is found to be a suitable technique. It also produces an accurate model for time series data. The new framework of the SGP4 model is expected and able to become a reliable predictor model. In the future, the study will further analyze by using simulation tools and real-time data. The accuracy and effectiveness of the improved model will be evaluated and the results will be compared with actual observations.","SGP4, Orbit Propagation, Deep Learning",27–32,6,Proceedings of the 2019 8th International Conference on Software and Computer Applications
565,@inproceedings: 10.1145/3316615.3316722,Cooperative Hierarchical Framework for Group Activity Recognition: From Group Detection to Multi-Activity Recognition,"Al-Habib, Mohammed and Huang, Dongjun and Al-Qatf, Majjed and Al-Sabahi, Kamal",2019,Association for Computing Machinery,,https://doi.org/10.1145/3316615.3316722,10.1145/3316615.3316722,"Deep neural network algorithms have shown promising performance for many tasks in computer vision field. Several neural network-based methods have been proposed to recognize group activities from video sequences. However, there are still several challenges that are related to multiple groups with different activities within a scene. The strong correlation that exists among individual motion, groups and activities can be utilized to detect groups and recognize their concurrent activities. Motivated by these observations, we propose a unified deep learning framework for detecting multiple groups and recognizing their corresponding collective activity based on Long Short-Term Memory (LSTM) network. In this framework, we use a pre-trained convolutional neural network (CNN) to extract features from the frames and appearances of persons. An objective function has been proposed to learn the amount of pairwise interaction between persons. The obtained individual features are passed to a clustering algorithm to detect groups in the scene. Then, an LSTM based model is used to recognize group activities. Together with this, a scene level CNN followed by LSTM is used to extract and learn scene level feature. Finally, the activities from the group level and the scene context level are integrated to infer the collective activity. The proposed method is evaluated on the benchmark collective activity dataset and compared with several baselines. The experimental results show its competitive performance for the collective activity recognition task.","Clustering Algorithm, CNN, LSTM, Group Activity Recognition",291–298,8,Proceedings of the 2019 8th International Conference on Software and Computer Applications
566,@inproceedings: 10.1145/3316615.3316712,A Review of Convolutional Neural Networks in Remote Sensing Image,"Liu, Xinni and Han, Fengrong and Ghazali, Kamarul Hawari and Mohamed, Izzeldin Ibrahim and Zhao, Yue",2019,Association for Computing Machinery,,https://doi.org/10.1145/3316615.3316712,10.1145/3316615.3316712,"Effectively analysis of remote-sensing images is very important in many practical applications, such as urban planning, geospatial object detection, military monitoring, vegetation mapping and precision agriculture. Recently, convolutional neural network based deep learning algorithm has achieved a series of breakthrough research results in the fields of objective detection, image semantic segmentation and image classification, etc. Their powerful feature learning capabilities have attracted more attention and have important research value. In this article, firstly we have summarized the basic structure and several classical convolutional neural network architectures. Secondly, the recent research problems on convolutional neural network are discussed. Later, we summarized the latest research results in convolutional neural network based remote sensing fields. Finally, the conclusion has made on the basis of current issue on convolutional neural networks and the future development direction.","Convolutional neural network, remote-sensing images, deep learning",263–267,5,Proceedings of the 2019 8th International Conference on Software and Computer Applications
567,@inproceedings: 10.1145/3316615.3316638,Combination of Facial Recognition and Interaction with Academic Portal in Automatic Attendance System,"Son, Ngo Tung and Chi, Le Phuong and Lam, Phan Truong and Van Dinh, Tran",2019,Association for Computing Machinery,,https://doi.org/10.1145/3316615.3316638,10.1145/3316615.3316638,"The computer vision has been advancing rapidly recently and it has become necessary to use the technology in the education area. As a part of it, the application of facial recognition technology plays an important role in building the intelligent education system, where teaching and learning can be measured to support quality improvement. The required-attendance taking activity at the schools is one of the important parts of the teaching process. The traditional attendance method causes several inconveniences such as wasting time and effort, easy to mistake and difficult to verify. In this paper, we introduce an automatic attendance system based on the combination of facial recognition technology and interaction with the existing academic portal. We conducted a review of several modern methods to select the most suitable open framework for individual tasks. Our proposed design is flexible and be able to apply to the large-scale set of students without compromising predictive accuracy. We have tested the system on more than 2200 freshmen at FPT University in Hanoi, Vietnam. Initial numerical results show the effectiveness in both aspects of the accuracy and performance of the prototype.","Facial Recognition, Face Detection, Academic Portal, Automatic Attendance System, Features Extraction, Classification",299–305,7,Proceedings of the 2019 8th International Conference on Software and Computer Applications
568,@inproceedings: 10.1145/3294032.3294079,Method Name Suggestion with Hierarchical Attention Networks,"Xu, Sihan and Zhang, Sen and Wang, Weijing and Cao, Xinya and Guo, Chenkai and Xu, Jing",2019,Association for Computing Machinery,,https://doi.org/10.1145/3294032.3294079,10.1145/3294032.3294079,"Method Rename has been a widely used refactoring operation that improves program comprehension and maintenance. Descriptive method names that summarize functionalities of source code can facilitate program comprehension. Much research has been done to suggest method names through source code summarization. However, unlike natural language, a code snippet consists of basic blocks organized by complicated structures. In this work, we observe a hierarchical structure --- tokens form basic blocks and basic blocks form a code snippet. Based on this observation, we exploit a hierarchical attention network to learn the representation of methods. Specifically, we apply two-level attention mechanism to learn the importance of each token in a basic block and that of a basic block in a method respectively. We evaluated our approach on 10 open source repositories and compared it against three state-of-the-art approaches. The results on these open-source data show the superiority of our hierarchical attention networks in terms of effectiveness.","Refactoring, Rename Method, Program Comprehension",10–21,12,Proceedings of the 2019 ACM SIGPLAN Workshop on Partial Evaluation and Program Manipulation
569,@inproceedings: 10.1145/3305160.3305194,An Overview of Learning Algorithms and Inference Techniques on Restricted Boltzmann Machines (RBMs),"Merindasari, Esti and Widyanto, M. Rahmat and Basaruddin, T.",2019,Association for Computing Machinery,,https://doi.org/10.1145/3305160.3305194,10.1145/3305160.3305194,"Restricted Boltzmann Machines (RBMs) is one of machine learning's methods which within past decades, the development of RBMs has quite increase. Researches of RBMs focused on theories and applications of RBMs. The application of RBMs has proofed that RBMs good at finishing many tasks, such as feature extraction method, document modeling, representation learning, classification and others. The RBMs' theories also have great movements, such as the development of the learning algorithm and inference techniques of RBMs. The key factors making the RBM success on finishing task are the learning algorithm and inference techniques. They motivated the development of inference techniques which successfully improved the deep neural network (DNN) performance. The aim of this research is reviewing the various types of RBMs as the application side, and the development of learning algorithm and inference techniques as theoretical side. Hopefully, it could motivate more development on the RBMs in order to contribute on overcoming implementation tasks especially on image processing tasks.","inference, learning algorithm, Restricted Boltzmann Machine",16–20,5,Proceedings of the 2nd International Conference on Software Engineering and Information Management
570,@inproceedings: 10.1145/3305160.3305216,Heterogeneous Data Integration Using Confidence Estimation of Unseen Visual Data for Zero-Shot Learning,"Seo, Sanghyun and Kim, Juntae",2019,Association for Computing Machinery,,https://doi.org/10.1145/3305160.3305216,10.1145/3305160.3305216,"Zero-shot learning is a learning methodology that can be used to recognize concepts that have never been seen during the training phase. Recently, interest in zero-shot learning has been increased by embedding multi-modal data into common vector space through heterogeneous data integration methodology. However, since the existing methodologies compare heterogeneous data focusing on the similarity between each vector, the performance of zero-shot learning decreases when the number of semantic candidates increases. We propose a heterogeneous data integration methodology using a confidence estimator for unseen visual data which estimates that whether input data is unseen data or not and output confidence measure. The proposed methodology constructs a more efficient zero-shot learning model by applying estimated confidence of input unseen visual data to the visual-semantic distance obtained from heterogeneous data integration model. Experiments have shown that the proposed methodology can improve zero-shot learning performance for unseen data despite a small performance decrease in the seen data.","Zero-shot Learning, Heterogeneous Data Integration, Visual Semantic Embedding, Confidence Estimation, Cross Modal Retrieval",171–174,4,Proceedings of the 2nd International Conference on Software Engineering and Information Management
571,@inproceedings: 10.1145/3305160.3305205,Person Re-Identification through Clustering and Partial Label Smoothing Regularization,"Ainam, Jean-Paul and Qin, Ke and Liu, Guisong and Luo, Guangchun",2019,Association for Computing Machinery,,https://doi.org/10.1145/3305160.3305205,10.1145/3305160.3305205,"In this paper, we propose a new label smoothing regularization scheme for person re-identification. We first use an unsupervised method for discriminative learning representation. We apply a clustering algorithm on the learned feature to partition the training set into k groups of equal variance and derive a shared space for similar images. Secondly, a GAN model is fed with each cluster to produce samples with relatively similar features to the original space. Our method consists of assigning an adaptive smooth label distribution to each generated sample according to their original cluster. To train our model, we define a new objective function which takes into account the generated samples and fine-tuned a CNN baseline using the objective function. Our model learns to exploit the samples generated by the GAN model to boost the performance of the person re-id by improving generalization. Extensive evaluations were conducted on four large-scale datasets to validate the advantage of the proposed model.","Smooth Label, Unsupervised Learning, Pedestrian Retrieval, Person ReID, GAN",189–193,5,Proceedings of the 2nd International Conference on Software Engineering and Information Management
572,@article: 10.1145/3293454,An Active Learning Approach for Improving the Accuracy of Automated Domain Model Extraction,"Arora, Chetan and Sabetzadeh, Mehrdad and Nejati, Shiva and Briand, Lionel",2019,Association for Computing Machinery,1049-331X,https://doi.org/10.1145/3293454,10.1145/3293454,"Domain models are a useful vehicle for making the interpretation and elaboration of natural-language requirements more precise. Advances in natural-language processing (NLP) have made it possible to automatically extract from requirements most of the information that is relevant to domain model construction. However, alongside the relevant information, NLP extracts from requirements a significant amount of information that is superfluous (not relevant to the domain model). Our objective in this article is to develop automated assistance for filtering the superfluous information extracted by NLP during domain model extraction. To this end, we devise an active-learning-based approach that iteratively learns from analysts’ feedback over the relevance and superfluousness of the extracted domain model elements and uses this feedback to provide recommendations for filtering superfluous elements. We empirically evaluate our approach over three industrial case studies. Our results indicate that, once trained, our approach automatically detects an average of ≈ 45% of the superfluous elements with a precision of ≈ 96%. Since precision is very high, the automatic recommendations made by our approach are trustworthy. Consequently, analysts can dispose of a considerable fraction – nearly half – of the superfluous elements with minimal manual work. The results are particularly promising, as they should be considered in light of the non-negligible subjectivity that is inherently tied to the notion of relevance.","active learning, domain modeling, Requirements engineering, case study research, natural-language requirement",,34,ACM Trans. Softw. Eng. Methodol.
573,@inproceedings: 10.1109/ASE.2019.00062,Machine Learning Based Recommendation of Method Names: How Far Are We,"Jiang, Lin and Liu, Hui and Jiang, He",2019,IEEE Press,,https://doi.org/10.1109/ASE.2019.00062,10.1109/ASE.2019.00062,"High quality method names are critical for the readability and maintainability of programs. However, constructing concise and consistent method names is often challenging, especially for inexperienced developers. To this end, advanced machine learning techniques have been recently leveraged to recommend method names automatically for given method bodies/implementation. Recent large-scale evaluations also suggest that such approaches are accurate. However, little is known about where and why such approaches work or don't work. To figure out the state of the art as well as the rationale for the success/failure, in this paper we conduct an empirical study on the state-of-the-art approach code2vec. We assess code2vec on a new dataset with more realistic settings. Our evaluation results suggest that although switching to new dataset does not significantly influence the performance, more realistic settings do significantly reduce the performance of code2vec. Further analysis on the successfully recommended method names also reveals the following findings: 1) around half (48.3%) of the accepted recommendations are made on getter/setter methods; 2) a large portion (19.2%) of the successfully recommended method names could be copied from the given bodies. To further validate its usefulness, we ask developers to manually score the difficulty in naming methods they developed. Code2vec is then applied to such manually scored methods to evaluate how often it works in need. Our evaluation results suggest that code2vec rarely works when it is really needed. Finally, to intuitively reveal the state of the art and to investigate the possibility of designing simple and straightforward alternative approaches, we propose a heuristics based approach to recommending method names. Evaluation results on large-scale dataset suggest that this simple heuristics-based approach significantly outperforms the state-of-the-art machine learning based approach, improving precision and recall by 65.25% and 22.45%, respectively. The comparison suggests that machine learning based recommendation of method names may still have a long way to go.","machine learning, code recommendation",602–614,13,Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering
574,@inproceedings: 10.1109/ASE.2019.00043,Apricot: A Weight-Adaptation Approach to Fixing Deep Learning Models,"Zhang, Hao and Chan, W. K.",2019,IEEE Press,,https://doi.org/10.1109/ASE.2019.00043,10.1109/ASE.2019.00043,"A deep learning (DL) model is inherently imprecise. To address this problem, existing techniques retrain a DL model over a larger training dataset or with the help of fault injected models or using the insight of failing test cases in a DL model. In this paper, we present Apricot, a novel weight-adaptation approach to fixing DL models iteratively. Our key observation is that if the deep learning architecture of a DL model is trained over many different subsets of the original training dataset, the weights in the resultant reduced DL model (rDLM) can provide insights on the adjustment direction and magnitude of the weights in the original DL model to handle the test cases that the original DL model misclassifies. Apricot generates a set of such reduced DL models from the original DL model. In each iteration, for each failing test case experienced by the input DL model (iDLM), Apricot adjusts each weight of this iDLM toward the average weight of these rDLMs correctly classifying the test case and/or away from that of these rDLMs misclassifying the same test case, followed by training the weight-adjusted iDLM over the original training dataset to generate a new iDLM for the next iteration. The experiment using five state-of-the-art DL models shows that Apricot can increase the test accuracy of these models by 0.87%-1.55% with an average of 1.08%. The experiment also reveals the complementary nature of these rDLMs in Apricot.","model fixing, optimization, model evolution, deep neural networks, debugging, model repair",376–387,12,Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering
575,@inproceedings: 10.1109/ASE.2019.00090,RENN: Efficient Reverse Execution with Neural-Network-Assisted Alias Analysis,"Mu, Dongliang and Guo, Wenbo and Cuevas, Alejandro and Chen, Yueqi and Gai, Jinxuan and Xing, Xinyu and Mao, Bing and Song, Chengyu",2019,IEEE Press,,https://doi.org/10.1109/ASE.2019.00090,10.1109/ASE.2019.00090,"Reverse execution and coredump analysis have long been used to diagnose the root cause of software crashes. Each of these techniques, however, face inherent challenges, such as insufficient capability when handling memory aliases. Recent works have used hypothesis testing to address this drawback, albeit with high computational complexity, making them impractical for real world applications. To address this issue, we propose a new deep neural architecture, which could significantly improve memory alias resolution. At the high level, our approach employs a recurrent neural network (RNN) to learn the binary code pattern pertaining to memory accesses. It then infers the memory region accessed by memory references. Since memory references to different regions naturally indicate a non-alias relationship, our neural architecture can greatly reduce the burden of doing hypothesis testing to track down non-alias relation in binary code.Different from previous researches that have utilized deep learning for other binary analysis tasks, the neural network proposed in this work is fundamentally novel. Instead of simply using off-the-shelf neural networks, we designed a new recurrent neural architecture that could capture the data dependency between machine code segments.To demonstrate the utility of our deep neural architecture, we implement it as RENN, a neural network-assisted reverse execution system. We utilize this tool to analyze software crashes corresponding to 40 memory corruption vulnerabilities from the real world. Our experiments show that RENN can significantly improve the efficiency of locating the root cause for the crashes. Compared to a state-of-the-art technique, RENN has 36.25% faster execution time on average, detects an average of 21.35% more non-alias pairs, and successfully identified the root cause of 12.5% more cases.","deep learning, memory alias, reverse execution",924–935,12,Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering
576,@inproceedings: 10.1109/ASE.2019.00148,Automatic Generation of Graphical User Interface Prototypes from Unrestricted Natural Language Requirements,"Kolthoff, Kristian",2019,IEEE Press,,https://doi.org/10.1109/ASE.2019.00148,10.1109/ASE.2019.00148,"High-fidelity GUI prototyping provides a meaningful manner for illustrating the developers' understanding of the requirements formulated by the customer and can be used for productive discussions and clarification of requirements and expectations. However, high-fidelity prototypes are time-consuming and expensive to develop. Furthermore, the interpretation of requirements expressed in informal natural language is often error-prone due to ambiguities and misunderstandings. In this dissertation project, we will develop a methodology based on Natural Language Processing (NLP) for supporting GUI prototyping by automatically translating Natural Language Requirements (NLR) into a formal Domain-Specific Language (DSL) describing the GUI and its navigational schema. The generated DSL can be further translated into corresponding target platform prototypes and directly provided to the user for inspection. Most related systems stop after generating artifacts, however, we introduce an intelligent and automatic interaction mechanism that allows users to provide natural language feedback on generated prototypes in an iterative fashion, which accordingly will be translated into respective prototype changes.","graphical user interface prototyping, intelligent interaction for automatic GUI Prototyping, automatic GUI generation, processing natural language requirements",1234–1237,4,Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering
577,@inproceedings: 10.1109/ASE.2019.00025,Automating App Review Response Generation,"Gao, Cuiyun and Zeng, Jichuan and Xia, Xin and Lo, David and Lyu, Michael R. and King, Irwin",2019,IEEE Press,,https://doi.org/10.1109/ASE.2019.00025,10.1109/ASE.2019.00025,"Previous studies showed that replying to a user review usually has a positive effect on the rating that is given by the user to the app. For example, Hassan et al. found that responding to a review increases the chances of a user updating their given rating by up to six times compared to not responding. To alleviate the labor burden in replying to the bulk of user reviews, developers usually adopt a template-based strategy where the templates can express appreciation for using the app or mention the company email address for users to follow up. However, reading a large number of user reviews every day is not an easy task for developers. Thus, there is a need for more automation to help developers respond to user reviews.Addressing the aforementioned need, in this work we propose a novel approach RRGen that automatically generates review responses by learning knowledge relations between reviews and their responses. RRGen explicitly incorporates review attributes, such as user rating and review length, and learns the relations between reviews and corresponding responses in a supervised way from the available training data. Experiments on 58 apps and 309,246 review-response pairs highlight that RRGen outperforms the baselines by at least 67.4% in terms of BLEU-4 (an accuracy measure that is widely used to evaluate dialogue response generation systems). Qualitative analysis also confirms the effectiveness of RRGen in generating relevant and accurate responses.","app reviews, response generation, neural machine translation",163–175,13,Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering
578,@inproceedings: 10.1109/ASE.2019.00051,ACTGAN: Automatic Configuration Tuning for Software Systems with Generative Adversarial Networks,"Bao, Liang and Liu, Xin and Wang, Fangzheng and Fang, Baoyin",2019,IEEE Press,,https://doi.org/10.1109/ASE.2019.00051,10.1109/ASE.2019.00051,"Complex software systems often provide a large number of parameters so that users can configure them for their specific application scenarios. However, configuration tuning requires a deep understanding of the software system, far beyond the abilities of typical system users. To address this issue, many existing approaches focus on exploring and learning good performance estimation models. The accuracy of such models often suffers when the number of available samples is small, a thorny challenge under a given tuning-time constraint. By contrast, we hypothesize that good configurations often share certain hidden structures. Therefore, instead of trying to improve the performance estimation of a given configuration, we focus on capturing the hidden structures of good configurations and utilizing such learned structure to generate potentially better configurations. We propose ACTGAN to achieve this goal. We have implemented and evaluated ACTGAN using 17 workloads with eight different software systems. Experimental results show that ACTGAN outperforms default configurations by 76.22% on average, and six state-of-the-art configuration tuning algorithms by 6.58%-64.56%. Furthermore, the ACTGAN-generated configurations are often better than those used in training and show certain features consisting with domain knowledge, both of which supports our hypothesis.","generative adversarial networks, software system, automatic configuration tuning",465–476,12,Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering
579,@inproceedings: 10.1109/ASE.2019.00024,"Discovering, Explaining and Summarizing Controversial Discussions in Community Q&amp;A Sites","Ren, Xiaoxue and Xing, Zhenchang and Xia, Xin and Li, Guoqiang and Sun, Jianling",2019,IEEE Press,,https://doi.org/10.1109/ASE.2019.00024,10.1109/ASE.2019.00024,"Developers often look for solutions to programming problems in community Q&amp;A sites like Stack Overflow. Due to the crowdsourcing nature of these Q&amp;A sites, many user-provided answers are wrong, less optimal or out-of-date. Relying on community-curated quality indicators (e.g., accepted answer, answer vote) cannot reliably identify these answer problems. Such problematic answers are often criticized by other users. However, these critiques are not readily discoverable when reading the posts. In this paper, we consider the answers being criticized and their critique posts as controversial discussions in community Q&amp;A sites. To help developers notice such controversial discussions and make more informed choices of appropriate solutions, we design an automatic open information extraction approach for systematically discovering and summarizing the controversies in Stack Overflow and exploiting official API documentation to assist the understanding of the discovered controversies. We apply our approach to millions of java/android-tagged Stack overflow questions and answers and discover a large scale of controversial discussions in Stack Overflow. Our manual evaluation confirms that the extracted controversy information is of high accuracy. A user study with 18 developers demonstrates the usefulness of our generated controversy summaries in helping developers avoid the controversial answers and choose more appropriate solutions to programming questions.","stack overflow, open information extraction, sentence embedding, controversial discussion",151–162,12,Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering
580,@inproceedings: 10.1109/ASE.2019.00079,Property Inference for Deep Neural Networks,"Gopinath, Divya and Converse, Hayes and P\u{a}s\u{a}reanu, Corina S. and Taly, Ankur",2019,IEEE Press,,https://doi.org/10.1109/ASE.2019.00079,10.1109/ASE.2019.00079,"We present techniques for automatically inferring formal properties of feed-forward neural networks. We observe that a significant part (if not all) of the logic of feed forward networks is captured in the activation status (on or off) of its neurons. We propose to extract patterns based on neuron decisions as preconditions that imply certain desirable output property e.g., the prediction being a certain class. We present techniques to extract input properties, encoding convex predicates on the input space that imply given output properties and layer properties, representing network properties captured in the hidden layers that imply the desired output behavior. We apply our techniques on networks for the MNIST and ACASXU applications. Our experiments highlight the use of the inferred properties in a variety of tasks, such as explaining predictions, providing robustness guarantees, simplifying proofs, and network distillation.",,797–809,13,Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering
581,@inproceedings: 10.5555/3370272.3370282,Classification of Histopathological Biopsy Images Using Ensemble of Deep Learning Networks,"Kassani, Sara Hosseinzadeh and Kassani, Peyman Hosseinzadeh and Wesolowski, Michal J. and Schneider, Kevin A. and Deters, Ralph",2019,IBM Corp.,,,,"Breast cancer is one of the leading causes of death across the world in women. Early diagnosis of this type of cancer is critical for treatment and patient care. Computer-aided detection (CAD) systems using convolutional neural networks (CNN) could assist in the classification of abnormalities. In this study, we proposed an ensemble deep learning-based approach for automatic binary classification of breast histology images. The proposed ensemble model adapts three pre-trained CNNs, namely VGG19, MobileNet, and DenseNet. The ensemble model is used for the feature representation and extraction steps. The extracted features are then fed into a multi-layer perceptron classifier to carry out the classification task. Various preprocessing and CNN tuning techniques such as stain-normalization, data augmentation, hyperparameter tuning, and fine-tuning are used to train the model. The proposed method is validated on four publicly available benchmark datasets, i.e., ICIAR, BreakHis, PatchCamelyon, and Bioimaging. The proposed multi-model ensemble method obtains better predictions than single classifiers and machine learning algorithms with accuracies of 98.13%, 95.00%, 94.64% and 83.10% for BreakHis, ICIAR, PatchCamelyon and Bioimaging datasets, respectively.","deep learning, computer-aided diagnosis, multimodel ensemble, transfer learning, feature extraction",92–99,8,Proceedings of the 29th Annual International Conference on Computer Science and Software Engineering
582,@inproceedings: 10.5555/3370272.3370324,Custom Visual Recognition Model with Watson Studio,"Ahmed, Imtihan and House, Rachael and Deilma, Neil and Luo, Li",2019,IBM Corp.,,,,"The explosive growth of cameras, image sensors, and computer vision as a discipline of Artificial Intelligence (AI) has garnered strong interest from researchers, developers, businesses and consumers. Image classification refers to a process to classify an image according to a model and match it to a set of classes or categories. Object detection is similar to image classification, which is a process to classify, locate and count multiple objects in an image and their respective locations within the image. Object tracking involves using object detection in each frame of a video to track the desired object through a series of image frames or video [1]. There are a number of use cases for computer vision including face recognition for application or device security, automatically counting and classifying items on a production line, and monitoring and responding to traffic conditions on busy road sections.Computer vision seeks to understand information in digital images through processing and analyzing digital images. This understanding is achieved through extracting high dimension data from images and processing them to produce usable information. Practical applications of computer vision in the context of machine learning include classification, segmentation, and tracking [2].IBM Watson Studio (https://www.ibm.com/watson) is IBM's suite of enterprise-ready AI services, applications and tooling. As a service on IBM Cloud, IBM Watson Visual Recognition uses deep learning algorithms to analyze images for scenes, faces, and objects. This service provides built-in models and can also be used to create and train custom models for specific needs. Watson Studio provides a collaborative platform on top of IBM Cloud's cloud computing capabilities to use existing models or train and deploy new models with minimal coding. Watson Studio has the added capability of setting up custom environments and Notebooks, allowing quick, cloud-enabled development machines that can scale as your projects scale.IBM PowerAI Vision (https://www.ibm.com/caen/marketplace/ibm-powerai-vision) is a Graphics Processing Unit (GPU) accelerated visual recognition solution running on IBM Power Systems. PowerAI Vision (https://www.ibm.com/caen/marketplace/ibm-powerai-vision) puts data science in the hands of subject matter experts. This tool simplifies building machine learning models with IBM Power Systems. As a result, users can build models and deploy them to the web without coding. The models can be accessed through an Application Program Interface (API). On the other hand, users can call the API from their own applications with a few lines of code.IBM provides developers free, open source, state-of-the-art assets for deep learning through the Model Asset Exchange (MAX) (https://developer.ibm.com/exchanges/models/) on IBM Developer. In the repository developers can find both assets for training deep learning models and pre-trained models to use in their projects.The first half of this workshop will focus on exploring the Watson Visual Recognition and Watson Machine Learning Services in IBM Cloud. We will begin by building and deploying a model on Watson Visual Recognition. We will focus on the key benefits of the service, including the ability of anyone with minimal coding experience to be able to train and deploy a computer vision model to the cloud. We will then demonstrate how easy it can be to integrate the model in any web-enabled application through a demo web application. Once this has been completed, we will give a soft introduction to Watson Machine Learning, including how to choose development environments, setting up a Jupyter notebook (https://jupyter.org/), and go over some prepared code snippets to train and analyze a model fully on the cloud. [We will then demonstrate how we can export the model and use it in our applications.In the second half of the workshop, we will demonstrate detecting and labeling objects within an image using PowerAI Vision object detection (https://github.com/IBM/powerai-vision-object-detection), based on customized training. Instead of writing code to train, deploy, and test the new model, we will only need to upload the images, and label the objects in the provided application. Once the model is deployed, we will use the PowerAI Vision user interface (UI) to test it. We will also use our application as a Representational State Transfer (REST) client to locate and count objects in an image using the provided REST API endpoint. At the end of the workshop we will briefly introduce Model Asset Exchange, we will demonstrate how to find a visual recognition model on MAX, deploy it as a microservice and test it.In summary, we will introduce some visual recognition services provided by IBM in this workshop. We together will develop an image classification model using Watson Visual Recognition service with Watson Studio. We also consume a visual recognition service from a client side. Then we discuss the features of PowerAI Vision and demonstrate object detection in PowerAI Vision. Finally, we introduce Model Asset Exchange.","IBM cloud, visual recognition, artificial intelligence, deep learning, machine learning, watson studio, model asset exchange, powerai vision, IBM Watson visual recognition",376–377,2,Proceedings of the 29th Annual International Conference on Computer Science and Software Engineering
583,@inproceedings: 10.5555/3370272.3370331,A Hands-on Tutorial on Deep Learning for Object and Pattern Recognition,"Zulkernine, Farhana and Isah, Haruna and Mahfuz, Sazia and Gasmallah, Mohammed and Lam, Jason and Khan, Shahzad",2019,IBM Corp.,,,,"Deep learning is a machine learning technique that is inspired by human brain. It enables information processing in multiple hierarchical layers to understand representations and features from raw data. Deep learning architectures have been applied to various fields including Computer Vision, Object and Speech Recognition, Natural Language Processing (NLP), and the Internet of Things (IoT). Deep learning methods in recent years have shown excellent performance in terms of accuracy on many challenging and complex learning tasks. This workshop was designed to provide an introduction and a hands-on tutorial on deep learning to researchers and industry practitioners interested in integrating the power of deep learning in their research or business applications.","Google colaboratory, deep learning, IoT, Python, object detection, tensorflow, tutorial, NLP",386–387,2,Proceedings of the 29th Annual International Conference on Computer Science and Software Engineering
584,@inproceedings: 10.5555/3370272.3370275,"A Fog Computing Framework for Autonomous Driving Assist: Architecture, Experiments, and Challenges","Maheswaran, Muthucumaru and Yang, Tianzi and Memon, Salman",2019,IBM Corp.,,,,"Autonomous driving is expected to provide a range of far-reaching economic, environmental and safety benefits. In this study, we propose a fog computing based framework to assist autonomous driving. Our framework relies on overhead views from cameras and data streams from vehicle sensors to create a network of distributed digital twins, called an edge twin, on fog machines. The edge twin will be continuously updated with the locations of both autonomous and human-piloted vehicles on the road segments. The vehicle locations will be harvested from overhead cameras as well as location feeds from the vehicles themselves. Although the edge twin can make fair road space allocations from a global viewpoint, there is a communication cost (delay) in reaching it from the cameras and vehicular sensors. To address this, we introduce a machine learning forecaster as a part of the edge twin which is responsible for predicting the future location of vehicles. Lastly, we introduce a box algorithm that will use the forecasted values to create a hazard map for the road segment which would be used by the framework to suggest safe manoeuvres for the autonomous vehicles such as lane changes and accelerations. We present the complete fog computing framework for autonomous driving assist and evaluate key portions of the proposed framework using simulations based on a real-world dataset of vehicle position traces on a highway.","edge twin, fog computing, machine learning, autonomous driving",24–33,10,Proceedings of the 29th Annual International Conference on Computer Science and Software Engineering
585,@inproceedings: 10.5555/3370272.3370313,Dogfooding: Using IBM Cloud Services to Monitor IBM Cloud Infrastructure,"Pourmajidi, William and Miranskyy, Andriy and Steinbacher, John and Erwin, Tony and Godwin, David",2019,IBM Corp.,,,,"The stability and performance of Cloud platforms are essential as they directly impact customers' satisfaction. Cloud service providers use Cloud monitoring tools to ensure that rendered services match the quality of service requirements indicated in established contracts such as service-level agreements.Given the enormous number of resources that need to be monitored, highly scalable and capable monitoring tools are designed and implemented by Cloud service providers such as Amazon, Google, IBM, and Microsoft. Cloud monitoring tools monitor millions of virtual and physical resources and continuously generate logs for each one of them. Considering that logs magnify any technical issue, they can be used for disaster detection, prevention, and recovery. However, logs are useless if they are not assessed and analyzed promptly. Thus, we argue that the scale of Cloud-generated logs makes it impossible for DevOps teams to analyze them effectively. This implies that one needs to automate the process of monitoring and analysis (e.g., using machine learning and artificial intelligence). If the automation will witness an anomaly in the logs --- it will alert DevOps staff.The automatic anomaly detectors require a reliable and scalable platform for gathering, filtering, and transforming the logs, executing the detector models, and sending out the alerts to the DevOps staff. In this work, we report on implementing a prototype of such a platform based on the 7-layered architecture pattern, which leverages micro-service principles to distribute tasks among highly scalable, resources-efficient modules. The modules interact with each other via an instance of the Publish-Subscribe architectural pattern. The platform is deployed on the IBM Cloud service infrastructure and is used to detect anomalies in logs emitted by the IBM Cloud services, hence the dogfooding. In particular, we leverage IBM Cloud Functions to deploy the computing modules, IBM Event Streams to establish communication among the modules, and IBM Cloud Object Storage and IBM Cloudant for persistent storage.The prototype efficiency is promising: it takes the platform 17 seconds or less from the point of receiving a new log record to emitting an alert to the IBM Cloud DevOps team.",,344–353,10,Proceedings of the 29th Annual International Conference on Computer Science and Software Engineering
586,@inproceedings: 10.1145/3358499.3361220,Actor-Based Incremental Tree Data Processing for Large-Scale Machine Learning Applications,"Sakurai, Kouhei and Shimizu, Taiki",2019,Association for Computing Machinery,,https://doi.org/10.1145/3358499.3361220,10.1145/3358499.3361220,"A number of online machine learning techniques based on tree model have been studied in order to cope with today's requirements of quickly processing large scale data-sets. We present a design pattern for incremental tree data processing as gradually constructing on-demand tree-model on memory. Our approach adopts the actor model as making use of multi-cores and distributed computers without largely rewriting code for algorithms. The pattern basically defines a node in the tree as an actor which is the unit of asynchronous processes and each data instance flows between actor nodes as a message. We study concrete two machine learning algorithms, VFDT for decision tree's top-down growth and BIRCH for hierarchical clustering's bottom up growth. For supporting VFDT, we propose an extension mechanism of replicating root nodes so that it can address bottleneck as starting of inputs. For supporting BIRCH, we split processes of recursive construction into asynchronous steps with correcting target node by traversing extra horizontal links between sibling nodes. We carried out machine learning tasks with our implementation on top of Akka Java, and we confirmed reasonable performance for the tasks with large scale data-sets.","Decision Tree, Actor Model, Hierarchical Clustering, Incremental Tree Data Processing",1–10,10,"Proceedings of the 9th ACM SIGPLAN International Workshop on Programming Based on Actors, Agents, and Decentralized Control"
587,@inproceedings: 10.1145/3332165.3347936,Mallard: Turn the Web into a Contextualized Prototyping Environment for Machine Learning,"Zhang, Xiong and Guo, Philip J.",2019,Association for Computing Machinery,,https://doi.org/10.1145/3332165.3347936,10.1145/3332165.3347936,"Machine learning (ML) can be hard to master, but what first trips up novices is something much more mundane: the incidental complexities of installing and configuring software development environments. Everyone has a web browser, so can we let people experiment with ML within the context of any webpage they visit? This paper's contribution is the idea that the web can serve as a contextualized prototyping environment for ML by enabling analyses to occur within the context of data on actual webpages rather than in isolated silos. We realized this idea by building Mallard, a browser extension that scaffolds acquiring and parsing web data, prototyping with pretrained ML models, and augmenting webpages with ML-driven results and interactions. To demonstrate the versatility of Mallard, we performed a case study where we used it to prototype nine ML-based browser apps, including augmenting Amazon and Twitter websites with sentiment analysis, augmenting restaurant menu websites with OCR-based search, using real-time face tracking to control a Pac-Man game, and style transfer on Google image search results. These case studies show that Mallard is capable of supporting a diverse range of hobbyist-level ML prototyping projects.","ml prototyping, contextualized machine learning",605–618,14,Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
588,@inproceedings: 10.1145/3332165.3347927,LabelAR: A Spatial Guidance Interface for Fast Computer Vision Image Collection,"Laielli, Michael and Smith, James and Biamby, Giscard and Darrell, Trevor and Hartmann, Bjoern",2019,Association for Computing Machinery,,https://doi.org/10.1145/3332165.3347927,10.1145/3332165.3347927,"Computer vision is applied in an ever expanding range of applications, many of which require custom training data to perform well. We present a novel interface for rapid collection of labeled training images to improve CV-based object detectors. LabelAR leverages the spatial tracking capabilities of an AR-enabled camera, allowing users to place persistent bounding volumes that stay centered on real-world objects. The interface then guides the user to move the camera to cover a wide variety of viewpoints. We eliminate the need for post hoc labeling of images by automatically projecting 2D bounding boxes around objects in the images as they are captured from AR-marked viewpoints. In a user study with 12 participants, LabelAR significantly outperforms existing approaches in terms of the trade-off between detection performance and collection time.","augmented reality, spatial interfaces, computer vision, image collection",987–998,12,Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
589,@inproceedings: 10.1145/3332165.3347867,Opisthenar: Hand Poses and Finger Tapping Recognition by Observing Back of Hand Using Embedded Wrist Camera,"Yeo, Hui-Shyong and Wu, Erwin and Lee, Juyoung and Quigley, Aaron and Koike, Hideki",2019,Association for Computing Machinery,,https://doi.org/10.1145/3332165.3347867,10.1145/3332165.3347867,"We introduce a vision-based technique to recognize static hand poses and dynamic finger tapping gestures. Our approach employs a camera on the wrist, with a view of the opisthenar (back of the hand) area. We envisage such cameras being included in a wrist-worn device such as a smartwatch, fitness tracker or wristband. Indeed, selected off-the-shelf smartwatches now incorporate a built-in camera on the side for photography purposes. However, in this configuration, the fingers are occluded from the view of the camera. The oblique angle and placement of the camera make typical vision-based techniques difficult to adopt. Our alternative approach observes small movements and changes in the shape, tendons, skin and bones on the opisthenar area. We train deep neural networks to recognize both hand poses and dynamic finger tapping gestures. While this is a challenging configuration for sensing, we tested the recognition with a real-time user test and achieved a high recognition rate of 89.4% (static poses) and 67.5% (dynamic gestures). Our results further demonstrate that our approach can generalize across sessions and to new users. Namely, users can remove and replace the wrist-worn device while new users can employ a previously trained system, to a certain degree. We conclude by demonstrating three applications and suggest future avenues of work based on sensing the back of the hand.","finger tapping, back of the hand, opisthenar, hand pose",963–971,9,Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
590,@inproceedings: 10.1145/3332165.3347878,Sketchforme: Composing Sketched Scenes from Text Descriptions for Interactive Applications,"Huang, Forrest and Canny, John F.",2019,Association for Computing Machinery,,https://doi.org/10.1145/3332165.3347878,10.1145/3332165.3347878,"Sketching is an effective communication medium that augments and enhances what can be communicated in text. We introduce Sketchforme, the first neural-network-based system that can generate complex sketches based on text descriptions specified by users. Sketchforme's key contribution is to factor complex sketch rendering into layout and rendering subtasks using neural networks. The sketches composed by Sketchforme are expressive and realistic: we show in our user study that these sketches convey descriptions better than human-generated sketches in several cases, and 36.5% of those sketches were identified as human-generated. We develop some interactive applications using these generated sketches, and show that Sketchforme can significantly improve language learning applications and support intelligent language-based sketching assistants.","sketching, neural networks, deep learning, transformer, generative models, interactive machine learning, interactive applications, natural language",209–220,12,Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
591,@inproceedings: 10.1145/3332165.3347873,StateLens: A Reverse Engineering Solution for Making Existing Dynamic Touchscreens Accessible,"Guo, Anhong and Kong, Junhan and Rivera, Michael and Xu, Frank F. and Bigham, Jeffrey P.",2019,Association for Computing Machinery,,https://doi.org/10.1145/3332165.3347873,10.1145/3332165.3347873,"Blind people frequently encounter inaccessible dynamic touchscreens in their everyday lives that are difficult, frustrating, and often impossible to use independently. Touchscreens are often the only way to control everything from coffee machines and payment terminals, to subway ticket machines and in-flight entertainment systems. Interacting with dynamic touchscreens is difficult non-visually because the visual user interfaces change, interactions often occur over multiple different screens, and it is easy to accidentally trigger interface actions while exploring the screen. To solve these problems, we introduce StateLens - a three-part reverse engineering solution that makes existing dynamic touchscreens accessible. First, StateLens reverse engineers the underlying state diagrams of existing interfaces using point-of-view videos found online or taken by users using a hybrid crowd-computer vision pipeline. Second, using the state diagrams, StateLens automatically generates conversational agents to guide blind users through specifying the tasks that the interface can perform, allowing the StateLens iOS application to provide interactive guidance and feedback so that blind users can access the interface. Finally, a set of 3D-printed accessories enable blind people to explore capacitive touchscreens without the risk of triggering accidental touches on the interface. Our technical evaluation shows that StateLens can accurately reconstruct interfaces from stationary, hand-held, and web videos; and, a user study of the complete system demonstrates that StateLens successfully enables blind users to access otherwise inaccessible dynamic touchscreens.","accessibility, touchscreen appliances, reverse engineering, dynamic interfaces, conversational agents, crowdsourcing, computer vision",371–385,15,Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology
592,@inproceedings: 10.1145/3332167.3356880,Performance-Based Expressive Character Animation,"Aneja, Deepali",2019,Association for Computing Machinery,,https://doi.org/10.1145/3332167.3356880,10.1145/3332167.3356880,"For decades, animation has been a popular storytelling technique. Traditional tools for creating animations are labor-intensive requiring animators to painstakingly draw frames and motion curves by hand. An alternative workflow is to equip animators with direct real-time control over digital characters via performance, which offers a more immediate and efficient way to create animation. Even when using these existing expression transfer and lip sync methods, producing convincing facial animation in real-time is a challenging task. In this position paper, I describe my past and proposed future research in developing interactive systems for perceptually-valid expression retargeting from humans to stylized characters, real-time lip sync for 2D animation, and building an expressive style aligned embodied conversational agent.","deep learning, expression retargeting, lip sync, multi-modality, embodied conversational agents",166–169,4,The Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology
593,@inproceedings: 10.1145/3349567.3351728,EAST-DNN: Expediting Architectural SimulaTions Using Deep Neural Networks: Work-in-Progress,"Dutt, Arko and Narasimman, Govind and Jie, Lin and Chandrasekhar, Vijay Ramaseshan and Sabry, Mohamed M.",2019,Association for Computing Machinery,,https://doi.org/10.1145/3349567.3351728,10.1145/3349567.3351728,"A rapid and accurate architectural simulator is a cornerstone for an efficient design-space exploration of computing systems. In this paper, we introduce EAST-DNN, a feed-forward deep neural network, to accelerate architectural simulations. EAST-DNN achieves &gt; 106X speedup with an average prediction error of 4.3% over the baseline simulator. It also achieves an average of 2X better accuracy with at least 2.3X speedup compared to state-of-the-art.","deep neural network, hardware accelerators",,2,Proceedings of the International Conference on Hardware/Software Codesign and System Synthesis Companion
594,@inproceedings: 10.1145/3349567.3351721,BPNet: Branch-Pruned Conditional Neural Network for Systematic Time-Accuracy Tradeoff in DNN Inference: Work-in-Progress,"Park, Kyungchul and Yi, Youngmin",2019,Association for Computing Machinery,,https://doi.org/10.1145/3349567.3351721,10.1145/3349567.3351721,"Recently, there have been attempts to execute the neural network conditionally with auxiliary classifiers allowing early termination depending on the difficulty of the input, which can reduce the execution time or energy consumption without any or with negligible accuracy decrease. However, these studies do not consider how many or where the auxiliary classifiers, or branches, should be added in a systematic fashion. In this paper, we propose Branch-pruned Conditional Neural Network (BPNet) and its methodology in which the time-accuracy tradeoff for the conditional neural network can be found systematically. We applied BPNet to SqueezeNet, ResNet-20, and VGG-16 with CIFAR-10 and 100. BPNet achieves on average 2.0X of speedups without any accuracy drop on average compared to the base network.",,,2,Proceedings of the International Conference on Hardware/Software Codesign and System Synthesis Companion
595,@article: 10.1145/3360586,Generating Precise Error Specifications for C: A Zero Shot Learning Approach,"Wu, Baijun and Campora III, John Peter and He, Yi and Schlecht, Alexander and Chen, Sheng",2019,Association for Computing Machinery,,https://doi.org/10.1145/3360586,10.1145/3360586,"In C programs, error specifications, which specify the value range that each function returns to indicate failures, are widely used to check and propagate errors for the sake of reliability and security. Various kinds of C analyzers employ error specifications for different purposes, e.g., to detect error handling bugs, yet a general approach for generating precise specifications is still missing. This limits the applicability of those tools.  In this paper, we solve this problem by developing a machine learning-based approach named MLPEx. It generates error specifications by analyzing only the source code, and is thus general. We propose a novel machine learning paradigm based on transfer learning, enabling MLPEx to require only one-time minimal data labeling from us (as the tool developers) and zero manual labeling efforts from users. To improve the accuracy of generated error specifications, MLPEx extracts and exploits project-specific information. We evaluate MLPEx on 10 projects, including 6 libraries and 4 applications. An investigation of 3,443 functions and 17,750 paths reveals that MLPEx generates error specifications with a precision of 91% and a recall of 94%, significantly higher than those of state-of-the-art approaches. To further demonstrate the usefulness of the generated error specifications, we use them to detect 57 bugs in 5 tested projects.","machine learning, project-specific features, Error specification generatio",,30,Proc. ACM Program. Lang.
596,@article: 10.1145/3360612,ApproxHPVM: A Portable Compiler IR for Accuracy-Aware Optimizations,"Sharif, Hashim and Srivastava, Prakalp and Huzaifa, Muhammad and Kotsifakou, Maria and Joshi, Keyur and Sarita, Yasmin and Zhao, Nathan and Adve, Vikram S. and Misailovic, Sasa and Adve, Sarita",2019,Association for Computing Machinery,,https://doi.org/10.1145/3360612,10.1145/3360612,"We propose ApproxHPVM, a compiler IR and system designed to enable accuracy-aware performance and energy tuning on heterogeneous systems with multiple compute units and approximation methods. ApproxHPVM automatically translates end-to-end application-level quality metrics into accuracy requirements for individual operations. ApproxHPVM uses a hardware-agnostic accuracy-tuning phase to do this translation that provides greater portability across heterogeneous hardware platforms and enables future capabilities like accuracy-aware dynamic scheduling and design space exploration.  ApproxHPVM incorporates three main components: (a) a compiler IR with hardware-agnostic approximation metrics, (b) a hardware-agnostic accuracy-tuning phase to identify error-tolerant computations, and (c) an accuracy-aware hardware scheduler that maps error-tolerant computations to approximate hardware components. As ApproxHPVM does not incorporate any hardware-specific knowledge as part of the IR, it can serve as a portable virtual ISA that can be shipped to all kinds of hardware platforms.  We evaluate our framework on nine benchmarks from the deep learning domain and five image processing benchmarks. Our results show that our framework can offload chunks of approximable computations to special-purpose accelerators that provide significant gains in performance and energy, while staying within user-specified application-level quality metrics with high probability. Across the 14 benchmarks, we observe from 1-9x performance speedups and 1.1-11.3x energy reduction for very small reductions in accuracy.","Heterogeneous Systems, Approximate Computing, Deep Neural Networks, Compiler, Virtual IS",,30,Proc. ACM Program. Lang.
597,@article: 10.1145/3358205,Memory- and Communication-Aware Model Compression for Distributed Deep Learning Inference on IoT,"Bhardwaj, Kartikeya and Lin, Ching-Yi and Sartor, Anderson and Marculescu, Radu",2019,Association for Computing Machinery,1539-9087,https://doi.org/10.1145/3358205,10.1145/3358205,"Model compression has emerged as an important area of research for deploying deep learning models on Internet-of-Things (IoT). However, for extremely memory-constrained scenarios, even the compressed models cannot fit within the memory of a single device and, as a result, must be distributed across multiple devices. This leads to a distributed inference paradigm in which memory and communication costs represent a major bottleneck. Yet, existing model compression techniques are not communication-aware. Therefore, we propose Network of Neural Networks (NoNN), a new distributed IoT learning paradigm that compresses a large pretrained ‘teacher’ deep network into several disjoint and highly-compressed ‘student’ modules, without loss of accuracy. Moreover, we propose a network science-based knowledge partitioning algorithm for the teacher model, and then train individual students on the resulting disjoint partitions. Extensive experimentation on five image classification datasets, for user-defined memory/performance budgets, show that NoNN achieves higher accuracy than several baselines and similar accuracy as the teacher model, while using minimal communication among students. Finally, as a case study, we deploy the proposed model for CIFAR-10 dataset on edge devices and demonstrate significant improvements in memory footprint (up to 24\texttimes{}), performance (up to 12\texttimes{}), and energy per node (up to 14\texttimes{}) compared to the large teacher model. We further show that for distributed inference on multiple edge devices, our proposed NoNN model results in up to 33\texttimes{} reduction in total latency w.r.t. a state-of-the-art model compression baseline.","Network of neural networks, communities, model compressio",,22,ACM Trans. Embed. Comput. Syst.
598,@article: 10.1145/3358174,Aggressive Energy Reduction for Video Inference with Software-Only Strategies,"Gon\c{c}alves, Larissa Rozales and Moura, Rafael F\~{a}o De and Carro, Luigi",2019,Association for Computing Machinery,1539-9087,https://doi.org/10.1145/3358174,10.1145/3358174,"In the past years, several works have proposed custom hardware and software-based techniques for the acceleration of Convolutional Neural Networks (CNNs). Most of these works focus on saving computations by changing the used precision or modifying frame processing. To reach a more aggressive energy reduction, in this paper we propose software-only modifications to the CNNs inference process.Our approach exploits the inherent locality in videos by replacing entire frame computations with a movement prediction algorithm. Furthermore, when a frame must be processed, we avoid energy-demanding floating-point operations, and at the same time reduce memory accesses by employing look-up tables in place of the original convolutions.Using the proposed approach, one can reach significant energy gains of more than 25\texttimes{} for security cameras, and 12\texttimes{} for moving vehicles applications, with only small software modifications.","Convolutional Neural Networks, computation reuse, frame predicto",,20,ACM Trans. Embed. Comput. Syst.
599,@article: 10.1145/3358179,ECAx: Balancing Error Correction Costs in Approximate Accelerators,"Castro-God\'{\i}nez, Jorge and Shafique, Muhammad and Henkel, J\""{o}rg",2019,Association for Computing Machinery,1539-9087,https://doi.org/10.1145/3358179,10.1145/3358179,"Approximate computing has emerged as a design paradigm amenable to error-tolerant applications. It enables trading the quality of results for efficiency improvement in terms of delay, power, and energy consumption under user-provided tolerable quality degradation. Approximate accelerators have been proposed to expedite frequently executing code sections of error-resilient applications while meeting a defined quality level. However, these accelerators may produce unacceptable errors at run time if the input data changes or dynamic adjustments are made for a defined output quality constraint. State-of-the-art approaches in approximate computing address this issue by correctly re-computing those accelerator invocations that produce unacceptable errors; this is achieved by using the host processor or an alternate exact accelerator, which is activated on-demand. Nevertheless, such approaches can nullify the benefits of approximate computing, especially when input data variations are high at run time and errors due to approximations are above a tolerable threshold. As a robust and general solution to this problem, we propose ECAx, a novel methodology to explore low-overhead error correction in approximate accelerators by selectively correcting most significant errors, in terms of their magnitude, without losing the gains of approximations. We particularly consider the case of approximate accelerators built with approximate functional units such as approximate adders. Our novel methodology reduces the required exact re-computations on the host processor, achieving up to 20% performance gain compared to state-of-the-art approaches.",Approximate computin,,20,ACM Trans. Embed. Comput. Syst.
600,@inbook: 10.5555/3492252.3492264,Patterns for Text Classification (Part 1),"Weiss, Michael and Bathula, Swarupini and Muegge, Steven and Nazari, Ali",2019,The Hillside Group,,,,"This paper describes patterns for text classification. The patterns address problems related to creating machine learning models for a corpus of documents and describe common solutions to solve those problems. The target audience for these patterns includes developers who are familiar with basic machine learning algorithms, but do not have much experience with applying machine learning to text classification.",,,,Proceedings of the 26th Conference on Pattern Languages of Programs
601,@inproceedings: 10.1145/3350768.3350785,EmbSE: A Word Embeddings Model Oriented Towards Software Engineering Domain,"De Bortoli F\'{a}vero, Eliane Maria and Casanova, Dalcimar and Pimentel, Andrey Ricardo",2019,Association for Computing Machinery,,https://doi.org/10.1145/3350768.3350785,10.1145/3350768.3350785,"The representation of contexts is essential in tasks involving Natural Language Processing (NLP). In the field of software engineering, classifying similar texts within a specific context has been a complex task, considering the informality and the complexity inherent of the texts produced through many software development processes (e.g. agile methods). Word embeddings capture semantic and syntactic information about unique words, allowing them to be represented in a dense and low-dimensional format. This property makes the embeddings vectors an important input feature for machine learning algorithms that aim to classify texts. Although there has been much research around the application of word embeddings in several areas, up to this moment, there is no knowledge about studies that have explored its application in the creation of a specific model for the domain of the area of software engineering. Thus, this article presents the proposal to generate an embedding model, called embeddings model for software engineering (EmbSE), which can recognize specific and relevant terms in the software engineering context. This model can be used as the main entry in the classification of several textual artifacts generated during the software development project process. The results are promising, presenting a 48% improvement in the mAP values for the EmbSE concerning the model trained on the generic corpus. This reinforces the hypothesis that a model of this nature can bring significant improvements in the classification of texts of the area.","software engineering, machine learning, word embedding, pre-treined model, domain-specific model",172–180,9,Proceedings of the XXXIII Brazilian Symposium on Software Engineering
602,@inproceedings: 10.1145/3307630.3342419,Reusability in Artificial Neural Networks: An Empirical Study,"Ghofrani, Javad and Kozegar, Ehsan and Bozorgmehr, Arezoo and Soorati, Mohammad Divband",2019,Association for Computing Machinery,,https://doi.org/10.1145/3307630.3342419,10.1145/3307630.3342419,"Machine learning, especially deep learning has aroused interests of researchers and practitioners for the last few years in development of intelligent systems such as speech, natural language, and image processing. Software solutions based on machine learning techniques attract more attention as alternatives to conventional software systems. In this paper, we investigate how reusability techniques are applied in implementation of artificial neural networks (ANNs). We conducted an empirical study with an online survey among experts with experience in developing solutions with ANNs. We analyze the feedback of more than 100 experts to our survey. The results show existing challenges and some of the applied solutions in an intersection between reusability and ANNs.","artificial neural networks, reusability, survey, empirical study, systematic reuse",122–129,8,Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B
603,@inproceedings: 10.1145/3340482.3342745,Classifying Non-Functional Requirements Using RNN Variants for Quality Software Development,"Rahman, Md. Abdur and Haque, Md. Ariful and Tawhid, Md. Nurul Ahad and Siddik, Md. Saeed",2019,Association for Computing Machinery,,https://doi.org/10.1145/3340482.3342745,10.1145/3340482.3342745,"Non-Functional Requirements (NFR), a set of quality attributes, required for software architectural design. Which are usually scattered in SRS and must be extracted for quality software development to meet user expectations. Researchers show that functional and non-functional requirements are mixed together within the same SRS, which requires a mammoth effort for distinguishing them. Automatic NFR classification would be a feasible way to characterize those requirements, where several techniques have been recommended e.g. IR, linguistic knowledge, etc. However, conventional supervised machine learning methods suffered for word representation problem and usually required hand-crafted features, which will be overcome by proposed research using RNN variants to categories NFR. The NFR are interrelated and one task happens after another, which is the ideal situation for RNN. In this approach, requirements are processed to eliminate unnecessary contents, which are used to extract features using word2vec to fed as input of RNN variants LSTM and GRU. Performance has been evaluated using PROMISE dataset considering several statistical analyses. Among those models, precision, recall, and f1-score of LSTM validation are 0.973, 0.967 and 0.966 respectively, which is higher over CNN and GRU models. LSTM also correctly classified minimum 60% and maximum 80% unseen requirements. In addition, classification accuracy of LSTM is 6.1% better than GRU, which concluded that RNN variants can lead to better classification results, and LSTM is more suitable for NFR classification from textual requirements.","RNN, Non Functional Requirements, Deep Learning, NLP",25–30,6,Proceedings of the 3rd ACM SIGSOFT International Workshop on Machine Learning Techniques for Software Quality Evaluation
604,@inproceedings: 10.1145/3338906.3338924,SAR: Learning Cross-Language API Mappings with Little Knowledge,"Bui, Nghi D. Q. and Yu, Yijun and Jiang, Lingxiao",2019,Association for Computing Machinery,,https://doi.org/10.1145/3338906.3338924,10.1145/3338906.3338924,"To save effort, developers often translate programs from one programming language to another, instead of implementing it from scratch. Translating application program interfaces (APIs) used in one language to functionally equivalent ones available in another language is an important aspect of program translation. Existing approaches facilitate the translation by automatically identifying the API mappings across programming languages. However, these approaches still require large amount of parallel corpora, ranging from pairs of APIs or code fragments that are functionally equivalent, to similar code comments.  To minimize the need of parallel corpora, this paper aims at an automated approach that can map APIs across languages with much less a priori knowledge than other approaches. The approach is based on an realization of the notion of domain adaption, combined with code embedding, to better align two vector spaces. Taking as input large sets of programs, our approach first generates numeric vector representations of the programs (including the APIs used in each language), and it adapts generative adversarial networks (GAN) to align the vectors in different spaces of two languages. For a better alignment, we initialize the GAN with parameters derived from API mapping seeds that can be identified accurately with a simple automatic signature-based matching heuristic. Then the cross language API mappings can be identified via nearest-neighbors queries in the aligned vector spaces. We have implemented the approach (SAR, named after three main technical components in the approach) in a prototype for mapping APIs across Java and C# programs. Our evaluation on about 2 million Java files and 1 million C# files shows that the approach can achieve 54% and 82% mapping accuracy in its top-1 and top-10 API mapping results with only 174 automatically identified seeds, more accurate than other approaches using the same or much more mapping seeds.","cross-language API mapping, domain adaptation, vector space alignment, Generative Adversarial Network, word embedding",796–806,11,Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
605,@inproceedings: 10.1145/3338906.3338943,On Using Machine Learning to Identify Knowledge in API Reference Documentation,"Fucci, Davide and Mollaalizadehbahnemiri, Alireza and Maalej, Walid",2019,Association for Computing Machinery,,https://doi.org/10.1145/3338906.3338943,10.1145/3338906.3338943,"Using API reference documentation like JavaDoc is an integral part of software development. Previous research introduced a grounded taxonomy that organizes API documentation knowledge in 12 types, including knowledge about the Functionality, Structure, and Quality of an API. We study how well modern text classification approaches can automatically identify documentation containing specific knowledge types. We compared conventional machine learning (k-NN and SVM) with deep learning approaches trained on manually-annotated Java and .NET API documentation (n = 5,574). When classifying the knowledge types individually (i.e., multiple binary classifiers) the best AUPRC was up to 87","machine learning, API documentation, information needs",109–119,11,Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
606,@inproceedings: 10.1145/3338906.3338977,SEntiMoji: An Emoji-Powered Learning Approach for Sentiment Analysis in Software Engineering,"Chen, Zhenpeng and Cao, Yanbin and Lu, Xuan and Mei, Qiaozhu and Liu, Xuanzhe",2019,Association for Computing Machinery,,https://doi.org/10.1145/3338906.3338977,10.1145/3338906.3338977,"Sentiment analysis has various application scenarios in software engineering (SE), such as detecting developers' emotions in commit messages and identifying their opinions on Q&amp;A forums. However, commonly used out-of-the-box sentiment analysis tools cannot obtain reliable results on SE tasks and the misunderstanding of technical jargon is demonstrated to be the main reason. Then, researchers have to utilize labeled SE-related texts to customize sentiment analysis for SE tasks via a variety of algorithms. However, the scarce labeled data can cover only very limited expressions and thus cannot guarantee the analysis quality. To address such a problem, we turn to the easily available emoji usage data for help. More specifically, we employ emotional emojis as noisy labels of sentiments and propose a representation learning approach that uses both Tweets and GitHub posts containing emojis to learn sentiment-aware representations for SE-related texts. These emoji-labeled posts can not only supply the technical jargon, but also incorporate more general sentiment patterns shared across domains. They as well as labeled data are used to learn the final sentiment classifier. Compared to the existing sentiment analysis methods used in SE, the proposed approach can achieve significant improvement on representative benchmark datasets. By further contrast experiments, we find that the Tweets make a key contribution to the power of our approach. This finding informs future research not to unilaterally pursue the domain-specific resource, but try to transform knowledge from the open domain through ubiquitous signals such as emojis.","Emoji, Software engineering, Sentiment analysis",841–852,12,Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
607,@inproceedings: 10.1145/3338906.3338930,Boosting Operational DNN Testing Efficiency through Conditioning,"Li, Zenan and Ma, Xiaoxing and Xu, Chang and Cao, Chun and Xu, Jingwei and L\""{u}, Jian",2019,Association for Computing Machinery,,https://doi.org/10.1145/3338906.3338930,10.1145/3338906.3338930,"With the increasing adoption of Deep Neural Network (DNN) models as integral parts of software systems, efficient operational testing of DNNs is much in demand to ensure these models' actual performance in field conditions. A challenge is that the testing often needs to produce precise results with a very limited budget for labeling data collected in field.  Viewing software testing as a practice of reliability estimation through statistical sampling, we re-interpret the idea behind conventional structural coverages as conditioning for variance reduction. With this insight we propose an efficient DNN testing method based on the conditioning on the representation learned by the DNN model under testing. The representation is defined by the probability distribution of the output of neurons in the last hidden layer of the model. To sample from this high dimensional distribution in which the operational data are sparsely distributed, we design an algorithm leveraging cross entropy minimization.  Experiments with various DNN models and datasets were conducted to evaluate the general efficiency of the approach. The results show that, compared with simple random sampling, this approach requires only about a half of labeled inputs to achieve the same level of precision.","Software testing, Coverage criteria, Neural networks",499–509,11,Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
608,@inproceedings: 10.1145/3338906.3338963,A Learning-Based Approach for Automatic Construction of Domain Glossary from Source Code and Documentation,"Wang, Chong and Peng, Xin and Liu, Mingwei and Xing, Zhenchang and Bai, Xuefang and Xie, Bing and Wang, Tuo",2019,Association for Computing Machinery,,https://doi.org/10.1145/3338906.3338963,10.1145/3338906.3338963,"A domain glossary that organizes domain-specific concepts and their aliases and relations is essential for knowledge acquisition and software development. Existing approaches use linguistic heuristics or term-frequency-based statistics to identify domain specific terms from software documentation, and thus the accuracy is often low. In this paper, we propose a learning-based approach for automatic construction of domain glossary from source code and software documentation. The approach uses a set of high-quality seed terms identified from code identifiers and natural language concept definitions to train a domain-specific prediction model to recognize glossary terms based on the lexical and semantic context of the sentences mentioning domain-specific concepts. It then merges the aliases of the same concepts to their canonical names, selects a set of explanation sentences for each concept, and identifies ""is a"", ""has a"", and ""related to"" relations between the concepts. We apply our approach to deep learning domain and Hadoop domain and harvest 5,382 and 2,069 concepts together with 16,962 and 6,815 relations respectively. Our evaluation validates the accuracy of the extracted domain glossary and its usefulness for the fusion and acquisition of knowledge from different documents of different projects.","learning, documentation, concept, domain glossary, knowledge",97–108,12,Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
609,@inproceedings: 10.1145/3338906.3338942,Generating Effective Test Cases for Self-Driving Cars from Police Reports,"Gambi, Alessio and Huynh, Tri and Fraser, Gordon",2019,Association for Computing Machinery,,https://doi.org/10.1145/3338906.3338942,10.1145/3338906.3338942,"Autonomous driving carries the promise to drastically reduce the number of car accidents; however, recently reported fatal crashes involving self-driving cars show that such an important goal is not yet achieved. This calls for better testing of the software controlling self-driving cars, which is difficult because it requires producing challenging driving scenarios. To better test self-driving car soft- ware, we propose to specifically test car crash scenarios, which are critical par excellence. Since real car crashes are difficult to test in field operation, we recreate them as physically accurate simulations in an environment that can be used for testing self-driving car software. To cope with the scarcity of sensory data collected during real car crashes which does not enable a full reproduction, we extract the information to recreate real car crashes from the police reports which document them. Our extensive evaluation, consisting of a user study involving 34 participants and a quantitative analysis of the quality of the generated tests, shows that we can generate accurate simulations of car crashes in a matter of minutes. Compared to tests which implement non critical driving scenarios, our tests effectively stressed the test subject in different ways and exposed several shortcomings in its implementation.","procedural content generation, self-driving cars, automatic test generation, natural language processing",257–267,11,Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
610,@inproceedings: 10.1145/3338906.3338931,Robust Log-Based Anomaly Detection on Unstable Log Data,"Zhang, Xu and Xu, Yong and Lin, Qingwei and Qiao, Bo and Zhang, Hongyu and Dang, Yingnong and Xie, Chunyu and Yang, Xinsheng and Cheng, Qian and Li, Ze and Chen, Junjie and He, Xiaoting and Yao, Randolph and Lou, Jian-Guang and Chintalapati, Murali and Shen, Furao and Zhang, Dongmei",2019,Association for Computing Machinery,,https://doi.org/10.1145/3338906.3338931,10.1145/3338906.3338931,"Logs are widely used by large and complex software-intensive systems for troubleshooting. There have been a lot of studies on log-based anomaly detection. To detect the anomalies, the existing methods mainly construct a detection model using log event data extracted from historical logs. However, we find that the existing methods do not work well in practice. These methods have the close-world assumption, which assumes that the log data is stable over time and the set of distinct log events is known. However, our empirical study shows that in practice, log data often contains previously unseen log events or log sequences. The instability of log data comes from two sources: 1) the evolution of logging statements, and 2) the processing noise in log data. In this paper, we propose a new log-based anomaly detection approach, called LogRobust. LogRobust extracts semantic information of log events and represents them as semantic vectors. It then detects anomalies by utilizing an attention-based Bi-LSTM model, which has the ability to capture the contextual information in the log sequences and automatically learn the importance of different log events. In this way, LogRobust is able to identify and handle unstable log events and sequences. We have evaluated LogRobust using logs collected from the Hadoop system and an actual online service system of Microsoft. The experimental results show that the proposed approach can well address the problem of log instability and achieve accurate and robust results on real-world, ever-changing log data.","Log Instability, Anomaly Detection, Data Quality, Deep Learning, Log Analysis",807–817,11,Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
611,@inproceedings: 10.1145/3338906.3338947,Assessing the Quality of the Steps to Reproduce in Bug Reports,"Chaparro, Oscar and Bernal-C\'{a}rdenas, Carlos and Lu, Jing and Moran, Kevin and Marcus, Andrian and Di Penta, Massimiliano and Poshyvanyk, Denys and Ng, Vincent",2019,Association for Computing Machinery,,https://doi.org/10.1145/3338906.3338947,10.1145/3338906.3338947,"A major problem with user-written bug reports, indicated by developers and documented by researchers, is the (lack of high) quality of the reported steps to reproduce the bugs. Low-quality steps to reproduce lead to excessive manual effort spent on bug triage and resolution. This paper proposes Euler, an approach that automatically identifies and assesses the quality of the steps to reproduce in a bug report, providing feedback to the reporters, which they can use to improve the bug report. The feedback provided by Euler was assessed by external evaluators and the results indicate that Euler correctly identified 98% of the existing steps to reproduce and 58% of the missing ones, while 73% of its quality annotations are correct.","Dynamic Software Analysis, Bug Report Quality, Textual Analysis",86–96,11,Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
612,@inproceedings: 10.1145/3338906.3338951,Maximal Multi-Layer Specification Synthesis,"Chen, Yanju and Martins, Ruben and Feng, Yu",2019,Association for Computing Machinery,,https://doi.org/10.1145/3338906.3338951,10.1145/3338906.3338951,"There has been a significant interest in applying programming-by-example to automate repetitive and tedious tasks. However, due to the incomplete nature of input-output examples, a synthesizer may generate programs that pass the examples but do not match the user intent. In this paper, we propose MARS, a novel synthesis framework that takes as input a multi-layer specification composed by input-output examples, textual description, and partial code snippets that capture the user intent. To accurately capture the user intent from the noisy and ambiguous description, we propose a hybrid model that combines the power of an LSTM-based sequence-to-sequence model with the apriori algorithm for mining association rules through unsupervised learning. We reduce the problem of solving a multi-layer specification synthesis to a Max-SMT problem, where hard constraints encode well-typed concrete programs and soft constraints encode the user intent learned by the hybrid model. We instantiate our hybrid model to the data wrangling domain and compare its performance against Morpheus, a state-of-the-art synthesizer for data wrangling tasks. Our experiments demonstrate that our approach outperforms MORPHEUS in terms of running time and solved benchmarks. For challenging benchmarks, our approach can suggest candidates with rankings that are an order of magnitude better than MORPHEUS which leads to running times that are 15x faster than MORPHEUS.","Max-SMT, neural networks, machine learning, program synthesis",602–612,11,Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
613,@inproceedings: 10.1145/3338906.3338954,DeepStellar: Model-Based Quantitative Analysis of Stateful Deep Learning Systems,"Du, Xiaoning and Xie, Xiaofei and Li, Yi and Ma, Lei and Liu, Yang and Zhao, Jianjun",2019,Association for Computing Machinery,,https://doi.org/10.1145/3338906.3338954,10.1145/3338906.3338954,"Deep Learning (DL) has achieved tremendous success in many cutting-edge applications. However, the state-of-the-art DL systems still suffer from quality issues. While some recent progress has been made on the analysis of feed-forward DL systems, little study has been done on the Recurrent Neural Network (RNN)-based stateful DL systems, which are widely used in audio, natural languages and video processing, etc. In this paper, we initiate the very first step towards the quantitative analysis of RNN-based DL systems. We model RNN as an abstract state transition system to characterize its internal behaviors. Based on the abstract model, we design two trace similarity metrics and five coverage criteria which enable the quantitative analysis of RNNs. We further propose two algorithms powered by the quantitative measures for adversarial sample detection and coverage-guided test generation. We evaluate DeepStellar on four RNN-based systems covering image classification and automated speech recognition. The results demonstrate that the abstract model is useful in capturing the internal behaviors of RNNs, and confirm that (1) the similarity metrics could effectively capture the differences between samples even with very small perturbations (achieving 97% accuracy for detecting adversarial samples) and (2) the coverage criteria are useful in revealing erroneous behaviors (generating three times more adversarial samples than random testing and hundreds times more than the unrolling approach).","model-based analysis, testing, adversarial sample, Deep learning, recurrent neural network",477–487,11,Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
614,@article: 10.1145/3324916,Neural Network-Based Detection of Self-Admitted Technical Debt: From Performance to Explainability,"Ren, Xiaoxue and Xing, Zhenchang and Xia, Xin and Lo, David and Wang, Xinyu and Grundy, John",2019,Association for Computing Machinery,1049-331X,https://doi.org/10.1145/3324916,10.1145/3324916,"Technical debt is a metaphor to reflect the tradeoff software engineers make between short-term benefits and long-term stability. Self-admitted technical debt (SATD), a variant of technical debt, has been proposed to identify debt that is intentionally introduced during software development, e.g., temporary fixes and workarounds. Previous studies have leveraged human-summarized patterns (which represent n-gram phrases that can be used to identify SATD) or text-mining techniques to detect SATD in source code comments. However, several characteristics of SATD features in code comments, such as vocabulary diversity, project uniqueness, length, and semantic variations, pose a big challenge to the accuracy of pattern or traditional text-mining-based SATD detection, especially for cross-project deployment. Furthermore, although traditional text-mining-based method outperforms pattern-based method in prediction accuracy, the text features it uses are less intuitive than human-summarized patterns, which makes the prediction results hard to explain. To improve the accuracy of SATD prediction, especially for cross-project prediction, we propose a Convolutional Neural Network-- (CNN) based approach for classifying code comments as SATD or non-SATD. To improve the explainability of our model’s prediction results, we exploit the computational structure of CNNs to identify key phrases and patterns in code comments that are most relevant to SATD. We have conducted an extensive set of experiments with 62,566 code comments from 10 open-source projects and a user study with 150 comments of another three projects. Our evaluation confirms the effectiveness of different aspects of our approach and its superior performance, generalizability, adaptability, and explainability over current state-of-the-art traditional text-mining-based methods for SATD classification.","Self-admitted technical debt, convolutional neural network, cross project prediction, model adaptability, model explainability, model generalizabilit",,45,ACM Trans. Softw. Eng. Methodol.
615,@inbook: 10.1145/3293882.3330579,DeepHunter: A Coverage-Guided Fuzz Testing Framework for Deep Neural Networks,"Xie, Xiaofei and Ma, Lei and Juefei-Xu, Felix and Xue, Minhui and Chen, Hongxu and Liu, Yang and Zhao, Jianjun and Li, Bo and Yin, Jianxiong and See, Simon",2019,Association for Computing Machinery,,https://doi.org/10.1145/3293882.3330579,,"The past decade has seen the great potential of applying deep neural network (DNN) based software to safety-critical scenarios, such as autonomous driving. Similar to traditional software, DNNs could exhibit incorrect behaviors, caused by hidden defects, leading to severe accidents and losses. In this paper, we propose DeepHunter, a coverage-guided fuzz testing framework for detecting potential defects of general-purpose DNNs. To this end, we first propose a metamorphic mutation strategy to generate new semantically preserved tests, and leverage multiple extensible coverage criteria as feedback to guide the test generation. We further propose a seed selection strategy that combines both diversity-based and recency-based seed selection. We implement and incorporate 5 existing testing criteria and 4 seed selection strategies in DeepHunter. Large-scale experiments demonstrate that (1) our metamorphic mutation strategy is useful to generate new valid tests with the same semantics as the original seed, by up to a 98% validity ratio; (2) the diversity-based seed selection generally weighs more than recency-based seed selection in boosting the coverage and in detecting defects; (3) DeepHunter outperforms the state of the arts by coverage as well as the quantity and diversity of defects identified; (4) guided by corner-region based criteria, DeepHunter is useful to capture defects during the DNN quantization for platform migration.",,146–157,1,Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis
616,@inproceedings: 10.1145/3314221.3314652,Wootz: A Compiler-Based Framework for Fast CNN Pruning via Composability,"Guan, Hui and Shen, Xipeng and Lim, Seung-Hwan",2019,Association for Computing Machinery,,https://doi.org/10.1145/3314221.3314652,10.1145/3314221.3314652,"Convolutional Neural Networks (CNN) are widely used for Deep Learning tasks. CNN pruning is an important method to adapt a large CNN model trained on general datasets to fit a more specialized task or a smaller device. The key challenge is on deciding which filters to remove in order to maximize the quality of the pruned networks while satisfying the constraints. It is time-consuming due to the enormous configuration space and the slowness of CNN training.  The problem has drawn many efforts from the machine learning field, which try to reduce the set of network configurations to explore. This work tackles the problem distinctively from a programming systems perspective, trying to speed up the evaluations of the remaining configurations through computation reuse via a compiler-based framework. We empirically uncover the existence of composability in the training of a collection of pruned CNN models, and point out the opportunities for computation reuse. We then propose composability-based CNN pruning, and design a compression-based algorithm to efficiently identify the set of CNN layers to pre-train for maximizing their reuse benefits in CNN pruning. We further develop a compiler-based framework named Wootz, which, for an arbitrary CNN, automatically generates code that builds a Teacher-Student scheme to materialize composability-based pruning. Experiments show that network pruning enabled by Wootz shortens the state-of-art pruning process by up to 186X while producing significantly better pruning results.","CNN, network pruning, compiler, composability",717–730,14,Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation
617,@inproceedings: 10.1145/3314221.3314594,Genie: A Generator of Natural Language Semantic Parsers for Virtual Assistant Commands,"Campagna, Giovanni and Xu, Silei and Moradshahi, Mehrad and Socher, Richard and Lam, Monica S.",2019,Association for Computing Machinery,,https://doi.org/10.1145/3314221.3314594,10.1145/3314221.3314594,"To understand diverse natural language commands, virtual assistants today are trained with numerous labor-intensive, manually annotated sentences. This paper presents a methodology and the Genie toolkit that can handle new compound commands with significantly less manual effort. We advocate formalizing the capability of virtual assistants with a Virtual Assistant Programming Language (VAPL) and using a neural semantic parser to translate natural language into VAPL code. Genie needs only a small realistic set of input sentences for validating the neural model. Developers write templates to synthesize data; Genie uses crowdsourced paraphrases and data augmentation, along with the synthesized data, to train a semantic parser. We also propose design principles that make VAPL languages amenable to natural language translation. We apply these principles to revise ThingTalk, the language used by the Almond virtual assistant. We use Genie to build the first semantic parser that can support compound virtual assistants commands with unquoted free-form parameters. Genie achieves a 62% accuracy on realistic user inputs. We demonstrate Genie’s generality by showing a 19% and 31% improvement over the previous state of the art on a music skill, aggregate functions, and access control.","data augmentation, training data generation, virtual assistants, data engineering, semantic parsing",394–410,17,Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation
618,@inproceedings: 10.1145/3314221.3314628,CHET: An Optimizing Compiler for Fully-Homomorphic Neural-Network Inferencing,"Dathathri, Roshan and Saarikivi, Olli and Chen, Hao and Laine, Kim and Lauter, Kristin and Maleki, Saeed and Musuvathi, Madanlal and Mytkowicz, Todd",2019,Association for Computing Machinery,,https://doi.org/10.1145/3314221.3314628,10.1145/3314221.3314628,"Fully Homomorphic Encryption (FHE) refers to a set of encryption schemes that allow computations on encrypted data without requiring a secret key. Recent cryptographic advances have pushed FHE into the realm of practical applications. However, programming these applications remains a huge challenge, as it requires cryptographic domain expertise to ensure correctness, security, and performance.  CHET is a domain-specific optimizing compiler designed to make the task of programming FHE applications easier. Motivated by the need to perform neural network inference on encrypted medical and financial data, CHET supports a domain-specific language for specifying tensor circuits. It automates many of the laborious and error prone tasks of encoding such circuits homomorphically, including encryption parameter selection to guarantee security and accuracy of the computation, determining efficient tensor layouts, and performing scheme-specific optimizations.  Our evaluation on a collection of popular neural networks shows that CHET generates homomorphic circuits that outperform expert-tuned circuits and makes it easy to switch across different encryption schemes. We demonstrate its scalability by evaluating it on a version of SqueezeNet, which to the best of our knowledge, is the deepest neural network to be evaluated homomorphically.","neural networks, Homomorphic encryption, privacy-preserving machine learning, domain-specific compiler",142–156,15,Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation
619,@inproceedings: 10.1145/3314221.3314613,Supporting Peripherals in Intermittent Systems with Just-in-Time Checkpoints,"Maeng, Kiwan and Lucia, Brandon",2019,Association for Computing Machinery,,https://doi.org/10.1145/3314221.3314613,10.1145/3314221.3314613,"Batteryless energy-harvesting devices have the potential to be the foundation of applications for which batteries are infeasible. Just-In-Time checkpointing supports intermittent execution on energy-harvesting devices by checkpointing processor state right before a power failure. While effective for software execution, Just-In-Time checkpointing remains vulnerable to unrecoverable failures involving peripherals(e.g., sensors and accelerators) because checkpointing during a peripheral operation may lead to inconsistency between peripheral and program state. Additionally, a peripheral operation that uses more energy than a device can buffer never completes, causing non-termination.  This paper presents Samoyed, a Just-In-Time checkpointing system that safely supports peripherals. Samoyed correctly runs user-annotated peripheral functions by selectively disabling checkpoints and undo-logging. Samoyed guarantees progress by energy profiling, dynamic peripheral workload scaling, and a user-provided software fallback routine. Our evaluation shows that Samoyed correctly executes peripheral operations that fail with existing systems, achieving up to 122.9x speedup by using accelerators. Samoyed preserves the performance benefit of Just-In-Time checkpointing, showing 4.11x mean speedup compared to a recent possible alternative. Moreover, Samoyed’s unique ability to profile energy and to dynamically scale large peripheral operations simplifies programming.","intermittent computing, energy-harvesting",1101–1116,16,Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation
620,@inproceedings: 10.1145/3314221.3314597,Compiling KB-Sized Machine Learning Models to Tiny IoT Devices,"Gopinath, Sridhar and Ghanathe, Nikhil and Seshadri, Vivek and Sharma, Rahul",2019,Association for Computing Machinery,,https://doi.org/10.1145/3314221.3314597,10.1145/3314221.3314597,"Recent advances in machine learning (ML) have produced KiloByte-size models that can directly run on constrained IoT devices. This approach avoids expensive communication between IoT devices and the cloud, thereby enabling energy-efficient real-time analytics. However, ML models are expressed typically in floating-point, and IoT hardware typically does not support floating-point. Therefore, running these models on IoT devices requires simulating IEEE-754 floating-point using software, which is very inefficient. We present SeeDot, a domain-specific language to express ML inference algorithms and a compiler that compiles SeeDot programs to fixed-point code that can efficiently run on constrained IoT devices. We propose 1)&nbsp;a novel compilation strategy that reduces the search space for some key parameters used in the fixed-point code, and 2)&nbsp;new efficient implementations of expensive operations. SeeDot compiles state-of-the-art KB-sized models to various microcontrollers and low-end FPGAs. We show that SeeDot outperforms 1) software emulation of floating-point (Arduino), 2) high-bitwidth fixed-point (MATLAB), 3) post-training quantization (TensorFlow-Lite), and 4) floating- and fixed-point FPGA implementations generated using high-level synthesis tools.","Compiler, Microcontroller, Programming Language, IoT device, FPGA, Machine Learning, Fixed-point",79–95,17,Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation
621,@inproceedings: 10.1145/3314221.3314633,Scenic: A Language for Scenario Specification and Scene Generation,"Fremont, Daniel J. and Dreossi, Tommaso and Ghosh, Shromona and Yue, Xiangyu and Sangiovanni-Vincentelli, Alberto L. and Seshia, Sanjit A.",2019,Association for Computing Machinery,,https://doi.org/10.1145/3314221.3314633,10.1145/3314221.3314633,"We propose a new probabilistic programming language for the design and analysis of perception systems, especially those based on machine learning. Specifically, we consider the problems of training a perception system to handle rare events, testing its performance under different conditions, and debugging failures. We show how a probabilistic programming language can help address these problems by specifying distributions encoding interesting types of inputs and sampling these to generate specialized training and test sets. More generally, such languages can be used for cyber-physical systems and robotics to write environment models, an essential prerequisite to any formal analysis. In this paper, we focus on systems like autonomous cars and robots, whose environment is a scene, a configuration of physical objects and agents. We design a domain-specific language, Scenic, for describing scenarios that are distributions over scenes. As a probabilistic programming language, Scenic allows assigning distributions to features of the scene, as well as declaratively imposing hard and soft constraints over the scene. We develop specialized techniques for sampling from the resulting distribution, taking advantage of the structure provided by Scenic's domain-specific syntax. Finally, we apply Scenic in a case study on a convolutional neural network designed to detect cars in road images, improving its performance beyond that achieved by state-of-the-art synthetic data generation methods.","deep learning, fuzz testing, probabilistic programming, scenario description language, synthetic data, automatic test generation",63–78,16,Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation
622,@inproceedings: 10.1109/SE4Science.2019.00013,Learning Units-of-Measure from Scientific Code,"Danish, Matthew and Allamanis, Miltiadis and Brockschmidt, Marc and Rice, Andrew and Orchard, Dominic",2019,IEEE Press,,https://doi.org/10.1109/SE4Science.2019.00013,10.1109/SE4Science.2019.00013,"CamFort is our multi-purpose tool for lightweight analysis and verification of scientific Fortran code. One core feature provides units-of-measure verification (dimensional analysis) of programs, where users partially annotate programs with units-of-measure from which our tool checks consistency and infers any missing specifications. However, many users find it onerous to provide units-of-measure information for existing code, even in part. We have noted however that there are often many common patterns and clues about the intended units-of-measure contained within variable names, comments, and surrounding code context. In this work-in-progress paper, we describe how we are adapting our approach, leveraging machine-learning techniques to reconstruct units-of-measure information automatically thus saving programmer effort and increasing the likelihood of adoption.","machine learning, units-of-measure",43–46,4,Proceedings of the 14th International Workshop on Software Engineering for Science
