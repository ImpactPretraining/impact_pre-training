,id,title,author,year,publisher,issn,url,doi,abstract,keyword,pages,num_pages,publication_venue,content_type,source
0,,Improving the prediction of continuous integration build failures using deep learning,"Saidani I., Ouni A., Mkaouer M.W.",2022,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123344189&doi=10.1007%2fs10515-021-00319-5&partnerID=40&md5=4d0c792e31310b01454a9f3513e96bf2,10.1007/s10515-021-00319-5,"Continuous Integration (CI) aims at supporting developers in integrating code changes constantly and quickly through an automated build process. However, the build process is typically time and resource-consuming as running failed builds can take hours until discovering the breakage; which may cause disruptions in the development process and delays in the product release dates. Hence, preemptively detecting when a software state is most likely to trigger a failure during the build is of crucial importance for developers. Accurate build failures prediction techniques can cut the expenses of CI build cost by early predicting its potential failures. However, developing accurate prediction models is a challenging task as it requires learning long- and short-term dependencies in the historical CI build data as well as extensive feature engineering to derive informative features to learn from. In this paper, we introduce DL-CIBuild a novel approach that uses Long Short-Term Memory (LSTM)-based Recurrent Neural Networks (RNN) to construct prediction models for CI build outcome prediction. The problem is comprised of a single series of CI build outcomes and a model is required to learn from the series of past observations to predict the next CI build outcome in the sequence. In addition, we tailor Genetic Algorithm (GA) to tune the hyper-parameters for our LSTM model. We evaluate our approach and investigate the performance of both cross-project and online prediction scenarios on a benchmark of 91,330 CI builds from 10 large and long-lived software projects that use the Travis CI build system. The statistical analysis of the obtained results shows that the LSTM-based model outperforms traditional Machine Learning (ML) models with both online and cross-project validations. DL-CIBuild has shown also a less sensitivity to the training set size and an effective robustness to the concept drift. Additionally, by considering several Hyper-Parameter Optimization (HPO) methods as baseline for GA, we demonstrate that the latter performs the best. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",Build prediction; Concept drift; Continuous integration; Genetic algorithm; Hyper-parameters optimization; Long short term memory; Machine learning; Travis CI,,,Automated Software Engineering,Article,Scopus
1,,Opinion mining for app reviews: an analysis of textual representation and predictive models,"Araujo A.F., Gôlo M.P.S., Marcacini R.M.",2022,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119969463&doi=10.1007%2fs10515-021-00301-1&partnerID=40&md5=5d40eb0d9084b03467f2ad98ed088a8e,10.1007/s10515-021-00301-1,"Popular mobile applications receive millions of user reviews. These reviews contain relevant information for software maintenance, such as bug reports and improvement suggestions. The review’s information is a valuable knowledge source for software requirements engineering since the apps review analysis helps make strategic decisions to improve the app quality. However, due to the large volume of texts, the manual extraction of the relevant information is an impracticable task. Opinion mining is the field of study for analyzing people’s sentiments and emotions through opinions expressed on the web, such as social networks, forums, and community platforms for products and services recommendation. In this paper, we investigate opinion mining for app reviews. In particular, we compare textual representation techniques for classification, sentiment analysis, and utility prediction from app reviews. We discuss and evaluate different techniques for the textual representation of reviews, from traditional Bag-of-Words (BoW) to the most recent state-of-the-art Neural Language models (NLM). Our findings show that the traditional Bag-of-Words model, combined with a careful analysis of text pre-processing techniques, is still competitive. It obtains results close to the NLM in the classification, sentiment analysis and utility prediction tasks. However, NLM proved to be more advantageous since they achieved very competitive performance in all the predictive tasks covered in this work, provide significant dimensionality reduction, and deals more adequately with semantic proximity between the reviews’ texts. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",Machine learning; Mobile applications; Opinion mining; Requirements engineering,,,Automated Software Engineering,Article,Scopus
2,,A study on improved deep learning structure based on DenseNet,"Yun S.-K., Kwon H.J., Kim J.",2022,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118626405&doi=10.4018%2fIJSI.289595&partnerID=40&md5=dee1eacf6e4e1351ba892c1bfd4e9510,10.4018/IJSI.289595,"The existing image-related deep learning research methods are conducted through algorithms based on feature identification and association, but there are limits to their accuracy and reliability. These methods are inefficient for artificial neural networks to extract features and learn because of the loss of spatial information in the process of removing background and flattening images and have a limit on increasing accuracy and reliability. The deep learning algorithm applied in this study was based on the DenseNet neural network which is recently the best in performance and accuracy, and its architecture was improved with a focus on increasing the learning performance. As a result of the experiment, both speed and accuracy of learning data were more increased than the existing DenseNet architecture, which means to diagnose more images than the existing methods within the same amount of time. Copyright © 2022, IGI Global.",AI; Artificial Neural Networks; CNN; Deep Learning; DenseNet; MLP; ResNet,,,International Journal of Software Innovation,Article,Scopus
3,,The effect of CT high-resolution imaging diagnosis based on deep residual network on the pathology of bladder cancer classification and staging,"Liu D., Wang S., Wang J.",2022,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123033755&doi=10.1016%2fj.cmpb.2022.106635&partnerID=40&md5=e915227492c60102e57f8e0ce1ed6583,10.1016/j.cmpb.2022.106635,"Background and objective: To study the high-resolution CT image based on deep residual network to efficiently and accurately predict the staging diagnosis of bladder tumors. Methods: The image was processed with super-resolution to restore the missing details of the image. The CT data of 75 bladder patients who were treated in our hospital from June to December 2013 were collected. And obtain the patient's classification and staging information through pathology, which is used to establish a model of ResNet structure combined with non-Local attention mechanism. The clinical data of 76 patients with bladder disease admitted to our hospital from May 2018 to August 2021 were randomly selected, and the imaging and accuracy of CT diagnosis were retrospectively analyzed. Results: 52 cases were diagnosed <T1 stage, 16 cases belonged to T2 stage, 2 cases T3 stage, and 2 cases T4 stage. The sensitivity rate of experimental diagnosis was 94.74%, which was not significantly different from the sensitivity rate of preoperative pathological diagnosis. Conclusion: CT based on deep residual network has high application value in the diagnosis and staging of bladder cancer, can effectively improve the diagnostic accuracy, and is worthy of clinical application. © 2022",CT; Deep learning; Non-local; Residual network; Staging of bladder cancer; Super-resolution processing,,,Computer Methods and Programs in Biomedicine,Article,Scopus
4,,Comparative analysis of U-Net and TLMDB GAN for the cardiovascular segmentation of the ventricles in the heart,"Zhang Y., Feng J., Guo X., Ren Y.",2022,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123032049&doi=10.1016%2fj.cmpb.2021.106614&partnerID=40&md5=ff8deef525a3e414ffe3c08b7d333136,10.1016/j.cmpb.2021.106614,"Objective: Magnetic Resonance Image (MRI) is an important imaging modality for diagnosing heart disease and analyzing heart function. The size and shape of the ventricle are important parameters for judging whether the heart is normal, and the ventricles in the MRI image is effectively segmented It is the key to obtain the ventricle size, shape and other parameters. Accurate segmentation of the entricle is the fundamental guarantee for the evaluation of cardiac function. However, in the heart image, the contrast between the ventricle area and the background area is not obvious, the boundary is blurred, and there is noise in most of the images. The accurate segmentation of the ventricle becomes a challenging problem. Methods: We performed scanning of short-axis cardiac MR image sequences based on 33 subjects. Each subject has 8 to 15 sequences, each pertaining to a 20-frame sequence. Based on the U-Net neural network structure, the high-resolution information directly transferred from the encoder to the same-height decoder through the connection operation can provide more refined features for segmentation, such as gradients. The MRI left ventricular image segmentation method based on transfer learning and multi-scale discriminant Generative Adversarial Network (TLMDB GAN) solves the problem of insufficient ventricular image data. Results: According to the experimental results of TLMDB GAN and U-Net network on the data set, the Dice coefficients of TLMDB GAN segmentation of the inner cardiac wall and outer cardiac wall of the ventricle are 0.9399 and 0.9697, respectively, which are 0.01 higher than other methods. The Dice coefficients of U-Net segmentation of the inner cardiac wall and outer cardiac wall of the ventricle are 0.8829 and 0.9292, respectively; Conclusion: The experimental results show that the TLMDB GAN based on transfer learning and multi-scale discrimination significantly improves the segmentation accuracy when compared with the U-Net segmentation model. © 2022 Elsevier B.V.",Cardiac MRI; Generative adversarial network; U-Net; Ventricle segmentation,,,Computer Methods and Programs in Biomedicine,Article,Scopus
5,,Learning how to search: generating effective test cases through adaptive fitness function selection,"Almulla H., Gay G.",2022,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122924823&doi=10.1007%2fs10664-021-10048-8&partnerID=40&md5=acd8ea7d236f42ca67e901e7ed627caf,10.1007/s10664-021-10048-8,"Search-based test generation is guided by feedback from one or more fitness functions—scoring functions that judge solution optimality. Choosing informative fitness functions is crucial to meeting the goals of a tester. Unfortunately, many goals—such as forcing the class-under-test to throw exceptions, increasing test suite diversity, and attaining Strong Mutation Coverage—do not have effective fitness function formulations. We propose that meeting such goals requires treating fitness function identification as a secondary optimization step. An adaptive algorithm that can vary the selection of fitness functions could adjust its selection throughout the generation process to maximize goal attainment, based on the current population of test suites. To test this hypothesis, we have implemented two reinforcement learning algorithms in the EvoSuite unit test generation framework, and used these algorithms to dynamically set the fitness functions used during generation for the three goals identified above. We have evaluated our framework, EvoSuiteFIT, on a set of Java case examples. EvoSuiteFIT techniques attain significant improvements for two of the three goals, and show limited improvements on the third when the number of generations of evolution is fixed. Additionally, for two of the three goals, EvoSuiteFIT detects faults missed by the other techniques. The ability to adjust fitness functions allows strategic choices that efficiently produce more effective test suites, and examining these choices offers insight into how to attain our testing goals. We find that adaptive fitness function selection is a powerful technique to apply when an effective fitness function does not already exist for achieving a testing goal. © 2022, The Author(s).",Automated test generation; Hyperheuristic search; Reinforcement learning; Search-based test generation,,,Empirical Software Engineering,Article,Scopus
6,,Exploring convolutional neural networks with transfer learning for diagnosing Lyme disease from skin lesion images,"Hossain S.I., de Goër de Herve J., Hassan M.S., Martineau D., Petrosyan E., Corbin V., Beytout J., Lebert I., Durand J., Carravieri I., Brun-Jacob A., Frey-Klett P., Baux E., Cazorla C., Eldin C., Hansmann Y., Patrat-Delon S., Prazuck T., Raffetin A., Tattevin P., Vourc'h G., Lesens O., Nguifo E.M.",2022,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122813912&doi=10.1016%2fj.cmpb.2022.106624&partnerID=40&md5=789f265e2272fd3fbb23be6505eb2114,10.1016/j.cmpb.2022.106624,"Background and objective: Lyme disease which is one of the most common infectious vector-borne diseases manifests itself in most cases with erythema migrans (EM) skin lesions. Recent studies show that convolutional neural networks (CNNs) perform well to identify skin lesions from images. Lightweight CNN based pre-scanner applications for resource-constrained mobile devices can help users with early diagnosis of Lyme disease and prevent the transition to a severe late form thanks to appropriate antibiotic therapy. Also, resource-intensive CNN based robust computer applications can assist non-expert practitioners with an accurate diagnosis. The main objective of this study is to extensively analyze the effectiveness of CNNs for diagnosing Lyme disease from images and to find out the best CNN architectures considering resource constraints. Methods: First, we created an EM dataset with the help of expert dermatologists from Clermont-Ferrand University Hospital Center of France. Second, we benchmarked this dataset for twenty-three CNN architectures customized from VGG, ResNet, DenseNet, MobileNet, Xception, NASNet, and EfficientNet architectures in terms of predictive performance, computational complexity, and statistical significance. Third, to improve the performance of the CNNs, we used custom transfer learning from ImageNet pre-trained models as well as pre-trained the CNNs with the skin lesion dataset HAM10000. Fourth, for model explainability, we utilized Gradient-weighted Class Activation Mapping to visualize the regions of input that are significant to the CNNs for making predictions. Fifth, we provided guidelines for model selection based on predictive performance and computational complexity. Results: Customized ResNet50 architecture gave the best classification accuracy of 84.42% ±1.36, AUC of 0.9189±0.0115, precision of 83.1%±2.49, sensitivity of 87.93%±1.47, and specificity of 80.65%±3.59. A lightweight model customized from EfficientNetB0 also performed well with an accuracy of 83.13%±1.2, AUC of 0.9094±0.0129, precision of 82.83%±1.75, sensitivity of 85.21% ±3.91, and specificity of 80.89%±2.95. All the trained models are publicly available at https://dappem.limos.fr/download.html, which can be used by others for transfer learning and building pre-scanners for Lyme disease. Conclusion: Our study confirmed the effectiveness of even some lightweight CNNs for building Lyme disease pre-scanner mobile applications to assist people with an initial self-assessment and referring them to expert dermatologist for further diagnosis. © 2022 Elsevier B.V.",CNN; Erythema migrans; Explainability; Lyme disease; Transfer learning,,,Computer Methods and Programs in Biomedicine,Article,Scopus
7,,Saliency map-guided hierarchical dense feature aggregation framework for breast lesion classification using ultrasound image,"Di X., Zhong S., Zhang Y.",2022,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122648456&doi=10.1016%2fj.cmpb.2021.106612&partnerID=40&md5=e5059a231dbbbf40ca32adb0816a941a,10.1016/j.cmpb.2021.106612,"Deep learning methods, especially convolutional neural networks, have advanced the breast lesion classification task using breast ultrasound (BUS) images. However, constructing a highly-accurate classification model still remains challenging due to complex pattern, relatively-low contrast and fuzzy boundary existing between lesion regions (i.e., foreground) and the surrounding tissues (i.e., background). Few studies have separated foreground and background for learning domain-specific representations, and then fused them for improving performance of models. In this paper, we propose a saliency map-guided hierarchical dense feature aggregation framework for breast lesion classification using BUS images. Specifically, we first generate saliency maps for foreground and background via super-pixel clustering and multi-scale region grouping. Then, a triple-branch network, including two feature extraction branches and a feature aggregation branch, is constructed to learn and fuse discriminative representations under the guidance of priors provided by saliency maps. In particular, two feature extraction branches take the original image and corresponding saliency map as input for extracting foreground- and background-specific representations. Subsequently, a hierarchical feature aggregation branch receives and fuses the features from different stages of two feature extraction branches, for lesion classification in a task-oriented manner. The proposed model was evaluated on three datasets using 5-fold cross validation, and experimental results have demonstrated that it outperforms several state-of-the-art deep learning methods on breast lesion diagnosis using BUS images. © 2021",Breast lesion classification; Deep learning; Hierarchical dense feature aggregation; Saliency map; Ultrasound image,,,Computer Methods and Programs in Biomedicine,Article,Scopus
8,,ExAID: A multimodal explanation framework for computer-aided diagnosis of skin lesions,"Lucieri A., Bajwa M.N., Braun S.A., Malik M.I., Dengel A., Ahmed S.",2022,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122635590&doi=10.1016%2fj.cmpb.2022.106620&partnerID=40&md5=d4c94eac4b86f6422e44de08689c37a0,10.1016/j.cmpb.2022.106620,"Background and objectives: One principal impediment in the successful deployment of Artificial Intelligence (AI) based Computer-Aided Diagnosis (CAD) systems in everyday clinical workflows is their lack of transparent decision-making. Although commonly used eXplainable AI (XAI) methods provide insights into these largely opaque algorithms, such explanations are usually convoluted and not readily comprehensible. The explanation of decisions regarding the malignancy of skin lesions from dermoscopic images demands particular clarity, as the underlying medical problem definition is ambiguous in itself. This work presents ExAID (Explainable AI for Dermatology), a novel XAI framework for biomedical image analysis that provides multi-modal concept-based explanations, consisting of easy-to-understand textual explanations and visual maps, to justify the predictions. Methods: Our framework relies on Concept Activation Vectors to map human-understandable concepts to those learned by an arbitrary Deep Learning (DL) based algorithm, and Concept Localisation Maps to highlight those concepts in the input space. This identification of relevant concepts is then used to construct fine-grained textual explanations supplemented by concept-wise location information to provide comprehensive and coherent multi-modal explanations. All decision-related information is presented in a diagnostic interface for use in clinical routines. Moreover, the framework includes an educational mode providing dataset-level explanation statistics as well as tools for data and model exploration to aid medical research and education processes. Results: Through rigorous quantitative and qualitative evaluation of our framework on a range of publicly available dermoscopic image datasets, we show the utility of multi-modal explanations for CAD-assisted scenarios even in case of wrong disease predictions. We demonstrate that concept detectors for the explanation of pre-trained networks reach accuracies of up to 81.46%, which is comparable to supervised networks trained end-to-end. Conclusions: We present a new end-to-end framework for the multi-modal explanation of DL-based biomedical image analysis in Melanoma classification and evaluate its utility on an array of datasets. Since perspicuous explanation is one of the cornerstones of any CAD system, we believe that ExAID will accelerate the transition from AI research to practice by providing dermatologists and researchers with an effective tool that they can both understand and trust. ExAID can also serve as the basis for similar applications in other biomedical fields. © 2022",Artificial intelligence in dermatology; Computer-aided diagnosis; Explainable artificial intelligence; Interpretability; Medical image processing; Textual explanations,,,Computer Methods and Programs in Biomedicine,Article,Scopus
9,,Robust asynchronous control of ERP-Based brain-Computer interfaces using deep learning,"Santamaría-Vázquez E., Martínez-Cagigal V., Pérez-Velasco S., Marcos-Martínez D., Hornero R.",2022,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122631601&doi=10.1016%2fj.cmpb.2022.106623&partnerID=40&md5=07ac1162fdda738158d0899f45e0b5c9,10.1016/j.cmpb.2022.106623,"Background and Objective. Brain-computer interfaces (BCI) based on event-related potentials (ERP) are a promising technology for alternative and augmented communication in an assistive context. However, most approaches to date are synchronous, requiring the intervention of a supervisor when the user wishes to turn his attention away from the BCI system. In order to bring these BCIs into real-life applications, a robust asynchronous control of the system is required through monitoring of user attention. Despite the great importance of this limitation, which prevents the deployment of these systems outside the laboratory, it is often overlooked in research articles. This study was aimed to propose a novel method to solve this problem, taking advantage of deep learning for the first time in this context to overcome the limitations of previous strategies based on hand-crafted features. Methods. The proposed method, based on EEG-Inception, a novel deep convolutional neural network, divides the problem in 2 stages to achieve the asynchronous control: (i) the model detects user's control state, and (ii) decodes the command only if the user is attending to the stimuli. Additionally, we used transfer learning to reduce the calibration time, even exploring a calibration-less approach. Results. Our method was evaluated with 22 healthy subjects, analyzing the impact of the calibration time and number of stimulation sequences on the system's performance. For the control state detection stage, we report average accuracies above 91% using only 1 sequence of stimulation and 30 calibration trials, reaching a maximum of 96.95% with 15 sequences. Moreover, our calibration-less approach also achieved suitable results, with a maximum accuracy of 89.36%, showing the benefits of transfer learning. As for the overall asynchronous system, which includes both stages, the maximum information transfer rate was 35.54 bpm, a suitable value for high-speed communication. Conclusions. The proposed strategy achieved higher performance with less calibration trials and stimulation sequences than former approaches, representing a promising step forward that paves the way for more practical applications of ERP-based spellers. © 2022 The Authors",Asynchrony; Brain–computer interfaces; Control state detection; Convolutional neural networks; Deep learning; Event-related potentials; P300,,,Computer Methods and Programs in Biomedicine,Article,Scopus
10,,Domain adaptation based on rough adjoint inconsistency and optimal transport for identifying autistic patients,"Shi C.-L., Xin X.-W., Zhang J.-C.",2022,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122537101&doi=10.1016%2fj.cmpb.2021.106615&partnerID=40&md5=557ff7f9b29a18b2297e64ce836d768e,10.1016/j.cmpb.2021.106615,"Background and Objective: Computer aided diagnosis technology has been widely used to diagnose autism spectrum disorder (ASD) from neural images. The performance of the model usually depends largely on a sufficient number of training samples that reflect the real sample distribution. Due to the lack of labelled neural images data, multisite data are often pooled together to expand the sample size. However, the heterogeneity among sites will inevitably lead to a decline in the generalization of models. To solve this problem, we propose a multisource unsupervised domain adaptation method using rough adjoint inconsistency and optimal transport. Methods: First, we define the concept of rough adjoint inconsistency and propose a double quantization method based on rough adjoint inconsistency and Dempster-Shafer (D-S) evidence theory to estimate the weight coefficient of each source domain to accurately describe the importance of each source domain to the target domain. Second, using optimal transport theory, we weaken the data distribution differences between domains and solve the problem of class imbalance by adjusting the sampling weights among classes. Results: The ASD recognition accuracy of the proposed method is improved on all eight tasks, which are 70.67%, 64.86%, 62.50%, 70.80%, 73.08%, 71.19%, 75.41% and 75.76%, respectively. Our proposed model achieves superior performance compared to traditional machine learning methods and other recently proposed deep learning model. Conclusions: Our method demonstrates that the fusion of rough adjoint inconsistency and optimal transport can be a powerful tool for identifying ASD and quantifying the correlations between domains. © 2021",Autism spectrum disorder; Domain adaptation; Optimal transport; Rough adjoint inconsistency,,,Computer Methods and Programs in Biomedicine,Article,Scopus
11,,Unsupervised seizure identification on EEG,"Yıldız İ., Garner R., Lai M., Duncan D.",2022,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122243973&doi=10.1016%2fj.cmpb.2021.106604&partnerID=40&md5=da3a0c6bbea1b662d3e73ae49edefcb0,10.1016/j.cmpb.2021.106604,"Background and Objective: Epilepsy is one of the most common neurological disorders, whose development is typically detected via early seizures. Electroencephalogram (EEG) is prevalently employed for seizure identification due to its routine and low expense collection. The stochastic nature of EEG makes manual seizure inspections laborsome, motivating automated seizure identification. The relevant literature focuses mostly on supervised machine learning. Despite their success, supervised methods require expert labels indicating seizure segments, which are difficult to obtain on clinically-acquired EEG. Thus, we aim to devise an unsupervised method for seizure identification on EEG. Methods: We propose the first fully-unsupervised deep learning method for seizure identification on raw EEG, using a variational autoencoder (VAE). In doing so, we train the VAE on recordings without seizures. As training captures non-seizure activity, we identify seizures with respect to the reconstruction errors at inference time. Moreover, we extend the traditional VAE training loss to suppress EEG artifacts. Our method does not require ground-truth expert labels indicating seizure segments or manual feature extraction. Results: We implement our method using the PyTorch library and execute experiments on an NVIDIA V100 GPU. We evaluate our method on three benchmark EEG datasets: (i) intracranial recordings from the University of Pennsylvania and the Mayo Clinic, (ii) scalp recordings from the Temple University Hospital of Philadelphia, and (iii) scalp recordings from the Massachusetts Institute of Technology and the Boston Children's Hospital. To assess performance, we report accuracy, precision, recall, Area under the Receiver Operating Characteristics Curve (AUC), and p-value under the Welch t-test for distinguishing seizure vs. non-seizure EEG windows. Our approach can successfully distinguish seizures from non-seizure activity, with up to 0.83 AUC on intracranial recordings. Moreover, our algorithm has the potential for real-time inference, by processing at least 10 s of EEG in a second. Conclusion: We take the first successful steps in deep learning-based unsupervised seizure identification on raw EEG. Our approach has the potential of alleviating the burden on clinical experts regarding laborsome EEG inspections for seizures. Furthermore, aiding the identification of early seizures via our method could facilitate successful detection of epilepsy development and initiate antiepileptogenic therapies. © 2021 Elsevier B.V.",EEG; Epilepsy; Seizure; Sparsity; Unsupervised learning; Variational autoencoder,,,Computer Methods and Programs in Biomedicine,Article,Scopus
12,,Using Machine Learning to Identify Intravenous Contrast Phases on Computed Tomography,"Muhamedrahimov R., Bar A., Laserson J., Akselrod-Ballin A., Elnekave E.",2022,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122034615&doi=10.1016%2fj.cmpb.2021.106603&partnerID=40&md5=9c871baba9974955b5b24f3741d21943,10.1016/j.cmpb.2021.106603,"Purpose: The purpose of the present work is to demonstrate the application of machine learning (ML) techniques to automatically identify the presence and physiologic phase of intravenous (IV) contrast in Computed Tomography (CT) scans of the Chest, Abdomen and Pelvis. Materials and Methods: Training, testing and validation data were acquired from a dataset of 82,690 chest and abdomen CT examinations performed at 17 different institutions. Free text in DICOM metadata was utilized as weak labels for semi-supervised classification training. Contrast phase identification was approached as a classification task, using a 12-layer CNN and ResNet18 with four contrast-phase output. The model was reformulated to fit a regression task aimed to predict actual seconds from time of IV contrast administration to series image acquisition. Finally, transfer learning was used to optimize the model to predict contrast presence on CT Chest. Results: By training based on labels inferred from noisy, free text DICOM information, contrast phase was predicted with 93.3% test accuracy (95% CI: 89.3%, 96.6%). Regression analysis resulted in delineation of early vs late arterial phases and a nephrogenic phase in between the portal venous and delayed excretory phase. Transfer learning applied to Chest CT achieved an AUROC of 0.776 (95% CI: 0.721, 0.832) directly using the model trained for abdomen CT and 0.999 (95% CI: 0.998, 1.000) by fine-tuning. Conclusions: The presence and phase of contrast on CT examinations of the Abdomen-pelvis accurately and automatically be ascertained by a machine learning algorithm. Transfer learning applied to CT Chest achieves high precision with as little as 100 labeled samples. © 2021",,,,Computer Methods and Programs in Biomedicine,Article,Scopus
13,,Multi-scale information with attention integration for classification of liver fibrosis in B-mode US image,"Feng X., Chen X., Dong C., Liu Y., Liu Z., Ding R., Huang Q.",2022,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122001895&doi=10.1016%2fj.cmpb.2021.106598&partnerID=40&md5=f4babdf0f382612f58f2956b654631ea,10.1016/j.cmpb.2021.106598,"Background and objective: Chronic hepatitis B (CHB) is one of the most common liver diseases in the world, which threats a lot to people's usual life. The increased deposition of fibrotic tissues in livers for patients with CHB may lead to the development of liver cirrhosis, hepatocellular carcinoma, or even liver failure. Accurate fibrosis staging is very important for the targeted treatment of liver fibrosis and its recovery. Methods: In this paper, we propose a new deep convolutional neural network (DCNN) with functions of multi-scale information extraction and attention integration for more accurate liver fibrosis classification from ultrasound (US) images. The proposed network uses two pyramid-structured CNN elements to extract multi-scale features from US images. Such a design significantly enlarges the receptive field of the convolution layer, such that more useful information can be explored by the neural network to associate with the final classification. Based on this, a new feature distillation method is also proposed to enhance the ability of deep features derived from multi-scale information. The proposed distillation method employs attention maps to automatically extract class-related features from multi-scale information, which effectively suppress the influence of potential distractors. Results: Experimental results on the US liver fibrosis dataset collected from 286 participants show that the proposed deep framework achieves promising classification performance. The proposed method achieves a classification accuracy of 95.66% on the test dataset. Conclusion: Our proposed framework could stage liver fibrosis highly accurately. It might provide effective suggestions for the clinical treatment of liver fibrosis that can facilitate its recovery. © 2021 Elsevier B.V.",Attention maps; Liver fibrosis; Multi-scale information; US image,,,Computer Methods and Programs in Biomedicine,Article,Scopus
14,,A computer-aided grading of glioma tumor using deep residual networks fusion,"Tripathi P.C., Bag S.",2022,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121969852&doi=10.1016%2fj.cmpb.2021.106597&partnerID=40&md5=9d8b5e54e619f310faae886ef2d19aad,10.1016/j.cmpb.2021.106597,"Background and objectives: Among different cancer types, glioma is considered as a potentially fatal brain cancer that arises from glial cells. Early diagnosis of glioma helps the physician in offering effective treatment to the patients. Magnetic Resonance Imaging (MRI)-based Computer-Aided Diagnosis for the brain tumors has attracted a lot of attention in the literature in recent years. In this paper, we propose a novel deep learning-based computer-aided diagnosis for glioma tumors. Methods: The proposed method incorporates a two-level classification of gliomas. Firstly, the tumor is classified into low-or high-grade and secondly, the low-grade tumors are classified into two types based on the presence of chromosome arms 1p/19q. The feature representations of four residual networks have been used to solve the problem by utilizing transfer learning approach. Furthermore, we have fused these trained models using a novel Dempster-shafer Theory (DST)-based fusion scheme in order to enhance the classification performance. Extensive data augmentation strategies are also utilized to avoid over-fitting of the discrimination models. Results: Extensive experiments have been performed on an MRI dataset to show the effectiveness of the method. It has been found that our method achieves 95.87% accuracy for glioma classification. The results obtained by our method have also been compared with different existing methods. The comparative study reveals that our method not only outperforms traditional machine learning-based methods, but it also produces better results to state-of-the-art deep learning-based methods. Conclusion: The fusion of different residual networks enhances the tumor classification performance. The experimental findings indicates that Dempster-shafer Theory (DST)-based fusion technique produces superior performance in comparison to other fusion schemes. © 2021 Elsevier B.V.",Convolutional neural network; Deep learning; Glioma; Magnetic resonance imaging,,,Computer Methods and Programs in Biomedicine,Article,Scopus
15,,PRHAN: Automated Pull Request Description Generation Based on Hybrid Attention Network,"Fang S., Zhang T., Tan Y.-S., Xu Z., Yuan Z.-X., Meng L.-Z.",2022,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121254484&doi=10.1016%2fj.jss.2021.111160&partnerID=40&md5=61e11ca00bfc50b74b66c72848d78463,10.1016/j.jss.2021.111160,"Descriptions of pull requests (PRs) are posted by developers for describing the modifications that they have made and the corresponding reasons in these PRs. Although PRs help developers improve the development efficiency, some developers usually ignore writing the descriptions for PRs. To alleviate the above problem, researchers generally utilize text summarization model to automatically generate descriptions for PRs. However, current RNN-based models still face the challenges such as low efficiency and out-of-vocabulary (OOV), which may influence the further performance improvement to their models. To break this bottleneck, we propose a novel model aiming at the above challenges, named PRHAN (Pull Requests Description Generation Based on Hybrid Attention Network). Specifically, the core of PRHAN is the hybrid attention network, which has faster execution efficiency than RNN-based model. Moreover, we address the OOV problem by the utilizing byte-pair encoding algorithm to build a vocabulary at the sub-word level. Such a vocabulary can represent the OOV words by combining sub-word units. To reduce the sensitivity of the model, we take a simple but effective method into the cross-entropy loss function, named label smoothing. We choose three baseline models, including LeadCM, Transformer and the state-of-the-art model built by Liu et al. and evaluate all the models on the open-source dataset through ROUGE, BLEU, and human evaluation. The experimental results demonstrate that PRHAN is more effective than baselines. Moreover, PRHAN can execute faster than the state-of-the-art model proposed by Liu et al. © 2021 Elsevier Inc.",Byte-pair encoding; Hybrid attention; Label smoothing; PR description,,,Journal of Systems and Software,Article,Scopus
16,,Test case selection and prioritization using machine learning: a systematic literature review,"Pan R., Bagherzadeh M., Ghaleb T.A., Briand L.",2022,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121144062&doi=10.1007%2fs10664-021-10066-6&partnerID=40&md5=a6be798e73969d51859f9d1d3569ff8e,10.1007/s10664-021-10066-6,"Regression testing is an essential activity to assure that software code changes do not adversely affect existing functionalities. With the wide adoption of Continuous Integration (CI) in software projects, which increases the frequency of running software builds, running all tests can be time-consuming and resource-intensive. To alleviate that problem, Test case Selection and Prioritization (TSP) techniques have been proposed to improve regression testing by selecting and prioritizing test cases in order to provide early feedback to developers. In recent years, researchers have relied on Machine Learning (ML) techniques to achieve effective TSP (ML-based TSP). Such techniques help combine information about test cases, from partial and imperfect sources, into accurate prediction models. This work conducts a systematic literature review focused on ML-based TSP techniques, aiming to perform an in-depth analysis of the state of the art, thus gaining insights regarding future avenues of research. To that end, we analyze 29 primary studies published from 2006 to 2020, which have been identified through a systematic and documented process. This paper addresses five research questions addressing variations in ML-based TSP techniques and feature sets for training and testing ML models, alternative metrics used for evaluating the techniques, the performance of techniques, and the reproducibility of the published studies. We summarize the results related to our research questions in a high-level summary that can be used as a taxonomy for classifying future TSP studies. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",Continuous integration; Machine learning; Software testing; Systematic literature review; Test case prioritization; Test case selection,,,Empirical Software Engineering,Article,Scopus
17,,Automated data function extraction from textual requirements by leveraging semi-supervised CRF and language model,"Li M., Shi L., Wang Y., Wang J., Wang Q., Hu J., Peng X., Liao W., Pi G.",2022,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120487992&doi=10.1016%2fj.infsof.2021.106770&partnerID=40&md5=c1f2c458d096a124ba1cbdae23650aae,10.1016/j.infsof.2021.106770,"Context: Function Point Analysis (FPA) provides an objective, comparative measure for size estimation in the early stage of software development. When practicing FPA, analysts typically abide by the following steps: data function (DF) extraction, transactional function extraction, function type classification and adjustment factor determination. However, due to lack of approach and tool support, these steps are usually conduct by human efforts in practice. Related approaches can hardly be applied in the FPA due to the following three challenges, i.e., FPA rule-driven extraction, domain-specific parsing, and expensive labeled resources. Objective: In this paper, we aim to automate the extraction of DFs, which is the starting and fundamental step in FPA. Method: We propose an automated approach named DEX to extract data functions from textual requirements. Specifically, DEX introduces the popularly-used conditional random field (CRF) model to predict the boundary of a data function. Besides, DEX employs the bootstrapping-based algorithm and DF-oriented language model to further boost the performance. Results: We evaluate DEX from two aspects: evaluation on a real industrial dataset and a manual review by domain experts. The evaluation on the real industrial dataset shows that DEX could achieve 80% precision, 84% recall, and 82% F1, and outperforms three state-of-the-art baselines. The expert review suggests that DEX could increase 16% precision and 13% recall, compared with those produced by engineers. Conclusion: DEX could achieve promising results under a small number of labeled requirements and outperform the state-of-the-art approaches. Moreover, DEX could help engineers produce more accurate and complete DFs in the industrial environment. © 2021 Elsevier B.V.",Bootstrapping; Conditional random field; Function point analysis; Language model; Size estimation,,,Information and Software Technology,Article,Scopus
18,,Review4Repair: Code review aided automatic program repairing,"Huq F., Hasan M., Haque M.M.A., Mahbub S., Iqbal A., Ahmed T.",2022,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120382419&doi=10.1016%2fj.infsof.2021.106765&partnerID=40&md5=76178c9c25b7a092bdbe544875ce310d,10.1016/j.infsof.2021.106765,"Context: Learning-based automatic program repair techniques are showing promise to provide quality fix suggestions for detected bugs in the source code of the software. These tools mostly exploit historical data of buggy and fixed code changes and are heavily dependent on bug localizers while applying to a new piece of code. With the increasing popularity of code review, dependency on bug localizers can be reduced. Besides, the code review-based bug localization is more trustworthy since reviewers’ expertise and experience are reflected in these suggestions. Objective: The natural language instructions scripted on the review comments are enormous sources of information about the bug's nature and expected solutions. However, none of the learning-based tools has utilized the review comments to fix programming bugs to the best of our knowledge. In this study, we investigate the performance improvement of repair techniques using code review comments. Method: We train a sequence-to-sequence model on 55,060 code reviews and associated code changes. We also introduce new tokenization and preprocessing approaches that help to achieve significant improvement over state-of-the-art learning-based repair techniques. Results: We boost the top-1 accuracy by 20.33% and top-10 accuracy by 34.82%. We could provide a suggestion for stylistics and non-code errors unaddressed by prior techniques. Conclusion: We believe that the automatic fix suggestions along with code review generated by our approach would help developers address the review comment quickly and correctly and thus save their time and effort. © 2021 Elsevier B.V.",Automatic program repair; Code review; Deep learning,,,Information and Software Technology,Article,Scopus
19,,Systematic review of question answering over knowledge bases,"Pereira A., Trifan A., Lopes R.P., Oliveira J.L.",2022,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123301494&doi=10.1049%2fsfw2.12028&partnerID=40&md5=0b2bab680ef5c21c814f457b85a910e6,10.1049/sfw2.12028,"Over the years, a growing number of semantic data repositories have been made available on the web. However, this has created new challenges in exploiting these resources efficiently. Querying services require knowledge beyond the typical user’s expertise, which is a critical issue in adopting semantic information solutions. Several proposals to overcome this difficulty have suggested using question answering (QA) systems to provide user-friendly interfaces and allow natural language use. Because question answering over knowledge bases (KBQAs) is a very active research topic, a comprehensive view of the field is essential. The purpose of this study was to conduct a systematic review of methods and systems for KBQAs to identify their main advantages and limitations. The inclusion criteria rationale was English full-text articles published since 2015 on methods and systems for KBQAs. Sixty-six articles were reviewed to describe their underlying reference architectures. © 2021 The Authors. IET Software published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology.",,"1, 13",,IET Software,Review,Scopus
20,,Software defect prediction based on stacked sparse denoising autoencoders and enhanced extreme learning machine,"Zhang N., Ying S., Zhu K., Zhu D.",2022,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123288982&doi=10.1049%2fsfw2.12029&partnerID=40&md5=2555ad851dff7c2aaa6d8d3f9065d3a3,10.1049/sfw2.12029,"Software defect prediction is an important software quality assurance technique. Nevertheless, the prediction performance of the constructed model is easily susceptible to irrelevant or redundant features in the software projects and is not predominant enough. To address these two issues, a novel defect prediction model called SSEPG based on Stacked Sparse Denoising AutoEncoders (SSDAE) and Extreme Learning Maching (ELM) optimised by Particle Swarm Optimisation (PSO) and another complementary Gravitational Search Algorithm (GSA) are proposed in this paper, which has two main merits: (1) employ a novel deep neural network – SSDAE to extract new combined features, which can effectively learn the robust deep semantic feature representation. (2) integrate strong exploitation capacity of PSO with strong exploration capability of GSA to optimise the input weights and hidden layer biases of ELM, and utilise the superior discriminability of the enhanced ELM to predict the defective modules. The SSDAE is compared with eleven state-of-the-art feature extraction methods in effect and efficiency, and the SSEPG model is compared with multiple baseline models that contain five classic defect predictors and three variants across 24 software defect projects. The experimental results exhibit the superiority of the SSDAE and the SSEPG on six evaluation metrics. © 2021 The Authors. IET Software published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology.",,"29, 47",,IET Software,Article,Scopus
21,,GERNERMED: An open German medical NER model[Formula presented],"Frei J., Kramer F.",2022,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122526991&doi=10.1016%2fj.simpa.2021.100212&partnerID=40&md5=aff7ff2ce9c134a4703fdb8896ec53a6,10.1016/j.simpa.2021.100212,"Recent advancements in natural language processing (NLP) have been achieved by the use of increasingly complex neural networks. In clinical context, NLP is a key technique to access highly relevant information from unstructured texts such as clinical notes. We evaluate the feasibility of training our neural model GERNERMED on annotated German training data generated by automated translation from a public English dataset. The work guides other researchers about the use of machine-translation methods for dataset acquisition. Due to the public origin of the dataset, our trained software can be used by fellow researchers without any legal access restrictions. © 2021 The Authors",Clinical text mining; Machine learning; Named entity recognition; Natural language processing,,,Software Impacts,Article,Scopus
22,,"COV-ADSX: An Automated Detection System using X-ray Images, Deep Learning, and XGBoost for COVID-19","Hasani S., Nasiri H.",2022,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122496640&doi=10.1016%2fj.simpa.2021.100210&partnerID=40&md5=e22fea66f8bd5e82f2c28c41bde87124,10.1016/j.simpa.2021.100210,"Following the COVID-19 pandemic, scientists have been looking for different ways to diagnose COVID-19, and these efforts have led to a variety of solutions. One of the common methods of detecting infected people is chest radiography. In this paper, an Automated Detection System using X-ray images (COV-ADSX) is proposed, which employs a deep neural network and XGBoost to detect COVID-19. COV-ADSX was implemented using the Django web framework, which allows the user to upload an X-ray image and view the results of the COVID-19 detection and image's heatmap, which helps the expert to evaluate the chest area more accurately. © 2021 The Author(s)",Chest X-ray Images; COVID-19; Deep Neural Networks; DenseNet169; XGBoost,,,Software Impacts,Article,Scopus
23,,A CNN-transformer hybrid approach for decoding visual neural activity into text,"Zhang J., Li C., Liu G., Min M., Wang C., Li J., Wang Y., Yan H., Zuo Z., Huang W., Chen H.",2022,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121795531&doi=10.1016%2fj.cmpb.2021.106586&partnerID=40&md5=5756709c67265601d0fc6f0ee2741c6b,10.1016/j.cmpb.2021.106586,"Background and Objective: Most studies used neural activities evoked by linguistic stimuli such as phrases or sentences to decode the language structure. However, compared to linguistic stimuli, it is more common for the human brain to perceive the outside world through non-linguistic stimuli such as natural images, so only relying on linguistic stimuli cannot fully understand the information perceived by the human brain. To address this, an end-to-end mapping model between visual neural activities evoked by non-linguistic stimuli and visual contents is demanded. Methods: Inspired by the success of the Transformer network in neural machine translation and the convolutional neural network (CNN) in computer vision, here a CNN-Transformer hybrid language decoding model is constructed in an end-to-end fashion to decode functional magnetic resonance imaging (fMRI) signals evoked by natural images into descriptive texts about the visual stimuli. Specifically, this model first encodes a semantic sequence extracted by a two-layer 1D CNN from the multi-time visual neural activity into a multi-level abstract representation, then decodes this representation, step by step, into an English sentence. Results: Experimental results show that the decoded texts are semantically consistent with the corresponding ground truth annotations. Additionally, by varying the encoding and decoding layers and modifying the original positional encoding of the Transformer, we found that a specific architecture of the Transformer is required in this work. Conclusions: The study results indicate that the proposed model can decode the visual neural activities evoked by natural images into descriptive text about the visual stimuli in the form of sentences. Hence, it may be considered as a potential computer-aided tool for neuroscientists to understand the neural mechanism of visual information processing in the human brain in the future. © 2021",Brain decoding; CNN; Deep learning; fMRI; Transformer,,,Computer Methods and Programs in Biomedicine,Article,Scopus
24,,Deep reconstruction-recoding network for unsupervised domain adaptation and multi-center generalization in colonoscopy polyp detection,"Xu J., Zhang Q., Yu Y., Zhao R., Bian X., Liu X., Wang J., Ge Z., Qian D.",2022,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121134476&doi=10.1016%2fj.cmpb.2021.106576&partnerID=40&md5=0ffa2a351798df8cdf2c72434d1bea23,10.1016/j.cmpb.2021.106576,"Background and objective: Currently, the best performing methods in colonoscopy polyp detection are primarily based on deep neural networks (DNNs), which are usually trained on large amounts of labeled data. However, different hospitals use different endoscope models and set different imaging parameters, which causes the collected endoscopic images and videos to vary greatly in style. There may be variations in the color space, brightness, contrast, and resolution, and there are also differences between white light endoscopy (WLE) and narrow band image endoscopy (NBIE). We call these variations the domain shift. The DNN performance may decrease when the training data and the testing data come from different hospitals or different endoscope models. Additionally, it is quite difficult to collect enough new labeled data and retrain a new DNN model before deploying that DNN to a new hospital or endoscope model. Methods: To solve this problem, we propose a domain adaptation model called Deep Reconstruction-Recoding Network (DRRN), which jointly learns a shared encoding representation for two tasks: i) a supervised object detection network for labeled source data, and ii) an unsupervised reconstruction-recoding network for unlabeled target data. Through the DRRN, the object detection network's encoder not only learns the features from the labeled source domain, but also encodes useful information from the unlabeled target domain. Therefore, the distribution difference of the two domains’ feature spaces can be reduced. Results: We evaluate the performance of the DRRN on a series of cross-domain datasets. Compared with training the polyp detection network using only source data, the performance of the DRRN on the target domain is improved. Through feature statistics and visualization, it is demonstrated that the DRRN can learn the common distribution and feature invariance of the two domains. The distribution difference between the feature spaces of the two domains can be reduced. Conclusion: The DRRN can improve cross-domain polyp detection. With the DRRN, the generalization performance of the DNN-based polyp detection model can be improved without additional labeled data. This improvement allows the polyp detection model to be easily transferred to datasets from different hospitals or different endoscope models. © 2021 Elsevier B.V.",Adversarial learning; Domain adaptation; Multi center generalization; Polyp detection,,,Computer Methods and Programs in Biomedicine,Article,Scopus
25,,Detection of retinopathy disease using morphological gradient and segmentation approaches in fundus images,Toğaçar M.,2022,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120968775&doi=10.1016%2fj.cmpb.2021.106579&partnerID=40&md5=4fb8a71531693cd6c32c499cc0fdcf8b,10.1016/j.cmpb.2021.106579,"Background and objective: Diabetes-related cases can cause glaucoma, cataracts, optic neuritis, paralysis of the eye muscles, or various retinal damages over time. Diabetic retinopathy is the most common form of blindness that occurs with diabetes. Diabetic retinopathy is a disease that occurs when the blood vessels in the retina of the eye become damaged, leading to loss of vision in advanced stages. This disease can occur in any diabetic patient, and the most important factor in treating the disease is early diagnosis. Nowadays, deep learning models and machine learning methods, which are open to technological developments, are already used in early diagnosis systems. In this study, two publicly available datasets were used. The datasets consist of five types according to the severity of diabetic retinopathy. The objectives of the proposed approach in diabetic retinopathy detection are to positively contribute to the performance of CNN models by processing fundus images through preprocessing steps (morphological gradient and segmentation approaches). The other goal is to detect efficient sets from type-based activation sets obtained from CNN models using Atom Search Optimization method and increase the classification success. Methods: The proposed approach consists of three steps. In the first step, the Morphological Gradient method is used to prevent parasitism in each image, and the ocular vessels in fundus images are extracted using the segmentation method. In the second step, the datasets are trained with transfer learning models and the activations for each class type in the last fully connected layers of these models are extracted. In the last step, the Atom Search optimization method is used, and the most dominant activation class is selected from the extracted activations on a class basis. Results: When classified by the severity of diabetic retinopathy, an overall accuracy of 99.59% was achieved for dataset #1 and 99.81% for dataset #2. Conclusions: In this study, it was found that the overall accuracy achieved with the proposed approach increased. To achieve this increase, the application of preprocessing steps and the selection of the dominant activation sets from the deep learning models were implemented using the Atom Search optimization method. © 2021 Elsevier B.V.",Atom search optimization; Diabetic retinopathy; Morphological gradient and segmentation; Selection of dominant activations,,,Computer Methods and Programs in Biomedicine,Article,Scopus
26,,Patch-based 3D U-Net and transfer learning for longitudinal piglet brain segmentation on MRI,"Coupeau P., Fasquel J.-B., Mazerand E., Menei P., Montero-Menei C.N., Dinomais M.",2022,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120658149&doi=10.1016%2fj.cmpb.2021.106563&partnerID=40&md5=354693ab25d6c2844069606a79e1b7f7,10.1016/j.cmpb.2021.106563,"Background and Objectives:In order to study neural plasticity in immature brain following early brain lesion, large animal model are needed. Because of its morphological similarities with the human developmental brain, piglet is a suitable but little used one. Its study from Magnetic Resonance Imaging (MRI) requires the development of automatic algorithms for the segmentation of the different structures and tissues. A crucial preliminary step consists in automatically segmenting the brain. Methods: We propose a fully automatic brain segmentation method applied to piglets by combining a 3D patch-based U-Net and a post-processing pipeline for spatial regularization and elimination of false positives. Our approach also integrates a transfer-learning strategy for managing an automated longitudinal monitoring evaluated for four developmental stages (2, 6, 10 and 18 weeks), facing the issue of MRI changes resulting from the rapid brain development. It is compared to a 2D approach and the Brain Extraction Tool (BET) as well as techniques adapted to other animals (rodents, macaques). The influence of training patches size and distribution is studied as well as the benefits of spatial regularization. Results: Results show that our approach is efficient in terms of average Dice score (0.952) and Hausdorff distance (8.51), outperforming the use of a 2D U-Net (Dice: 0.919, Hausdorff distance: 11.06) and BET (Dice: 0.764, Hausdorff distance: 25.91). The transfer-learning strategy achieves a good performance on older piglets (Dice of 0.934 at 6 weeks, 0.956 at 10 weeks and 0.958 at 18 weeks) compared to a standard training strategy with few data (Dice of 0.636 at 6 weeks, 0.907 at 10 weeks, not calculable at 18 weeks because of too few training piglets). Conclusions: In conclusion, we provide a method for longitudinal MRI piglet brain segmentation based on 3D U-Net and transfer learning which can be used for future morphometric studies and applied to other animals. © 2021",3D U-Net; Brain development; MRI; Patches; Piglet brain segmentation; Transfer learning,,,Computer Methods and Programs in Biomedicine,Article,Scopus
27,,Domain- and task-specific transfer learning for medical segmentation tasks,"Zoetmulder R., Gavves E., Caan M., Marquering H.",2022,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120474842&doi=10.1016%2fj.cmpb.2021.106539&partnerID=40&md5=08b92c3497f02227d390c0556ac0f6a1,10.1016/j.cmpb.2021.106539,"Background and objectives: Transfer learning is a valuable approach to perform medical image segmentation in settings with limited cases available for training convolutional neural networks (CNN). Both the source task and the source domain influence transfer learning performance on a given target medical image segmentation task. This study aims to assess transfer learning-based medical segmentation task performance for various source task and domain combinations. Methods: CNNs were pre-trained on classification, segmentation, and self-supervised tasks on two domains: natural images and T1 brain MRI. Next, these CNNs were fine-tuned on three target T1 brain MRI segmentation tasks: stroke lesion, MS lesions, and brain anatomy segmentation. In all experiments, the CNN architecture and transfer learning strategy were the same. The segmentation accuracy on all target tasks was evaluated using the mIOU or Dice coefficients. The detection accuracy was evaluated for the stroke and MS lesion target tasks only. Results: CNNs pre-trained on a segmentation task on the same domain as the target tasks resulted in higher or similar segmentation accuracy compared to other source task and domain combinations. Pre-training a CNN on ImageNet resulted in a comparable, but not consistently higher lesion detection rate, despite the amount of training data used being 10 times larger. Conclusions: This study suggests that optimal transfer learning for medical segmentation is achieved with a similar task and domain for pre-training. As a result, CNNs can be effectively pre-trained on smaller datasets by selecting a source domain and task similar to the target domain and task. © 2021 Elsevier B.V.",Deep learning; Domain adaptation; MRI; Task transfer learning,,,Computer Methods and Programs in Biomedicine,Article,Scopus
28,,Classification of Imbalanced Electrocardiosignal Data using Convolutional Neural Network,"Du C., Liu P.X., Zheng M.",2022,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120425972&doi=10.1016%2fj.cmpb.2021.106483&partnerID=40&md5=c32fe27bf29167850ee48285eaa18d4c,10.1016/j.cmpb.2021.106483,"Background and objective: In the application of wearable heart-monitors, it is of great significance to analyze electrocardiogram (ECG) signals for anomaly detection. ECG arrhythmia classification remains an open problem in that it cannot easily recognize data from minority classes due to the imbalanced dataset and particular characteristic of the time series signal. In this study, a novel method is presented as a possible solution to imbalanced classification problems. Methods: An improved data augmentation method based on variational auto-encoder (VAE) and auxiliary classifier generative adversarial network (ACGAN) is implemented to address the difficulties resulting from the imbalanced dataset. Based on the augmented dataset, convolutional neural network (CNN) classifiers are employed to automatically recognize arrhythmias using two-dimensional ECG images. Results: In experimental studies conducted with the MIT-BIH arrhythmia database, the proposed method achieves 98.45% accuracy and 97.03% sensitivity. The sensitivities of two minority classes achieve 95.83% and 97.37%, respectively. Conclusion: In imbalanced classification, the sensitivity of minority class is a key evaluation indicator. One of the significant contributions of this study is that the proposed method can obtain higher sensitivity of minority class. The experimental results demonstrate that the proposed method for ECG arrhythmia calssification under imbalanced data has better performance compared with traditional cropping augmentation methods and traditional classifiers. © 2021",Convolutional nerual network; Data augmentation; Electrocardiogram arrhythmia; Imbalanced datasets,,,Computer Methods and Programs in Biomedicine,Article,Scopus
29,,Detection of ataxia with hybrid convolutional neural network using static plantar pressure distribution model in patients with multiple sclerosis,"Kaya M., Karakuş S., Tuncer S.A.",2022,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120173621&doi=10.1016%2fj.cmpb.2021.106525&partnerID=40&md5=cfbecb5119528f4ad244f1ded1efaeb5,10.1016/j.cmpb.2021.106525,"Objective: In this study, it is aimed to detect ataxia for Persons with Multiple Sclerosis (PwMS) through a deep learning-based approach using an image dataset containing static plantar pressure distribution. Here, an alternative and objective method will be proposed to assist physicians who diagnose PwMS in the early stages. Methods: A total of 406 static bipedal pressure distribution image data for 43 ataxic PwMS and 62 healthy individuals were used in the study. After preprocessing, these images were given as input to pre-trained deep learning models such as VGG16, VGG19, ResNet, DenseNet, MobileNet, and NasNetMobile. The data of each model is utilized to generate its feature vectors. Finally, feature vectors obtained from static pressure distribution images were classified by SVM (Support Vector Machine), K-NN (K-Nearest Neighbors), and ANN (Artificial Neural Network). In addition, a cross-validation method was used to examine the validity of the classifier. Results: The performance of the proposed models was evaluated with accuracy, sensitivity, specificity, and F1-measure criteria. The VGG19-SVM hybrid model showed the best performance with 95.12% acc, 94.91% sen, 95.31% spe, and 94.44% F1. Conclusions: In this study, a specific and sensitive automatic test evaluation system was proposed for Ataxic syndromes using digital images to observe the motor skills of the subjects. Comparative results show that the proposed method can be applied in practice for ataxia that is clinically difficult to detect or not yet symptomatic. It can be defined using only static plantar pressure distribution in the early stage and it can be recommended as an assistant system to physicians in clinical practice. © 2021 Elsevier B.V.",Ataxia; CNN; Deep learning; Multiple sclerosis; Static plantar pressure distribution,,,Computer Methods and Programs in Biomedicine,Article,Scopus
30,,Precise Learning of Source Code Contextual Semantics via Hierarchical Dependence Structure and Graph Attention Networks,"Zhao Z., Yang B., Li G., Liu H., Jin Z.",2022,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118759414&doi=10.1016%2fj.jss.2021.111108&partnerID=40&md5=b2e6deb1d8c0668848a5d5f83bc18516,10.1016/j.jss.2021.111108,"Deep learning is being used extensively in a variety of software engineering tasks, e.g., program classification and defect prediction. Although the technique eliminates the required process of feature engineering, the construction of source code model significantly affects the performance on those tasks. Most recent works was mainly focused on complementing AST-based source code models by introducing contextual dependencies extracted from CFG. However, all of them pay little attention to the representation of basic blocks, which are the basis of contextual dependencies. In this paper, we integrated AST and CFG and proposed a novel source code model embedded with hierarchical dependencies. Based on that, we also designed a neural network that depends on the graph attention mechanism. Specifically, we introduced the syntactic structural of the basic block, i.e., its corresponding AST, in source code model to provide sufficient information and fill the gap. We have evaluated this model on three practical software engineering tasks and compared it with other state-of-the-art methods. The results show that our model can significantly improve the performance. For example, compared to the best performing baseline, our model reduces the scale of parameters by 50% and achieves 4% improvement on accuracy on program classification task. © 2021 Elsevier Inc.",Abstract syntax Tree; Control flow graph; Deep learning; Graph neural network; Program analysis,,,Journal of Systems and Software,Article,Scopus
31,,Sharing runtime permission issues for developers based on similar-app review mining,"Gao H., Guo C., Bai G., Huang D., He Z., Wu Y., Xu J.",2022,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118496709&doi=10.1016%2fj.jss.2021.111118&partnerID=40&md5=ccaa53e71999220b75a58b9e7501e7e8,10.1016/j.jss.2021.111118,"The Android operating system introduces an ask-on-first-use permission policy after 6.0 version to regulate access to user data, which raises Permission-Related Issues (PRIS for short). Relevant research has been conducted to identify the PRIS through investigating users’ opinions towards runtime permissions. These efforts mainly focus on helping users understand and be aware of permissions, but neglect to assist developers in discovering permission requirements. In this paper, we propose a novel framework named PRISharer, which mines potential permission issues from the reviews of similar apps to assist developers in discovering possible permission requirements at runtime. PRISharer first builds a deep fine-grained classifier to identify similar apps, and then employs sentiment analysis based keywords extraction to mine permission-related reviews from similar apps’ reviews. Finally, the <category, permission, issues> mappings based on a multi-label learning method are generated to provide a PRIS profile for developers. The results of comparative experiments on more than 12 million reviews of 17,741 Android apps demonstrate that PRISharer achieves (i) superior performance in terms of F1-score for PRIS analysis, with an average improvement of 24.4%, (ii) the best recall (89.3%) in extracting permission-related reviews and (iii) 82.4% positive responses by expert developers, through which the effectiveness of PRISharer is well verified. © 2021 Elsevier Inc.",Android; Machine learning; Permission-Related Issues (PRIS); Runtime permission; User reviews,,,Journal of Systems and Software,Article,Scopus
32,,SHSE: A subspace hybrid sampling ensemble method for software defect number prediction,"Tong H., Lu W., Xing W., Liu B., Wang S.",2022,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117945474&doi=10.1016%2fj.infsof.2021.106747&partnerID=40&md5=9dd2a7f4d0d7c0b7cd0996e9e09f9932,10.1016/j.infsof.2021.106747,"Context: Software defect number prediction (SDNP) helps allocate limited testing resources by ranking software modules according to the predicted defect numbers. However, the highly skewed distribution of defects greatly degrades the performance of SDNP models by preventing SDNP models from ranking software modules accurately. Objective: This paper introduces a novel subspace hybrid sampling ensemble (SHSE) method based on feature subspace construction, hybrid sampling, and ensemble learning for building high-performance SDNP models. Method: Specifically, we first construct a series of feature subspace to ensure the diversity of base learners. In each of feature subspace, we then use the proposed hybrid sampling method to balance the training subset without losing too much information and introducing lots of noisy data caused by only using undersampling or oversampling techniques. Finally, we train each base learner and combine them by using the proposed weighted ensemble strategy. Experiments are performed on 27 public defect datasets. We compare SHSE with five state-of-the-art resampling-based models and four zero-inflated/hurdle models in terms of the ranking performance measure fault-percentile-average (FPA). To demonstrate the effectiveness of SHSE, two statistical testing methods including Wilcoxon Signed-rank test and Scott–Knott Effect Size Difference test are utilized. Cliff's δ is also computed for quantifying the difference when there is significant difference between SHSE and each baseline. Results: The experimental results show that SHSE significantly outperforms the baselines and improves the performance over each baseline with as least medium effect size on most datasets. On average, SHSE improves the performance over the resampling-based methods by 8.7%∼14.4% and the zero-inflate/hurdle models by 10.3%∼15.2%. Conclusion: It can be concluded that SHSE is a more promising alternative for software defect number prediction. © 2021 Elsevier B.V.",Ensemble learning; Feature subspace; Hybrid sampling for regression; Imbalanced data; Software defect number prediction,,,Information and Software Technology,Article,Scopus
33,,An empirical evaluation of deep learning-based source code vulnerability detection: Representation versus models,"Semasaba A.O.A., Zheng W., Wu X., Agyemang S.A., Liu T., Ge Y.",2022,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123353974&doi=10.1002%2fsmr.2422&partnerID=40&md5=71afa0dd30ad9234d1b780c0e41f35f6,10.1002/smr.2422,"Vulnerabilities in the source code of the software are critical issues in the realm of software engineering. Coping with vulnerabilities in software source code is becoming more challenging due to several aspects such as complexity and volume. Deep learning has gained popularity throughout the years as a means of addressing such issues. This paper proposes an evaluation of vulnerability detection performance on source code representations and evaluates how machine learning (ML) strategies can improve them. The structure of our experiment consists of three deep neural networks (DNNs) in conjunction with five different source code representations: abstract syntax trees (ASTs), code gadgets (CGs), semantics-based vulnerability candidates (SeVCs), lexed code representations (LCRs), and composite code representations (CCRs). Experimental results show that employing different ML strategies in conjunction with the base model structure influences the performance results to a varying degree. However, ML-based techniques suffer from poor performance on class imbalance handling and dimensionality reduction when used in conjunction with source code representations. © 2022 John Wiley & Sons, Ltd.",,,,Journal of Software: Evolution and Process,Article,Scopus
34,,"Machine Learning Testing: Survey, Landscapes and Horizons","Zhang J.M., Harman M., Ma L., Liu Y.",2022,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123224381&doi=10.1109%2fTSE.2019.2962027&partnerID=40&md5=6bcb2de3ce3cbcef90ece1cf886dda83,10.1109/TSE.2019.2962027,"This paper provides a comprehensive survey of techniques for testing machine learning systems; Machine Learning Testing (ML testing) research. It covers 144 papers on testing properties (e.g., correctness, robustness, and fairness), testing components (e.g., the data, learning program, and framework), testing workflow (e.g., test generation and test evaluation), and application scenarios (e.g., autonomous driving, machine translation). The paper also analyses trends concerning datasets, research trends, and research focus, concluding with research challenges and promising research directions in ML testing. © 1976-2012 IEEE.",deep neural network; Machine learning; software testing,,,IEEE Transactions on Software Engineering,Article,Scopus
35,,An Empirical Study of Model-Agnostic Techniques for Defect Prediction Models,"Jiarpakdee J., Tantithamthavorn C.K., Dam H.K., Grundy J.",2022,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123222595&doi=10.1109%2fTSE.2020.2982385&partnerID=40&md5=3ee77f77a734899419d4d565aba3f0ef,10.1109/TSE.2020.2982385,"Software analytics have empowered software organisations to support a wide range of improved decision-making and policy-making. However, such predictions made by software analytics to date have not been explained and justified. Specifically, current defect prediction models still fail to explain why models make such a prediction and fail to uphold the privacy laws in terms of the requirement to explain any decision made by an algorithm. In this paper, we empirically evaluate three model-agnostic techniques, i.e., two state-of-the-art Local Interpretability Model-agnostic Explanations technique (LIME) and BreakDown techniques, and our improvement of LIME with Hyper Parameter Optimisation (LIME-HPO). Through a case study of 32 highly-curated defect datasets that span across 9 open-source software systems, we conclude that (1) model-agnostic techniques are needed to explain individual predictions of defect models; (2) instance explanations generated by model-agnostic techniques are mostly overlapping (but not exactly the same) with the global explanation of defect models and reliable when they are re-generated; (3) model-agnostic techniques take less than a minute to generate instance explanations; and (4) more than half of the practitioners perceive that the contrastive explanations are necessary and useful to understand the predictions of defect models. Since the implementation of the studied model-agnostic techniques is available in both Python and R, we recommend model-agnostic techniques be used in the future. © 1976-2012 IEEE.",defect prediction models; Explainable software analytics; model-agnostic techniques; software quality assurance,"166, 185",,IEEE Transactions on Software Engineering,Article,Scopus
36,,Are duplicates really harmful? An empirical study on bug report summarization techniques,"Hao R., Li Y., Feng Y., Chen Z.",2022,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122964806&doi=10.1002%2fsmr.2424&partnerID=40&md5=7dd4f4c2743cbf4dbe9cb35b6906dafd,10.1002/smr.2424,"Recent research works have proven that duplicate bug reports can provide helpful information to assist developers in software tasks such as fault localization and program fixing, while thoroughly reading duplicate bug reports is time-consuming and inefficient. Summarization is a possible solution for gaining essential information quickly. However, there are many challenges when applying existing summarizing techniques on duplicate bug reports. Duplicate bug reports describe the same problem from different views and vary in quality, content, and writing style. Moreover, the code snippet understanding and the semantic gap between natural and programming languages make the summary generation even more difficult. Thus, in this paper, we want to investigate whether the state-of-the-art summarization approaches can overcome the resistance and generate an effective summary for duplicate bug reports. We collected more than 8,000 groups of duplicate reports from GitHub and labeled 60 groups with 149 reports manually for the evaluation. Results showed that although the existing summarization approaches can work on duplicate bug reports, there are significant differences between them when it comes to code snippet summarization. Moreover, several methods can be very sluggish for summarizing long bug reports. Our study provides insights and guidelines for choosing proper summarization approaches in different scenarios. © 2022 John Wiley & Sons, Ltd.",,,,Journal of Software: Evolution and Process,Article,Scopus
37,,DDA-Net: Unsupervised cross-modality medical image segmentation via dual domain adaptation,"Bian X., Luo X., Wang C., Liu W., Lin X.",2022,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122536153&doi=10.1016%2fj.cmpb.2021.106531&partnerID=40&md5=739da11015e24adb65c92a89e6e28c33,10.1016/j.cmpb.2021.106531,"Background and Objective: Deep convolutional networks are powerful tools for single-modality medical image segmentation, whereas generally require semantic labelling or annotation that is laborious and time-consuming. However, domain shift among various modalities critically deteriorates the performance of deep convolutional networks if only trained by single-modality labelling data. Methods: In this paper, we propose an end-to-end unsupervised cross-modality segmentation network, DDA-Net, for accurate medical image segmentation without semantic annotation or labelling on the target domain. To close the domain gap, different images with domain shift are mapped into a shared domain-invariant representation space. In addition, spatial position information, which benefits the spatial structure consistency for semantic information, is preserved by an introduced cross-modality auto-encoder. Results: We validated the proposed DDA-Net method on cross-modality medical image datasets of brain images and heart images. The experimental results show that DDA-Net effectively alleviates domain shift and suppresses model degradation. Conclusions: The proposed DDA-Net successfully closes the domain gap between different modalities of medical image, and achieves state-of-the-art performance in cross-modality medical image segmentation. It also can be generalized for other semi-supervised or unsupervised segmentation tasks in some other field. © 2021",Cross-modality; Domain adaptation; Medical image; Segmentation; Unsupervised learning,,,Computer Methods and Programs in Biomedicine,Article,Scopus
38,,A graph sequence neural architecture for code completion with semantic structure features,"Yang K., Yu H., Fan G., Yang X., Huang Z.",2022,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122136280&doi=10.1002%2fsmr.2414&partnerID=40&md5=970b6c7fa148f561f4d2147c0e4ad322,10.1002/smr.2414,"Code completion plays an important role in intelligent software development for accelerating coding efficiency. Recently, the prediction models based on deep learning have achieved good performance in code completion task. However, the existing models cannot avoid three drawbacks: (i) In the existing models, the code representation loses the information (parent–child information between nodes) and lacks many effective features (orientation between nodes). (ii) The known code structure information is not fully utilized, which will cause the model to generate completely irrelevant results. (iii) Simple sequence modeling ignores repeated patterns and structural information. Besides, previous works cannot capture the characteristics of correlation and directionality between nodes. In this paper, we propose a Code Completion approach named CC-GGNN, which is graph model based on Gated Graph Neural Networks (GGNNs) to address the problems. We introduce a new architecture to obtain the effective code features from code representation. In order to utilize the known information, we propose Classification Mechanism, which classifies the representation of the node using the known parent node and constructs training graph in the model. The experimental results show that our model outperforms the state-of-the-art methods MRR@5 at most 9.2% and ACC at most 11.4% in datasets. © 2022 John Wiley & Sons, Ltd.",,,,Journal of Software: Evolution and Process,Article,Scopus
39,,Rap4DQ: Learning to recommend relevant API documentation for developer questions,"Li Y., Wang S., Wang W., Nguyen T.N., Wang Y., Ye X.",2022,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120156541&doi=10.1007%2fs10664-021-10067-5&partnerID=40&md5=a01d7c9dbd39aae0182f474653c8f1d8,10.1007/s10664-021-10067-5,"Developers often face difficulties in using different API methods during the software development process. Answering API related questions on API Q&A forums often costs API development teams a lot of time. To help save time for API development teams, we propose a deep learning-based approach, namely Rap4DQ, to identify relevant web API documentation for developer’s API related questions on API Q&A forums. Rap4DQ learns representation vectors for questions and API documentation separately using Gated Recurrent Unit (GRU) and adds different weights to reflect the various importance of varied API documents during training. Rap4DQ is designed to train on positive and negative samples with a loss function that minimizes the distances between questions and their relevant documentation, but maximizes the distances between questions and their irrelevant documentation. In the end, we construct a learning-to-rank layer to rank the API documentation based on learned representation vectors from GRUs. We have conducted several experiments to evaluate Rap4DQ on three popular and large API Q&A forums, Twitter, eBay, and AdWords. The results show that Rap4DQ can outperform all baselines by having a relative improvement up to 84.3% in terms of AUC. Rap4DQ can obtain a high AUC of 0.84, 0.88, and 0.94 on identifying relevant API documentation on Twitter, eBay, and AdWords, respectively. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",API documentation; Deep learning; Developer forums; Learning-to-Rank; Question answering,,,Empirical Software Engineering,Article,Scopus
40,,ECSD-Net: A joint optic disc and cup segmentation and glaucoma classification network based on unsupervised domain adaptation,"Liu B., Pan D., Shuai Z., Song H.",2022,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119428452&doi=10.1016%2fj.cmpb.2021.106530&partnerID=40&md5=6adfa968c9e476f1e610adf8a8a3e906,10.1016/j.cmpb.2021.106530,"Background and objectives: Glaucoma can cause irreversible vision loss and even blindness, and early diagnosis can help prevent vision loss. Analyzing the optic disc and optic cup helps diagnose glaucoma, which motivates many computer-aided diagnosis methods based on deep learning networks. However, the performance of the trained model on new datasets is seriously hindered due to the distribution gap between different datasets. Therefore, we aim to develop an unsupervised learning method to solve this problem and improve the prediction performance of the model on new datasets. Methods: In this paper, we propose a novel unsupervised model based on adversarial learning to perform the optic disc and cup segmentation and glaucoma screening tasks in a more generalized and efficient manner. We adopt an efficient segmentation and classification network and employ unsupervised domain adaptation technology on the output space of the segmentation network to solve the domain shift problem. We conduct glaucoma screening task by combining classification and segmentation networks to obtain more stable and efficient glaucoma screening prediction. Results: We verify the effectiveness and efficiency of our proposed method on three public datasets, the REFUGE, DRISHTI-GS and RIM-ONE-r3 datasets. The experimental results demonstrate that the proposed method can effectively alleviate the deterioration of segmentation performance caused by domain shift and improve the accuracy of glaucoma screening. Furthermore, the proposed method outperforms state-of-the-art unsupervised optic disc and cup segmentation domain adaptation methods. Conclusions: The proposed method can assist clinicians in screening and diagnosis of glaucoma and is suitable for real-world applications. © 2021 Elsevier B.V.",Adversarial learning; Domain adaptation; Glaucoma screening; Optic cup segmentation; Optic disc segmentation,,,Computer Methods and Programs in Biomedicine,Review,Scopus
41,,Improved U-Net based on contour prediction for efficient segmentation of rectal cancer,"Li D., Chu X., Cui Y., Zhao J., Zhang K., Yang X.",2022,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118575108&doi=10.1016%2fj.cmpb.2021.106493&partnerID=40&md5=be7deee15d1ef3b1ce0bd13457bfb741,10.1016/j.cmpb.2021.106493,"Background and objective: Segmentation of rectal cancerous regions using 2D Magnetic Resonance Imaging (MRI) images is a critical step in radiation therapy. The shape of rectal cancer has significant variations and the shape of some surrounding organs is similar to that of rectal cancer; these conditions significantly affect the segmentation accuracy of rectal cancer and lead to incorrect segmentation. Therefore, automatic segmentation of rectal cancer is urgently needed, and it is a great challenge. For this task, the existing deep learning-based approaches have two shortcomings: 1) The U-Net network plays an important role in the field of medical segmentation. However, the designs of encoders and decoders in traditional U-Net networks are relatively simple and cannot extract good features, resulting in incorrect segmentation results. 2) Conventional neural networks extract high-level features that often do not include sufficient high-resolution contour information, resulting in ambiguity in contour segmentation. In this paper, we propose an improved U-Net network based on contour prediction, aiming at effective segmentation of rectal cancer. Methods: We designed a new U-Net network by improving the traditional U-Net network. We made four improvements: 1) We replaced the encoders with the SENet network. 2) A global pooling layer was added after the last encoder. 3) We added the Spatial and Channel Squeeze & Excitation (SCSE) attention mechanism module to each decoder. 4) We concatenated the output results of each decoder. In addition, the model implemented content segmentation and contour segmentation for rectal cancer in parallel, so that both the content and contour information was learned by the network to enhance the segmentation accuracy. Results: Our data were obtained from the Shanxi Provincial Cancer Hospital and included 3773 2D MRI rectal cancer images. The proposed method achieved an Mean Intersection over Union of 0.894 (MIoU) on the test set. Compared with state-of-the-art methods, our method had the best performance on the test set, and its MIoU metric was 0.123 higher than that of the second-best model. At the same time, the effectiveness of the improvements to our method was demonstrated through ablation experiments. Conclusions: Our method can help radiologists to segment effectively, save their time and energy, and enable them to focus on cases that are not easily segmented because of the complex shape of rectal cancer. © 2021",Artificial intelligence; Rectal cancer; Semantic segmentation; U-Net,,,Computer Methods and Programs in Biomedicine,Article,Scopus
42,,One-dimensional convolutional neural network and hybrid deep-learning paradigm for classification of specific language impaired children using their speech,"Sharma Y., Singh B.K.",2022,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118532070&doi=10.1016%2fj.cmpb.2021.106487&partnerID=40&md5=25118e3c3ae17b4a229f8e7ad2158515,10.1016/j.cmpb.2021.106487,Background and objective: Screening children for communicational disorders such as specific language impairment (SLI) is always challenging as it requires clinicians to follow a series of steps to evaluate the subjects. Artificial intelligence and computer-aided diagnosis have supported health professionals in making swift and error-free decisions about the neurodevelopmental state of children vis-à-vis language comprehension and production. Past studies have claimed that typical developing (TD) and SLI children show distinct vocal characteristics that can serve as discriminating facets between them. The objective of this study is to group children in SLI or TD categories by processing their raw speech signals using two proposed approaches: a customized convolutional neural network (CNN) model and a hybrid deep-learning framework where CNN is combined with long-short-term-memory (LSTM). Method: We considered a publicly available speech database of SLI and typical children of Czech accents for this study. The convolution filters in both the proposed CNN and hybrid models (CNN-LSTM) estimated fuzzy-automated features from the speech utterance. We performed the experiments in five separate sessions. Data augmentations were performed in each of those sessions to enhance the training strength. Results: Our hybrid model exhibited a perfect 100% accuracy and F-measure for almost all the session-trials compared to CNN alone which achieved an average accuracy close to 90% and F-measure ≥ 92%. The models have further illustrated their robust classification essences by securing values of reliability indexes over 90%. Conclusion: The results confirm the effectiveness of proposed approaches for the detection of SLI in children using their raw speech signals. Both the models do not require any dedicated feature extraction unit for their operations. The models may also be suitable for screening SLI and other neurodevelopmental disorders in children of different linguistic accents. © 2021 Elsevier B.V.,Convolutional neural network; Long-short-term-memory; Neurodevelopmental disorder; Specific language impairment; Speech signal processing,,,Computer Methods and Programs in Biomedicine,Article,Scopus
43,,Conclusion stability for natural language based mining of design discussions,"Mahadi A., Ernst N.A., Tongay K.",2022,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117726027&doi=10.1007%2fs10664-021-10009-1&partnerID=40&md5=6fb1cafc8822987601a714d4a6249c9e,10.1007/s10664-021-10009-1,"Developer discussions range from in-person hallway chats to comment chains on bug reports. Being able to identify discussions that touch on software design would be helpful in documentation and refactoring software. Design mining is the application of machine learning techniques to correctly label a given discussion artifact, such as a pull request, as pertaining (or not) to design. In this paper we demonstrate a simple example of how design mining works. We then show how conclusion stability is poor on different artifact types and different projects. We show two techniques—augmentation and context specificity—that greatly improve the conclusion stability and cross-project relevance of design mining. Our new approach achieves AUC of 0.88 on within dataset classification and 0.80 on the cross-dataset classification task. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",Conclusion stability; Mining software design; Supervised learning,,,Empirical Software Engineering,Article,Scopus
44,,Information retrieval versus deep learning approaches for generating traceability links in bilingual projects,"Lin J., Liu Y., Cleland-Huang J.",2022,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117709914&doi=10.1007%2fs10664-021-10050-0&partnerID=40&md5=fb18745b51cfea856892a90ae4156f24,10.1007/s10664-021-10050-0,"Software traceability links are established between diverse artifacts of the software development process in order to support tasks such as compliance analysis, safety assurance, and requirements validation. However, practice has shown that it is difficult and costly to create and maintain trace links in non-trivially sized projects. For this reason, many researchers have proposed and evaluated automated approaches based on information retrieval and deep-learning. Generating trace links automatically can also be challenging – especially in multi-national projects which include artifacts written in multiple languages. The intermingled language use can reduce the efficiency of automated tracing solutions. In this work, we analyze patterns of intermingled language that we observed in several different projects, and then comparatively evaluate different tracing algorithms. These include Information Retrieval techniques, such as the Vector Space Model (VSM), Latent Semantic Indexing (LSI), Latent Dirichlet Allocation (LDA), and various models that combine mono- and cross-lingual word embeddings with the Generative Vector Space Model (GVSM), and a deep-learning approach based on a BERT language model. Our experimental analysis of trace links generated for 14 Chinese-English projects indicates that our MultiLingual Trace-BERT approach performed best in large projects with close to 2-times the accuracy of the best IR approach, while the IR-based GVSM with neural machine translation and a monolingual word embedding performed best on small projects. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",BERT; Cross-lingual information retrieval; Generalized Vector Space Model; Software traceability,,,Empirical Software Engineering,Article,Scopus
45,,Better Data Labelling with EMBLEM (and how that Impacts Defect Prediction),"Tu H., Yu Z., Menzies T.",2022,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098414166&doi=10.1109%2fTSE.2020.2986415&partnerID=40&md5=638abf74d76cd60698871916833f90ae,10.1109/TSE.2020.2986415,"Standard automatic methods for recognizing problematic development commits can be greatly improved via the incremental application of human+artificial expertise. In this approach, called EMBLEM, an AI tool first explore the software development process to label commits that are most problematic. Humans then apply their expertise to check those labels (perhaps resulting in the AI updating the support vectors within their SVM learner). We recommend this human+AI partnership, for several reasons. When a new domain is encountered, EMBLEM can learn better ways to label which comments refer to real problems. Also, in studies with 9 open source software projects, labelling via EMBLEM's incremental application of human+AI is at least an order of magnitude cheaper than existing methods (≈ eight times). Further, EMBLEM is very effective. For the data sets explored here, EMBLEM better labelling methods significantly improved P_opt20Popt20 and G-scores performance in nearly all the projects studied here. © 1976-2012 IEEE.",data labelling; defect prediction; Human-in-the-loop AI; software analytics,"278, 294",,IEEE Transactions on Software Engineering,Article,Scopus
46,,Implementation of Neural Machine Translation for Nahuatl as a Web Platform: A Focus on Text Translation,"García S.K.B., Lucero E.S., Huerta E.B., Hernández J.C.H., Cruz J.F.R., Méndez B.E.P.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121783249&doi=10.1134%2fS0361768821080168&partnerID=40&md5=5a2d3072a71fa95ec01b6f77d30cca62,10.1134/S0361768821080168,"Abstract: There are few on-line platforms related to Natural Language Processing and zero services of machine translation for Nahuatl as a low-resource language. However, Nahuatl has had academical implementations on machine translation, from Statistical Machine Translation (SMT) to Neural Machine Translation (NMT), in specific Recurrent Neural Networks (RNNs). This research aims to create a platform that can address this issue with text, voice and Text-To-Speech features. In particular, the current paper presents several advancements on text translation as a comparative analysis between two attention architectures, transformers and RNNs using several models that combine such architectures, two parallel corpuses, and two tokenization techniques. Additionally, the development of a platform and iOS application client is described. A new and bigger corpus, over 35,000 pairs, is made to improve the state of the art, where a conscious cleaning of it shows a reduction on the religious bias presented on the source text. The model performance is evaluated with % BLEU in order to conduct a direct comparative on previous Nahuatl machine translation works. The results outperformed those works with a score of 66.45 at best using transformers compared to 34.78 and 14.28 for RNNs and SMT respectively, confirming that transformers and a sub-word tokenization are the best combination so far for Nahuatl Machine translation. Moreover, emerging behaviors were observed in the Transformers, where a subtle pleonasm seen only in rural locations where Mexican Spanish is spoken arouse from the model, linking its origin to Nahuatl, as well as the ability of the model of transforming numbers from base 10 to base 20. Finally, some out of corpus translations were presented to a Nahuatl speaker where the model demonstrated a good performance and retention of information for its size. This research seeks to be used as a framework of how a polysynthetic language can be manipulated to be used for different languages like Spanish, English or Russian. This research work was carried out at the “Tecnológico Nacional de México” (TecNM), campus “Instituto Tecnológico de Apizaco” (ITA). © 2021, Pleiades Publishing, Ltd.",,"778, 792",,Programming and Computer Software,Article,Scopus
47,,An Empirical Study on Heterogeneous Defect Prediction Approaches,"Chen H., Jing X.-Y., Li Z., Wu D., Peng Y., Huang Z.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121752915&doi=10.1109%2fTSE.2020.2968520&partnerID=40&md5=159e01b345db5d8f6a3bf4e08ff112e4,10.1109/TSE.2020.2968520,"Software defect prediction has always been a hot research topic in the field of software engineering owing to its capability of allocating limited resources reasonably. Compared with cross-project defect prediction (CPDP), heterogeneous defect prediction (HDP) further relaxes the limitation of defect data used for prediction, permitting different metric sets to be contained in the source and target projects. However, there is still a lack of a holistic understanding of existing HDP studies due to different evaluation strategies and experimental settings. In this paper, we provide an empirical study on HDP approaches. We review the research status systematically and compare the HDP approaches proposed from 2014 to June 2018. Furthermore, we also investigate the feasibility of HDP approaches in CPDP. Through extensive experiments on 30 projects from five datasets, we have the following findings: (1) metric transformation-based HDP approaches usually result in better prediction effects, while metric selection-based approaches have better interpretability. Overall, the HDP approach proposed by Li et al. (CTKCCA) currently has the best performance. (2) Handling class imbalance problems can boost the prediction effects, but the improvements are usually limited. In addition, utilizing mixed project data cannot improve the performance of HDP approaches consistently since the label information in the target project is not used effectively. (3) HDP approaches are feasible for cross-project defect prediction in which the source and target projects have the same metric set. © 1976-2012 IEEE.",cross-project; empirical study; Heterogeneous defect prediction; metric selection; metric transformation,"2803, 2822",,IEEE Transactions on Software Engineering,Article,Scopus
48,,Whence to Learn? Transferring Knowledge in Configurable Systems Using BEETLE,"Krishna R., Nair V., Jamshidi P., Menzies T.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121700933&doi=10.1109%2fTSE.2020.2983927&partnerID=40&md5=b30fd3925c57a126bb451fb3ab0670b4,10.1109/TSE.2020.2983927,"As software systems grow in complexity and the space of possible configurations increases exponentially, finding the near-optimal configuration of a software system becomes challenging. Recent approaches address this challenge by learning performance models based on a sample set of configurations. However, collecting enough sample configurations can be very expensive since each such sample requires configuring, compiling, and executing the entire system using a complex test suite. When learning on new data is too expensive, it is possible to use Transfer Learning to 'transfer' old lessons to the new context. Traditional transfer learning has a number of challenges, specifically, (a) learning from excessive data takes excessive time, and (b) the performance of the models built via transfer can deteriorate as a result of learning from a poor source. To resolve these problems, we propose a novel transfer learning framework called BEETLE, which is a 'bellwether'-based transfer learner that focuses on identifying and learning from the most relevant source from amongst the old data. This paper evaluates BEETLE with 57 different software configuration problems based on five software systems (a video encoder, an SAT solver, a SQL database, a high-performance C-compiler, and a streaming data analytics tool). In each of these cases, BEETLE found configurations that are as good as or better than those found by other state-of-the-art transfer learners while requiring only a fraction {1}{7}17th of the measurements needed by those other methods. Based on these results, we say that BEETLE is a new high-water mark in optimally configuring software. © 1976-2012 IEEE.",bellwether; Performance optimization; SBSE; transfer learning,"2956, 2972",,IEEE Transactions on Software Engineering,Article,Scopus
49,,Recommendation Approach Based on Attentive Federated Distillation [一种基于注意力联邦蒸馏的推荐方法],"Chen M., Zhang L., Ma T.-Y.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120857342&doi=10.13328%2fj.cnki.jos.006128&partnerID=40&md5=5785c93fa85e0b641b1805b31eb552bb,10.13328/j.cnki.jos.006128,"Data privacy protection has become one of the major challenges of recommendation systems. With the release of the Cybersecurity Law of the People's Republic of China and the general data protection regulation in the European Union, data privacy and security have become a worldwide concern. Federated learning can train the global model without exchanging user data, thus protecting users' privacy. Nevertheless, federated learning is still facing many issues, such as the small size of local data in each device, over-fitting of local model, and the data sparsity, which makes it difficult to reach higher accuracy. Meanwhile, with the advent of 5G (the 5th generation mobile communication technology) era, the data volume and transmission rate of personal devices are expected to be 10 to 100 times higher than the current ones, which requires higher model efficiency. Knowledge distillation can transfer the knowledge from the teacher model to a more compact student model so that the student model can approach or surpass the performance of teacher model, thus effectively solve the problems of large model parameter and high communication cost. However, the accuracy of student model is lower than teacher model after knowledge distillation. Therefore, a federated distillation approach is proposed with attentional mechanisms for recommendation systems. First, the method introduces Kullback-Leibler divergence and regularization term to the objective function of federated distillation to reduce the impact of heterogeneity between teacher network and student network; then it introduces multi-head attention mechanism to improve model accuracy by adding information to the embeddings. Finally, an improved adaptive training mechanism is introduced for learning rate to automatically switch optimizers and choose appropriate learning rates, thus increasing convergence speed of model. Experiment results validate efficiency of the proposed methods: compared to the baselines, the training time of the proposed model is reduced by 52%, the accuracy is increased by 13%, the average error is reduced by 17%, and the NDCG is increased by 10%. © Copyright 2021, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Attentive mechanism; Distributed learning; Federated distillation; Federated learning; Recommendation systems,"3852, 3868",,Ruan Jian Xue Bao/Journal of Software,Article,Scopus
50,,Corpus Construction for Chinese Zero Anaphora from Discourse Perspective [篇章视角的汉语零指代语料库构建],"Kong F., Ge H.-Z., Zhou G.-D.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120855590&doi=10.13328%2fj.cnki.jos.006119&partnerID=40&md5=5206f8dc2203be2fee546ed398c10ce1,10.13328/j.cnki.jos.006119,"As a common phenomenon in Chinese, zero anaphora plays an important role in many natural language processing tasks, such as machine translation, text summarization and machine reading comprehension. Currently, it has become a research hotspot in the field of natural language processing. Towards better discourse analysis, this study proposes a representation architecture for Chinese zero anaphora from the discourse perspective. Firstly, the elementary discourse unit is taken as the investigation object to determine whether it contains zero elements. Secondly, according to the roles of zero elements in the elementary discourse unit, the zero elements are divided into two categories: the core type and the modifier type. Thirdly, the discourse rhetorical tree of the paragraph is used as the basic unit to evaluate the Chinese zero coreferential relationship. According to the positional relationship between the antecedent and the zero element, the coreferential relationship is classified into two types, i.e., Intra-EDU and Inter-EDU. After that, for Inter-EDU type, the coreferential relationship is furtherly divided into four categories according to the status of the antecedent, i.e., entity, event, union, and others. Finally, this study selects the overlapped 325 texts of the Chinese treebank (CTB), the connective-driven Chinese discourse treebank (CDTB), and the OntoNotes corpus to annotate the Chinese zero anaphora. System evaluation shows the high quality of the constructed corpus for Chinese zero anaphora. Moreover, a complete zero anaphor resolution baseline system is constructed to show the appropriateness and the effectiveness of the proposed representation architecture for Chinese zero anaphora from computability perspective. © Copyright 2021, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Corpus construction; Discourse analysis; Elementary discourse unit; Zero anaphora; Zero pronouns,"3782, 3801",,Ruan Jian Xue Bao/Journal of Software,Article,Scopus
51,,Evaluation of a machine learning classifier for metamodels,"Nguyen P.T., Di Rocco J., Iovino L., Di Ruscio D., Pierantonio A.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114606004&doi=10.1007%2fs10270-021-00913-x&partnerID=40&md5=a23dbe04192a75afa1ec7fd1028640f7,10.1007/s10270-021-00913-x,"Modeling is a ubiquitous activity in the process of software development. In recent years, such an activity has reached a high degree of intricacy, guided by the heterogeneity of the components, data sources, and tasks. The democratized use of models has led to the necessity for suitable machinery for mining modeling repositories. Among others, the classification of metamodels into independent categories facilitates personalized searches by boosting the visibility of metamodels. Nevertheless, the manual classification of metamodels is not only a tedious but also an error-prone task. According to our observation, misclassification is the norm which leads to a reduction in reachability as well as reusability of metamodels. Handling such complexity requires suitable tooling to leverage raw data into practical knowledge that can help modelers with their daily tasks. In our previous work, we proposed AURORA as a machine learning classifier for metamodel repositories. In this paper, we present a thorough evaluation of the system by taking into consideration different settings as well as evaluation metrics. More importantly, we improve the original AURORA tool by changing its internal design. Experimental results demonstrate that the proposed amendment is beneficial to the classification of metamodels. We also compared our approach with two baseline algorithms, namely gradient boosted decision tree and support vector machines. Eventually, we see that AURORA outperforms the baselines with respect to various quality metrics. © 2021, The Author(s).",GBDT; Machine learning; Model-driven engineering; Neural networks; SVM,"1797, 1821",,Software and Systems Modeling,Article,Scopus
52,,Learning software configuration spaces: A systematic literature review,"Pereira J.A., Acher M., Martin H., Jézéquel J.-M., Botterweck G., Ventresque A.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114450561&doi=10.1016%2fj.jss.2021.111044&partnerID=40&md5=ecba3100562f3148699814dafb924493,10.1016/j.jss.2021.111044,"Most modern software systems (operating systems like Linux or Android, Web browsers like Firefox or Chrome, video encoders like ffmpeg, x264 or VLC, mobile and cloud applications, etc.) are highly configurable. Hundreds of configuration options, features, or plugins can be combined, each potentially with distinct functionality and effects on execution time, security, energy consumption, etc. Due to the combinatorial explosion and the cost of executing software, it is quickly impossible to exhaustively explore the whole configuration space. Hence, numerous works have investigated the idea of learning it from a small sample of configurations’ measurements. The pattern “sampling, measuring, learning” has emerged in the literature, with several practical interests for both software developers and end-users of configurable systems. In this systematic literature review, we report on the different application objectives (e.g., performance prediction, configuration optimization, constraint mining), use-cases, targeted software systems, and application domains. We review the various strategies employed to gather a representative and cost-effective sample. We describe automated software techniques used to measure functional and non-functional properties of configurations. We classify machine learning algorithms and how they relate to the pursued application. Finally, we also describe how researchers evaluate the quality of the learning process. The findings from this systematic review show that the potential application objective is important; there are a vast number of case studies reported in the literature related to particular domains or software systems. Yet, the huge variant space of configurable systems is still challenging and calls to further investigate the synergies between artificial intelligence and software engineering. © 2021 Elsevier Inc.",Configurable systems; Machine learning; Software product lines; Systematic literature review,,,Journal of Systems and Software,Article,Scopus
53,,MLaaS4HEP: Machine Learning as a Service for HEP,"Kuznetsov V., Giommi L., Bonacorsi D.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109294734&doi=10.1007%2fs41781-021-00061-3&partnerID=40&md5=650bc574b1956c60e0d20c5d3afe9411,10.1007/s41781-021-00061-3,"Machine Learning (ML) will play a significant role in the success of the upcoming High-Luminosity LHC (HL-LHC) program at CERN. An unprecedented amount of data at the exascale will be collected by LHC experiments in the next decade, and this effort will require novel approaches to train and use ML models. In this paper, we discuss a Machine Learning as a Service pipeline for HEP (MLaaS4HEP) which provides three independent layers: a data streaming layer to read High-Energy Physics (HEP) data in their native ROOT data format; a data training layer to train ML models using distributed ROOT files; a data inference layer to serve predictions using pre-trained ML models via HTTP protocol. Such modular design opens up the possibility to train data at large scale by reading ROOT files from remote storage facilities, e.g., World-Wide LHC Computing Grid (WLCG) infrastructure, and feed the data to the user’s favorite ML framework. The inference layer implemented as TensorFlow as a Service (TFaaS) may provide an easy access to pre-trained ML models in existing infrastructure and applications inside or outside of the HEP domain. In particular, we demonstrate the usage of the MLaaS4HEP architecture for a physics use-case, namely, the tt¯ Higgs analysis in CMS originally performed using custom made Ntuples. We provide details on the training of the ML model using distributed ROOT files, discuss the performance of the MLaaS and TFaaS approaches for the selected physics analysis, and compare the results with traditional methods. © 2021, The Author(s).",BigData; Data management; LHC; Machine learning,,,Computing and Software for Big Science,Article,Scopus
54,,Human-cyber-physical Services Dispatch Approach for Data Characteristics [面向数据特征的人机物融合服务分派方法],"Yuan M., Chen Z., Xu B.-Q.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118955148&doi=10.13328%2fj.cnki.jos.006090&partnerID=40&md5=b66bb93d821b5703a666d612a992c3db,10.13328/j.cnki.jos.006090,"With the continuous development of the industrial Internet, big data and artificial intelligence contribute to the comprehensive interconnection in human-cyber-physical system. The amount of task data generated by users using the service is growing exponentially. While recommending services for online users to meet personalized needs, and for services that need to be completed through human- cyber-physical interaction, it has become a challenging problem how to integrate the various offline and online resources to dispatch the right person to complete the task quickly and effectively. In order to ensure the accuracy of services dispatch, this study proposes a cross-domain collaborative service dispatch method that takes into account the data characteristics of all these factors in human-cyber-physical system. In order to get a more reasonable dispatch, the sentiment characteristics of user evaluation and the similarity of business data are analyzed respectively, and then the attributes inherent in the real world are added of which have an impact on business processes. Finally, taking the doctor-patient assignment of an online diagnosis and treatment platform on the Internet as an example, the results show that the method proposed in this study has high accuracy and can improve the efficiency of task execution. © Copyright 2021, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Cross-domain integration; Intelligent service; Sentiment analysis; Service dispatch; User preference,"3404, 3422",,Ruan Jian Xue Bao/Journal of Software,Article,Scopus
55,,Unsupervised Fine-grained Video Categorization via Adaptation Learning Across Domains and Modalities [跨域和跨模态适应学习的无监督细粒度视频分类],"He X.-T., Peng Y.-X.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118927048&doi=10.13328%2fj.cnki.jos.006058&partnerID=40&md5=371e43c96eba8a896244daeeb053049a,10.13328/j.cnki.jos.006058,"Fine-grained video categorization is a highly challenging task to discriminate similar subcategories that belong to the same basic-level category. Due to the significant advances in fine-grained image categorization and expensive cost of labeling video data, it is intuitive to adapt the knowledge learned from image to video in an unsupervised manner. However, there is a clear gap to directly apply the models learned from image to recognize the fine-grained instances in video, due to domain distinction and modality distinction between image and video. Therefore, this study proposes the unsupervised discriminative adaptation network (UDAN), which transfers the ability of discrimination localization from image to video. A progressive pseudo labeling strategy is adopted to iteratively guide UDAN to approximate the distribution of the target video data. To verify the effectiveness of the proposed UDAN approach, adaptation tasks between image and video are performed, adapting the knowledge learned from CUB-200-2011/Cars-196 datasets (image) to YouTube Birds/YouTube Cars datasets (video). Experimental results illustrate the advantage of the proposed UDAN approach for unsupervised fine-grained video categorization. © Copyright 2021, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Domain adaption; Domain distinction; Fine-grained video categorization; Modality distinction; Unsupervised discriminative adaptation network,"3482, 3495",,Ruan Jian Xue Bao/Journal of Software,Article,Scopus
56,,Generative adversarial network based data augmentation and gender-last training strategy with application to bone age assessment,"Su L., Fu X., Hu Q.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118805625&doi=10.1016%2fj.cmpb.2021.106456&partnerID=40&md5=f9da3bfefea9fd084c87a345de3443ad,10.1016/j.cmpb.2021.106456,"Background and objectives: Bone age assessment (BAA) is widely used in determination of discrepancy between skeletal age and chronological age. Manual approaches are complicated which require experienced experts, while existing automatic approaches are perplexed with small and imbalanced samples which is a big challenge in deep learning. Methods: In this study, we proposed a new deep learning based method to improve the BAA training in both pre-training and training architecture. In pre-training, we proposed a framework using a new distance metric of cosine distance in the framework of optimal transport for data augmentation (CNN-GAN-OTD). In the training architecture, we explored the order of gender label and bone age information, supervised and semi-supervised training. Results: We found that the training architecture with the CNN-GAN-OTD based data augmentation and supervised gender-last classification with supervised Inception v3 network yielded the best assessment (mean average error of 4.23 months). Conclusions: The proposed data augmentation framework could be a potential built-in component of general deep learning networks and the training strategy with different label order could inspire more and deeper consideration of label priority in multi-label tasks. © 2021",bone age assessment; data augmentation; gender-last training; regression; semi-supervised generative adversarial network,,,Computer Methods and Programs in Biomedicine,Article,Scopus
57,,Application of a brain-inspired deep imitation learning algorithm in autonomous driving[Formula presented],"Ahmedov H.B., Yi D., Sui J.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118741003&doi=10.1016%2fj.simpa.2021.100165&partnerID=40&md5=86dcecd1e80530c8ce60f3523cef251a,10.1016/j.simpa.2021.100165,"Autonomous driving has attracted great attention from both academics and industries. To realise autonomous driving, Deep Imitation Learning (DIL) is treated as one of the most promising solutions, because it improves autonomous driving systems by automatically learning a complex mapping from human driving data, compared to manually designing the driving policy. However, existing DIL methods cannot generalise well across domains, that is, a network trained on the data of source domain gives rise to poor generalisation on the data of target domain. In the present study, we propose a novel brain-inspired deep imitation method that builds on the evidence from human brain functions, to improve the generalisation ability of DNN so that autonomous driving systems can perform well in various scenarios. Specifically, humans have a strong generalisation ability which is beneficial from the structural and functional asymmetry of the two sides of the brain. Here, we design dual Neural Circuit Policy (NCP) architectures in DNN based on the asymmetry of human neural networks. Experimental results demonstrate that our brain-inspired method outperforms existing methods regarding generalisation when dealing with unseen data. Our source codes and pretrained models are available at https://github.com/Intenzo21/Brain-Inspired-Deep-Imitation-Learning-for-Autonomous-Driving-Systems. © 2021 The Author(s)",Autonomous vehicles; Brain-inspired AI; Imitation Learning,,,Software Impacts,Article,Scopus
58,,GeneCoNet: A web application server for constructing cancer patient-specific gene correlation networks with prognostic gene pairs,"Park B., Lee W., Han K.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118729038&doi=10.1016%2fj.cmpb.2021.106465&partnerID=40&md5=28abd8d3c284d181f990db3731f6709d,10.1016/j.cmpb.2021.106465,"Background and objective: Most prognostic gene signatures that have been known for cancer are either individual genes or combination of genes. Both individual genes and combination of genes do not provide information on gene-gene relations, and often have less prognostic significance than random genes associated with cell proliferation. Several methods for generating sample-specific gene networks have been proposed, but programs implementing the methods are not publicly available. Methods: We have developed a method that builds gene correlation networks specific to individual cancer patients and derives prognostic gene correlations from the networks. A gene correlation network specific to a patient is constructed by identifying gene-gene relations that are significantly different from normal samples. Prognostic gene pairs are obtained by carrying out the Cox proportional hazards regression and the log-rank test for every gene pair. Results: We built a web application server called GeneCoNet with thousands of tumor samples in TCGA. Given a tumor sample ID of TCGA, GeneCoNet dynamically constructs a gene correlation network specific to the sample as output. As an additional output, it provides information on prognostic gene correlations in the network. GeneCoNet found several prognostic gene correlations for six types of cancer, but there were no prognostic gene pairs common to multiple cancer types. Conclusion: Extensive analysis of patient-specific gene correlation networks suggests that patients with a larger subnetwork of prognostic gene pairs have shorter survival time than the others and that patients with a subnetwork that contains more genes participating in prognostic gene pairs have shorter survival time than the others. GeneCoNet can be used as a valuable resource for generating gene correlation networks specific to individual patients and for identifying prognostic gene correlations. It is freely accessible at http://geneconet.inha.ac.kr. © 2021 Elsevier B.V.",Biological networks; Cancer; Visualization,,,Computer Methods and Programs in Biomedicine,Article,Scopus
59,,Deep embeddings and logistic regression for rapid active learning in histopathological images,"Jiao Y., Yuan J., Qiang Y., Fei S.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118333751&doi=10.1016%2fj.cmpb.2021.106464&partnerID=40&md5=4fd4f11f2c7fe34c74cfdd5103119214,10.1016/j.cmpb.2021.106464,"Background and Objective: Recognizing different tissue components is one of the most fundamental and essential works in digital pathology. Current methods are often based on convolutional neural networks (CNNs), which need numerous annotated samples for training. Creating large-scale histopathological datasets is labor-intensive, where interactive data annotation is a potential solution. Methods: We propose DELR (Deep Embedding-based Logistic Regression) to enable rapid model training and inference for histopathological image analysis. DELR utilizes a pretrained CNN to encode images as compact embeddings with low computational cost. The embeddings are then used to train a Logistic Regression model efficiently. We implemented DELR in an active learning framework, and validated it on three histopathological problems (binary, 4-category, and 8-category classification challenge for lung, breast, and colorectal cancer, respectively). We also investigated the influence of active learning strategy and type of the encoder. Results: On all the three datasets, DELR can achieve an area under curve (AUC) metric higher than 0.95 with only 100 image patches per class. Although its AUC is slightly lower than a fine-tuned CNN counterpart, DELR can be 536, 316, and 1481 times faster after pre-encoding. Moreover, DELR is proved to be compatible with a variety of active learning strategies and encoders. Conclusions: DELR can achieve comparable accuracy to CNN with rapid running speed. These advantages make it a potential solution for real-time interactive data annotation. © 2021",Active learning; Computer-aided diagnosis; Data annotation; Deep learning; Digital pathology; Tissue classification,,,Computer Methods and Programs in Biomedicine,Article,Scopus
60,,Super-resolution of Pneumocystis carinii pneumonia CT via self-attention GAN,"Xie H., Zhang T., Song W., Wang S., Zhu H., Zhang R., Zhang W., Yu Y., Zhao Y.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117904319&doi=10.1016%2fj.cmpb.2021.106467&partnerID=40&md5=c73f0fcb4d3154c6f710720863be7d2e,10.1016/j.cmpb.2021.106467,"Background and objective: Computed tomography (CT) examination plays an important role in screening suspected and confirmed patients in pneumocystis carinii pneumonia (PCP), and the efficient acquisition of high-quality medical CT images is essential for the clinical application of computer-aided diagnosis technology. Therefore, improving the resolution of CT images of pneumonia is a very important task. Methods: Aiming at the problem of how to recover the texture details of the reconstructed PCP CT super-resolution image, we propose the image super-resolution reconstruction model based on self-attention generation adversarial network (SAGAN). In the SAGAN algorithm, a generator based on self-attention mechanism and residual module is used to transform a low-resolution image into a super-resolution image. A discriminator based on depth convolution network tries to distinguish the difference between the reconstructed super-resolution image and the real super-resolution image. In terms of loss function construction, on the one hand, the Charbonnier content loss function is used to improve the accuracy of image reconstruction, and on the other hand, the feature value before activation of the pre-trained VGGNet is used to calculate the perceptual loss to achieve accurate texture detail reconstruction of super-resolution images. Results: Experimental results show that our SAGAN algorithm is superior to other state-of-the-art algorithms in both peak signal-to-noise ratio (PSNR) and structural similarity score (SSIM). Specifically, our SAGAN method can obtain 31.94 dB which is 1.53 dB better than SRGAN on Set5 dataset for 4 enlargements. Conclusion: Our SAGAN method can reconstruct more realistic PCP CT images with clear texture, which can help experts diagnose the condition of PCP. © 2021",Convolutional neural network; Generative adversarial network; Pneumocystis carinii pneumonia; Self-attention mechanism; Super-resolution,,,Computer Methods and Programs in Biomedicine,Article,Scopus
61,,MBNM: Multi-branch network based on memory features for long-tailed medical image recognition,"Zhang R., E H., Yuan L., He J., Zhang H., Zhang S., Wang Y., Song M., Wang L.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117201482&doi=10.1016%2fj.cmpb.2021.106448&partnerID=40&md5=7fb2f1f06bd41ab34a596697cd3d965f,10.1016/j.cmpb.2021.106448,"Background and objectives: Deep learning algorithms show revolutionary potential in computer-aided diagnosis. These computer-aided diagnosis techniques often rely on large-scale, balanced standard datasets. However, there are many rare diseases in real clinical scenarios, which makes the medical datasets present a highly imbalanced long-tailed distribution, leading to the poor generalization ability of deep learning models. Currently, most algorithms to solve this problem involve more complex modules and loss functions. But for complicated tasks in the medical domain, usually suffer from issues such as increased inference time and unstable performance. Therefore, it is a great challenge to develop a computer-aided diagnosis algorithm for long-tailed medical data. Methods: We proposed the Multi-Branch Network based on Memory Features (MBNM) for Long-Tailed Medical Image Recognition. MBNM includes three branches, where each branch focuses on a different learning task: 1) the regular learning branch learns the generalizable feature representations; 2) the tail learning branch gains extra intra-class diversity for the tail classes through the feature memory module and the improved reverse sampler to improve the classification performance of the tail classes; 3) the fusion balance branch integrates various decision-making advantages and introduces an adaptive loss function to re-balance the classification performance of easy and difficult samples. Results: We conducted experiments on the multi-disease Ophthalmic OCT datasets with imbalance factors of 98.48 and skin image datasets Skin-7 with imbalance factors of 58.3. The Accuracy, MCR, F1-Score, Precision, and AUC of our model were significantly improved over the strong baselines in the auxiliary diagnosis scenario where the clinical medical data is extremely imbalanced. Furthermore, we demonstrated that MBNM outperforms the state-of-the-art models on the publicly available natural image datasets (CIFAR-10 and CIFAR-100). Conclusions: The proposed algorithm can solve the problem of imbalanced data distribution with little added cost. In addition, the memory module does not act in the inference phase, thereby saving inference time. And it shows outstanding performance on medical images and natural images with a variety of imbalance factors. © 2021",Deep learning; Fusion model; Imbalanced medical image; Memory features,,,Computer Methods and Programs in Biomedicine,Article,Scopus
62,,Deep learning based microscopic cell images classification framework using multi-level ensemble,"Maurya R., Pathak V.K., Dutta M.K.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116701657&doi=10.1016%2fj.cmpb.2021.106445&partnerID=40&md5=00ec76aeac2a7b9fbedcb258dd0139fe,10.1016/j.cmpb.2021.106445,"Background and objectives: Advancement of the ultra-fast microscopic images acquisition and generation techniques give rise to the automated artificial intelligence (AI)-based microscopic images classification systems. The earlier cell classification systems classify the cell images of a specific type captured using a specific microscopy technique, therefore the motivation behind the present study is to develop a generic framework that can be used for the classification of cell images of multiple types captured using a variety of microscopic techniques. Methods: The proposed framework for microscopic cell images classification is based on the transfer learning-based multi-level ensemble approach. The ensemble is made by training the same base model with different optimisation methods and different learning rates. An important contribution of the proposed framework lies in its ability to capture different granularities of features extracted from multiple scales of an input microscopic cell image. The base learners used in the proposed ensemble encapsulates the aggregation of low-level coarse features and high-level semantic features, thus, represent the different granular microscopic cell image features present at different scales of input cell images. The batch normalisation layer has been added to the base models for the fast convergence in the proposed ensemble for microscopic cell images classification. Results: The general applicability of the proposed framework for microscopic cell image classification has been tested with five different public datasets. The proposed method has outperformed the experimental results obtained in several other similar works. Conclusions: The proposed framework for microscopic cell classification outperforms the other state-of-the-art classification methods in the same domain with a comparatively lesser amount of training data. © 2021","Convolutional neural networks, Microscopic cell images classification; Deep learning; Multi-level ensemble; Transfer learning",,,Computer Methods and Programs in Biomedicine,Article,Scopus
63,,A disease-specific language representation model for cerebrovascular disease research,"Lin C.-H., Hsu K.-C., Liang C.-K., Lee T.-H., Liou C.-W., Lee J.-D., Peng T.-I., Shih C.-S., Fann Y.C.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116552816&doi=10.1016%2fj.cmpb.2021.106446&partnerID=40&md5=3c214e58c84c43fa541a7b5ba890902c,10.1016/j.cmpb.2021.106446,"Background: Effectively utilizing disease-relevant text information from unstructured clinical notes for medical research presents many challenges. BERT (Bidirectional Encoder Representation from Transformers) related models such as BioBERT and ClinicalBERT, pre-trained on biomedical corpora and general clinical information, have shown promising performance in various biomedical language processing tasks. Objectives: This study aims to explore whether a BERT-based model pre-trained on disease-related clinical information can be more effective for cerebrovascular disease-relevant research. Methods: This study proposed the StrokeBERT which was initialized from BioBERT and pre-trained on large-scale cerebrovascular disease related clinical text information. The pre-trained corpora contained 113,590 discharge notes, 105,743 radiology reports, and 38,199 neurological reports. Two real-world empirical clinical tasks were conducted to validate StrokeBERT's performance. The first task identified extracranial and intracranial artery stenosis from two independent sets of radiology angiography reports. The second task predicted the risk of recurrent ischemic stroke based on patients’ first discharge information. Results: In stenosis detection, StrokeBERT showed improved performance on targeted carotid arteries, with an average AUC compared to that of ClinicalBERT of 0.968 ± 0.021 and 0.956 ± 0.018, respectively. In recurrent ischemic stroke prediction, after 10-fold cross-validation on 1,700 discharge information, StrokeBERT presented better prediction ability (AUC±SD = 0.838 ± 0.017) than ClinicalBERT (AUC±SD = 0.808 ± 0.045). The attention scores of StrokeBERT showed better ability to detect and associate cerebrovascular disease related terms than current BERT based models. Conclusions: This study shows that a disease-specific BERT model improved the performance and accuracy of various disease-specific language processing tasks and can readily be fine-tuned to advance cerebrovascular disease research and further developed for clinical applications. © 2021",Cerebrovascular disease; Natural language processing; Specific language representation model,,,Computer Methods and Programs in Biomedicine,Article,Scopus
64,,A novel combined dynamic ensemble selection model for imbalanced data to detect COVID-19 from complete blood count,"Wu J., Shen J., Xu M., Shao M.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116096500&doi=10.1016%2fj.cmpb.2021.106444&partnerID=40&md5=6c04ad0219d5f1bc4624893f93a28c1d,10.1016/j.cmpb.2021.106444,"Background: As blood testing is radiation-free, low-cost and simple to operate, some researchers use machine learning to detect COVID-19 from blood test data. However, few studies take into consideration the imbalanced data distribution, which can impair the performance of a classifier. Method: A novel combined dynamic ensemble selection (DES) method is proposed for imbalanced data to detect COVID-19 from complete blood count. This method combines data preprocessing and improved DES. Firstly, we use the hybrid synthetic minority over-sampling technique and edited nearest neighbor (SMOTE-ENN) to balance data and remove noise. Secondly, in order to improve the performance of DES, a novel hybrid multiple clustering and bagging classifier generation (HMCBCG) method is proposed to reinforce the diversity and local regional competence of candidate classifiers. Results: The experimental results based on three popular DES methods show that the performance of HMCBCG is better than only use bagging. HMCBCG+KNE obtains the best performance for COVID-19 screening with 99.81% accuracy, 99.86% F1, 99.78% G-mean and 99.81% AUC. Conclusion: Compared to other advanced methods, our combined DES model can improve accuracy, G-mean, F1 and AUC of COVID-19 screening. © 2021",Candidate classifier generation; COVID-19 screening; Dynamic ensemble selection; Hybrid multiple clustering and bagging; Imbalanced data,,,Computer Methods and Programs in Biomedicine,Article,Scopus
65,,End-to-end multimodal clinical depression recognition using deep neural networks: A comparative analysis,"Muzammel M., Salam H., Othmani A.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116079993&doi=10.1016%2fj.cmpb.2021.106433&partnerID=40&md5=97dddb2f74633fcc51de68446bb29485,10.1016/j.cmpb.2021.106433,"Background and Objective: Major Depressive Disorder is a highly prevalent and disabling mental health condition. Numerous studies explored multimodal fusion systems combining visual, audio, and textual features via deep learning architectures for clinical depression recognition. Yet, no comparative analysis for multimodal depression analysis has been proposed in the literature. Methods: In this paper, an up-to-date literature overview of multimodal depression recognition is presented and an extensive comparative analysis of different deep learning architectures for depression recognition is performed. First, audio features based Convolutional Neural Networks (CNNs) and Long Short-Term Memory (LSTM) are studied. Then, early-level and model-level fusion of deep audio features with visual and textual features through LSTM and CNN architectures are investigated. Results: The performance of the proposed architectures using an hold-out strategy on the DAIC-WOZ dataset (80% training, 10% validation, 10% test split) for binary and severity levels of depression recognition is tested. Using this strategy, a set of experiments have been performed and they have demonstrated: (1) LSTM-based audio features perform slightly better than CNN ones with an accuracy of 66.25% versus 65.60% for binary depression classes. (2) the model level fusion of deep audio and visual features using LSTM network performed the best with an accuracy of 77.16%, a precision of 53% for the depressed class, and a precision of 83% for the non-depressed class. The given network obtained a normalized Root Mean Square Error (RMSE) of 0.15 for depression severity level prediction. Using a Leave-One-Subject-Out strategy, this network achieved an accuracy of 95.38% for binary depression detection, and a normalized RMSE of 0.1476 for depression severity level prediction. Our best-performing architecture outperforms all state-of-the-art approaches on DAIC-WOZ dataset. Conclusions: The obtained results show that the proposed LSTM-based surpass the proposed CNN-based architectures allowing to learn temporal dynamics representations of multimodal features. Furthermore, model-level fusion of audio and visual features using an LSTM network leads to the best performance. Our best-performing architecture successfully detects depression using a speech segment of less than 8 seconds, and an average prediction computation time of less than 6ms; making it suitable for real-world clinical applications. © 2021 Elsevier B.V.",Biomedical informatics; Biomedical information processing; Deep learning; Features fusion; Multimodal depression recognition,,,Computer Methods and Programs in Biomedicine,Article,Scopus
66,,Lightweight deep neural networks for cholelithiasis and cholecystitis detection by point-of-care ultrasound,"Yu C.-J., Yeh H.-J., Chang C.-C., Tang J.-H., Kao W.-Y., Chen W.-C., Huang Y.-J., Li C.-H., Chang W.-H., Lin Y.-T., Sufriyana H., Su E.C.-Y.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115252572&doi=10.1016%2fj.cmpb.2021.106382&partnerID=40&md5=ea104971d3ed8b381720ad6679666a89,10.1016/j.cmpb.2021.106382,"Background and objective: Emergency physicians (EPs) frequently deal with abdominal pain, including that is caused by either gallstones or acute cholecystitis. Easy access and low cost justify point-of-care ultrasound (POCUS) use as a first-line test to detect these diseases; yet, the detection performance of POCUS by EPs is unreliable, causing misdiagnoses with serious impacts. This study aimed to develop a machine learning system to detect and localize gallstones and to detect acute cholecystitis by ultrasound (US) still images taken by physicians or technicians for preliminary diagnoses. Methods: Abdominal US images (> 89,000) were collected from 2386 patients in a hospital database. We constructed training sets for gallstones with or without cholecystitis (N = 10,971) and cholecystitis with or without gallstones (N = 7348) as positives. Validation sets were also constructed for gallstones (N = 2664) and cholecystitis (N = 1919). We applied a single-shot multibox detector (SSD) and a feature pyramid network (FPN) to classify and localize objects using image features extracted by ResNet-50 for gallstones, and MobileNet V2 to classify cholecystitis. The deep learning models were pretrained using the COCO-2017 and ILSVRC-2012 datasets. Results: Using the validation sets, the SSD-FPN-ResNet-50 and MobileNet V2 achieved areas under the receiver operating characteristics curve of 0.92 and 0.94, respectively. The inference speeds were 21 (47.6 frames per second, fps) and 7 ms (142.9 fps). Conclusions: A machine learning system was developed to detect and localize gallstones, and to detect cholecystitis, with acceptable discrimination and speed. This is the first study to develop this system for either gallstone or cholecystitis detection with absence or presence of each one. After clinical trials, this system may be used to assist EPs, including those in remote areas, for detecting these diseases. © 2021 Elsevier B.V.",Abdomen; Computer-aided diagnosis; Machine learning; Neural network; Pattern recognition; Ultrasound,,,Computer Methods and Programs in Biomedicine,Article,Scopus
67,,DR-MIL: deep represented multiple instance learning distinguishes COVID-19 from community-acquired pneumonia in CT images,"Qi S., Xu C., Li C., Tian B., Xia S., Ren J., Yang L., Wang H., Yu H.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114939477&doi=10.1016%2fj.cmpb.2021.106406&partnerID=40&md5=94e1ae7da705629a157be79becce4e67,10.1016/j.cmpb.2021.106406,"Background and objective: Given that the novel coronavirus disease 2019 (COVID-19) has become a pandemic, a method to accurately distinguish COVID-19 from community-acquired pneumonia (CAP) is urgently needed. However, the spatial uncertainty and morphological diversity of COVID-19 lesions in the lungs, and subtle differences with respect to CAP, make differential diagnosis non-trivial. Methods: We propose a deep represented multiple instance learning (DR-MIL) method to fulfill this task. A 3D volumetric CT scan of one patient is treated as one bag and ten CT slices are selected as the initial instances. For each instance, deep features are extracted from the pre-trained ResNet-50 with fine-tuning and represented as one deep represented instance score (DRIS). Each bag with a DRIS for each initial instance is then input into a citation k-nearest neighbor search to generate the final prediction. A total of 141 COVID-19 and 100 CAP CT scans were used. The performance of DR-MIL is compared with other potential strategies and state-of-the-art models. Results: DR-MIL displayed an accuracy of 95% and an area under curve of 0.943, which were superior to those observed for comparable methods. COVID-19 and CAP exhibited significant differences in both the DRIS and the spatial pattern of lesions (p<0.001). As a means of content-based image retrieval, DR-MIL can identify images used as key instances, references, and citers for visual interpretation. Conclusions: DR-MIL can effectively represent the deep characteristics of COVID-19 lesions in CT images and accurately distinguish COVID-19 from CAP in a weakly supervised manner. The resulting DRIS is a useful supplement to visual interpretation of the spatial pattern of lesions when screening for COVID-19. © 2021",Community-acquired pneumonia; Convolutional neural network; COVID-19; Deep learning; Lung CT image; Multiple instance learning,,,Computer Methods and Programs in Biomedicine,Article,Scopus
68,,Fusion of CNN1 and CNN2-based magnetic resonance image diagnosis of knee meniscus injury and a comparative analysis with computed tomography,"Qiu X., Liu Z., Zhuang M., Cheng D., Zhu C., Zhang X.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114831733&doi=10.1016%2fj.cmpb.2021.106297&partnerID=40&md5=ce3385147a5b8fca8f7ce36bc4fc57b4,10.1016/j.cmpb.2021.106297,"Purpose: We used convolutional neural network (CNN) technology to improve the accuracy of diagnosis of knee meniscus injury and shorten the diagnosis time. Method: We propose a meniscus detection method based on Fusion of CNN1 and CNN2 (CNNf), which uses Magnetic Resonance Imaging (MRI) and Computer tomography (CT) to compare the diagnosis results, verifies the proposed method through 2460 images collected from 205 patients in the hospital. We used accuracy, sensitivity, specificity, receiver operating characteristics (ROC), and damage total rate to evaluate performance. Results: The accuracy of our model was 93.86%, the sensitivity was 91.35%, the specificity was 94.65%, and the area under the receiver operating characteristic curve was 96.78%. The total damage rate of MRI is 91.57%, which is far greater than the total damage rate of CT diagnosis of 80.13%. Conclusion: CNNf-based MRI technology of knee meniscus injury has high practical value in clinical practice. It can effectively improve the accuracy of diagnosis and reduce the rate of misdiagnosis. © 2021",Convolutional neural network; Diagnostic value; Knee joint; Magnetic Resonance Imaging; Meniscus injury,,,Computer Methods and Programs in Biomedicine,Article,Scopus
69,,SSD based waste separation in smart garbage using augmented clustering NMS,"Karthikeyan M., Subashini T.S., Jebakumar R.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113945093&doi=10.1007%2fs10515-021-00296-9&partnerID=40&md5=2f4a015631760f634f19c8f574318ea7,10.1007/s10515-021-00296-9,"Object detection plays a pivotal role in autonomous systems helps to build the machines to be intelligent as humans that leads to build an Artificial Intelligence application used for society,Industries,face-recognition and so-on. Nowadays, it is difficult to manage the waste generated by humans and industries which is increasing rapidly day by day needs to address the problem to make automation for separating the Bio-degradable and Non-Bio degradable waste. Although humans are tried to manage impact of waste management in society to maintain the eco-system by implementing a separate trash for Bio and Non-bio waste. Sometimes it is difficult to follow for the separation of waste manually by humans. There is no existing sensor to identify the types of wastes. In this proposed system, it evolves the implementation of the bio-degradable and non-bio degradable object detection method to help to detect these objects automatically with the augmented clustering NMS using Single-shot detector methods. The enhanced augmented clustering algorithm effectively detects the multiple objects in the video along with the respective bio or non-bio classification custom object detection model. With the build thousand images for each class custom dataset model to train the objects using deep learning neural network. A custom object detection data model is built with the help of NVIDIA GPU RTX 4 GB using tensorflow model. Here the results are interpreted with the mean average precision value of 0.965 with ACNMS Single shot object detector which is effectively detected with the new enhanced technique. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",ACNMS; Object detection; Single-shot detector; Smart garbage,,,Automated Software Engineering,Article,Scopus
70,,SeCNN: A semantic CNN parser for code comment generation,"Li Z., Wu Y., Peng B., Chen X., Sun Z., Liu Y., Yu D.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109986339&doi=10.1016%2fj.jss.2021.111036&partnerID=40&md5=a4ae9930818fec0e7df764501a8c6cf2,10.1016/j.jss.2021.111036,"A code comment generation system can summarize the semantic information of source code and generate a natural language description, which can help developers comprehend programs and reduce time cost spent during software maintenance. Most of state-of-the-art approaches use RNN (Recurrent Neural Network)-based encoder–decoder neural networks. However, this kind of method may not generate high-quality description when summarizing the information among several code blocks that are far from each other (i.e., the long-dependency problem). In this paper, we propose a novel Semantic CNN parser SeCNN for code comment generation. In particular, we use a CNN (Convolutional Neural Network) to alleviate the long-dependency problem and design several novel components, including source code-based CNN and AST-based CNN, to capture the semantic information of the source code. The evaluation is conducted on a widely-used large-scale dataset of 87,136 Java methods. Experimental results show that SeCNN achieves better performance (i.e., 44.69% in terms of BLEU and 26.88% in terms of METEOR) and has lower execution time cost when compared with five state-of-the-art baselines. © 2021",Code comment generation; Convolutional Neural Network; Long short-term memory network; Program comprehension,,,Journal of Systems and Software,Article,Scopus
71,,Investigation on the stability of SMOTE-based oversampling techniques in software defect prediction,"Feng S., Keung J., Yu X., Xiao Y., Zhang M.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109045933&doi=10.1016%2fj.infsof.2021.106662&partnerID=40&md5=1d86beac241092b7979f466381047a21,10.1016/j.infsof.2021.106662,"Context: In practice, software datasets tend to have more non-defective instances than defective ones, which is referred to as the class imbalance problem in software defect prediction (SDP). Synthetic Minority Oversampling TEchnique (SMOTE) and its variants alleviate the class imbalance problem by generating synthetic defective instances. SMOTE-based oversampling techniques were widely adopted as the baselines to compare with the newly proposed oversampling techniques in SDP. However, randomness is introduced during the procedure of SMOTE-based oversampling techniques. If the performance of SMOTE-based oversampling techniques is highly unstable, the conclusion drawn from the comparison between SMOTE-based oversampling techniques and the newly proposed techniques may be misleading and less convincing. Objective: This paper aims to investigate the stability of SMOTE-based oversampling techniques. Moreover, a series of stable SMOTE-based oversampling techniques are proposed to improve the stability of SMOTE-based oversampling techniques. Method: Stable SMOTE-based oversampling techniques reduce the randomness in each step of SMOTE-based oversampling techniques by selecting defective instances in turn, distance-based selection of K neighbor instances, and evenly distributed interpolation. Besides, we mathematically prove and also empirically investigate the stability of SMOTE-based and stable SMOTE-based oversampling techniques on four common classifiers across 26 datasets in terms of AUC, balance, and MCC. Results: The analysis of SMOTE-based and stable SMOTE-based oversampling techniques shows that the performance of stable SMOTE-based oversampling techniques is more stable and better than that of SMOTE-based oversampling techniques. The difference between the worst and best performances of SMOTE-based oversampling techniques is up to 23.3%, 32.6%, and 204.2% in terms of AUC, balance, and MCC, respectively. Conclusion: Stable SMOTE-based oversampling techniques should be considered as a drop-in replacement for SMOTE-based oversampling techniques. © 2021 Elsevier B.V.",Class imbalance; Empirical Software Engineering; Oversampling; SMOTE; Software defect prediction,,,Information and Software Technology,Article,Scopus
72,,The impact of using biased performance metrics on software defect prediction research,"Yao J., Shepperd M.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108424593&doi=10.1016%2fj.infsof.2021.106664&partnerID=40&md5=41b79d339223ed24eed1ecc28f428489,10.1016/j.infsof.2021.106664,"Context: Software engineering researchers have undertaken many experiments investigating the potential of software defect prediction algorithms. Unfortunately some widely used performance metrics are known to be problematic, most notably F1, but nevertheless F1 is widely used. Objective: To investigate the potential impact of using F1 on the validity of this large body of research. Method: We undertook a systematic review to locate relevant experiments and then extract all pairwise comparisons of defect prediction performance using F1 and the unbiased Matthews correlation coefficient (MCC). Results: We found a total of 38 primary studies. These contain 12,471 pairs of results. Of these comparisons, 21.95% changed direction when the MCC metric is used instead of the biased F1 metric. Unfortunately, we also found evidence suggesting that F1 remains widely used in software defect prediction research. Conclusion: We reiterate the concerns of statisticians that the F1 is a problematic metric outside of an information retrieval context, since we are concerned about both classes (defect-prone and not defect-prone units). This inappropriate usage has led to a substantial number (more than one fifth) of erroneous (in terms of direction) results. Therefore we urge researchers to (i) use an unbiased metric and (ii) publish detailed results including confusion matrices such that alternative analyses become possible. © 2021",Classification metrics; Computational experiment; Machine learning; Software defect prediction; Software engineering,,,Information and Software Technology,Review,Scopus
73,,Cybersecurity protection on in-vehicle networks for distributed automotive cyber-physical systems: State-of-the-art and future challenges,"Xie Y., Zhou Y., Xu J., Zhou J., Chen X., Xiao F.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102822700&doi=10.1002%2fspe.2965&partnerID=40&md5=79595eafa677046239b00fc2a259fc13,10.1002/spe.2965,"The ever-evolving trip mode of human being leads the automobiles moving toward connected, autonomous, sharing, and electrified vehicles rapidly. But the connection introduces new cybersecurity problems on in-vehicle networks, which poses great challenges for safety guarantee of distributed automotive cyber-physical systems. This article first analyzes the cybersecurity vulnerabilities and defines the security requirements for in-vehicle networks, and then introduces the architecture evolution of in-vehicle network. Based on the definition on architecture of in-vehicle networks, this article defines a security protection framework for it. And then, it surveys the state-of-the-art works for availability protection, integrity protection, and confidentiality protection of in-vehicle networks, respectively, and detailed analysis and comparisons are given about the proposed cybersecurity protection mechanisms. Finally, it summarizes the future challenges for cybersecurity protection of in-vehicle networks, and proposes possible solutions for these challenges. © 2021 John Wiley & Sons Ltd.",controller area network; cybersecurity protection; distributed automotive cyber-physical systems; in-vehicle networks; system design and optimization,"2108, 2127",,Software - Practice and Experience,Conference Paper,Scopus
74,,Deep domain adversarial residual neural network for sustainable wind turbine cyber-physical system fault diagnosis,"Jin Y., Feng Q., Zhang X., Lu P., Shen J., Tu Y., Wu Z.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101907624&doi=10.1002%2fspe.2937&partnerID=40&md5=932de8e4383523384ec5fb4ac6df20f0,10.1002/spe.2937,"As a popular renewable energy generation technology, wind turbine system has become a critical enabler for building the sustainable cyber-physical system (CPS). The main shaft bearing is an important part of the wind turbine CPS and often runs under variable working conditions. Thus, the reliable bearing diagnosis method can timely discover the main shaft bearing fault, which reduces the maintenance cost of wind turbines. Inspired by the idea of domain adaptation, we combined domain adversarial neural network and residual network and proposed a novel deep domain adversarial residual neural network (DDA-RNN) for diagnosing bearing fault and improving model performance on the unlabeled dataset. This proposed software and hardware co-design method was evaluated by our bearing dataset, which was collected from two wind turbine CPSs from Sanmenxia in Henan Province. Besides, F1 score and accuracy are served as model metrics, which reflect the diagnosis performance. Compared with other methods, the experimental results show that DDA-RNN can improve model performance. Meanwhile, DDA-RNN extracts diagnosis knowledge from labeled dataset and improves the model performance on the unlabeled dataset under different working condition. Therefore, the proposed method can be potentially used to benefit many practical scenarios in the future. © 2021 John Wiley & Sons, Ltd.",bearing fault diagnosis; domain adversarial learning; residual block; sustainable wind turbine cyber-physical system,"2128, 2142",,Software - Practice and Experience,Conference Paper,Scopus
75,,On the Costs and Profit of Software Defect Prediction,Herbold S.,2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076314124&doi=10.1109%2fTSE.2019.2957794&partnerID=40&md5=5a1fc3489e5550ddd78baa6e7d8180f7,10.1109/TSE.2019.2957794,"Defect prediction can be a powerful tool to guide the use of quality assurance resources. However, while lots of research covered methods for defect prediction as well as methodological aspects of defect prediction research, the actual cost saving potential of defect prediction is still unclear. Within this article, we close this research gap and formulate a cost model for software defect prediction. We derive mathematically provable boundary conditions that must be fulfilled by defect prediction models such that there is a positive profit when the defect prediction model is used. Our cost model includes aspects like the costs for quality assurance, the costs of post-release defects, the possibility that quality assurance fails to reveal predicted defects, and the relationship between software artifacts and defects. We initialize the cost model using different assumptions, perform experiments to show trends of the behavior of costs on real projects. Our results show that the unrealistic assumption that defects only affect a single software artifact, which is a standard practice in the defect prediction literature, leads to inaccurate cost estimations. Moreover, the results indicate that thresholds for machine learning metrics are also not suited to define success criteria for software defect prediction. © 1976-2012 IEEE.",costs; Defect prediction; return on investment,"2617, 2631",,IEEE Transactions on Software Engineering,Article,Scopus
76,,Natural language-guided programming,"Heyman G., Huysegems R., Justen P., Van Cutsem T.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119076848&doi=10.1145%2f3486607.3486749&partnerID=40&md5=e18667eb0393783b755bb457461943d5,10.1145/3486607.3486749,"In today's software world with its cornucopia of reusable software libraries, when a programmer is faced with a programming task that they suspect can be completed through the use of a library, they often look for code examples using a search engine and then manually adapt found examples to their specific context of use. We put forward a vision based on a new breed of developer tools that have the potential to largely automate this process. The key idea is to adapt code autocompletion tools such that they take into account not only the developer's already-written code but also the intent of the task the developer is trying to achieve next, formulated in plain natural language. We call this practice of enriching the code with natural language intent to facilitate its completion natural language-guided programming. To show that this idea is feasible we design, implement and benchmark a tool that solves this problem in the context of a specific domain (data science) and a specific programming language (Python). Central to the tool is the use of language models trained on a large corpus of documented code. Our initial experiments confirm the feasibility of the idea but also make it clear that we have only scratched the surface of what may become possible in the future. We end the paper with a comprehensive research agenda to stimulate additional research in the budding area of natural language-guided programming. © 2021 ACM.",code completion; code prediction; example-centric programming; natural language-guided programming,"39, 55",,"Onward! 2021 - Proceedings of the 2021 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software, co-located with SPLASH 2021",Conference Paper,Scopus
77,,Programming with neural surrogates of programs,"Renda A., Ding Y., Carbin M.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119054640&doi=10.1145%2f3486607.3486748&partnerID=40&md5=e0fd265593d5f7ba6d534bc11202e392,10.1145/3486607.3486748,"Surrogates, models that mimic the behavior of programs, form the basis of a variety of development workflows. We study three surrogate-based design patterns, evaluating each in case studies on a large-scale CPU simulator. With surrogate compilation, programmers develop a surrogate that mimics the behavior of a program to deploy to end-users in place of the original program. Surrogate compilation accelerates the CPU simulator under study by 1.6×. With surrogate adaptation, programmers develop a surrogate of a program then retrain that surrogate on a different task. Surrogate adaptation decreases the simulator's error by up to 50%. With surrogate optimization, programmers develop a surrogate of a program, optimize input parameters of the surrogate, then plug the optimized input parameters back into the original program. Surrogate optimization finds simulation parameters that decrease the simulator's error by 5% compared to the error induced by expert-set parameters. In this paper we formalize this taxonomy of surrogate-based design patterns. We further describe the programming methodology common to all three design patterns. Our work builds a foundation for the emerging class of workflows based on programming with surrogates of programs. © 2021 Owner/Author.",machine learning; neural networks; programming languages; surrogate models,"18, 38",,"Onward! 2021 - Proceedings of the 2021 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software, co-located with SPLASH 2021",Conference Paper,Scopus
78,,Per Garment Capture and Synthesis for Real-time Virtual Try-on,"Chong T., Shen I.-C., Umetani N., Igarashi T.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118231110&doi=10.1145%2f3472749.3474762&partnerID=40&md5=df013c7ed8637fa6f007d08dabc55074,10.1145/3472749.3474762,"Virtual try-on is a promising application of computer graphics and human computer interaction that can have a profound real-world impact especially during this pandemic. Existing image-based works try to synthesize a try-on image from a single image of a target garment, but it inherently limits the ability to react to possible interactions. It is difficult to reproduce the change of wrinkles caused by pose and body size change, as well as pulling and stretching of the garment by hand. In this paper, we propose an alternative per garment capture and synthesis workflow to handle such rich interactions by training the model with many systematically captured images. Our workflow is composed of two parts: garment capturing and clothed person image synthesis. We designed an actuated mannequin and an efficient capturing process that collects the detailed deformations of the target garments under diverse body sizes and poses. Furthermore, we proposed to use a custom-designed measurement garment, and we captured paired images of the measurement garment and the target garments. We then learn a mapping between the measurement garment and the target garments using deep image-to-image translation. The customer can then try on the target garments interactively during online shopping. The proposed workflow requires certain manual labor, but we believe that the cost is acceptable given that the retailers are already paying significant costs for hiring professional photographers and models, stylists, and editors to take photographs for promotion. Our method can remove the need of hiring these costly professionals. We evaluated the effectiveness of the proposed system with ablation studies and quality comparison with previous virtual try-on methods. We perform a user study to show our promising virtual try-on performances. Moreover, we also demonstrate that we use our method for changing virtual costumes in video conferences. Finally, we provide the collected dataset as the cloth dataset parameterized by various viewing angles, body poses, and sizes. © 2021 ACM.",Deep image synthesis; Virtual Try-on,"457, 469",,UIST 2021 - Proceedings of the 34th Annual ACM Symposium on User Interface Software and Technology,Conference Paper,Scopus
79,,HoloBoard: A Large-format Immersive Teaching Board based on pseudo HoloGraphics,"Gong J., Han T., Guo S., Li J., Zha S., Zhang L., Tian F., Wang Q., Rui Y.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118226954&doi=10.1145%2f3472749.3474761&partnerID=40&md5=f8112ee1f17386947cd7c2bab435af4d,10.1145/3472749.3474761,"In this paper, we present HoloBoard, an interactive large-format pseduo-holographic display system for lecture based classes. With its unique properties of immersive visual display and transparent screen, we designed and implemented a rich set of novel interaction techniques like immersive presentation, role-play, and lecturing behind the scene that are potentially valuable for lecturing in class. We conducted a controlled experimental study to compare a HoloBoard class with a normal class through measuring students' learning outcomes and three dimensions of engagement (i.e., behavioral, emotional, and cognitive engagement). We used pre-/post- knowledge tests and multimodal learning analytics to measure students' learning outcomes and learning experiences. Results indicated that the lecture-based class utilizing HoloBoard lead to slightly better learning outcomes and a significantly higher level of student engagement. Given the results, we discussed the impact of HoloBoard as an immersive media in the classroom setting and suggest several design implications for deploying HoloBoard in immersive teaching practices. © 2021 ACM.",hologram; immersive learning; large-format display; mixed reality; teaching board,"441, 456",,UIST 2021 - Proceedings of the 34th Annual ACM Symposium on User Interface Software and Technology,Conference Paper,Scopus
80,,Just Speak It: Minimize Cognitive Load for Eyes-Free Text Editing with a Smart Voice Assistant,"Fan J., Xu C., Yu C., Shi Y.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118215776&doi=10.1145%2f3472749.3474795&partnerID=40&md5=473006c5417df702670f586750702808,10.1145/3472749.3474795,"Entering text precisely by voice, users might encounter colloquial inserts, inappropriate wording, and recognition errors, which brings difficulties to voice editing. Users need to locate the errors and then correct them. In eyes-free scenarios, this select-modify mode brings a cognitive burden and a risk of error. This paper introduces neural networks and pre-trained models to understand users' revision intention based on semantics, reducing the need for the information from users' statements. We present two strategies. One is to remove the colloquial inserts automatically. The other is to allow users to edit by just speaking out the target words without having to say the context and the incorrect text. Accordingly, our approach can predict whether to insert or replace, the incorrect text to replace, and the position to insert. We implement these strategies in SmartEdit, an eyes-free voice input agent controlled with earphone buttons. The evaluation shows that our techniques reduce the cognitive load and decrease the average failure rate by 54.1% compared to descriptive command or re-speaking. © 2021 ACM.",eyes-free; natural language processing.; Text editing; voice user interfaces; voice-based text editing,"910, 921",,UIST 2021 - Proceedings of the 34th Annual ACM Symposium on User Interface Software and Technology,Conference Paper,Scopus
81,,Screen Parsing: Towards Reverse Engineering of UI Models from Screenshots,"Wu J., Zhang X., Nichols J., Bigham J.P.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118209871&doi=10.1145%2f3472749.3474763&partnerID=40&md5=8efcc87866d1e028ed1d3dbbae7bd9c3,10.1145/3472749.3474763,"Automated understanding of user interfaces (UIs) from their pixels can improve accessibility, enable task automation, and facilitate interface design without relying on developers to comprehensively provide metadata. A first step is to infer what UI elements exist on a screen, but current approaches are limited in how they infer how those elements are semantically grouped into structured interface definitions. In this paper, we motivate the problem of screen parsing, the task of predicting UI elements and their relationships from a screenshot. We describe our implementation of screen parsing and provide an effective training procedure that optimizes its performance. In an evaluation comparing the accuracy of the generated output, we find that our implementation significantly outperforms current systems (up to 23%). Finally, we show three example applications that are facilitated by screen parsing: (i) UI similarity search, (ii) accessibility enhancement, and (iii) code generation from UI screenshots. © 2021 Owner/Author.",hierarchy prediction; ui semantics; user interface modeling,"470, 483",,UIST 2021 - Proceedings of the 34th Annual ACM Symposium on User Interface Software and Technology,Conference Paper,Scopus
82,,Hierarchical Summarization for Longform Spoken Dialog,"Li D., Chen T., Tung A., Chilton L.B.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118206532&doi=10.1145%2f3472749.3474771&partnerID=40&md5=25fc3b6af6f5f1d1bbf94ca414987de4,10.1145/3472749.3474771,"Every day we are surrounded by spoken dialog. This medium delivers rich diverse streams of information auditorily; however, systematically understanding dialog can often be non-trivial. Despite the pervasiveness of spoken dialog, automated speech understanding and quality information extraction remains markedly poor, especially when compared to written prose. Furthermore, compared to understanding text, auditory communication poses many additional challenges such as speaker disfluencies, informal prose styles, and lack of structure. These concerns all demonstrate the need for a distinctly speech tailored interactive system to help users understand and navigate the spoken language domain. While individual automatic speech recognition (ASR) and text summarization methods already exist, they are imperfect technologies; neither consider user purpose and intent nor address spoken language induced complications. Consequently, we design a two stage ASR and text summarization pipeline and propose a set of semantic segmentation and merging algorithms to resolve these speech modeling challenges. Our system enables users to easily browse and navigate content as well as recover from errors in these underlying technologies. Finally, we present an evaluation of the system which highlights user preference for hierarchical summarization as a tool to quickly skim audio and identify content of interest to the user. © 2021 ACM.",automatic speech recognition; information retrieval; machine learning applications; natural language interaction; summarization,"582, 597",,UIST 2021 - Proceedings of the 34th Annual ACM Symposium on User Interface Software and Technology,Conference Paper,Scopus
83,,Etna: Harvesting Action Graphs from Websites,"Riva O., Kace J.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118198946&doi=10.1145%2f3472749.3474752&partnerID=40&md5=8fa1bc7e7064ab01a25ba1bc54b543eb,10.1145/3472749.3474752,"Knowledge bases, such as Google knowledge graph, contain millions of entities (people, places, etc.) and billions of facts about them. While much is known about entities, little is known about the actions these entities relate to. On the other hand, the Web has lots of information about human tasks. A website for restaurant reservations, for example, implicitly knows about various restaurant-related actions (making reservations, delivering food, etc.), the inputs these actions require and their expected output; it can also be automated to execute those actions. To harvest action knowledge from websites, we propose Etna. Users demonstrate how to accomplish various tasks in a website, and Etna constructs an action-state model of the website visualized as an action graph. An action graph includes definitions of tasks and actions, knowledge about their start/end states, and execution scripts for their automation. We report on our experience in building action-state models of many commercial websites and use cases that leveraged them. © 2021 ACM.",Action graphs; graphical user interfaces; programming by demonstration; UI automation.; web,"312, 331",,UIST 2021 - Proceedings of the 34th Annual ACM Symposium on User Interface Software and Technology,Conference Paper,Scopus
84,,Situated Live Programming for Human-Robot Collaboration,"Senft E., Hagenow M., Radwin R., Zinn M., Gleicher M., Mutlu B.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118198228&doi=10.1145%2f3472749.3474773&partnerID=40&md5=41b48986e267277308332fa028f915da,10.1145/3472749.3474773,"We present situated live programming for human-robot collaboration, an approach that enables users with limited programming experience to program collaborative applications for human-robot interaction. Allowing end users, such as shop floor workers, to program collaborative robots themselves would make it easy to ""retask""robots from one process to another, facilitating their adoption by small and medium enterprises. Our approach builds on the paradigm of trigger-action programming (TAP) by allowing end users to create rich interactions through simple trigger-action pairings. It enables end users to iteratively create, edit, and refine a reactive robot program while executing partial programs. This live programming approach enables the user to utilize the task space and objects by incrementally specifying situated trigger-action pairs, substantially lowering the barrier to entry for programming or reprogramming robots for collaboration. We instantiate situated live programming in an authoring system where users can create trigger-action programs by annotating an augmented video feed from the robot's perspective and assign robot actions to trigger conditions. We evaluated this system in a study where participants (n = 10) developed robot programs for solving collaborative light-manufacturing tasks. Results showed that users with little programming experience were able to program HRC tasks in an interactive fashion and our situated live programming approach further supported individualized strategies and workflows. We conclude by discussing opportunities and limitations of the proposed approach, our system implementation, and our study and discuss a roadmap for expanding this approach to a broader range of tasks and applications. © 2021 ACM.",end-user programming; human-robot collaboration; human-robot interaction; trigger-action programming,"613, 625",,UIST 2021 - Proceedings of the 34th Annual ACM Symposium on User Interface Software and Technology,Conference Paper,Scopus
85,,A Spatial Music Listening Experience in Augmented Reality,"Lim L., Goh W.-Y., Downing M., Sra M.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117914835&doi=10.1145%2f3474349.3480218&partnerID=40&md5=4d2aadd3bea89a23fc83d9543df4746c,10.1145/3474349.3480218,"Live music provides a more immersive and social experience that recorded music cannot replicate. In a live music setting, listeners perceive sounds differently based on their position with respect to the musicians and can enjoy the experience with others. To make recorded music a dynamic listening experience, we propose and implement an app that adds a spatial dimension to music using augmented reality to allow users to listen to a song as if it were played live. The app lets users place virtual instruments around a physical space and plays the instrument track for each instrument. Users can move around in the space and change the importance of various sound localization aspects to customize their experience. Finally, users can record and livestream to share their listening experience with others. © 2021 Owner/Author.",Audio; Augmented Reality; Music; Sound Localization,"23, 25",,"Adjunct Publication of the 34th Annual ACM Symposium on User Interface Software and Technology, UIST 2021",Conference Paper,Scopus
86,,Author reputation measurement on question and answer sites by the classification of author-generated content,"Sezerer E., Tenekeci S., Acar A., Baloǧlu B., Tekir S.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119595051&doi=10.1142%2fS0218194021500479&partnerID=40&md5=964174191bde34940561ddc44538c223,10.1142/S0218194021500479,"In the field of software engineering, practitioners' share in the constructed knowledge cannot be underestimated and is mostly in the form of grey literature (GL). GL is a valuable resource though it is subjective and lacks an objective quality assurance methodology. In this paper, a quality assessment scheme is proposed for question and answer (Q&A) sites. In particular, we target stack overflow (SO) and stack exchange (SE) sites. We model the problem of author reputation measurement as a classification task on the author-provided answers. The authors' mean, median, and total answer scores are used as inputs for class labeling. State-of-the-art language models (BERT and DistilBERT) with a softmax layer on top are utilized as classifiers and compared to SVM and random baselines. Our best model achieves 63.8% accuracy in binary classification in SO design patterns tag and 71.6% accuracy in SE software engineering category. Superior performance in SE software engineering can be explained by its larger dataset size. In addition to quantitative evaluation, we provide qualitative evidence, which supports that the system's predicted reputation labels match the quality of provided answers. © 2021 World Scientific Publishing Company.",Author reputation measurement; Question and answer sites; Stack exchange; Stack overflow; Text classification,"1421, 1445",,International Journal of Software Engineering and Knowledge Engineering,Article,Scopus
87,,Improving AMR-to-text Generation with Multi-task Pre-training [基于多任务预训练的AMR文本生成研究],"Xu D.-Q., Li J.-H., Zhu M.-H., Zhou G.-D.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117004108&doi=10.13328%2fj.cnki.jos.006207&partnerID=40&md5=1ec3f06a980f9cdb66188c6ec2b7c716,10.13328/j.cnki.jos.006207,"Given an AMR (abstract meaning representation) graph, AMR-to-text generation aims to generate text with the same meaning. Related studies show that the performance of AMR-to-text severely suffers from the size of the manually annotated dataset. To alleviate the dependence on manually annotated dataset, this study proposes a novel multi-task pre-training for AMR-to-text generation. In particular, based on a large-scale automatic AMR dataset, three relevant pre-training tasks are defined, i.e., AMR denoising auto-encoder, sentence denoising auto-encoder, and AMR-to-text generation itself. In addition, to fine-tune the pre-training models, the vanilla fine-tuning method is further extended to multi-task learning fine-tuning, which enables the final model to maintain performance on both AMR-to-text and pre-training tasks. With the automatic dataset of 0.39M sentences, detailed experimentation on two AMR benchmarks shows that the proposed pre-training approach significantly improves the performance of AMR-to-text generation, with the improvement of 12.27 BLEU on AMR2.0 and 7.57 on AMR3.0, respectively. This greatly advances the state-of-the-art performance with 40.30 BLEU on AMR2.0 and 38.97 on AMR 3.0, respectively. To the best knowledge, this is the best result achieved so far on AMR 2.0 while AMR-to- text generation performance on AMR 3.0 is firstly reported. © Copyright 2021, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Abstract meaning representation (AMR); AMR-to-text generation; Multi-task pre-training; Sequence-to-sequence,"3036, 3050",,Ruan Jian Xue Bao/Journal of Software,Article,Scopus
88,,Fusion of 3D lung CT and serum biomarkers for diagnosis of multiple pathological types on pulmonary nodules,"Fu Y., Xue P., Li N., Zhao P., Xu Z., Ji H., Zhang Z., Cui W., Dong E.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114354926&doi=10.1016%2fj.cmpb.2021.106381&partnerID=40&md5=a3b73c9307d0f08edecf0d8af0168867,10.1016/j.cmpb.2021.106381,"Background and Objective:: Current researches on pulmonary nodules mainly focused on the binary-classification of benign and malignant pulmonary nodules. However, in clinical applications, it is not enough to judge whether pulmonary nodules are benign or malignant. In this paper, we proposed a fusion model based on the Lung Information Dataset Containing 3D CT Images and Serum Biomarkers (LIDC[sbnd]CISB) we constructed to accurately diagnose the types of pulmonary nodules in squamous cell carcinoma, adenocarcinoma, inflammation and other benign diseases. Methods:: Using single modal information of lung 3D CT images and single modal information of Lung Tumor Biomarkers (LTBs) in LIDC[sbnd]CISB, a Multi-resolution 3D Multi-classification deep learning model (Mr-Mc) and a Multi-Layer Perceptron machine learning model (MLP) were constructed for diagnosing multiple pathological types of pulmonary nodules, respectively. To comprehensively use the double modal information of CT images and LTBs, we used transfer learning to fuse Mr-Mc and MLP, and constructed a multimodal information fusion model that could classify multiple pathological types of benign and malignant pulmonary nodules. Results:: Experiments showed that the constructed Mr-Mc model can achieve an average accuracy of 0.805 and MLP model can achieve an average accuracy of 0.887. The fusion model was verified on a dataset containing 64 samples, and achieved an average accuracy of 0.906. Conclusions:: This is the first study to simultaneously use CT images and LTBs to diagnose multiple pathological types of benign and malignant pulmonary nodules, and experiments showed that our research was more advanced and more suitable for practical clinical applications. © 2021",Machine learning; Multi-resolution deep learning; Multimodal information fusion; Multiple pathological types; Pulmonary nodules,,,Computer Methods and Programs in Biomedicine,Article,Scopus
89,,"Deep learning, explained: Fundamentals, explainability, and bridgeability to process-based modelling",Razavi S.,2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114319353&doi=10.1016%2fj.envsoft.2021.105159&partnerID=40&md5=3d45ea302786d3802f3dfbc27e8e22b8,10.1016/j.envsoft.2021.105159,"Recent breakthroughs in artificial intelligence (AI), and particularly in deep learning (DL), have created tremendous excitement and opportunities in the earth and environmental sciences communities. To leverage these new ‘data-driven’ technologies, however, one needs to understand the fundamental concepts that give rise to DL and how they differ from ‘process-based’, mechanistic modelling. This paper revisits those fundamentals and addresses 10 questions that might be posed by earth and environmental scientists, and with the aid of a real-world modelling experiment, it explains some critical, but often ignored, issues DL may face in practice. The overarching objective is to contribute to a future of AI-assisted earth and environmental sciences where AI models can (1) embrace the typically ignored knowledge base available, (2) function credibly in ‘true’ out-of-sample prediction, and (3) handle non-stationarity in earth and environmental systems. Comparing and contrasting earth and environmental problems with prominent AI applications, such as playing chess and trading in stock markets, provides critical insights for better directing future research in this field. © 2021 The Author(s)",Artificial intelligence; Artificial neural networks; Deep learning; Earth systems; Hydrology; Machine learning; Process-based modelling,,,Environmental Modelling and Software,Article,Scopus
90,,"Integrating a spoken dialogue system, nursing records, and activity data collection based on smartphones","Mairittha T., Mairittha N., Inoue S.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114312554&doi=10.1016%2fj.cmpb.2021.106364&partnerID=40&md5=95e21961a7dd8cae49a6359aab8936ae,10.1016/j.cmpb.2021.106364,"Background and Objective: This study describes the integration of a spoken dialogue system and nursing records on an Android smartphone application intending to help nurses reduce documentation time and improve the overall experience of a healthcare setting. The application also incorporates with collecting personal sensor data and activity labels for activity recognition. Methods: We developed a joint model based on a bidirectional long-short term memory and conditional random fields (Bi-LSTM-CRF) to identify user intention and extract record details from user utterances. Then, we transformed unstructured data into record inputs on the smartphone application. Results: The joint model achieved the highest F1-score at 96.79%. Moreover, we conducted an experiment to demonstrate the proposed model's capability and feasibility in recording in realistic settings. Our preliminary evaluation results indicate that when using the dialogue-based, we could increase the percentage of documentation speed to 58.13% compared to the traditional keyboard-based. Conclusions: Based on our findings, we highlight critical and promising future research directions regarding the design of the efficient spoken dialogue system and nursing records. © 2021 Elsevier B.V.",Activity recognition; Dialogue system; Electronic health record; Entity extraction; Intent classification; Nursing record,,,Computer Methods and Programs in Biomedicine,Article,Scopus
91,,Combined Transfer Learning and Test-Time Augmentation Improves Convolutional Neural Network-Based Semantic Segmentation of Prostate Cancer from Multi-Parametric MR Images,"Hoar D., Lee P.Q., Guida A., Patterson S., Bowen C.V., Merrimen J., Wang C., Rendon R., Beyea S.D., Clarke S.E.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114302101&doi=10.1016%2fj.cmpb.2021.106375&partnerID=40&md5=0181490d89442e17b0ad0bd9e1eddfa8,10.1016/j.cmpb.2021.106375,"Purpose: Multiparametric MRI (mp-MRI) is a widely used tool for diagnosing and staging prostate cancer. The purpose of this study was to evaluate whether transfer learning, unsupervised pre-training and test-time augmentation significantly improved the performance of a convolutional neural network (CNN) for pixel-by-pixel prediction of cancer vs. non-cancer using mp-MRI datasets. Methods: 154 subjects undergoing mp-MRI were prospectively recruited, 16 of whom subsequently underwent radical prostatectomy. Logistic regression, random forest and CNN models were trained on mp-MRI data using histopathology as the gold standard. Transfer learning, unsupervised pre-training and test-time augmentation were used to boost CNN performance. Models were evaluated using Dice score and area under the receiver operating curve (AUROC) with leave-one-subject-out cross validation. Permutation feature importance testing was performed to evaluate the relative value of each MR contrast to CNN model performance. Statistical significance (p<0.05) was determined using the paired Wilcoxon signed rank test with Benjamini-Hochberg correction for multiple comparisons. Results: Baseline CNN outperformed logistic regression and random forest models. Transfer learning and unsupervised pre-training did not significantly improve CNN performance over baseline; however, test-time augmentation resulted in significantly higher Dice scores over both baseline CNN and CNN plus either of transfer learning or unsupervised pre-training. The best performing model was CNN with transfer learning and test-time augmentation (Dice score of 0.59 and AUROC of 0.93). The most important contrast was apparent diffusion coefficient (ADC), followed by Ktrans and T2, although each contributed significantly to classifier performance. Conclusions: The addition of transfer learning and test-time augmentation resulted in significant improvement in CNN segmentation performance in a small set of prostate cancer mp-MRI data. Results suggest that these techniques may be more broadly useful for the optimization of deep learning algorithms applied to the problem of semantic segmentation in biomedical image datasets. However, further work is needed to improve the generalizability of the specific model presented herein. © 2021 Elsevier B.V.",Computer aided diagnosis; Convolutional neural network; Machine learning; MRI; Prostate cancer; Segmentation,,,Computer Methods and Programs in Biomedicine,Article,Scopus
92,,Diagnosis and grading of vesicoureteral reflux on voiding cystourethrography images in children using a deep hybrid model,"EROGLU Y., YILDIRIM K., ÇINAR A., YILDIRIM M.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114023780&doi=10.1016%2fj.cmpb.2021.106369&partnerID=40&md5=fd11384b9a6f78c244cbbb90cdc1f010,10.1016/j.cmpb.2021.106369,"Background and objective: Vesicoureteral reflux is the leakage of urine from the bladder into the ureter. As a result, urinary tract infections and kidney scarring can occur in children. Voiding cystourethrography is the primary radiological imaging method used to diagnose vesicoureteral reflux in children with a history of recurrent urinary tract infection. Besides the diagnosis of reflux, it is graded with voiding cystourethrography. In this study, we aimed to diagnose and grade vesicoureteral reflux in Voiding cystourethrography images using hybrid CNN in deep learning methods. Methods: Images of pediatric patients diagnosed with VUR between 2016 and 2021 in our hospital (Firat University Hospital) were graded according to the international vesicoureteral reflux radiographic grading system. VCUG images of 236 normal and 992 with vesicoureteral reflux pediatric patients were available. A total of 6 classes were created as normal and graded 1-5 patients. Results: In this study, a hybrid-based mRMR (Minimum Redundancy Maximum Relevance) using CNN (Convolutional Neural Networks) model is developed for the diagnosis and grading of vesicoureteral reflux on voiding cystourethrography images. Googlenet, MobilenetV2, and Densenet201 models are used as a part of the hybrid architecture. The obtained features from these architectures are examined in concatenating process. Then, these features are classified in machine learning classifiers after optimizing with the mRMR method. Among the models used in the study, the highest accuracy value was obtained in the proposed model with an accuracy rate of 96.9%. Conclusions: It shows that the hybrid model developed according to the findings of our study can be used in the diagnosis and grading of vesicoureteral reflux in voiding cystourethrography images. © 2021",Children; Classifiers; Deep learning; mRMR; Vesicoureteral Reflux; Voiding cystourethrography,,,Computer Methods and Programs in Biomedicine,Article,Scopus
93,,Deep learning enabled brain shunt valve identification using mobile phones,"Sujit S.J., Bonfante E., Aein A., Coronado I., Riascos-Castaneda R., Giancardo L.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113799144&doi=10.1016%2fj.cmpb.2021.106356&partnerID=40&md5=cb91cc56b4f5ac6c2cf79d1efd0af062,10.1016/j.cmpb.2021.106356,"Background and Objective: Accurate information concerning implanted medical devices prior to a Magnetic resonance imaging (MRI) examination is crucial to assure safety of the patient and to address MRI induced unintended changes in device settings. The identification of these devices still remains a very challenging task. In this paper, with the aim of providing a faster device detection, we propose the adoption of deep learning for medical device detection from X-rays. Method: In particular, we propose a pipeline for the identification of implanted programmable cerebrospinal fluid shunt valves using X-ray images of the radiologist workstation screens captured with mobile phone integrated cameras at different angles and illuminations. We compare the proposed convolutional neural network with published methods. Results: Experimental results show that this approach outperforms methods trained on images digitally transferred directly from the scanners and then applied on mobile phones images (mean accuracy 95% vs 77%, Avg. Precision 0.96 vs 0.77, Avg. Recall 0.95 vs 0.77, Avg. F1-score 0.95 vs 0.77) and existing published methods based on transfer learning fine-tuned directly on the mobile phone images (mean accuracy 94% vs 75%, Avg. Precision 0.94 vs 0.75, Avg. Recall 0.94 vs 0.75, Avg. F1-score 0.94 vs 0.75). Conclusion: An automated shunt valve identification system is a promising safety tool for radiologists to efficiently coordinate the care of patients with implanted devices. An image-based safety system able to be deployed on a mobile phone would have significant advantages over methods requiring direct input from X-ray scanners or clinical picture archiving and communication system (PACS) in terms of ease of integration in the hospital or clinical ecosystems. © 2021",Deep learning; Magnetic resonance imaging; Mobile phone camera; Programmable cerebrospinal fluid shunt valve,,,Computer Methods and Programs in Biomedicine,Article,Scopus
94,,Deep learning-enhanced extraction of drainage networks from digital elevation models,"Mao X., Chow J.K., Su Z., Wang Y.-H., Li J., Wu T., Li T.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110660809&doi=10.1016%2fj.envsoft.2021.105135&partnerID=40&md5=0c8d984791e305d4049e392d8a66dd5a,10.1016/j.envsoft.2021.105135,"Drainage network extraction is essential for different research and applications. However, traditional methods have low efficiency, low accuracy for flat regions, and difficulties in detecting channel heads. Although deep learning techniques have been used to solve these problems, different challenges remain unsolved. Therefore, we introduced distributed representations of aspect features to facilitate the deep learning model calculating the flow direction; adopted a semantic segmentation model, U-Net, to improve the accuracy and efficiency in predicting flow directions and in pixel classifications; and used postprocessing to delineate the flowlines. Our proposed framework achieved state-of-the-art results compared with the traditional methods and the published deep-learning-based methods. Further, case study results demonstrated that our framework can extract drainage networks with high accuracy for rivers of different widths flowing through terrains of different characteristics. This framework, requiring no parameters provided by users, can also produce waterbody polygons and allow cyclic graphs in the drainage network. © 2021",Deep learning; Digital elevation model; Drainage network extraction; Semantic segmentation,,,Environmental Modelling and Software,Article,Scopus
95,,ARTINALI++: Multi-dimensional Specification Mining for Complex Cyber-Physical System Security,"Aliabadi M.R., Asl M.V., Ghavamizadeh R.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108356860&doi=10.1016%2fj.jss.2021.111016&partnerID=40&md5=50ce2fb8b563c4764b0a38db445b9142,10.1016/j.jss.2021.111016,"Cyber-Physical Systems (CPSes) have been investigated as a key area of research since they are the core of Internet of Things. CPSs integrate computing and communication with control and monitoring of entities in the physical world. Due to the tight coupling of cyber and physical domains, and to the possible catastrophic consequences of the malicious attacks on critical infrastructures, security is one of the key concerns. However, the exponential growth of IoT has led to deployment of CPSes without support for enforcing important security properties. Specification-based Intrusion Detection Systems (IDS) have been shown to be effective for securing these systems. Mining the specifications of CPSes by experts is a cumbersome and error-prone task. Therefore, it is essential to dynamically monitor the CPS to learn its common behaviors and formulate specifications for detecting malicious bugs and security attacks. Existing solutions for specification mining only combine data and events, but not time. However, time is a semantic property in CPS systems, and hence incorporating time in addition to data and events, is essential for obtaining high accuracy. This paper proposes ARTINALI++, which dynamically mines specifications in CPS systems with arbitrary size and complexity. ARTINALI++ captures the security properties by incorporating time as a substantial property of the system, and generate a multi-dimensional model for the general CPS systems. Moreover, it enhances the model through discovering invariants that represent the physical motions and distinct operational modes in complex CPS systems. We build Intrusion Detection Systems based on ARTINALI++ for three CPSes with various levels of complexity including smart meter, smart artificial pancreas and unmanned aerial vehicle, and measure their detection accuracy. We find that the ARTINALI++ significantly reduces the ratio of false positives and false negatives by 23.45% and 73.6% on average, respectively, over other dynamic specification mining tools on the three CPS platforms. © 2021 Elsevier Inc.",Cyber-Physical Systems; Intrusion Detection Systems; Program analysis; Safety; Security; Specification mining,,,Journal of Systems and Software,Article,Scopus
96,,Software defect prediction based on enhanced metaheuristic feature selection optimization and a hybrid deep neural network,"Zhu K., Ying S., Zhang N., Zhu D.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108275396&doi=10.1016%2fj.jss.2021.111026&partnerID=40&md5=dc85a8949d2f1e47bc510ac279fc9284,10.1016/j.jss.2021.111026,"Software defect prediction aims to identify the potential defects of new software modules in advance by constructing an effective prediction model. However, the model performance is susceptible to irrelevant and redundant features. In addition, previous studies mainly use traditional data mining or machine learning techniques for defect prediction, the prediction performance is not superior enough. For the first issue, motivated by the idea of search based software engineering, we leverage the recently proposed whale optimization algorithm (WOA) and another complementary simulated annealing (SA) to construct an enhanced metaheuristic search based feature selection algorithm named EMWS, which can effectively select fewer but closely related representative features. For the second issue, we employ a hybrid deep neural network — convolutional neural network (CNN) and kernel extreme learning machine (KELM) to construct a unified defect prediction predictor called WSHCKE, which can further integrate the selected features into the abstract deep semantic features by CNN and boost the prediction performance by taking full advantage of the strong classification capacity of KELM. We conduct extensive experiments for feature selection or extraction and defect prediction across 20 widely-studied software projects on four evaluation indicators. Experimental results demonstrate the superiority of EMWS and WSHCKE. © 2021",Convolutional neural network; Kernel extreme learning machine; Metaheuristic feature selection; Software defect prediction; Whale optimization algorithm,,,Journal of Systems and Software,Article,Scopus
97,,Easy-to-Deploy API Extraction by Multi-Level Feature Embedding and Transfer Learning,"Ma S., Xing Z., Chen C., Chen C., Qu L., Li G.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073564213&doi=10.1109%2fTSE.2019.2946830&partnerID=40&md5=d8b8d08d7bd47cb0e468854f3d1637d9,10.1109/TSE.2019.2946830,"Application Programming Interfaces (APIs) have been widely discussed on social-technical platforms (e.g., Stack Overflow). Extracting API mentions from such informal software texts is the prerequisite for API-centric search and summarization of programming knowledge. Machine learning based API extraction has demonstrated superior performance than rule-based methods in informal software texts that lack consistent writing forms and annotations. However, machine learning based methods have a significant overhead in preparing training data and effective features. In this paper, we propose a multi-layer neural network based architecture for API extraction. Our architecture automatically learns character-, word- and sentence-level features from the input texts, thus removing the need for manual feature engineering and the dependence on advanced features (e.g., API gazetteers) beyond the input texts. We also propose to adopt transfer learning to adapt a source-library-trained model to a target-library, thus reducing the overhead of manual training-data labeling when the software text of multiple programming languages and libraries need to be processed. We conduct extensive experiments with six libraries of four programming languages which support diverse functionalities and have different API-naming and API-mention characteristics. Our experiments investigate the performance of our neural architecture for API extraction in informal software texts, the importance of different features, the effectiveness of transfer learning. Our results confirm not only the superior performance of our neural architecture than existing machine learning based methods for API extraction in informal software texts, but also the easy-to-deploy characteristic of our neural architecture. © 1976-2012 IEEE.",API extraction; CNN; LSTM; transfer learning; word embedding,"2296, 2311",,IEEE Transactions on Software Engineering,Article,Scopus
98,,OHTLoc: an online heterogeneous transfer method on wifi-based indoor localization system: work-in-progress,"Han L., Bian C.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117465107&doi=10.1145%2f3477244.3477612&partnerID=40&md5=1198bdedcc83355f39b64c11ae5a4b7b,10.1145/3477244.3477612,"With the development of wireless network technology, the WiFi-based indoor localization methods incorporating machine learning have attracted wide attention due to its easy deployment and low cost characteristics. However, the existing learning methods are limited to locating homogeneous and tagged target data. Such strict conditions do not exist in the actual indoor positioning environment, and therefore cannot meet people's locational needs. In this article, we design an Online Heterogeneous Transfer method in Indoor Localization(OHTLoc), a novel transfer learning approach that can realize online location prediction based on the RSS(Received Signal Strength) fingerprint and CSI(Channel State Information) data using WLANs. In particular, OHTLoc does not require any tags on the target data. This is the first time this type of algorithm has been proposed in the field of indoor localization. The prediction results of the target demonstrate showed in the experiment part demonstrate the effectiveness of the proposed technique. © 2021 ACM.",CSI; heterogeneous transfer learning,"25, 26",,"Proceedings - 2021 International Conference on Embedded Software, EMSOFT 2021",Conference Paper,Scopus
99,,A Preliminary Evaluation of CPDP Approaches on Just-in-Time Software Defect Prediction,"Amasaki S., Aman H., Yokogawa T.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119197756&doi=10.1109%2fSEAA53835.2021.00042&partnerID=40&md5=3b1456b8500705e2743691b9567c2aa4,10.1109/SEAA53835.2021.00042,CONTEXT: Just-in-Time defect prediction is to specify the suspicious code commits that might make a product cause defects. Building JIT defect prediction models require a commit history and their fixed defect records. The shortage of commits of new projects motivated research of JIT cross-project defect prediction (CPDP). CPDP approaches proposed for component-level defect prediction were barely evaluated under JIT CPDP. OBJECTIVE: To explore the effects of CPDP approaches for component-level defect prediction where JIT CPDP is adopted. METHOD: A case study was conducted through two commit dataset suites provided in past studies for JIT defect prediction. JIT defect predictions with and without 21 CPDP approaches were compared regarding the classification performance using AUC. The CPDP approaches were also compared with each other. RESULTS: Most CPDP approaches changed the prediction performance of a baseline that simply combined all CP data. A few CPDP approaches could improve the prediction performance significantly. Not a few approaches worsened the performance significantly. The results based on the two suites could specify two CPDP approaches safer than the baseline. The results were inconsistent with a previous study. CONCLUSIONS: CPDP approaches for component-level might be effective for JIT CPDP. Further evaluations were needed to bring a firm conclusion. © 2021 IEEE.,cross-project defect prediction; empirical study; just-in-time defect prediction,"279, 286",,"Proceedings - 2021 47th Euromicro Conference on Software Engineering and Advanced Applications, SEAA 2021",Conference Paper,Scopus
100,,Personalized API Recommendations,"Yang W., Zhou Y., Huang Z.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116755122&doi=10.1142%2fS021819402150042X&partnerID=40&md5=9ba3ff01814989b4e8a253ada2284f3b,10.1142/S021819402150042X,"Application Programming Interfaces (APIs) play an important role in modern software development. Developers interact with APIs on a daily basis and thus need to learn and memorize those APIs suitable for implementing the required functions. This can be a burden even for experienced developers since there exists a mass of available APIs. API recommendation techniques focus on assisting developers in selecting suitable APIs. However, existing API recommendation techniques have not taken the developers personal characteristics into account. As a result, they cannot provide developers with personalized API recommendation services. Meanwhile, they lack the support for self-defined APIs in the recommendation. To this end, we aim to propose a personalized API recommendation method that considers developers' differences. Our API recommendation method is based on statistical language. We propose a model structure that combines the N-gram model and the long short-Term memory (LSTM) neural network and train predictive models using API invoking sequences extracted from GitHub code repositories. A general language model trained on all sorts of code data is first acquired, based on which two personalized language models that recommend personalized library APIs and self-defined APIs are trained using the code data of the developer who needs personalized services. We evaluate our personalized API recommendation method on real-world developers, and the experimental results show that our approach achieves better accuracy in recommending both library APIs and self-defined APIs compared with the state-of-The-Art. The experimental results also confirm the effectiveness of our hybrid model structure and the choice of the LSTM's size. © 2021 World Scientific Publishing Company.",API recommendation; LSTM; N-gram; personalized recommendation; transfer learning,"1299, 1327",,International Journal of Software Engineering and Knowledge Engineering,Article,Scopus
101,,Automatic Logging Decision Through Program Layered Structure Tree and Transfer Learning [基于程序层次树的日志打印位置决策方法],"Jia T., Li Y., Zhang Q.-X., Wu Z.-H.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115443822&doi=10.13328%2fj.cnki.jos.005990&partnerID=40&md5=75f77474df0ebff3ff4bae8014109fa9,10.13328/j.cnki.jos.005990,"With the development of AIOps, log-based failure diagnosis has become more and more important. However, this technique has a key bottleneck-the quality of logs. Today, the lack of log printing specifications and guidance for programmers is a key factor of poor log quality, thus the need of automatic logging decision so as to improve log quality is becoming urgent. This study focuses on automatic logging decision. Specifically, the aim is to propose a general logging point decision approach. Different from existing works, an automatic feature vector generation method is proposed based on program layered structure tree and reverse composition, which can be applied to software systems written in different programming languages. In addition, this study leverages transfer learning algorithms to achieve cross-component and cross-project logging point decision. The approach is evaluated on five popular open source software systems, namely, OpenStack, Tensorflow, SaltCloud, Hadoop, and Angel, in three typical application scenarios including software upgrading, new component development, and new project development. Results show that the proposed approach performs about 95% accuracy in Java projects and 70% accuracy in Python projects on average. © Copyright 2021, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Logging decision; Program layered structure tree; Transfer learning,"2713, 2728",,Ruan Jian Xue Bao/Journal of Software,Article,Scopus
102,,Kernel spectral embedding transfer ensemble for heterogeneous defect prediction,"Tong H., Liu B., Wang S.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115233465&doi=10.1109%2fTSE.2019.2939303&partnerID=40&md5=e2fd18304262bfe76cf51c49712ef9d7,10.1109/TSE.2019.2939303,"Cross-project defect prediction (CPDP) refers to predicting defects in the target project lacking of defect data by using prediction models trained on the historical defect data of other projects (i.e., source data). However, CPDP requires the source and target projects have common metric set (CPDP-CM). Recently, heterogeneous defect prediction (HDP) has drawn the increasing attention, which predicts defects across projects having heterogeneous metric sets. However, building high-performance HDP methods remains a challenge owing to several serious challenges including class imbalance problem, nonlinear, and the distribution differences between source and target datasets. In this paper, we propose a novel kernel spectral embedding transfer ensemble (KSETE) approach for HDP. KSETE first addresses the class-imbalance problem of the source data and then tries to find the latent common feature space for the source and target datasets by combining kernel spectral embedding, transfer learning, and ensemble learning. Experiments are performed on 22 public projects in both HDP and CPDP-CM scenarios in terms of multiple well-known performance measures such as, AUC, G-Measure, and MCC. The experimental results show that (1) KSETE improves the performance over previous HDP methods by at least 22.7, 138.9, and 494.4 percent in terms of AUC, G-Measure, and MCC, respectively. (2) KSETE improves the performance over previous CPDP-CM methods by at least 4.5, 30.2, and 17.9 percent in AUC, G-Measure, and MCC, respectively. It can be concluded that the proposed KSETE is very effective in both the HDP scenario and the CPDP-CM scenario. © 1976-2012 IEEE.",class imbalance learning; cross-project defect prediction; ensemble learning; Heterogeneous defect prediction; multiple kernel learning; spectral embedding; transfer learning,"1886, 1906",,IEEE Transactions on Software Engineering,Article,Scopus
103,,Which variables should i log?,"Liu Z., Xia X., Lo D., Xing Z., Hassan A.E., Li S.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115222248&doi=10.1109%2fTSE.2019.2941943&partnerID=40&md5=db49f1d6ab7d0ab12b6f6bcff3bd027b,10.1109/TSE.2019.2941943,"Developers usually depend on inserting logging statements into the source code to collect system runtime information. Such logged information is valuable for software maintenance. A logging statement usually prints one or more variables to record vital system status. However, due to the lack of rigorous logging guidance and the requirement of domain-specific knowledge, it is not easy for developers to make proper decisions about which variables to log. To address this need, in this work, we propose an approach to recommend logging variables for developers during development by learning from existing logging statements. Different from other prediction tasks in software engineering, this task has two challenges: 1) Dynamic labels - different logging statements have different sets of accessible variables, which means in this task, the set of possible labels of each sample is not the same. 2) Out-of-vocabulary words - identifiers' names are not limited to natural language words and the test set usually contains a number of program tokens which are out of the vocabulary built from the training set and cannot be appropriately mapped to word embeddings. To deal with the first challenge, we convert this task into a representation learning problem instead of a multi-label classification problem. Given a code snippet which lacks a logging statement, our approach first leverages a neural network with an RNN (recurrent neural network) layer and a self-attention layer to learn the proper representation of each program token, and then predicts whether each token should be logged through a unified binary classifier based on the learned representation. To handle the second challenge, we propose a novel method to map program tokens into word embeddings by making use of the pre-trained word embeddings of natural language tokens. We evaluate our approach on 9 large and high-quality Java projects. Our evaluation results show that the average MAP of our approach is over 0.84, outperforming random guess and an information-retrieval-based method by large margins. © 1976-2012 IEEE.",Log; logging variable; representation learning; word embedding,"2012, 2031",,IEEE Transactions on Software Engineering,Review,Scopus
104,,Automatic arteriosclerotic retinopathy grading using four-channel with image merging,"Gao S., Gao L., Quan X., Zhang H., Bai H., Kang C.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114211467&doi=10.1016%2fj.cmpb.2021.106274&partnerID=40&md5=1995808a4d0623715fb5095e5bc16e49,10.1016/j.cmpb.2021.106274,"BACKGROUND AND OBJECTIVE: Arteriosclerosis can reflect the severity of hypertension, which is one of the main diseases threatening human life safety. But Arteriosclerosis retinopathy detection involves costly and time-consuming manual assessment. To meet the urgent needs of automation, this paper developed a novel arteriosclerosis retinopathy grading method based on convolutional neural network. METHODS: Firstly, we propose a good scheme for extracting features facing the fundus blood vessel background using image merging for contour enhancement. In this step, the original image is dealt with adaptive threshold processing to generate the new contour channel, which merge with the original three-channel image. Then, we employ the pre-trained convolutional neural network with transfer learning to speed up training and contour image channel parameter with Kaiming initialization. Moreover, ArcLoss is applied to increase inter-class differences and intra-class similarity aiming to the high similarity of images of different classes in the dataset. RESULTS: The accuracy of arteriosclerosis retinopathy grading achieved by the proposed method is up to 65.354%, which is nearly 4% higher than those of the exiting methods. The Kappa of our method is 0.508 in arteriosclerosis retinopathy grading. CONCLUSIONS: An experimental study on multiple metrics demonstrates the superiority of our method, which will be a useful to the toolbox for arteriosclerosis retinopathy grading. Copyright © 2021. Published by Elsevier B.V.",ArcLossdeep; Arteriosclerotic retinopathy grading; Contour channel; convolutional neural network; Image merge,"106274, nan",,Computer methods and programs in biomedicine,Article,Scopus
105,,Naturally!: How Breakthroughs in Natural Language Processing Can Dramatically Help Developers,"Sawant A.A., Devanbu P.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113389171&doi=10.1109%2fMS.2021.3086338&partnerID=40&md5=010536b7a812fd5106dd1216417284b8,10.1109/MS.2021.3086338,"Taking advantage of the naturalness hypothesis for code, recent development, and research has focused on applying machine learning (ML) techniques originally developed for natural language processing (NLP) to drive a new wave of tools and applications aimed specifically for software engineering (SE) tasks. This drive to apply ML and deep learning (DL) has been animated by the large-scale availability of software development data (e.g., source code, code comments, code review comments, commit data, and so on) available from open source platforms such as GitHub and Bitbucket. © 1984-2012 IEEE.",,"118, 123",,IEEE Software,Article,Scopus
106,,"Prediction of COVID Criticality Score with Laboratory, Clinical and CT Images using Hybrid Regression Models","Perumal V., Narayanan V., Rajasekar S.J.S.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112555404&doi=10.1016%2fj.cmpb.2021.106336&partnerID=40&md5=7e8cb9f725a0ba8a33cd8b1d62541431,10.1016/j.cmpb.2021.106336,"Background and Objective: Rapid and precise diagnosis of COVID-19 is very critical in hotspot regions. The main aim of this proposed work is to investigate the baseline, laboratory and CT features of COVID-19 affected patients of two groups (Early and Critical stages). The detection model for COVID-19 is built depending upon the manifestations that define the severity of the disease. Methods: The CT scan images are fed into the various deep learning, machine learning and hybrid learning models to mine the necessary features and predict CT Score. The predicted CT score along with other clinical, laboratory and CT scan image features are then passed to train the various Regression models for predicting the COVID Criticality (CC) Score. These baseline, laboratory and CT features of COVID-19 are reduced using Statistical analysis and Univariate logistic regression analysis. Results: When analysing the prediction of CT scores using images alone, AlexNet+Lasso yields better outcome with regression score of 0.9643 and RMSE of 0.0023 when compared with Decision tree (RMSE of 0.0034; Regression score of 0.9578) and GRU (RMSE of 0.1253; regression score of 0.9323). When analysing the prediction of CC scores using CT scores and other baseline, laboratory and CT features, VGG-16+Linear Regression yields better results with regression score of 0.9911 and RMSE of 0.0002 when compared with Linear SVR (RMSE of 0.0006; Regression score of 0.9911) and LSTM (RMSE of 0.0005; Regression score of 0.9877). The correlation analysis is performed to identify the significance of utilizing other features in prediction of CC Score. The correlation coefficient of CT scores with actual value is 0.93 and 0.92 for Early stage group and Critical stage group respectively. The correlation coefficient of CC scores with actual value is 0.96 for Early stage group and 0.95 for Critical stage group.The classification of COVID-19 patients are carried out with the help of predicted CC Scores. Conclusions: This proposed work is carried out in the motive of helping radiologists in faster categorization of COVID patients as Early or Severe staged using CC Scores. The automated prediction of COVID Criticality Score using our diagnostic model can help radiologists and physicians save time for carrying out further treatment and procedures. © 2021 Elsevier B.V.",Chest CT images; Clinical features; Convolutional neural network (CNN); COVID Criticality score; Laboratory features; Regression models,,,Computer Methods and Programs in Biomedicine,Article,Scopus
107,,TEM virus images: Benchmark dataset and deep learning classification,"Matuszewski D.J., Sintorn I.-M.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111954042&doi=10.1016%2fj.cmpb.2021.106318&partnerID=40&md5=6430bfb41bcdddd4e7f2f1b55de32eb6,10.1016/j.cmpb.2021.106318,"Background and Objective: To achieve the full potential of deep learning (DL) models, such as understanding the interplay between model (size), training strategy, and amount of training data, researchers and developers need access to new dedicated image datasets; i.e., annotated collections of images representing real-world problems with all their variations, complexity, limitations, and noise. Here, we present, describe and make freely available an annotated transmission electron microscopy (TEM) image dataset. It constitutes an interesting challenge for many practical applications in virology and epidemiology; e.g., virus detection, segmentation, classification, and novelty detection. We also present benchmarking results for virus detection and recognition using some of the top-performing (large and small) networks as well as a handcrafted very small network. We compare and evaluate transfer learning and training from scratch hypothesizing that with a limited dataset, transfer learning is crucial for good performance of a large network whereas our handcrafted small network performs relatively well when training from scratch. This is one step towards understanding how much training data is needed for a given task. Methods: The benchmark dataset contains 1245 images of 22 virus classes. We propose a representative data split into training, validation, and test sets for this dataset. Moreover, we compare different established DL networks and present a baseline DL solution for classifying a subset of the 14 most-represented virus classes in the dataset. Results: Our best model, DenseNet201 pre-trained on ImageNet and fine-tuned on the training set, achieved a 0.921 F1-score and 93.1% accuracy on the proposed representative test set. Conclusions: Public and real biomedical datasets are an important contribution and a necessity to increase the understanding of shortcomings, requirements, and potential improvements for deep learning solutions on biomedical problems or deploying solutions in clinical settings. We compared transfer learning to learning from scratch on this dataset and hypothesize that for limited-sized datasets transfer learning is crucial for achieving good performance for large models. Last but not least, we demonstrate the importance of application knowledge in creating datasets for training DL models and analyzing their results. © 2021",CNN; Convolutional neural networks; Dataset curation; Transfer learning; Transmission electron microscopy; Virus recognition,,,Computer Methods and Programs in Biomedicine,Article,Scopus
108,,Comparative analysis of pulmonary nodules segmentation using multiscale residual U-Net and fuzzy C-means clustering,"Shi J., Ye Y., Zhu D., Su L., Huang Y., Huang J.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111928389&doi=10.1016%2fj.cmpb.2021.106332&partnerID=40&md5=8348f7764455e47cca8448960a6b50df,10.1016/j.cmpb.2021.106332,"Background and Objective: Pulmonary nodules have different shapes and uneven density, and some nodules adhere to blood vessels, pleura and other anatomical structures, which increase the difficulty of nodule segmentation. The purpose of this paper is to use multiscale residual U-Net to accurately segment lung nodules with complex geometric shapes, while comparing it with fuzzy C-means clustering and manual segmentation. Method: We selected 58 computed tomography (CT) scan images of patients with different lung nodules for image segmentation. This paper proposes an automatic segmentation algorithm for lung nodules based on multiscale residual U-Net. In order to verify the accuracy of the method, we also conducted comparative experiments, while comparing it with fuzzy C-means clustering. Results: Compared with the other two methods, the segmentation of lung nodules based on multiscale residual U-Net has a higher accuracy, with an accuracy rate of 94.57%. This method not only maintains a high accuracy rate, but also shortens the recognition time significantly with a segmentation time of 3.15 s. Conclusions: The diagnosis method of lung nodules combined with deep learning has a good market prospect and can improve the efficiency of doctors in diagnosing benign and malignant lung nodules. © 2021",Convolutional neural network; Fuzzy C-means clustering; Multiscale residual U-Net; Pulmonary nodules,,,Computer Methods and Programs in Biomedicine,Article,Scopus
109,,A novel multiscale and multipath convolutional neural network based age-related macular degeneration detection using OCT images,"Thomas A., Harikrishnan P.M., Ramachandran, Ramachandran S., Manoj R., Palanisamy P., Gopi V.P.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111875342&doi=10.1016%2fj.cmpb.2021.106294&partnerID=40&md5=3dcf3aa452e306f9296e78da4ef7fd8c,10.1016/j.cmpb.2021.106294,"Background and Objective: One of the significant retinal diseases that affected older people is called Age-related Macular Degeneration (AMD). The first stage creates a blur effect on vision and later leads to central vision loss. Most people overlooked the primary stage blurring and converted it into an advanced stage. There is no proper treatment to cure the disease. So the early detection of AMD is essential to prevent its extension into the advanced stage. This paper proposes a novel deep Convolutional Neural Network (CNN) architecture to automate AMD diagnosis early from Optical Coherence Tomographic (OCT) images. Methods: The proposed architecture is a multiscale and multipath CNN with six convolutional layers. The multiscale convolution layer permits the network to produce many local structures with various filter dimensions. The multipath feature extraction permits CNN to merge more features regarding the sparse local and fine global structures. The performance of the proposed architecture is evaluated through ten-fold cross-validation methods using different classifiers like support vector machine, multi-layer perceptron, and random forest. Results: The proposed CNN with the random forest classifier gives the best classification accuracy results. The proposed method is tested on data set 1, data set 2, data set 3, data set 4, and achieved an accuracy of 0.9666, 0.9897, 0.9974, and 0.9978 respectively, with random forest classifier. Also, we tested the combination of first three data sets and achieved an accuracy of 0.9902. Conclusions: An efficient algorithm for detecting AMD from OCT images is proposed based on a multiscale and multipath CNN architecture. Comparison with other approaches produced results that exhibit the efficiency of the proposed algorithm in the detection of AMD. The proposed architecture can be applied in rapid screening of the eye for the early detection of AMD. Due to less complexity and fewer learnable parameters. © 2021 Elsevier B.V.",Age-related macular degeneration; Classification; Multiscale and multipath CNN; Ten-fold cross-validation,,,Computer Methods and Programs in Biomedicine,Article,Scopus
110,,Spatial-Frequency dual-branch attention model for determining KRAS mutation status in colorectal cancer with T2-weighted MRI,"Ma Y., Wang J., Song K., Qiang Y., Jiao X., Zhao J.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111609813&doi=10.1016%2fj.cmpb.2021.106311&partnerID=40&md5=6bfef76207dd31d28680b4c77c0963ad,10.1016/j.cmpb.2021.106311,"Background and Objective: Identifying the KRAS mutation status accurately in medical images is very important for the diagnosis and treatment of colorectal cancer. Despite the substantial progress achieved by existing methods, it remains challenging due to limited annotated dataset, large intra-class variances, and a high degree of inter-class similarities. Methods: To tackle these challenges, we propose a spatial-frequency dual-branch attention model (SF-DBAM) to determine the KRAS mutation status of colorectal cancer patients using a limited T2-weighted MRI dataset. The dataset contains 169 wild-type patients (2151 images) and 137 mutation-type patients (1666 images). The first branch utilizes part of the pre-trained Xception model to capture spatial-domain information and alleviate the small-scale dataset problem. The second branch builds frequency-domain information into cube columns using block-based discrete cosine transform and channel rearrangement. Then the cube columns are fed into convolutional long short-term memory (convLSTM) to explore the effective information between the reconstructed frequency-domain channels. Also, we design a channel enhanced attention module (CEAM) at the end of each branch to make them focus on the lesion areas. Finally, we concatenate the two branches and output the classified results through fully connected layers. Results: The proposed method achieves 88.03% overall accuracy with AUC of 94.27% and specificity of 90.75% in 10-fold cross-validation, which is better than the current non-invasive methods for determining KRAS mutation status. Conclusions: We believe that the proposed method can assist physicians to diagnose the KRAS mutation status in patients with colorectal cancer, and other medical problems can benefit from the spatial and frequency domains information. © 2021",Attention module; Colorectal cancer; Frequency-domain; KRASmutation status; Spatial-domain; T2-weighted MRI,,,Computer Methods and Programs in Biomedicine,Article,Scopus
111,,Early Alzheimer's disease diagnosis with the contrastive loss using paired structural MRIs,"Qiao H., Chen L., Ye Z., Zhu F.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111526924&doi=10.1016%2fj.cmpb.2021.106282&partnerID=40&md5=046913d768dd8cc03051d422583a97ed,10.1016/j.cmpb.2021.106282,"Background and objective: Alzheimer's Disease (AD) is a chronic and fatal neurodegenerative disease with progressive impairment of memory. Brain structural magnetic resonance imaging (sMRI) has been widely applied as important biomarkers of AD. Various machine learning approaches, especially deep learning-based models, have been proposed for the early diagnosis of AD and monitoring the disease progression on sMRI data. However, the requirement for a large number of training images still hinders the extensive usage of AD diagnosis. In addition, due to the similarities in human whole-brain structure, finding the subtle brain changes is essential to extract discriminative features from limited sMRI data effectively. Methods: In this work, we proposed two types of contrastive losses with paired sMRIs to promote the diagnostic performance using group categories (G-CAT) and varying subject mini-mental state examination (S-MMSE) information, respectively. Specifically, G-CAT contrastive loss layer was used to learn the closer feature representation from sMRIs with the same categories, while ranking information from S-MMSE assists the model to explore subtle changes between individuals. Results: The model was trained on ADNI-1. Comparison with baseline methods was performed on MIRIAD and ADNI-2. For the classification task on MIRIAD, S-MMSE achieves 93.5% of accuracy, 96.6% of sensitivity, and 94.9% of specificity, respectively. G-CAT and S-MMSE both reach remarkable performance in terms of classification sensitivity and specificity respectively. Comparing with state-of-the-art methods, we found this proposed method could achieve comparable results with other approaches. Conclusion: The proposed model could extract discriminative features under whole-brain similarity. Extensive experiments also support the accuracy of this model, i.e., it provides better ability to identify uncertain samples, especially for the classification task of subjects with MMSE in 22–27. Source code is freely available at https://github.com/fengduqianhe/ADComparative. © 2021 Elsevier B.V.",Alzheimer's disease (AD); Contrastive loss; Convolutional neural network (CNN); Magnetic resonance imaging (MRI); Mini-mental state examination (MMSE),,,Computer Methods and Programs in Biomedicine,Article,Scopus
112,,Development of convolutional neural networks for recognition of tenogenic differentiation based on cellular morphology,"Dursun G., Tandale S.B., Gulakala R., Eschweiler J., Tohidnezhad M., Markert B., Stoffel M.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111511077&doi=10.1016%2fj.cmpb.2021.106279&partnerID=40&md5=6ec2db97a539e62527ef63b767667c56,10.1016/j.cmpb.2021.106279,"Background and objective: The use of automated systems for image recognition is highly preferred for regenerative medicine applications to evaluate stem cell differentiation early in the culturing state with non-invasive methodologies instead of invasive counterparts. Bone marrow-derived mesenchymal stem cells (BMSCs) are able to differentiate into desired cell phenotypes, and thereby promise a proper cell source for tendon regeneration. The therapeutic success of stem cell therapy requires cellular characterization prior to the implantation of cells. The foremost problem is that traditional characterization techniques require cellular material which would be more useful for cell therapy, complex laboratory procedures, and human expertise. Convolutional neural networks (CNNs), a class of deep neural networks, have recently made great improvements in image-based classifications, recognition, and detection tasks. We, therefore, aim to develop a potential CNN model in order to recognize differentiated stem cells by learning features directly from image data of unlabelled cells. Methods: The differentiation of bone marrow mesenchymal stem cells (BMSCs) into tenocytes was induced with the treatment of bone morphogenetic protein-12 (BMP-12). Following the treatment and incubation step, the phase-contrast images of cells were obtained and immunofluorescence staining has been applied to characterize the differentiated state of BMSCs. CNN models were developed and trained with the phase-contrast cell images. The comparison of CNN models was performed with respect to prediction performance and training time. Moreover, we have evaluated the effect of image enhancement method, data augmentation, and fine-tuning training strategy to increase classification accuracy of CNN models. The best model was integrated into a mobile application. Results: All the CNN models can fit the biological data extracted from immunofluorescence characterization. CNN models enable the cell classification with satisfactory accuracies. The best result in terms of accuracy and training time is achieved by the model proposed based on Inception-ResNet V2 trained from scratch using image enhancement and data augmentation strategies (96.80%, 434.55 sec). Conclusion: Our study reveals that the CNN models show good performance by identifying stem cell differentiation. Importantly this technique provides a faster and real-time tool in comparison to traditional methods enabling the adjustment of culture conditions during cultivation to improve the yield of therapeutic stem cells. © 2021 Elsevier B.V.",Convolutional neural network; Image recognition; Stem cell differentiation,,,Computer Methods and Programs in Biomedicine,Article,Scopus
113,,Cross-Gram matrices and their use in transfer learning: Application to automatic REM detection using heart rate,"Muller B., Lengellé R.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111321100&doi=10.1016%2fj.cmpb.2021.106280&partnerID=40&md5=95563f98ca903a8d811756e6ee9bcbed,10.1016/j.cmpb.2021.106280,"Background and objectives: while traditional sleep staging is achieved through the visual - expert-based - annotation of a polysomnography, it has the disadvantages of being unpractical and expensive. Alternatives have been developed over the years to relieve sleep staging from its heavy requirements, through the collection of more easily assessable signals and its automation using machine learning. However, these alternatives have their limitations, some due to variabilities among and between subjects, other inherent to their use of sub-discriminative signals. Many new solutions rely on the evaluation of the Autonomic Nervous System (ANS) activation through the assessment of the heart-rate (HR); the latter is modulated by the aforementioned variabilities, which may result in data and concept shifts between what was learned and what we want to classify. Such adversary effects are usually tackled by Transfer Learning, dealing with problems where there are differences between what is known (source) and what we want to classify (target). In this paper, we propose two new kernel-based methods of transfer learning and assess their performances in Rapid-Eye-Movement (REM) sleep stage detection, using solely the heart rate. Methods: our first contribution is the introduction of Kernel-Cross Alignment (KCA), a measure of similarity between a source and a target, which is a direct extension of Kernel-Target Alignment (KTA). To our knowledge, KCA has currently never been studied in the literature. Our second contribution is two alignment-based methods of transfer learning: Kernel-Target Alignment Transfer Learning (KTATL) and Kernel-Cross Alignment Transfer Learning (KCATL). Both methods differ from KTA, whose traditional use is kernel-tuning: in our methods, the kernel has been fixed beforehand, and our objective is the improvement of the estimation of unknown target labels by taking into account how observations relate to each other, which, as it will be explained, allows to transfer knowledge (transfer learning). Results: we compare performances with transfer learning (KCATL, KTATL) to performances without transfer using a fixed classifier (a Support Vector Classifier - SVC). In most cases, both transfer learning methods result in an improvement of performances (higher detection rates for a fixed false-alarm rate). Our methods do not require iterative computations. Conclusion: we observe improved performances using our transfer methods, which are computationally efficient, as they only require the computation of a kernel matrix and are non-iterative. However, some optimisation aspects are still under investigation. © 2021 Elsevier B.V.",Data & concept shift; Kernel methods; REM Detection; Sleep staging; Transfer learning,,,Computer Methods and Programs in Biomedicine,Article,Scopus
114,,Machine learning based natural language processing of radiology reports in orthopaedic trauma,"Olthof A.W., Shouche P., Fennema E.M., IJpma F.F.A., Koolstra R.H.C., Stirler V.M.A., van Ooijen P.M.A., Cornelissen L.J.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111254500&doi=10.1016%2fj.cmpb.2021.106304&partnerID=40&md5=37e2cb60bb55b2be339f1bf073b69456,10.1016/j.cmpb.2021.106304,"Objectives: To compare different Machine Learning (ML) Natural Language Processing (NLP) methods to classify radiology reports in orthopaedic trauma for the presence of injuries. Assessing NLP performance is a prerequisite for downstream tasks and therefore of importance from a clinical perspective (avoiding missed injuries, quality check, insight in diagnostic yield) as well as from a research perspective (identification of patient cohorts, annotation of radiographs). Methods: Datasets of Dutch radiology reports of injured extremities (n = 2469, 33% fractures) and chest radiographs (n = 799, 20% pneumothorax) were collected in two different hospitals and labeled by radiologists and trauma surgeons for the presence or absence of injuries. NLP classification was applied and optimized by testing different preprocessing steps and different classifiers (Rule-based, ML, and Bidirectional Encoder Representations from Transformers (BERT)). Performance was assessed by F1-score, AUC, sensitivity, specificity and accuracy. Results: The deep learning based BERT model outperforms all other classification methods which were assessed. The model achieved an F1-score of (95 ± 2)% and accuracy of (96 ± 1)% on a dataset of simple reports (n= 2469), and an F1 of (83 ± 7)% with accuracy (93 ± 2)% on a dataset of complex reports (n= 799). Conclusion: BERT NLP outperforms traditional ML and rule-base classifiers when applied to Dutch radiology reports in orthopaedic trauma. © 2021",(MeSH); Informatics; Machine learning; Natural language processing; Orthopaedic trauma; Radiology,,,Computer Methods and Programs in Biomedicine,Article,Scopus
115,,Magnetic resonance image diagnosis of femoral head necrosis based on ResNet18 network,"Liu Y., She G.-R., Chen S.-X.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111069449&doi=10.1016%2fj.cmpb.2021.106254&partnerID=40&md5=6c105df982fd864fef8cc9002170dd6f,10.1016/j.cmpb.2021.106254,"Purpose: In order to enhance the practicability of the application of Magnetic Resonance Imaging (MRI) in the diagnosis of femoral head necrosis, combined with the convolutional neural network (CNN), we propose an automatic identification of femoral head necrosis model based on the ResNet18 network. Methods: In order to verify that MRI has a higher detection rate for early femoral head necrosis, we collected 360 cases of femoral MRI and the same number of femoral CT. Combining this method with ResNet18, AlexNet, and VGG16, compare the clinical staging and typical signs of femoral head necrosis with 8 diagnostic methods. Results: The total detection rate of MRI combined with ResNet18 is as high as 99.27%, which is much higher than the other three comparison methods. The sensitivity is 97%, the specificity is 98.99%, and the accuracy is 98.23%. The difference is statistically significant. Conclusion: The automatic recognition femoral MRI model based on the ResNet18 network has a high detection rate for early femoral head necrosis, and can effectively detect bone marrow edema, line-like signs and other signs, providing a reliable reference for early treatment. © 2021 Elsevier B.V.",Computed Tomography; Convolutional neural network; Early treatment; Magnetic Resonance Imaging; ResNet18,,,Computer Methods and Programs in Biomedicine,Article,Scopus
116,,Computer auxiliary diagnosis technique of detecting cholangiocarcinoma based on medical imaging: A review,"Wang S., Liu X., Zhao J., Liu Y., Liu S., Liu Y., Zhao J.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111068951&doi=10.1016%2fj.cmpb.2021.106265&partnerID=40&md5=5bd67ce38ff9c9672ecc8031df48fa34,10.1016/j.cmpb.2021.106265,"Background and objectives: Cholangiocarcinoma (CCA) is one of the most aggressive human malignant tumors and is becoming one of the main factors of death and disability globally. Specifically, 60% to 70% of CCA patients were diagnosed with local invasion or distant metastasis and lost the chance of radical operation. The overall median survival time was less than 12 months. As a non-invasive diagnostic technology, medical imaging consisting of computed tomography (CT) imaging, magnetic resonance imaging (MRI), and ultrasound (US) imaging, is the most effectively and commonly used method to detect CCA. The computer auxiliary diagnosis (CAD) system based on medical imaging is helpful for rapid diagnosis and provides credible “second opinion” for specialists. The purpose of this review is to categorize and review the CAD technique of detecting CCA based on medical imaging. Methods: This work applies a four-level screening process to choose suitable publications. 125 research papers published in different academic research databases were selected and analyzed according to specific criteria. From the five steps of medical image acquisition, processing, analysis, understanding and verification of CAD combined with artificial intelligence algorithms, we obtain the most advanced insights related to CCA detection. Results: This work provides a comprehensive analysis and comparison analysis of the current CAD systems of detecting CCA. After careful investigation, we find that the main detection methods are traditional machine learning method and deep learning method. For the detection, the most commonly used method is semi-automatic segmentation algorithm combined with support vector machine classifier method, combination of which has good detection performance. The end-to-end training mode makes deep learning method more and more popular in CAD systems. However, due to the limited medical training data, the accuracy of deep learning method is unsatisfactory. Conclusions: Based on analysis of artificial intelligence methods applied in CCA, this work is expected to be truly applied in clinical practice in the future to improve the level of clinical diagnosis and treatment of it. This work concludes by providing a prediction of future trends, which will be of great significance for researchers in the medical imaging of CCA and artificial intelligence. © 2021",Cholangiocarcinoma; Computer auxiliary diagnosis; Convolutional neural network; Feature extraction and selection; Machine learning; Medical image processing,,,Computer Methods and Programs in Biomedicine,Review,Scopus
117,,Video recognition of simple mastoidectomy using convolutional neural networks: Detection and segmentation of surgical tools and anatomical regions,"Choi J., Cho S., Chung J.W., Kim N.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111062078&doi=10.1016%2fj.cmpb.2021.106251&partnerID=40&md5=9591f323d21993eb96a723bd9ff4eaf0,10.1016/j.cmpb.2021.106251,"A simple mastoidectomy is used to remove inflammation of the mastoid cavity and to create a route to the skull base and middle ear. However, due to the complexity and difficulty of the simple mastoidectomy, implementing robot vision for assisted surgery is a challenge. To overcome this issue using a convolutional neural network architecture in a surgical environment, each surgical instrument and anatomical region must be distinguishable in real time. To meet this condition, we used the latest instance segmentation architecture, YOLACT. In this study, a data set comprising 5,319 extracted frames from 70 simple mastoidectomy surgery videos were used. Six surgical tools and five anatomic regions were identified for the training. The YOLACT-based model in the surgical environment was trained and evaluated for real-time object detection and semantic segmentation. Detection accuracies of surgical tools and anatomic regions were 91.2% and 56.5% in mean average precision, respectively. Additionally, the dice similarity coefficient metric for segmentation of the five anatomic regions was 48.2%. The mean frames per second of this model was 32.3, which is sufficient for real-time robotic applications. © 2021 Elsevier B.V.",Anatomical region; Object detection; Semantic segmentation; Simple mastoidectomy; Surgical tool,,,Computer Methods and Programs in Biomedicine,Article,Scopus
118,,"TorchIO: A Python library for efficient loading, preprocessing, augmentation and patch-based sampling of medical images in deep learning","Pérez-García F., Sparks R., Ourselin S.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110998688&doi=10.1016%2fj.cmpb.2021.106236&partnerID=40&md5=a0c6cd739cf308682588fd61b432b3bb,10.1016/j.cmpb.2021.106236,"Background and objective: Processing of medical images such as MRI or CT presents different challenges compared to RGB images typically used in computer vision. These include a lack of labels for large datasets, high computational costs, and the need of metadata to describe the physical properties of voxels. Data augmentation is used to artificially increase the size of the training datasets. Training with image subvolumes or patches decreases the need for computational power. Spatial metadata needs to be carefully taken into account in order to ensure a correct alignment and orientation of volumes. Methods: We present TorchIO, an open-source Python library to enable efficient loading, preprocessing, augmentation and patch-based sampling of medical images for deep learning. TorchIO follows the style of PyTorch and integrates standard medical image processing libraries to efficiently process images during training of neural networks. TorchIO transforms can be easily composed, reproduced, traced and extended. Most transforms can be inverted, making the library suitable for test-time augmentation and estimation of aleatoric uncertainty in the context of segmentation. We provide multiple generic preprocessing and augmentation operations as well as simulation of MRI-specific artifacts. Results: Source code, comprehensive tutorials and extensive documentation for TorchIO can be found at http://torchio.rtfd.io/. The package can be installed from the Python Package Index (PyPI) running pip install torchio. It includes a command-line interface which allows users to apply transforms to image files without using Python. Additionally, we provide a graphical user interface within a TorchIO extension in 3D Slicer to visualize the effects of transforms. Conclusion: TorchIO was developed to help researchers standardize medical image processing pipelines and allow them to focus on the deep learning experiments. It encourages good open-science practices, as it supports experiment reproducibility and is version-controlled so that the software can be cited precisely. Due to its modularity, the library is compatible with other frameworks for deep learning with medical images. © 2021 The Author(s)",Data augmentation; Deep learning; Medical image computing; Preprocessing,,,Computer Methods and Programs in Biomedicine,Article,Scopus
119,,Deep fire topology: Understanding the role of landscape spatial patterns in wildfire occurrence using artificial intelligence,"Pais C., Miranda A., Carrasco J., Shen Z.-J.M.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110195888&doi=10.1016%2fj.envsoft.2021.105122&partnerID=40&md5=61117bd1b1dc0d5a97ac50887f76f52d,10.1016/j.envsoft.2021.105122,"Increasing wildfire activity globally has become an urgent issue with enormous ecological and social impacts. In this work, we focus on analyzing and quantifying the influence of landscape topology, understood as the spatial structure and interaction of multiple land-covers in an area, on fire ignition. We propose a deep learning framework, Deep Fire Topology, to estimate and predict wildfire ignition risk. We focus on understanding the impact of these topological attributes and the rationale behind the results to provide interpretable knowledge for territorial planning considering wildfire ignition uncertainty. We demonstrate the high performance and interpretability of the framework in a case study, accurately detecting risky areas by exploiting spatial patterns. This work reveals the strong potential of landscape topology in wildfire occurrence prediction and its implications to develop robust landscape management plans. We discuss potential extensions and applications of the proposed method, available as an open-source software. © 2021 Elsevier Ltd",Deep learning; Landscape topology; Machine learning; Territorial planning; Wildfire ignition risk; Wildfire management,,,Environmental Modelling and Software,Article,Scopus
120,,GapFill-Recon Net: A Cascade Network for simultaneously PET Gap Filling and Image Reconstruction,"Huang Y., Zhu H., Duan X., Hong X., Sun H., Lv W., Lu L., Feng Q.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110179924&doi=10.1016%2fj.cmpb.2021.106271&partnerID=40&md5=5c6a1674a19ac1abad16f4ea9255d5f8,10.1016/j.cmpb.2021.106271,"PET image reconstruction from incomplete data, such as the gap between adjacent detector blocks generally introduces partial projection data loss, is an important and challenging problem in medical imaging. This work proposes an efficient convolutional neural network (CNN) framework, called GapFill-Recon Net, that jointly reconstructs PET images and their associated sinogram data. GapFill-Recon Net including two blocks: the Gap-Filling block first address the sinogram gap and the Image-Recon block maps the filled sinogram onto the final image directly. A total of 43,660 pairs of synthetic 2D PET sinograms with gaps and images generated from the MOBY phantom are utilized for network training, testing and validation. Whole-body mouse Monte Carlo (MC) simulated data are also used for evaluation. The experimental results show that the reconstructed image quality of GapFill-Recon Net outperforms filtered back-projection (FBP) and maximum likelihood expectation maximization (MLEM) in terms of the structural similarity index metric (SSIM), relative root mean squared error (rRMSE), and peak signal-to-noise ratio (PSNR). Moreover, the reconstruction speed is equivalent to that of FBP and was nearly 83 times faster than that of MLEM. In conclusion, compared with the traditional reconstruction algorithm, GapFill-Recon Net achieves relatively optimal performance in image quality and reconstruction speed, which effectively achieves a balance between efficiency and performance. © 2021",deep learning; Gap filling; Image reconstruction; Incomplete projection; PET,,,Computer Methods and Programs in Biomedicine,Article,Scopus
121,,Bayesian convolutional neural network estimation for pediatric pneumonia detection and diagnosis,"Fernandes V., Junior G.B., de Paiva A.C., Silva A.C., Gattass M.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109928850&doi=10.1016%2fj.cmpb.2021.106259&partnerID=40&md5=4c4b8ff72382c3f2b6ac02ce69617744,10.1016/j.cmpb.2021.106259,"Background and objectives: Pneumonia is a disease that affects the lungs, making breathing difficult. Nowadays, pneumonia is the disease that kills the most children under the age of five in the world, and if no action is taken, pneumonia is estimated to kill 11 million children by the year 2030. Knowing that rapid and accurate diagnosis of pneumonia is a significant factor in reducing mortality, acceleration, or automation of the diagnostic process is highly desirable. The use of computational methods can decrease specialists’ workload and even offer a second opinion, increasing the number of accurate diagnostics. Methods: This work proposes a method for constructing a specific convolutional neural network architecture to detect pneumonia and classify viral and bacterial types using Bayesian optimization from pre-trained networks. Results: The results obtained are promising, in the order of 0.964 accuracy for pneumonia detection and 0.957 accuracy for pneumonia type classification. Conclusion: This research demonstrated the efficiency of CNN architecture estimation for detecting and diagnosing pneumonia using Bayesian optimization. The proposed network proved to have promising results, despite not using common preprocessing techniques such as histogram equalization and lung segmentation. This fact shows that the proposed method provides efficient and high-performance neural networks since image preprocessing is unnecessary. © 2021 Elsevier B.V.",Bayesian optimization; Convolutional neural networks; Pneumonia detection; Transfer learning,,,Computer Methods and Programs in Biomedicine,Article,Scopus
122,,Topic recommendation for software repositories using multi-label classification algorithms,"Izadi M., Heydarnoori A., Gousios G.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109715295&doi=10.1007%2fs10664-021-09976-2&partnerID=40&md5=fc9e8bc3a6f657cda7b26e1d9914c1ab,10.1007/s10664-021-09976-2,"Many platforms exploit collaborative tagging to provide their users with faster and more accurate results while searching or navigating. Tags can communicate different concepts such as the main features, technologies, functionality, and the goal of a software repository. Recently, GitHub has enabled users to annotate repositories with topic tags. It has also provided a set of featured topics, and their possible aliases, carefully curated with the help of the community. This creates the opportunity to use this initial seed of topics to automatically annotate all remaining repositories, by training models that recommend high-quality topic tags to developers. In this work, we study the application of multi-label classification techniques to predict software repositories’ topics. First, we map the large-space of user-defined topics to those featured by GitHub. The core idea is to derive more information from projects’ available documentation. Our data contains about 152K GitHub repositories and 228 featured topics. Then, we apply supervised models on repositories’ textual information such as descriptions, README files, wiki pages, and file names. We assess the performance of our approach both quantitatively and qualitatively. Our proposed model achieves Recall@5 and LRAP scores of 0.890 and 0.805, respectively. Moreover, based on users’ assessment, our approach is highly capable of recommending correct and complete set of topics. Finally, we use our models to develop an online tool named Repository Catalogue, that automatically predicts topics for GitHub repositories and is publicly available1. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",GitHub; Mining software repositories; Multi-label classification; Recommender systems; Topic tag recommendation,,,Empirical Software Engineering,Article,Scopus
123,,Machine learning based analysis of speech dimensions in functional oropharyngeal dysphagia,"Roldan-Vasco S., Orozco-Duque A., Suarez-Escudero J.C., Orozco-Arroyave J.R.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109711802&doi=10.1016%2fj.cmpb.2021.106248&partnerID=40&md5=1f7d73dd8fb211378b7acbacaf5eb1e1,10.1016/j.cmpb.2021.106248,"Background and objective: The normal swallowing process requires a complex coordination of anatomical structures driven by sensory and cranial nerves. Alterations in such coordination cause swallowing malfunctions, namely dysphagia. The dysphagia screening methods are quite subjective and experience dependent. Bearing in mind that the swallowing process and speech production share some anatomical structures and mechanisms of neurological control, this work aims to evaluate the suitability of automatic speech processing and machine learning techniques for screening of functional dysphagia. Methods: Speech recordings were collected from 46 patients with functional oropharyngeal dysphagia produced by neurological causes, and 46 healthy controls. The dimensions of speech including phonation, articulation, and prosody were considered through different speech tasks. Specific features per dimension were extracted and analyzed using statistical tests. Machine learning models were applied per dimension via nested cross-validation. Hyperparameters were selected using the AUC - ROC as optimization criterion. Results: The Random Forest in the articulation related speech tasks retrieved the highest performance measures (AUC=0.86±0.10, sensitivity=0.91±0.12) for individual analysis of dimensions. In addition, the combination of speech dimensions with a voting ensemble improved the results, which suggests a contribution of information from different feature sets extracted from speech signals in dysphagia conditions. Conclusions: The proposed approach based on speech related models is suitable for the automatic discrimination between dysphagic and healthy individuals. These findings seem to have potential use in the screening of functional oropharyngeal dysphagia in a non-invasive and inexpensive way. © 2021 Elsevier B.V.",Dysphagia; Feature extraction; Machine learning; Speech analysis; Speech processing; Voice changes,,,Computer Methods and Programs in Biomedicine,Article,Scopus
124,,FeatCompare: Feature comparison for competing mobile apps leveraging user reviews,"Assi M., Hassan S., Tian Y., Zou Y.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109651814&doi=10.1007%2fs10664-021-09988-y&partnerID=40&md5=0f89a6d71d1dfe3c5dc1f674ad13e6b3,10.1007/s10664-021-09988-y,"Given the competitive mobile app market, developers must be fully aware of users’ needs, satisfy users’ requirements, combat apps of similar functionalities (i.e., competing apps), and thus stay ahead of the competition. While it is easy to track the overall user ratings of competing apps, such information fails to provide actionable insights for developers to improve their apps over the competing apps (AlSubaihin et al., IEEE Trans Softw Eng, 1–1, 2019). Thus, developers still need to read reviews from all their interested competing apps and summarize the advantages and disadvantages of each app. Such a manual process can be tedious and even infeasible with thousands of reviews posted daily. To help developers compare users’ opinions among competing apps on high-level features, such as the main functionalities and the main characteristics of an app, we propose a review analysis approach named FeatCompare. FeatCompare can automatically identify high-level features mentioned in user reviews without any manually annotated resource. Then, FeatCompare creates a comparative table that summarizes users’ opinions for each identified feature across competing apps. FeatCompare features a novel neural network-based model named G lobal-L ocal sensitive F eature E xtractor (GLFE), which extends Attention-based Aspect Extraction (ABAE), a state-of-the-art model for extracting high-level features from reviews. We evaluate the effectiveness of GLFE on 480 manually annotated reviews sampled from five groups of competing apps. Our experiment results show that GLFE achieves a precision of 79%-82% and recall of 74%-77% in identifying the high-level features associated with reviews and outperforms ABAE by 14.7% on average. We also conduct a case study to demonstrate the usage scenarios of FeatCompare. A survey with 107 mobile app developers shows that more than 70% of developers agree that FeatCompare is of great benefit. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",Competing apps; Competitor analysis; Feature extraction; Google Play Store; Mobile applications; User reviews,,,Empirical Software Engineering,Article,Scopus
125,,To what extent do DNN-based image classification models make unreliable inferences?,"Tian Y., Ma S., Wen M., Liu Y., Cheung S.-C., Zhang X.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108082989&doi=10.1007%2fs10664-021-09985-1&partnerID=40&md5=c73cd9bdd102fe276b75627cc5c10d94,10.1007/s10664-021-09985-1,"Deep Neural Network (DNN) models are widely used for image classification. While they offer high performance in terms of accuracy, researchers are concerned about if these models inappropriately make inferences using features irrelevant to the target object in a given image. To address this concern, we propose a metamorphic testing approach that assesses if a given inference is made based on irrelevant features. Specifically, we propose two metamorphic relations (MRs) to detect such unreliable inferences. These relations expect (a) the classification results with different labels or the same labels but less certainty from models after corrupting the relevant features of images, and (b) the classification results with the same labels after corrupting irrelevant features. The inferences that violate the metamorphic relations are regarded as unreliable inferences. Our evaluation demonstrated that our approach can effectively identify unreliable inferences for single-label classification models with an average precision of 64.1% and 96.4% for the two MRs, respectively. As for multi-label classification models, the corresponding precision for MR-1 and MR-2 is 78.2% and 86.5%, respectively. Further, we conducted an empirical study to understand the problem of unreliable inferences in practice. Specifically, we applied our approach to 18 pre-trained single-label image classification models and 3 multi-label classification models, and then examined their inferences on the ImageNet and COCO datasets. We found that unreliable inferences are pervasive. Specifically, for each model, more than thousands of correct classifications are actually made using irrelevant features. Next, we investigated the effect of such pervasive unreliable inferences, and found that they can cause significant degradation of a model’s overall accuracy. After including these unreliable inferences from the test set, the model’s accuracy can be significantly changed. Therefore, we recommend that developers should pay more attention to these unreliable inferences during the model evaluations. We also explored the correlation between model accuracy and the size of unreliable inferences. We found the inferences of the input with smaller objects are easier to be unreliable. Lastly, we found that the current model training methodologies can guide the models to learn object-relevant features to certain extent, but may not necessarily prevent the model from making unreliable inferences. We encourage the community to propose more effective training methodologies to address this issue. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",Deep learning; Metamorphic testing; Software engineering for AI,,,Empirical Software Engineering,Article,Scopus
126,,A high resolution representation network with multi-path scale for retinal vessel segmentation,"Lin Z., Huang J., Chen Y., Zhang X., Zhao W., Li Y., Lu L., Zhan M., Jiang X., Liang X.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107964821&doi=10.1016%2fj.cmpb.2021.106206&partnerID=40&md5=9df47a0a8a240f35f98704b89cd33631,10.1016/j.cmpb.2021.106206,"Background and objectives: Automatic retinal vessel segmentation (RVS) in fundus images is expected to be a vital step in the early image diagnosis of ophthalmologic diseases. However, it is a challenging task to detect the retinal vessel accurately mainly due to the vascular intricacies, lesion areas and optic disc edges in retinal fundus images. Methods: In this paper, we propose a high resolution representation network with multi-path scale (MPS-Net) for RVS aiming to improve the performance of extracting the retinal blood vessels. In the MPS-Net, there exist one high resolution main road and two lower resolution branch roads where the proposed multi-path scale modules are embedded to enhance the representation ability of network. Besides, in order to guide the network focus on learning the features of hard examples in retinal images, we design a hard-focused cross-entropy loss function. Results: We evaluate our network structure on DRIVE, STARE, CHASE and synthetic images and the quantitative comparisons with respect to the existing methods are presented. The experimental results show that our approach is superior to most methods in terms of F1-score, sensitivity, G-mean and Matthews correlation coefficient. Conclusions: The promising segmentation performances reveal that our method has potential in real-world applications and can be exploited for other medical images with further analysis. © 2021 Elsevier B.V.",Deep learning; High resolution; Multi-path scale; Retinal vessels segmentation,,,Computer Methods and Programs in Biomedicine,Article,Scopus
127,,Breast ultrasound tumor image classification using image decomposition and fusion based on adaptive multi-model spatial feature fusion,"Zhuang Z., Yang Z., Raj A.N.J., Wei C., Jin P., Zhuang S.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107956573&doi=10.1016%2fj.cmpb.2021.106221&partnerID=40&md5=f681ce5980dfd2141a621742fee3d6bd,10.1016/j.cmpb.2021.106221,"Background and Objective: Breast cancer is a fatal threat to the health of women. Ultrasonography is a common method for the detection of breast cancer. Computer-aided diagnosis of breast ultrasound images can help doctors in diagnosing benign and malignant lesions. In this paper, by combining image decomposition and fusion techniques with adaptive spatial feature fusion technology, a reliable classification method for breast ultrasound images of tumors is proposed. Methods: First, fuzzy enhancement and bilateral filtering algorithms are used to process the original breast ultrasound image. Then, various decomposition images representing the clinical characteristics of breast tumors are obtained using the original and mask images. By considering the diversity of the benign and malignant characteristic information represented by each decomposition image, the decomposition images are fused through the RGB channel, and three types of fusion images are generated. Then, from a series of candidate deep learning models, transfer learning is used to select the best model as the base model to extract deep learning features. Finally, while training the classification network, adaptive spatial feature fusion technology is used to train the weight network to complete deep learning feature fusion and classification. Results: In this study, 1328 breast ultrasound images were collected for training and testing. The experimental results show that the values of accuracy, precision, specificity, sensitivity/recall, F1 score, and area under the curve of the proposed method were 0.9548, 0.9811, 0.9833, 0.9392, 0.9571, and 0.9883, respectively. Conclusion: Our research can automate breast cancer detection and has strong clinical utility. When compared to previous methods, our proposed method is expected to be more effective while assisting doctors in diagnosing breast ultrasound images. © 2021",Adaptive spatial feature fusion; Breast ultrasound tumor image classification; Deep learning; Image decomposition; Image fusion,,,Computer Methods and Programs in Biomedicine,Article,Scopus
128,,Estimation of PQ distance dispersion for atrial fibrillation detection,"Giraldo-Guzmán J., Kotas M., Castells F., Contreras-Ortiz S.H., Urina-Triana M.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107668437&doi=10.1016%2fj.cmpb.2021.106167&partnerID=40&md5=4260bc2badb054b1aef4c83a00881081,10.1016/j.cmpb.2021.106167,"Background and objective: Atrial fibrillation (AF) is the most common cardiac arrhythmia in the world. It is associated with significantly increased morbidity and mortality. Diagnosis of the disease can be based on the analysis of the electrical atrial activity, on quantification of the heart rate irregularity or on a mixture of the both approaches. Since the amplitude of the atrial waves is small, their analysis can lead to false results. On the other hand, the heart rate based analysis usually leads to many unnecessary warnings. Therefore, our goal is to develop a new method for effective AF detection based on the analysis of the electrical atrial waves. Methods: The proposed method employs the fact that there is a lack of repeatable P waves preceding QRS complexes during AF. We apply the operation of spatio-temporal filtering (STF) to magnify and detect the prominent spatio-temporal patterns (STP) within the P waves in multi-channel ECG recordings. Later we measure their distances (PQ) to the succeeding QRS complexes, and we estimate dispersion of the obtained PQ series. For signals with normal sinus rhythm, this dispersion is usually very low, and contrary, for AF it is much raised. This allows for effective discrimination of this cardiologic disorder. Results: Tested on an ECG database consisting of AF cases, normal rhythm cases and cases with normal rhythm restored by the use of cardioversion, the method proposed allowed for AF detection with the accuracy of 98.75% on the basis of both 8–channel and 2–channel signals of 12 s length. When the signals length was decreased to 6 s, the accuracy varied in the range of 95%−97.5% depending on the number of channels and the dispersion measure applied. Conclusions: Our approach allows for high accuracy of atrial fibrillation detection using the analysis of electrical atrial activity. The method can be applied to an early detection of the desease and can advantageously be used to decrease the number of false warnings in systems based on the analysis of the heart rate. © 2021 Elsevier B.V.",Atrial fibrillation; ECG processing; PQ dispersion; Spatio–temporal filtering; Spatio–temporal patterns,,,Computer Methods and Programs in Biomedicine,Article,Scopus
129,,Leveraging developer information for efficient effort-aware bug prediction,"Qu Y., Chi J., Yin H.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105696344&doi=10.1016%2fj.infsof.2021.106605&partnerID=40&md5=75f7687f24256e968a126bbedd88160b,10.1016/j.infsof.2021.106605,"Context: Software bug prediction techniques can provide informative guidance in software engineering practices. Over the past 15 years, developer information has been intensively used in bug prediction as features or basic data source to construct other useful models. Objective: Further leverage developer information from a new and straightforward perspective to improve effort-aware bug prediction. Methods: We propose to investigate the direct relations between the number of developers and the probability for a file to be buggy. Based on an empirical study on nine open-source Java systems with 32 versions, we observe a widely-existed and interesting tendency: when there are more developers working on a source file, there will be a stronger possibility for this file to be buggy. Based on the observed tendency, we propose an unsupervised algorithm and a supervised equation both called top-dev to improve effort-aware bug prediction. The key idea is to prioritize the ranking of files, whose number of developers is large, in the suspicious file list generated by effort-aware models. Results: Experimental results show that the proposed top-dev algorithm and equation significantly outperform the unsupervised and supervised baseline models (ManualUp, Rad, Rdd, Ree, CBS+, and top-core). Moreover, the unsupervised top-dev algorithm is comparable or superior to existing supervised baseline models. Conclusion: The proposed approaches are very useful in effort-aware bug prediction practices. Practitioners can use the top-dev algorithm to generate a high-quality and informative suspicious file list without training complex machine learning classifiers. On the other hand, when building supervised bug prediction model, the best practice is to combine existing models with the top-dev equation. © 2021 Elsevier B.V.",Defect prediction; Developer information; Effort-aware bug prediction; Empirical study; Software bug prediction; Unsupervised model,,,Information and Software Technology,Article,Scopus
130,,Joint feature representation learning and progressive distribution matching for cross-project defect prediction,"Zou Q., Lu L., Yang Z., Gu X., Qiu S.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103928221&doi=10.1016%2fj.infsof.2021.106588&partnerID=40&md5=93ae4fe9ea11db6a20e52b97288ad767,10.1016/j.infsof.2021.106588,"Context: Cross-Project Defect Prediction (CPDP) aims to leverage the knowledge from label-rich source software projects to promote tasks in a label-poor target software project. Existing CPDP methods have two major flaws. One is that previous CPDP methods only consider global feature representation and ignores local relationship between instances in the same category from different projects, resulting in ambiguous predictions near the decision boundary. The other one is that CPDP methods based on pseudo-labels assume that the conditional distribution can be well matched at one stroke, when instances of target project are correctly annotated pseudo labels. However, due to the great gap between projects, the pseudo-labels seriously deviate from the real labels. Objective: To address above issues, this paper proposed a novel CPDP method named Joint Feature Representation with Double Marginalized Denoising Autoencoders (DMDA_JFR). Method: Our method mainly includes two parts: joint feature representation learning and progressive distribution matching. We utilize two novel autoencoders to jointly learn the global and local feature representations simultaneously. To achieve progressive distribution matching, we introduce a repetitious pseudo-labels strategy, which makes it possible that distributions are matched after each stack layer learning rather than in one stroke. Results: The effectiveness of the proposed method was evaluated through experiments conducted on 10 open-source projects, including 29 software releases from PROMISE repository. Overall, experimental results show that our proposed method outperformed several state-of-the-art baseline CPDP methods. Conclusions: It can be concluded that (1) joint deep representations are promising for CPDP compared with only considering global feature representation methods, (2) progressive distribution matching is more effective for adapting probability distributions in CPDP compared with existing CPDP methods based on pseudo-labels. © 2021",Cross project defect prediction; Domain adaption; Feature representation; Progressive distribution matching,,,Information and Software Technology,Article,Scopus
131,,Spatial Steganalysis of Low Embedding Rate Based on Convolutional Neural Network [基于卷积神经网络的低嵌入率空域隐写分析],"Shen J., Liao X., Qin Z., Liu X.-C.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102546347&doi=10.13328%2fj.cnki.jos.005980&partnerID=40&md5=e167c658ff1cf1e39dfb54d49e80a815,10.13328/j.cnki.jos.005980,"In recent years, the research of spatial steganalysis based on deep learning has achieved sound results under high embedding rate, but the detection performance under low embedding rate is still not ideal. Therefore, a convolutional neural network structure is proposed, which uses the SRM filter for preprocessing to obtain implicit noise residuals, adopts three convolution layers and designs the size of convolution kernel reasonably, and selects appropriate batch normalization operations and activation functions to improve the network performance. The experimental results show that compared with the existing methods, the proposed network can achieve better detection performance for WOW, S-UNIWARD, and HILL, three common adaptive steganographic algorithms in spatial domain, and significant improvement in detection performance at low embedding rates of 0.2 bpp, 0.1 bpp, and 0.05 bpp. A step-by-step transfer learning method is also designed to further improve the steganalysis effect under low embedding rate conditions. © Copyright 2021, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Convolution neural network; Low embedding rate; Steganalysis; Transfer learning,"2901, 2915",,Ruan Jian Xue Bao/Journal of Software,Article,Scopus
132,,Unsupervised learning of general-purpose embeddings for code changes,"Pravilov M., Bogomolov E., Golubev Y., Bryksin T.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113878057&doi=10.1145%2f3472674.3473979&partnerID=40&md5=bad9bb2d8d7e9547fec8ea45e979f5d7,10.1145/3472674.3473979,"Applying machine learning to tasks that operate with code changes requires their numerical representation. In this work, we propose an approach for obtaining such representations during pre-training and evaluate them on two different downstream tasks - applying changes to code and commit message generation. During pre-training, the model learns to apply the given code change in a correct way. This task requires only code changes themselves, which makes it unsupervised. In the task of applying code changes, our model outperforms baseline models by 5.9 percentage points in accuracy. As for the commit message generation, our model demonstrated the same results as supervised models trained for this specific task, which indicates that it can encode code changes well and can be improved in the future by pre-training on a larger dataset of easily gathered code changes. © 2021 ACM.",Code changes; Commit message generation; Unsupervised learning,"7, 12",,"MaLTESQuE 2021 - Proceedings of the 5th International Workshop on Machine Learning Techniques for Software Quality Evolution, co-located with ESEC/FSE 2021",Conference Paper,Scopus
133,,Comparing within-and cross-project machine learning algorithms for code smell detection,"De Stefano M., Pecorelli F., Palomba F., De Lucia A.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113825122&doi=10.1145%2f3472674.3473978&partnerID=40&md5=ef08e821bd0aa076a2350dadfe5034f8,10.1145/3472674.3473978,"Code smells represent a well-known problem in software engineering, since they are a notorious cause of loss of comprehensibility and maintainability. The most recent efforts in devising automatic machine learning-based code smell detection techniques have achieved unsatisfying results so far. This could be explained by the fact that all these approaches follow a within-project classification, i.e. training and test data are taken from the same source project, which combined with the imbalanced nature of the problem, produces datasets with a very low number of instances belonging to the minority class (i.e. smelly instances). In this paper, we propose a cross-project machine learning approach and compare its performance with a within-project alternative. The core idea is to use transfer learning to increase the overall number of smelly instances in the training datasets. Our results have shown that cross-project classification provides very similar performance with respect to within-project. Despite this finding does not yet provide a step forward in increasing the performance of ML techniques for code smell detection, it sets the basis for further investigations. © 2021 ACM.",Code smells; Empirical Software Engineering; Transfer Learning,"1, 6",,"MaLTESQuE 2021 - Proceedings of the 5th International Workshop on Machine Learning Techniques for Software Quality Evolution, co-located with ESEC/FSE 2021",Conference Paper,Scopus
134,,Validation on machine reading comprehension software without annotated labels: A property-based method,"Chen S., Jin S., Xie X.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116305686&doi=10.1145%2f3468264.3468569&partnerID=40&md5=e3321fcd195374e99dbaa151efc7cca7,10.1145/3468264.3468569,"Machine Reading Comprehension (MRC) in Natural Language Processing has seen great progress recently. But almost all the current MRC software is validated with a reference-based method, which requires well-annotated labels for test cases and tests the software by checking the consistency between the labels and the outputs. However, labeling test cases of MRC could be very costly due to their complexity, which makes reference-based validation hard to be extensible and sufficient. Furthermore, solely checking the consistency and measuring the overall score may not be sensible and flexible for assessing the language understanding capability. In this paper, we propose a property-based validation method for MRC software with Metamorphic Testing to supplement the reference-based validation. It does not refer to the labels and hence can make much data available for testing. Besides, it validates MRC software against various linguistic properties to give a specific and in-depth picture on linguistic capabilities of MRC software. Comprehensive experimental results show that our method can successfully reveal violations to the target linguistic properties without the labels. Moreover, it can reveal problems that have been concealed by the traditional validation. Comparison according to the properties provides deeper and more concrete ideas about different language understanding capabilities of the MRC software. © 2021 ACM.",language understanding capability; machine reading comprehension; metamorphic relation; property-based validation,"590, 602",,ESEC/FSE 2021 - Proceedings of the 29th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,Conference Paper,Scopus
135,,Multi-objectivizing software configuration tuning,"Chen T., Li M.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116294210&doi=10.1145%2f3468264.3468555&partnerID=40&md5=2789ae7f809557fa2e4a5b4cd7a7f1ca,10.1145/3468264.3468555,"Automatically tuning software configuration for optimizing a single performance attribute (e.g., minimizing latency) is not trivial, due to the nature of the configuration systems (e.g., complex landscape and expensive measurement). To deal with the problem, existing work has been focusing on developing various effective optimizers. However, a prominent issue that all these optimizers need to take care of is how to avoid the search being trapped in local optima-a hard nut to crack for software configuration tuning due to its rugged and sparse landscape, and neighboring configurations tending to behave very differently. Overcoming such in an expensive measurement setting is even more challenging. In this paper, we take a different perspective to tackle this issue. Instead of focusing on improving the optimizer, we work on the level of optimization model. We do this by proposing a meta multi-objectivization model (MMO) that considers an auxiliary performance objective (e.g., throughput in addition to latency). What makes this model unique is that we do not optimize the auxiliary performance objective, but rather use it to make similarly-performing while different configurations less comparable (i.e. Pareto nondominated to each other), thus preventing the search from being trapped in local optima. Experiments on eight real-world software systems/environments with diverse performance attributes reveal that our MMO model is statistically more effective than state-of-the-art single-objective counterparts in overcoming local optima (up to 42% gain), while using as low as 24% of their measurements to achieve the same (or better) performance result. © 2021 ACM.",Configuration tuning; multi-objectivization; performance optimization; search-based software engineering,"453, 465",,ESEC/FSE 2021 - Proceedings of the 29th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,Conference Paper,Scopus
136,,StateFormer: Fine-grained type recovery from binaries using generative state modeling,"Pei K., Guan J., Broughton M., Chen Z., Yao S., Williams-King D., Ummadisetty V., Yang J., Ray B., Jana S.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116286319&doi=10.1145%2f3468264.3468607&partnerID=40&md5=db256e7896f41a84d919ec35ad8cb0a1,10.1145/3468264.3468607,"Binary type inference is a critical reverse engineering task supporting many security applications, including vulnerability analysis, binary hardening, forensics, and decompilation. It is a difficult task because source-level type information is often stripped during compilation, leaving only binaries with untyped memory and register accesses. Existing approaches rely on hand-coded type inference rules defined by domain experts, which are brittle and require nontrivial effort to maintain and update. Even though machine learning approaches have shown promise at automatically learning the inference rules, their accuracy is still low, especially for optimized binaries. We present StateFormer, a new neural architecture that is adept at accurate and robust type inference. StateFormer follows a two-step transfer learning paradigm. In the pretraining step, the model is trained with Generative State Modeling (GSM), a novel task that we design to teach the model to statically approximate execution effects of assembly instructions in both forward and backward directions. In the finetuning step, the pretrained model learns to use its knowledge of operational semantics to infer types. We evaluate StateFormer's performance on a corpus of 33 popular open-source software projects containing over 1.67 billion variables of different types. The programs are compiled with GCC and LLVM over 4 optimization levels O0-O3, and 3 obfuscation passes based on LLVM. Our model significantly outperforms state-of-the-art ML-based tools by 14.6% in recovering types for both function arguments and variables. Our ablation studies show that GSM improves type inference accuracy by 33%. © 2021 ACM.",Machine Learning for Program Analysis; Reverse Engineering; Transfer Learning; Type Inference,"690, 702",,ESEC/FSE 2021 - Proceedings of the 29th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,Conference Paper,Scopus
137,,A comprehensive study on learning-based PE malware family classification methods,"Ma Y., Liu S., Jiang J., Chen G., Li K.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116282495&doi=10.1145%2f3468264.3473925&partnerID=40&md5=9fe801c45a50f5bc2a61a9b88f298e8c,10.1145/3468264.3473925,"Driven by the high profit, Portable Executable (PE) malware has been consistently evolving in terms of both volume and sophistication. PE malware family classification has gained great attention and a large number of approaches have been proposed. With the rapid development of machine learning techniques and the exciting results they achieved on various tasks, machine learning algorithms have also gained popularity in the PE malware family classification task. Three mainstream approaches that use learning based algorithms, as categorized by the input format the methods take, are image-based, binary-based and disassembly-based approaches. Although a large number of approaches are published, there is no consistent comparisons on those approaches, especially from the practical industry adoption perspective. Moreover, there is no comparison in the scenario of concept drift, which is a fact for the malware classification task due to the fast evolving nature of malware. In this work, we conduct a thorough empirical study on learning-based PE malware classification approaches on 4 different datasets and consistent experiment settings. Based on the experiment results and an interview with our industry partners, we find that (1) there is no individual class of methods that significantly outperforms the others; (2) All classes of methods show performance degradation on concept drift (by an average F1-score of 32.23%); and (3) the prediction time and high memory consumption hinder existing approaches from being adopted for industry usage. © 2021 ACM.",Concept Drift; Deep Learning; Malware Classification,"1314, 1325",,ESEC/FSE 2021 - Proceedings of the 29th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,Conference Paper,Scopus
138,,Multi-location cryptographic code repair with neural-network-based methodologies,Xiao Y.,2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116276901&doi=10.1145%2f3468264.3473102&partnerID=40&md5=b917c480c7ba6df9470af23950b50234,10.1145/3468264.3473102,"Java Cryptographic API libraries are error-prone and result in vulnerabilities. The fixes of them often require security expertise and extra consideration for cryptographic consistency at multiple code locations. My Ph.D. research aims to help developers with a multi-location cryptographic code repair system. The proposed method relies on a precise static analysis for cryptographic code and a neural network based secure code generation solution. We focus on designing neural network based techniques guided by program analysis to learn from the secure code and give accurate suggestions. First, we conducted a comprehensive measurement to compare cryptographic API embeddings guided by different program analysis strategies. Then, we identified two previously unreported programming language-specific challenges, differentiating functionally similar APIs and capturing low-frequency code patterns. We address them by a specialized multi-path code suggestion architecture, and a novel low-frequency enhanced sequence learning technique. Existing results show that our approach achieves significant improvements on top-1 accuracy compared with the state-of-the-art.Our next step is an cryptographic consistent localization that enables our multi-location code repair. We publish our data and code as a large Java cryptographic code dataset. © 2021 ACM.",code embedding; code suggestion; cryptographic API misuse; neural networks,"1640, 1644",,ESEC/FSE 2021 - Proceedings of the 29th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,Conference Paper,Scopus
139,,Term interrelations and trends in software engineering,"Baskararajah J., Zhang L., Miranskyy A.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116276105&doi=10.1145%2f3468264.3473132&partnerID=40&md5=0d03de97a775a33f04d48edf5cc7a30e,10.1145/3468264.3473132,"The Software Engineering (SE) community is prolific, making it challenging for experts to keep up with the flood of new papers and for neophytes to enter the field. Therefore, we posit that the community may benefit from a tool extracting terms and their interrelations from the SE community's text corpus and showing terms' trends. In this paper, we build a prototyping tool using the word embedding technique. We train the embeddings on the SE Body of Knowledge handbook and 15,233 research papers' titles and abstracts. We also create test cases necessary for validation of the training of the embeddings. We provide representative examples showing that the embeddings may aid in summarizing terms and uncovering trends in the knowledge base. © 2021 ACM.",software engineering trends; term interrelations; word embeddings,"1471, 1474",,ESEC/FSE 2021 - Proceedings of the 29th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,Conference Paper,Scopus
140,,BiasRV: Uncovering biased sentiment predictions at runtime,"Yang Z., Asyrofi M.H., Lo D.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116267303&doi=10.1145%2f3468264.3473117&partnerID=40&md5=95c7b4aafa956ce467f09c0f9ed7a371,10.1145/3468264.3473117,"Sentiment analysis (SA) systems, though widely applied in many domains, have been demonstrated to produce biased results. Some research works have been done in automatically generating test cases to reveal unfairness in SA systems, but the community still lacks tools that can monitor and uncover biased predictions at runtime. This paper fills this gap by proposing BiasRV, the first tool to raise an alarm when a deployed SA system makes a biased prediction on a given input text. To implement this feature, BiasRV dynamically extracts a template from an input text and generates gender-discriminatory mutants (semantically-equivalent texts that only differ in gender information) from the template. Based on popular metrics used to evaluate the overall fairness of an SA system, we define the distributional fairness property for an individual prediction of an SA system. This property specifies a requirement that for one piece of text, mutants from different gender classes should be treated similarly. Verifying the distributional fairness property causes much overhead to the running system. To run more efficiently, BiasRV adopts a two-step heuristic: (1) sampling several mutants from each gender and checking if the system predicts them as of the same sentiment, (2) checking distributional fairness only when sampled mutants have conflicting results. Experiments show that when compared to directly checking the distributional fairness property for each input text, our two-step heuristic can decrease the overhead used for analyzing mutants by 73.81% while only resulting in 6.7% of biased predictions being missed. Besides, BiasRV can be used conveniently without knowing the implementation of SA systems. Future researchers can easily extend BiasRV to detect more types of bias, e.g., race and occupation. The demo video for BiasRV can be viewed at https://youtu.be/WPe4Ml77d3U and the source code can be found at https://github.com/soarsmu/BiasRV. © 2021 ACM.",Ethical AI; Fairness; Runtime Verification; Sentiment Analysis,"1540, 1544",,ESEC/FSE 2021 - Proceedings of the 29th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,Conference Paper,Scopus
141,,An empirical investigation of practical log anomaly detection for online service systems,"Zhao N., Wang H., Li Z., Peng X., Wang G., Pan Z., Wu Y., Feng Z., Wen X., Zhang W., Sui K., Pei D.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116265051&doi=10.1145%2f3468264.3473933&partnerID=40&md5=d01c61125dfca1d643ec9c5ecee9b20a,10.1145/3468264.3473933,"Log data is an essential and valuable resource of online service systems, which records detailed information of system running status and user behavior. Log anomaly detection is vital for service reliability engineering, which has been extensively studied. However, we find that existing approaches suffer from several limitations when deploying them into practice, including 1) inability to deal with various logs and complex log abnormal patterns; 2) poor interpretability; 3) lack of domain knowledge. To help understand these practical challenges and investigate the practical performance of existing work quantitatively, we conduct the first empirical study and an experimental study based on large-scale real-world data. We find that logs with rich information indeed exhibit diverse abnormal patterns (e.g., keywords, template count, template sequence, variable value, and variable distribution). However, existing approaches fail to tackle such complex abnormal patterns, producing unsatisfactory performance. Motivated by obtained findings, we propose a generic log anomaly detection system named LogAD based on ensemble learning, which integrates multiple anomaly detection approaches and domain knowledge, so as to handle complex situations in practice. About the effectiveness of LogAD, the average F1-score achieves 0.83, outperforming all baselines. Besides, we also share some success cases and lessons learned during our study. To our best knowledge, we are the first to investigate practical log anomaly detection in the real world deeply. Our work is helpful for practitioners and researchers to apply log anomaly detection to practice to enhance service reliability. © 2021 ACM.",Log Anomaly Detection; Online Service Systems; Practical Challenges,"1404, 1415",,ESEC/FSE 2021 - Proceedings of the 29th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,Conference Paper,Scopus
142,,GLIB: Towards automated test oracle for graphically-rich applications,"Chen K., Li Y., Chen Y., Fan C., Hu Z., Yang W.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116250781&doi=10.1145%2f3468264.3468586&partnerID=40&md5=564668efe6e8eb0cf5a1d61d4d925b23,10.1145/3468264.3468586,"Graphically-rich applications such as games are ubiquitous with attractive visual effects of Graphical User Interface (GUI) that offers a bridge between software applications and end-users. However, various types of graphical glitches may arise from such GUI complexity and have become one of the main component of software compatibility issues. Our study on bug reports from game development teams in NetEase Inc. indicates that graphical glitches frequently occur during the GUI rendering and severely degrade the quality of graphically-rich applications such as video games. Existing automated testing techniques for such applications focus mainly on generating various GUI test sequences and check whether the test sequences can cause crashes. These techniques require constant human attention to captures non-crashing bugs such as bugs causing graphical glitches. In this paper, we present the first step in automating the test oracle for detecting non-crashing bugs in graphically-rich applications. Specifically, we propose GLIB based on a code-based data augmentation technique to detect game GUI glitches. We perform an evaluation of GLIB on 20 real-world game apps (with bug reports available) and the result shows that GLIB can achieve 100% precision and 99.5% recall in detecting non-crashing bugs such as game GUI glitches. Practical application of GLIB on another 14 real-world games (without bug reports) further demonstrates that GLIB can effectively uncover GUI glitches, with 48 of 53 bugs reported by GLIB having been confirmed and fixed so far. © 2021 ACM.",Automated Test Oracle; Deep Learning; Game Testing; GUI Testing,"1093, 1104",,ESEC/FSE 2021 - Proceedings of the 29th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,Conference Paper,Scopus
143,,Towards automating code review at scale,"Hellendoorn V.J., Tsay J., Mukherjee M., Hirzel M.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116247511&doi=10.1145%2f3468264.3473134&partnerID=40&md5=74cd260e92b050b9cfe1f12a6f2709e0,10.1145/3468264.3473134,"As neural methods are increasingly used to support and automate software development tasks, code review is a natural next target. Yet, training models to imitate developers based on past code reviews is far from straightforward: reviews found in open-source projects vary greatly in quality, phrasing, and depth depending on the reviewer. In addition, changesets are often large, stretching the capacity of current neural models. Recent work reported modest success at predicting review resolutions, but largely side-stepped the above issues by focusing on small inputs where comments were already known to occur. This work examines the vision and challenges of automating code review at realistic scale. We collect hundreds of thousands of changesets across hundreds of projects that routinely conduct code review, many of which change thousands of tokens. We focus on predicting just the locations of comments, which are quite rare. By analyzing model performance and dataset statistics, we show that even this task is already challenging, in no small part because of tremendous variation and (apparent) randomness in code reviews. Our findings give rise to a research agenda for realistically and impactfully automating code review. © 2021 Owner/Author.",code review; neural networks,"1479, 1482",,ESEC/FSE 2021 - Proceedings of the 29th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,Conference Paper,Scopus
144,,Learning type annotation: Is big data enough?,"Jesse K., Devanbu P.T., Ahmed T.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116245693&doi=10.1145%2f3468264.3473135&partnerID=40&md5=fdedb37ac84a4d082c11c86bb591a338,10.1145/3468264.3473135,"TypeScript is a widely used optionally-typed language where developers can adopt ""pay as you go""typing: they can add types as desired, and benefit from static typing. The ""type annotation tax""or manual effort required to annotate new or existing TypeScript can be reduced by a variety of automatic methods. Probabilistic machine-learning (ML) approaches work quite well. ML approaches use different inductive biases, ranging from simple token sequences to complex graphical neural network (GNN) models capturing syntax and semantic relations. More sophisticated inductive biases are hand-engineered to exploit the formal nature of software. Rather than deploying fancy inductive biases for code, can we just use ""big data""to learn natural patterns relevant to typing? We find evidence suggesting that this is the case. We present TypeBert, demonstrating that even with simple token-sequence inductive bias used in BERT-style models and enough data, type-annotation performance of the most sophisticated models can be surpassed. © 2021 Owner/Author.",deep learning; transfer learning; Type inference; TypeScript,"1483, 1486",,ESEC/FSE 2021 - Proceedings of the 29th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,Conference Paper,Scopus
145,,Freeing hybrid distributed AI training configuration,Wang H.,2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116243854&doi=10.1145%2f3468264.3473104&partnerID=40&md5=cc3720231a6dda9c97857f7247069a09,10.1145/3468264.3473104,"Deep neural network (DNN) has become the leading technology to realize Artificial Intelligence (AI). As DNN models become larger and more complex, so do datasets. Being able to efficiently train DNNs in parallel has become a crucial need. Data Parallelism (DP) is the widest-used solution today to accelerate DNN training but could be inefficient when processing DNNs with large-size parameters. Hybrid Parallelism (HP), which applies different parallel strategies on different parts of DNNs, is more efficient but requires advanced configurations. Not all AI researchers are experts in parallel computing, thus automating the configuration of HP strategies is very desirable for all AI frameworks. We propose a parallel semantics analysis method, which can analyze the trade-offs among different kinds of parallelisms and systematically choose the HP strategies with good training time performance. We demonstrated experimentally 260% speedup when applying our method compared to using a conventional DP approach. With our proposal, AI researchers would be able to focus more on AI algorithm research without being disturbed by parallel analysis and engineering concerns. © 2021 ACM.",AI Framework; Distributed Deep Learning; Performance Analysis; Systematic Partitioning,"1620, 1624",,ESEC/FSE 2021 - Proceedings of the 29th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,Conference Paper,Scopus
146,,Domain adaptation for an automated classification of deontic modalities in software engineering contracts,"Joshi V., Anish P.R., Ghaisas S.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116226321&doi=10.1145%2f3468264.3473921&partnerID=40&md5=569f1688fbdc1652772891318e3f80c8,10.1145/3468264.3473921,"Contracts are agreements between parties engaging in economic transactions. They specify deontic modalities that the signatories should be held responsible for and state the penalties or actions to be taken if the stated agreements are not met. Additionally, contracts have also been known to be source of Software Engineering (SE) requirements. Identifying the deontic modalities in contracts can therefore add value to the Requirements Engineering (RE) phase of SE. The complex and ambiguous language of contracts make it difficult and time-consuming to identify the deontic modalities (obligations, permissions, prohibitions), embedded in the text. State-of-art neural network models are effective for text classification; however, they require substantial amounts of training data. The availability of contracts data is sparse owing to the confidentiality concerns of customers. In this paper, we leverage the linguistic and taxonomical similarities between regulations (available abundantly in the public domain) and contracts to demonstrate that it is possible to use regulations as training data for classifying deontic modalities in real-life contracts. We discuss the results of a range of experiments from the use of rule-based approach to Bidirectional Encoder Representations from Transformers (BERT) for automating the classification of deontic modalities. With BERT, we obtained an average precision and recall of 90% and 89.66% respectively. © 2021 ACM.",BERT; BiLSTM; Business Contract; Deep Learning Models; Deontic Modality; Domain Adaptation; Regulation,"1275, 1280",,ESEC/FSE 2021 - Proceedings of the 29th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,Conference Paper,Scopus
147,,Automating the removal of obsolete TODO comments,"Gao Z., Xia X., Lo D., Grundy J., Zimmermann T.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116210160&doi=10.1145%2f3468264.3468553&partnerID=40&md5=6ce12a520cb3da739efe05874cbde11d,10.1145/3468264.3468553,"TODO comments are very widely used by software developers to describe their pending tasks during software development. However, after performing the task developers sometimes neglect or simply forget to remove the TODO comment, resulting in obsolete TODO comments. These obsolete TODO comments can confuse development teams and may cause the introduction of bugs in the future, decreasing the software's quality and maintainability. Manually identifying obsolete TODO comments is time-consuming and expensive. It is thus necessary to detect obsolete TODO comments and remove them automatically before they cause any unwanted side effects. In this work, we propose a novel model, named TDCleaner, to identify obsolete TODO comments in software projects. TDCleaner can assist developers in just-in-time checking of TODO comments status and avoid leaving obsolete TODO comments. Our approach has two main stages: offline learning and online prediction. During offline learning, we first automatically establish <code_change, todo_comment, commit_msg> training samples and leverage three neural encoders to capture the semantic features of TODO comment, code change and commit message respectively. TDCleaner then automatically learns the correlations and interactions between different encoders to estimate the final status of the TODO comment. For online prediction, we check a TODO comment's status by leveraging the offline trained model to judge the TODO comment's likelihood of being obsolete. We built our dataset by collecting TODO comments from the top-10,000 Python and Java Github repositories and evaluated TDCleaner on them. Extensive experimental results show the promising performance of our model over a set of benchmarks. We also performed an in-the-wild evaluation with real-world software projects, we reported 18 obsolete TODO comments identified by TDCleaner to Github developers and 9 of them have already been confirmed and removed by the developers, demonstrating the practical usage of our approach. © 2021 ACM.",Bert; Code-Comment Inconsistency; TODO comment,"218, 229",,ESEC/FSE 2021 - Proceedings of the 29th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,Conference Paper,Scopus
148,,Learning-based extraction of first-order logic representations of API directives,"Liu M., Peng X., Marcus A., Treude C., Bai X., Lyu G., Xie J., Zhang X.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116209268&doi=10.1145%2f3468264.3468618&partnerID=40&md5=8da357bf632bdefc704b86a5d7b040bd,10.1145/3468264.3468618,"Developers often rely on API documentation to learn API directives, i.e., constraints and guidelines related to API usage. Failing to follow API directives may cause defects or improper implementations. Since there are no industry-wide standards on how to document API directives, they take many forms and are often hard to understand by developers or challenging to parse with tools. In this paper, we propose a learning based approach for extracting first-order logic representations of API directives (FOL directives for short). The approach, called LEADFOL, uses a joint learning method to extract atomic formulas by identifying the predicates and arguments involved in directive sentences, and recognizes the logical relations between atomic formulas, by parsing the sentence structures. It then parses the arguments and uses a learning based method to link API references to their corresponding API elements. Finally, it groups the formulas of the same class or method together and transforms them into conjunctive normal form. Our evaluation shows that LEADFOL can accurately extract more FOL directives than a state-of-the-art approach and that the extracted FOL directives are useful in supporting code reviews. © 2021 ACM.",API Documentation; Directive; First Order Logic,"491, 502",,ESEC/FSE 2021 - Proceedings of the 29th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,Conference Paper,Scopus
149,,Semantic bug seeding: A learning-based approach for creating realistic bugs,"Patra J., Pradel M.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116207842&doi=10.1145%2f3468264.3468623&partnerID=40&md5=2448d0e451e7334c5c8639aace055a84,10.1145/3468264.3468623,"When working on techniques to address the wide-spread problem of software bugs, one often faces the need for a large number of realistic bugs in real-world programs. Such bugs can either help evaluate an approach, e.g., in form of a bug benchmark or a suite of program mutations, or even help build the technique, e.g., in learning-based bug detection. Because gathering a large number of real bugs is difficult, a common approach is to rely on automatically seeded bugs. Prior work seeds bugs based on syntactic transformation patterns, which often results in unrealistic bugs and typically cannot introduce new, application-specific code tokens. This paper presents SemSeed, a technique for automatically seeding bugs in a semantics-aware way. The key idea is to imitate how a given real-world bug would look like in other programs by semantically adapting the bug pattern to the local context. To reason about the semantics of pieces of code, our approach builds on learned token embeddings that encode the semantic similarities of identifiers and literals. Our evaluation with real-world JavaScript software shows that the approach effectively reproduces real bugs and clearly outperforms a semantics-unaware approach. The seeded bugs are useful as training data for learning-based bug detection, where they significantly improve the bug detection ability. Moreover, we show that SemSeed-created bugs complement existing mutation testing operators, and that our approach is efficient enough to seed hundreds of thousands of bugs within an hour. © 2021 ACM.",bug injection; bugs; dataset; machine learning; token embeddings,"906, 918",,ESEC/FSE 2021 - Proceedings of the 29th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,Conference Paper,Scopus
150,,Does reusing pre-trained NLP model propagate bugs?,Chakraborty M.,2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116200524&doi=10.1145%2f3468264.3473494&partnerID=40&md5=02eef32dc4f71477e365c0660b2f559f,10.1145/3468264.3473494,"In this digital era, the textual content has become a seemingly ubiquitous part of our life. Natural Language Processing (NLP) empowers machines to comprehend the intricacies of textual data and eases human-computer interaction. Advancement in language modeling, continual learning, availability of a large amount of linguistic data, and large-scale computational power have made it feasible to train models for downstream tasks related to text analysis, including safety-critical ones, e.g., medical, airlines, etc. Compared to other deep learning (DL) models, NLP-based models are widely reused for various tasks. However, the reuse of pre-trained models in a new setting is still a complex task due to the limitations of the training dataset, model structure, specification, usage, etc. With this motivation, we study BERT, a vastly used language model (LM), from the direction of reusing in the code. We mined 80 posts from Stack Overflow related to BERT and found 4 types of bugs observed in clients' code. Our results show that 13.75% are fairness, 28.75% are parameter, 15% are token, and 16.25% are version-related bugs. © 2021 Owner/Author.",BERT; Bug; Deep Learning; NLP; Reuse,"1686, 1688",,ESEC/FSE 2021 - Proceedings of the 29th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,Conference Paper,Scopus
151,,Empirical study of transformers for source code,"Chirkova N., Troshin S.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116192638&doi=10.1145%2f3468264.3468611&partnerID=40&md5=46cbb215786705d879da5673d8ae45e8,10.1145/3468264.3468611,"Initially developed for natural language processing (NLP), Transformers are now widely used for source code processing, due to the format similarity between source code and text. In contrast to natural language, source code is strictly structured, i.e., it follows the syntax of the programming language. Several recent works develop Transformer modifications for capturing syntactic information in source code. The drawback of these works is that they do not compare to each other and consider different tasks. In this work, we conduct a thorough empirical study of the capabilities of Transformers to utilize syntactic information in different tasks. We consider three tasks (code completion, function naming and bug fixing) and re-implement different syntax-capturing modifications in a unified framework. We show that Transformers are able to make meaningful predictions based purely on syntactic information and underline the best practices of taking the syntactic information into account for improving the performance of the model. © 2021 Owner/Author.",code completion; function naming; neural networks; transformer; variable misuse detection,"703, 715",,ESEC/FSE 2021 - Proceedings of the 29th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,Conference Paper,Scopus
152,,Generalizable and interpretable learning for configuration extrapolation,"Ding Y., Pervaiz A., Carbin M., Hoffmann H.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116192082&doi=10.1145%2f3468264.3468603&partnerID=40&md5=31b5de0b6c108b2cfd4025b162845930,10.1145/3468264.3468603,"Modern software applications are increasingly configurable, which puts a burden on users to tune these configurations for their target hardware and workloads. To help users, machine learning techniques can model the complex relationships between software configuration parameters and performance. While powerful, these learners have two major drawbacks: (1) they rarely incorporate prior knowledge and (2) they produce outputs that are not interpretable by users. These limitations make it difficult to (1) leverage information a user has already collected (e.g., tuning for new hardware using the best configurations from old hardware) and (2) gain insights into the learner's behavior (e.g., understanding why the learner chose different configurations on different hardware or for different workloads). To address these issues, this paper presents two configuration optimization tools, GIL and GIL+, using the proposed generalizable and interpretable learning approaches. To incorporate prior knowledge, the proposed tools (1) start from known configurations, (2) iteratively construct a new linear model, (3) extrapolate better performance configurations from that model, and (4) repeat. Since the base learners are linear models, these tools are inherently interpretable. We enhance this property with a graphical representation of how they arrived at the highest performance configuration. We evaluate GIL and GIL+ by using them to configure Apache Spark workloads on different hardware platforms and find that, compared to prior work, GIL and GIL+ produce comparable, and sometimes even better performance configurations, but with interpretable results. © 2021 Owner/Author.",Configuration; generalizability; interpretability; machine learning,"728, 740",,ESEC/FSE 2021 - Proceedings of the 29th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,Conference Paper,Scopus
153,,A First Look at Developers' Live Chat on Gitter,"Shi L., Chen X., Yang Y., Jiang H., Jiang Z., Niu N., Wang Q.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115862638&doi=10.1145%2f3468264.3468562&partnerID=40&md5=5537a68a10c88bf977dc7a3c0a666579,10.1145/3468264.3468562,"Modern communication platforms such as Gitter and Slack play an increasingly critical role in supporting software teamwork, especially in open source development.Conversations on such platforms often contain intensive, valuable information that may be used for better understanding OSS developer communication and collaboration. However, little work has been done in this regard. To bridge the gap, this paper reports a first comprehensive empirical study on developers' live chat, investigating when they interact, what community structures look like, which topics are discussed, and how they interact. We manually analyze 749 dialogs in the first phase, followed by an automated analysis of over 173K dialogs in the second phase. We find that developers tend to converse more often on weekdays, especially on Wednesdays and Thursdays (UTC), that there are three common community structures observed, that developers tend to discuss topics such as API usages and errors, and that six dialog interaction patterns are identified in the live chat communities. Based on the findings, we provide recommendations for individual developers and OSS communities, highlight desired features for platform vendors, and shed light on future research directions. We believe that the findings and insights will enable a better understanding of developers' live chat, pave the way for other researchers, as well as a better utilization and mining of knowledge embedded in the massive chat history. © 2021 ACM.",Empirical Study; Live chat; Open source; Team communication,"391, 403",,ESEC/FSE 2021 - Proceedings of the 29th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,Conference Paper,Scopus
154,,Research on Chinese Minority Clothing Based on Deep Convolution Neural Network,"Zhang Y., Zhong W., Li X.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114965584&doi=10.1109%2fICSESS52187.2021.9522354&partnerID=40&md5=75cfd8e2d4613277ffd0e9b876276560,10.1109/ICSESS52187.2021.9522354,"There are 55 minority nationalities in China. The traditional clothing of minority nationality is a part of Chinese traditional culture. How to use computer technology to identify these clothing has great significance for the protection and inheritance of Chinese traditional culture. Some scholars have studied the methods of minority nationality clothing recognition, but these methods require people to manually mark the semantic attributes of clothing on the training set pictures. In this paper, an end-To-end method of deep convolution neural network is adopted to implement the Chinese minority nationality clothing classifier. On the basis of CNN pre-Training model mobilenet-v2, Fine-Tune training can classify minority nationality clothing without manual marking on the training set pictures, and the accuracy rate is 94.5%. © 2021 IEEE.",Chinese minority clothing; CNN; Deep convolution neural network; Image Recognition,"235, 238",,"Proceedings of the IEEE International Conference on Software Engineering and Service Sciences, ICSESS",Conference Paper,Scopus
155,,Named Entity Recognition Method for Educational Emergency Field Based on BERT,"Wei K., Wen B.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114965506&doi=10.1109%2fICSESS52187.2021.9522262&partnerID=40&md5=c34f27a3c954a86c2a0436943754495e,10.1109/ICSESS52187.2021.9522262,"Educational emergencies provide a lot of information. Aiming at the problem that traditional named entity recognition methods in this field cannot represent the ambiguity of a word, this article proposes a named entity recognition method BERT+BiLSTM-CRF in the field of educational emergencies based on the BERT pre-Training language model. Firstly, BERT is trained on the corpus of educational emergencies to obtain the vectorized representation of the words, and then the context encoding of the serialized text is obtained using BiLSTM, and then the sequence is decoded and annotated by CRF to obtain the corresponding entities in the educational emergencies. Experiments show that the BERT+BiLSTM-CRF fusion model has achieved an accuracy of 91.62% on the educational emergency data set, which is a significant improvement compared to the traditional named entity recognition model. © 2021 IEEE.",BERT; Education emergencies; Named entity recognition; Vectorization,"145, 149",,"Proceedings of the IEEE International Conference on Software Engineering and Service Sciences, ICSESS",Conference Paper,Scopus
156,,Research on Chinese Event Extraction Method Based on RoBERTa-WWM-CRF,"Li Z., Cheng N., Song W.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114963129&doi=10.1109%2fICSESS52187.2021.9522150&partnerID=40&md5=e812f43d0cc933808f577226b283b784,10.1109/ICSESS52187.2021.9522150,"To extract the event information contained in the Chinese text effectively, this paper takes Chinese event extraction as a sequential labeling task, and proposes a method to extract events based on the combination of RoBERTa-WWM (A Robustly Optimized BERT Pre-Training Approach-Whole Word Masking) and Conditional Random Fields (CRF). This method uses RoBERTa-WWM to generate semantic representation with prior knowledge, and then inputs them into the Conditional Random Fields (CRF) model. The argument is predicted by the output label sequence. The experimental results show that this method can effectively improve accuracy, recall, and F1-score on the Chinese event extraction dataset DUEE1.0, which Baidu recently released, and improve the performance of event extraction in Chinese text. © 2021 IEEE.",Chinese Event Extraction; component; Pretraining Model; RoBERTa-WWM-CRF,"100, 104",,"Proceedings of the IEEE International Conference on Software Engineering and Service Sciences, ICSESS",Conference Paper,Scopus
157,,The Impact of Feature Selection Techniques on Software Defect Identification Models,"Gong H., Zhang Y.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114962066&doi=10.1109%2fICSESS52187.2021.9522337&partnerID=40&md5=70637812b730fb53229a58ce0fd51a34,10.1109/ICSESS52187.2021.9522337,"Defect identification is an important task for ensuring the quality of software. Recently, researchers have begun to utilize artificial intelligence techniques to improve the usability of static analysis tools by automatically identifying true defects from the reported SA alarms. Existing methods mainly focus on using the static code features to represent the defective code. However, a challenge that threatens the performance of these machine learning methods is the irrelevant and redundant features. Feature selection techniques can be applied to alleviate this problem. Since many feature selection methods have been proposed, this paper conducts a rigorous experimental evaluation on the impact of feature selection techniques for defect identification and explores whether there is a smallest ratio when using the feature selection techniques for building defect identification models with acceptable performance. Additionally, this paper proposes an effective feature selection approach based on the idea of majority voting, combing the output results of different feature selection techniques. The experimental results for five open-source projects show that there is a best ratio (20%) for feature selection which achieves satisfied performance with far fewer features used for defect identification. This finding can serve as a practical guideline for software defect identification. © 2021 IEEE.",Feature Selection; Machine Learning; Model Evaluation; Software Defect Identification; Static Analysis,"12, 15",,"Proceedings of the IEEE International Conference on Software Engineering and Service Sciences, ICSESS",Conference Paper,Scopus
158,,Research on Entity Recognition Based on Multi-criteria Fusion Model,Qiyu Y.,2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114961552&doi=10.1109%2fICSESS52187.2021.9522307&partnerID=40&md5=a3b8fcdf9fc2d0f70b943252c8f8be62,10.1109/ICSESS52187.2021.9522307,"In order to fully and comprehensively utilize the Chinese named entity recognition corpus marked according to different labeling criteria, this paper, by applying the word-based BERT bidirectional language model, introducing multi-criteria shared connection layer and conditional random field (CRF) system, and using Microsoft Research Asia MSRA-NER corpus and Peking University's People's Daily part-of-speech tagging corpus (RMRB-98-1), firstly formed each Chinese named entity recognition model separately, and then mixed a multi-criteria fusion model with two corpora. Experiments show that the recognition effect of multi-criteria fusion model is better than that of each corpus independent model, reaching 94.46% F1 value and 94.32% F1 value respectively on MSRA-NER and RMRB-98-1 corpus. However, the experiment still has its limit in the scale of the corpus involved in the fusion. Later, integration of more corpora, and combination with entity recognition tasks in specific fields such as biology and military will be further explored to enhance the recognition effect. © 2021 IEEE.",attention mechanism; entity recognition; multicriteria learning; transfer learning,"85, 88",,"Proceedings of the IEEE International Conference on Software Engineering and Service Sciences, ICSESS",Conference Paper,Scopus
159,,A Few-shot Learning Method Based on Bidirectional Encoder Representation from Transformers for Relation Extraction,"Gao Y., Qi R., Li S.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114961497&doi=10.1109%2fICSESS52187.2021.9522191&partnerID=40&md5=5950168aefc11f390af74940265b829e,10.1109/ICSESS52187.2021.9522191,"Relation extraction is one of the fundamental subtasks of the information extraction. The purpose is to determine the implicit relation between two entities in a sentence. Therefore, Convolutional Neural Networks and Feature Attention-based Prototypical Networks (CNN-Proto-FATT), a typical few-shot learning method, is proposed and achieve competitive performance. However, convolutional neural networks suffer from the insufficient instances of relation in real scenes, leading to undesirable results. To extract long-distance features more comprehensively, the pre-Trained model Bidirectional Encoder Representation from Transformers (BERT) is incorporated into CNN-Proto-FATT. In this model, named Bidirectional Encoder Representation from Transformers and Feature Attention-based Prototypical Networks (BERT-Proto-FATT), the multi-head attention helps the network extract semantic features cross long-And short-distance to enhance the encoded representations. Experimental results indicate that BERT-Proto-FATT demonstrates significant improvements on the FewRel dataset. © 2021 IEEE.",few-shot learning; information extraction; pre-Trained model; relation extraction,"158, 161",,"Proceedings of the IEEE International Conference on Software Engineering and Service Sciences, ICSESS",Conference Paper,Scopus
160,,Visual Loop Closure Detection Based on Lightweight Convolutional Neural Network and Product Quantization,"Huang L., Zhu M., Zhang M.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114961165&doi=10.1109%2fICSESS52187.2021.9522158&partnerID=40&md5=f744c485dc56162e9a1fa3f5152a822e,10.1109/ICSESS52187.2021.9522158,"Mobile robots rely heavily on the creation of the scene map and the positioning in the map in an unknown environment, but no matter what type of map is created, it is inevitably affected by cumulative errors. This presents a huge challenge for loop closure detection technology. Using traditional loop closure detection methods to perform scene recognition is difficult to extract the appearance changes caused by time, weather, or seasonal conditions in the image and deep semantic information, and the speed of extracting image features is slow, which is difficult to meet the real-Time performance of robots. Because of the success of deep convolutional neural networks(CNN), it is possible to enrich the information of image features. First of all, this paper uses the pre-Trained CNN model SSE-Net to extract the deep visual appearance and semantic features of the image, and obtain the feature description vector. Then, after product quantization(PQ) and encoding, the final pair of candidate frames is quickly searched and matched to obtain the most similar pair of candidate frames and judged as a loop. After the verification of the New collage dataset and the City Center dataset, this algorithm has achieved a good Precision-Recall rate and a faster speed compared with the recently proposed large-scale convolution network VGG16 method and traditional feature extraction methods. © 2021 IEEE.",convolutional neural networks; loop closure detection; mobile robots; product quantization,"122, 126",,"Proceedings of the IEEE International Conference on Software Engineering and Service Sciences, ICSESS",Conference Paper,Scopus
161,,Automatic Deployment Method of USV Software Based on Functional Domain,"Qing C., Lei Z., Feng L.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114960687&doi=10.1109%2fICSESS52187.2021.9522160&partnerID=40&md5=097b64762338498c9edcda92b1b2ce11,10.1109/ICSESS52187.2021.9522160,"In order to meet the unmanned requirements of Unmanned Surface Vessels (USV), aiming at the automatic deployment of USV software system, based on functional semantic analysis and rough set reduction, the functional domain division of USV software is realized to support automatic software deployment. The Bidirectional Encoder Representation from Transformer (BERT) model is pre-Trained by the relevant standards and design documents of USV, and the semantic correlation between the function descriptions of USV software services is analyzed by using the trained model. The rough set information system for USV software system is constructed. The appropriate functional domain partition is extracted by rough set attribute reduction method, and the functional domain attribution of software service is obtained according to the equivalent partition of rough set. So USV software system can use functional domain to deploy automatically. The results show that the proposed method can meet the requirements of automatic deployment of USV software services developed from different sources and in different periods, and can realize automatic re deployment according to task changes. © 2021 IEEE.",automatic deployment; BERT; component; functional domain; rough set,"89, 95",,"Proceedings of the IEEE International Conference on Software Engineering and Service Sciences, ICSESS",Conference Paper,Scopus
162,,Understanding neural code intelligence through program simplification,"Rabin M.R.I., Hellendoorn V.J., Alipour M.A.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109390946&doi=10.1145%2f3468264.3468539&partnerID=40&md5=606cda3bd521eb2c5ead36d500438f1b,10.1145/3468264.3468539,"A wide range of code intelligence (CI) tools, powered by deep neural networks, have been developed recently to improve programming productivity and perform program analysis. To reliably use such tools, developers often need to reason about the behavior of the underlying models and the factors that affect them. This is especially challenging for tools backed by deep neural networks. Various methods have tried to reduce this opacity in the vein of ""transparent/interpretable-AI"". However, these approaches are often specific to a particular set of network architectures, even requiring access to the network's parameters. This makes them difficult to use for the average programmer, which hinders the reliable adoption of neural CI systems. In this paper, we propose a simple, model-agnostic approach to identify critical input features for models in CI systems, by drawing on software debugging research, specifically delta debugging. Our approach, SIVAND, uses simplification techniques that reduce the size of input programs of a CI model while preserving the predictions of the model. We show that this approach yields remarkably small outputs and is broadly applicable across many model architectures and problem domains. We find that the models in our experiments often rely heavily on just a few syntactic features in input programs. We believe that SIVAND's extracted features may help understand neural CI systems' predictions and learned behavior. © 2021 ACM.",Interpretable AI; Models of Code; Program Simplification,"441, 452",,ESEC/FSE 2021 - Proceedings of the 29th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,Conference Paper,Scopus
163,,CVEfixes: Automated collection of vulnerabilities and their fixes from open-source software,"Bhandari G., Naseer A., Moonen L.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113649225&doi=10.1145%2f3475960.3475985&partnerID=40&md5=587cce7d3192fade582f1bfb832c322f,10.1145/3475960.3475985,"Data-driven research on the automated discovery and repair of security vulnerabilities in source code requires comprehensive datasets of real-life vulnerable code and their fixes. To assist in such research, we propose a method to automatically collect and curate a comprehensive vulnerability dataset from Common Vulnerabilities and Exposures (CVE) records in the National Vulnerability Database (NVD). We implement our approach in a fully automated dataset collection tool and share an initial release of the resulting vulnerability dataset named CVEfixes. The CVEfixes collection tool automatically fetches all available CVE records from the NVD, gathers the vulnerable code and corresponding fixes from associated open-source repositories, and organizes the collected information in a relational database. Moreover, the dataset is enriched with meta-data such as programming language, and detailed code and security metrics at five levels of abstraction. The collection can easily be repeated to keep up-to-date with newly discovered or patched vulnerabilities. The initial release of CVEfixes spans all published CVEs up to 9 June 2021, covering 5365 CVE records for 1754 open-source projects that were addressed in a total of 5495 vulnerability fixing commits. CVEfixes supports various types of data-driven software security research, such as vulnerability prediction, vulnerability classification, vulnerability severity prediction, analysis of vulnerability-related code changes, and automated vulnerability repair. © 2021 Owner/Author.",dataset; Security vulnerabilities; software repository mining; source code repair; vulnerability classification; vulnerability prediction,"30, 39",,"PROMISE 2021 - Proceedings of the 17th International Conference on Predictive Models and Data Analytics in Software Engineering, co-located with ESEC/FSE 2021",Conference Paper,Scopus
164,,Multi-stream online transfer learning for software effort estimation: Is it necessary?,Minku L.L.,2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113636340&doi=10.1145%2f3475960.3475988&partnerID=40&md5=44828b2c505f5ccb4f07ef4d0fc7066d,10.1145/3475960.3475988,"Software Effort Estimation (SEE) may suffer from changes in the relationship between features describing software projects and their required effort over time, hindering predictive performance of machine learning models. To cope with that, most machine learning-based SEE approaches rely on receiving a large number of Within-Company (WC) projects for training over time, being prohibitively expensive. The approach Dycom reduces the number of required WC training projects by transferring knowledge from Cross-Company (CC) projects. However, it assumes that CC projects have no chronology and are entirely available before WC projects start being estimated. Given the importance of taking chronology into account to cope with changes, it may be beneficial to also take the chronology of CC projects into account. This paper thus investigates whether and under what circumstances treating CC projects as multiple data streams to be learned over time may be useful for improving SEE. For that, an extension of Dycom called OATES is proposed to enable multi-stream online learning, so that both incoming WC and CC data streams can be learnt over time. OATES is then compared against Dycom and five other approaches on a case study using four different scenarios derived from the ISBSG Repository. The results show that OATES improved predictive performance over the state-of-the-art when the number of CC projects available beforehand was small. Learning CC projects over time as multiple data streams is thus recommended for improving SEE in such scenario. When the number of CC projects available beforehand was large, OATES obtained similar predictive performance to the state-of-the-art. Therefore, CC data streams are unnecessary in this scenario, but are not detrimental either. © 2021 ACM.",concept drift; cross-company learning; data streams; ensembles; Software effort estimation; transfer learning,"11, 20",,"PROMISE 2021 - Proceedings of the 17th International Conference on Predictive Models and Data Analytics in Software Engineering, co-located with ESEC/FSE 2021",Conference Paper,Scopus
165,,"PROMISE 2021 - Proceedings of the 17th International Conference on Predictive Models and Data Analytics in Software Engineering, co-located with ESEC/FSE 2021",[No author name available],2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113586931&partnerID=40&md5=2eeb3cb63dba7895fe54fb5b9713c202,,The proceedings contain 5 papers. The topics discussed include: heterogeneous ensemble imputation for software development effort estimation; multi-stream online transfer learning for software effort estimation: is it necessary?; comparative study of random search hyper-parameter tuning for software effort estimation; CVEfixes: automated collection of vulnerabilities and their fixes from open-source software; and a classification of code changes and test types dependencies for improving machine learning based test selection.,,,54.0,"PROMISE 2021 - Proceedings of the 17th International Conference on Predictive Models and Data Analytics in Software Engineering, co-located with ESEC/FSE 2021",Conference Review,Scopus
166,,A Character-Level Convolutional Neural Network for Predicting Exploitability of Vulnerability,"Lyu J., Bai Y., Xing Z., Li X., Ge W.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116897421&doi=10.1109%2fTASE52547.2021.00014&partnerID=40&md5=19168c36eb04470aa90f7fa9605be5a3,10.1109/TASE52547.2021.00014,"The continuous discovery of software vulnerabilities have brought great challenges to the cyber security, which will lead to severe systematical or individual losses after being exploited. But the harshly increasing of software vulnerabilities overwhelms the time consuming vulnerability analysis. Security experts must pay more attention to the ones which have the highest priority to be repaired. In general, both severity and exploitability determine the severity of a software vulnerability. Compared with the severity evaluated by the Common Vulnerability Scoring System (CVSS score), the exploitability is still lack of a well-accepted standard. Furthermore, based on the perspective of attack and defense, we found that the exploitability of vulnerabilities is more attractive to hackers so that system or individual is severely affected by the exploitability rather than the severity. In this paper, we propose a deep learning based approach to predict the exploitability of the vulnerability by using the correlated textual description and characteristics. Specifically, our approach takes character-level Convolutional Neural Network (charCNN) to fetch more fine-grained character-level features from the vulnerability description instead of the word-level features considered by the previous literatures. And we highlight the importance of vulnerability characteristics such as Confidentiality Impact, Integrity Impact, Attack Vector etc. during the determination of vulnerability exploitability. Extensive experiments are set to prove the effectiveness of the given charCNN approach through the comparison on both different levels of features and different neural network models. Our approach achieves the best F1 values 93.1% (at least 2.2% more than the baselines). And we also investigate the efficiency of charCNN trained by historical vulnerability when predicting the exploitability of the newly published vulnerabilities. Finally, we further explore the robustness of the proposed model by changing the scale of training sets. For the prediction of vulnerability exploitability, we recommend to adopt 40.0% to 50.0% vulnerabilities to train a robust charCNN model. © 2021 IEEE.",Deep Learning; Mining Software Repositories; Vulnerability description; Vulnerability Exploitability Prediction,"119, 126",,"Proceedings - 2021 International Symposium on Theoretical Aspects of Software Engineering, TASE 2021",Conference Paper,Scopus
167,,A Data-oriented Approach for Detecting offensive Language in Arabic Tweets,Refaee E.A.,2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116169703&doi=10.1109%2fICSECS52883.2021.00051&partnerID=40&md5=3690e9319796c8b7d5b5a00a87e35bda,10.1109/ICSECS52883.2021.00051,"The growing popularity of social media (SM) platforms has made these platforms a crucial part of modern societies. Users from different cultures, backgrounds, demographics get aboard in an increasing manner to express their views, stances, and opinions on a varied range of topics. Since users on SM can easily hide their real identity, a closer look at daily posts on social medial platforms shows that users do not seem to reflect only their stances and views, but also, they get an opportunity for revealing their behaviors, which could be negative towards the others. Although only a small population of SM users can show negative behavior towards other individuals, groups, and society in general, the impact could be catastrophic. This has resulted in the emerge of terms like cyberbullying, online extremism/hatred/threatening, online trolling, online political-polarity discourse. To ensure safe social networking, the domain of automatic detection of offensive/hatred language has lately grown notably. This work focuses on utilizing a publicly available dataset of Arabic tweets labeled for offensive/non-offensive language. Unlike previous work which focuses merely on developing and tuning machine learning models to be as accurate as possible on the benchmark dataset used, we turn to focus on the characteristics of the offensive language used in SM. The purpose is to have an in-depth look into the dataset to disclose what seems to be hidden patterns in offensive language expressed daily online. Our findings reveal the benefit of using larger training dataset that covers a wide range of offensive language patterns to build robust machine learning classifiers with a better ability to generalize well on highly sparse data used in SM. © 2021 IEEE.",Arabic NLP; classification; machine learning; offensive language; twitter,"244, 248",,"Proceedings - 2021 International Conference on Software Engineering and Computer Systems and 4th International Conference on Computational Science and Information Management, ICSECS-ICOCSIM 2021",Conference Paper,Scopus
168,,Data pre-processing of website browsing record: An initial step for web page classification,"Apandi S.H., Sallim J., Mohamed R.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116131908&doi=10.1109%2fICSECS52883.2021.00129&partnerID=40&md5=41a81895dc690ca3899e736b271b0fb8,10.1109/ICSECS52883.2021.00129,"The Internet utilization has resulted in an increase in the number of web pages on the World Wide Web. The classification of web pages is required to organize the growing number of web pages. A web page classification system is proposed to be constructed using a deep learning algorithm. The initial step for web page classification is data pre-processing. The website browsing record is used as a dataset in this study. The raw dataset needs to be pre-processing to fetch the cleaned data by removing missing value data, redundant data, and error data. There are many steps in data pre-processing which include data cleaning and web content pre-processing. The main contribution of this paper is to investigate how to do data pre-processing on website browsing records that focusing on the Game and Online Video web pages that will be utilized as the dataset to construct the web page classification model. After doing the data pre-processing, the number of datasets will be reduced. This shows many datasets have been removed because it is inactive and not suitable to be used in this study as the dataset of Game and Online Video web pages. © 2021 IEEE.",data cleaning; data pre-processing; web content pre-processing; webpage classification; website browsing record,"679, 684",,"Proceedings - 2021 International Conference on Software Engineering and Computer Systems and 4th International Conference on Computational Science and Information Management, ICSECS-ICOCSIM 2021",Conference Paper,Scopus
169,,Word Embedding based Event Identification and Labeling of Connected Events from Tweets,"Das P.K., Al Banna M., Al Fahad M.A., Islam S., Hossain Mukta M.S.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116131379&doi=10.1109%2fICSECS52883.2021.00105&partnerID=40&md5=d25197f654a0a2f93a3901721b4b877f,10.1109/ICSECS52883.2021.00105,"Events conceived as facts which are fine grained entities that happen around us such as completing graduation, birthday celebration, and death of an individual. Events are regarded as happening in a certain place, during a particular interval of time which can be occurred planned or unplanned way. Social media is a platform where users share their attending events with others. In this paper, we present a novel machine learning based approach to identify events from social media, i.e., Twitter, by using Bidirectional Encoder Representations from Transformers (BERT) based word embedding technique. Events might be connected with each other which might have real life implications such as identifying causal-effect, investigating criminal activities, etc. We also demonstrate a mechanism which can organize relevant events into a cluster based on their spatiotemporal properties. Later, we develop an unsupervised connected event labeling technique by using BERT word embedding approach by exploiting its semantic strength from the content of tweets. Our model shows an outstanding performance which has an accuracy of 91%. We also compare our approach with two competitive baseline techniques (i.e., word2vec and tf-idf) to identify events and our model shows better performance (on an average 5% better accuracy) than that of those baseline models. © 2021 IEEE.",BERT; Classification; Event; Twitter; Word Embedding,"541, 546",,"Proceedings - 2021 International Conference on Software Engineering and Computer Systems and 4th International Conference on Computational Science and Information Management, ICSECS-ICOCSIM 2021",Conference Paper,Scopus
170,,Iris Recognition System Using Convolutional Neural Network,"Sallam A., Amery H.A., Al-Qudasi S., Al-Ghorbani S., Rassem T.H., Makbol N.M.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116122371&doi=10.1109%2fICSECS52883.2021.00027&partnerID=40&md5=230204f4508f460c6ff1f0d4dc3316be,10.1109/ICSECS52883.2021.00027,"Identification system is one of the important parts in security domains of the present time. The traditional protection methods considered to be inefficient and unreliable as they are subjected to the theft, imitation or forgetfulness. In contrast, biometrics such as facial recognition, fingerprints and the retina have emerged as modern protection methods, but still also suffer from some defects and violations. However, Iris recognition is an automated method that considered as a promising biometric identification due to the stability and the uniqueness of its patterns. In this paper, an iris recognition system based on Convolutional Neural Network (CNN) model was proposed. CNN is used to perform the required processes of feature extraction and classification. The proposed system was evaluated through CASIA-V1 and ATVS datasets, after the required pre-processing steps taken place, and achieved 98% and 97.83% as a result, respectively. © 2021 IEEE.",Convolution Neural Network; Deep Learning; Fuzzy Operation; Segmentation,"109, 114",,"Proceedings - 2021 International Conference on Software Engineering and Computer Systems and 4th International Conference on Computational Science and Information Management, ICSECS-ICOCSIM 2021",Conference Paper,Scopus
171,,Empirical Evaluation of Fault Localisation Using Code and Change Metrics,"Sohn J., Yoo S.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112763651&doi=10.1109%2fTSE.2019.2930977&partnerID=40&md5=64e1c691e4d788f901a7befaad742cf9,10.1109/TSE.2019.2930977,"Fault localisation aims to reduce the debugging efforts of human developers by highlighting the program elements that are suspected to be the root cause of the observed failure. Spectrum Based Fault Localisation (SBFL), a coverage based approach, has been widely studied in many researches as a promising localisation technique. Recently, however, it has been proven that SBFL techniques have reached the limit of further improvement. To overcome the limitation, we extend SBFL with code and change metrics that have been mainly studied in defect prediction, such as size, age, and churn. FLUCCS, our fault learn-to-rank localisation technique, employs both existing SBFL formulæ and these metrics as input. We investigate the effect of employing code and change metrics for fault localisation using four different learn-to-rank techniques: Genetic Programming, Gaussian Process Modelling, Support Vector Machine, and Random Forest. We evaluate the performance of FLUCCS with 386 real world faults collected from Defects4J repository. The results show that FLUCCS with code and change metrics places 144 faults at the top and 304 faults within the top ten. This is a significant improvement over the state-of-art SBFL formulæ, which can locate 65 and 212 faults at the top and within the top ten, respectively. We also investigate the feasibility of cross-project transfer learning of fault localisation. The results show that, while there exist project-specific properties that can be exploited for better localisation per project, ranking models learnt from one project can be applied to others without significant loss of effectiveness. © 1976-2012 IEEE.",Fault localisation; genetic programming; SBSE,"1605, 1625",,IEEE Transactions on Software Engineering,Article,Scopus
172,,Bearing Health Monitoring and Fault Diagnosis Based on Joint Feature Extraction in 1D- CNN [基于1D-CNN联合特征提取的轴承健康监测与故障诊断],"Liu L., Zhu J.-C., Han G.-J., Bi Y.-G.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112008519&doi=10.13328%2fj.cnki.jos.006188&partnerID=40&md5=4d08a204316d8714e2701651daae47e6,10.13328/j.cnki.jos.006188,"Data-driven fault diagnosis models for specific mechanical equipment lack generalization capabilities. As a core component of various types of machinery, the health status of bearings makes sense in analyzing derivative failures of different machinery. This study proposes a bearing health monitoring and fault diagnosis algorithm based on 1D-CNN (one-dimensional convolution neural network) joint feature extraction. The algorithm first partitions the original vibration signal of the bearing in segmentations. The signal segmentations are used as feature learning spaces and input into the 1D-CNN in parallel to extract the representative feature domain under each working condition. To avoid processing overlapping information generated by faults, a bearing health status discriminant model is built in advance based on the feature domain sensitive to health status. If the health model recognizes that the bearing is not in a healthy state, the feature domain will be reconstructed jointly with the original signal and coupled with an automatic encoder for failure mode classification. Bearing data provided by Case Western Reserve University are used to carry out experiments. Experimental results demonstrate that the proposed algorithm inherits the accuracy and robustness of the deep learning model, and has higher diagnosis accuracy and lower time delay. © Copyright 2021, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Bearing; Fault diagnosis; Industrial Internet of things; Joint feature; One-dimensional convolution neural network,"2379, 2390",,Ruan Jian Xue Bao/Journal of Software,Article,Scopus
173,,Chinese Fine-grained Name Entity Recognition Based on Associated Memory Networks [基于关联记忆网络的中文细粒度命名实体识别],"Ju S.-G., Li T.-N., Sun J.-P.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111956833&doi=10.13328%2fj.cnki.jos.006114&partnerID=40&md5=25c6dbcfe0e4a2889d74f3ef4346e566,10.13328/j.cnki.jos.006114,"Fine-grained named entity recognition is to locate entities in text and classify them into predefined fine-grained categories. At present, Chinese fine-grained named entity recognition only uses pre-trained language models to encode characters in sentences and does not take into account that the category label information can distinguish entity categories. Since the predicted sentence does not have the entity label, the associated memory network is used to capture the entity label information of the sentences in the training set and to incorporate label information into the representation of predicted sentences in this paper. In this method, sentences with entity labels in the training set are used as memory units, the pre-trained language model is used to obtain the contextual representations of the original sentence and the sentence in the memory unit. Then, the label information of the sentences in the memory unit is combined with the representation of the original sentence by the attention mechanism to improve the recognition effect. On the CLUENER 2020 Chinese fine-grained named entity recognition task, this method improves performance over the baseline methods. © Copyright 2021, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Associated memory network; Chinese fine-grained name entity recognition; Multi-headed self-attention; Pre-trained language model,"2545, 2556",,Ruan Jian Xue Bao/Journal of Software,Article,Scopus
174,,Comparison of convolutional neural network training strategies for cone-beam CT image segmentation,"Minnema J., Wolff J., Koivisto J., Lucka F., Batenburg K.J., Forouzanfar T., van Eijnatten M.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110593795&doi=10.1016%2fj.cmpb.2021.106192&partnerID=40&md5=a50222492fe4fbde8116510d04041d17,10.1016/j.cmpb.2021.106192,"Background and objective: Over the past decade, convolutional neural networks (CNNs) have revolutionized the field of medical image segmentation. Prompted by the developments in computational resources and the availability of large datasets, a wide variety of different two-dimensional (2D) and three-dimensional (3D) CNN training strategies have been proposed. However, a systematic comparison of the impact of these strategies on the image segmentation performance is still lacking. Therefore, this study aimed to compare eight different CNN training strategies, namely 2D (axial, sagittal and coronal slices), 2.5D (3 and 5 adjacent slices), majority voting, randomly oriented 2D cross-sections and 3D patches. Methods: These eight strategies were used to train a U-Net and an MS-D network for the segmentation of simulated cone-beam computed tomography (CBCT) images comprising randomly-placed non-overlapping cylinders and experimental CBCT images of anthropomorphic phantom heads. The resulting segmentation performances were quantitatively compared by calculating Dice similarity coefficients. In addition, all segmented and gold standard experimental CBCT images were converted into virtual 3D models and compared using orientation-based surface comparisons. Results: The CNN training strategy that generally resulted in the best performances on both simulated and experimental CBCT images was majority voting. When employing 2D training strategies, the segmentation performance can be optimized by training on image slices that are perpendicular to the predominant orientation of the anatomical structure of interest. Such spatial features should be taken into account when choosing or developing novel CNN training strategies for medical image segmentation. Conclusions: The results of this study will help clinicians and engineers to choose the most-suited CNN training strategy for CBCT image segmentation. © 2021",Cone-beam computed tomography; Convolutional neural networks; Deep learning; Medical image segmentation; Training strategies,,,Computer Methods and Programs in Biomedicine,Article,Scopus
175,,image2emmet: Automatic code generation from web user interface image,"Xu Y., Bo L., Sun X., Li B., Jiang J., Zhou W.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108958370&doi=10.1002%2fsmr.2369&partnerID=40&md5=41f2e947e4885c908ee6b5bfeb3a21e1,10.1002/smr.2369,"Web development usually follows with analyzing the functionality, designing the user interface (UI) prototype, implementing the UI by front-end (FE) developers and implementing the REpresentational State Transfer (RESTful) application programming interface (API) by back-end (BE) programmers. Unfortunately, web development is a tedious, cumbersome, and time-consuming task, which makes it a challenge for the FE programmers to work in an efficient way. In this paper, we propose an approach, image2emmet, to assist FE programmers in implementing the UI. First, we collect HyperText Markup Language, Cascading Style Sheets (HTML-CSS) dataset in an automatic and efficient way. The HTML-CSS dataset used for model training consists of HTML-CSS code and its display images. Second, the faster region-based convolutional neural network (CNN) (R-CNN) is utilized to detect the UI component. Finally, we build a model combining CNN and long short-term memory (LSTM) to transform the UI component into the HTML-CSS code. The empirical study demonstrates that image2emmet can achieve a precision of 80% on the UI component detection and 60% on the transformation of UI component into HTML-CSS code. © 2021 John Wiley & Sons, Ltd.",code generation; HTML-CSS code; UI component; web development,,,Journal of Software: Evolution and Process,Article,Scopus
176,,Convolutional neural network analysis of recurrence plots for high resolution melting classification,"Ozkok F.O., Celik M.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107696114&doi=10.1016%2fj.cmpb.2021.106139&partnerID=40&md5=a126755768ed2befe729e508517d6e59,10.1016/j.cmpb.2021.106139,"Background and Objective: High resolution melting (HRM) analysis is a rapid and correct method for identification of species, such as, microorganism, bacteria, yeast, virus, etc. HRM data are produced using real-time polymerase chain reaction (PCR) and unique for each species. Analysis of the HRM data is important for several applications, such as, for detection of diseases (e.g., influenza, zika virus, SARS-Cov-2 and Covid-19 diseases) in health, for identification of spoiled foods in food industry, for analysis of crime scene evidence in forensic investigation, etc. However, the characteristics of the HRM data can change due to the experimental conditions or instrumental settings. In addition, it becomes laborious and time-consuming process as the number of samples increases. Because of these reasons, the analysis and classification of the HRM data become challenging for species which have similar characteristics. Methods: To improve the classification accuracy of HRM data, we propose to use image (visual) representation of HRM data, which we call HRM images, that are generated using recurrence plots, and propose convolutional neural network (CNN) based models for classifying HRM images. In this study, two different types of recurrence plots are generated, which are black-white recurrence plots (BW-RP) and gray scale recurrence plots (GS-RP) and four different CNN models are proposed for classifying HRM data. Results: The classification performance of the proposed methods are evaluated based on average classification accuracy and F1 score, specificity, recall, and precision values for each yeast species. When BW-RP representation of HRM data is used as input to the CNN models, the best classification accuracy of 95.2% is obtained. The classification accuracies of CNN models for melting curve and GS-RP data representations of HRM data are 90.13% and 86.13%, respectively. The classification accuracy of support vector machines (SVM) model that take melting curve representation of HRM data is 86.53%. Moreover, when BW-RP representation of HRM data is used as input to the CNN models, the F1 score, specificity, recall and precision values are the highest for almost all of species. Conclusions: Experimental results show that using BW-RP representation of HRM data improved the classification accuracy of HRM data and CNN models that take these images as input outperformed CNN models that take melting curve and GS-RP representations of HRM data as inputs and SVM model that take melting curve representation of HRM data as input. © 2021 Elsevier B.V.",Classification; Convolutional neural network; Deep learning; High resolution melting; HRM analysis; Recurrence plot,,,Computer Methods and Programs in Biomedicine,Article,Scopus
177,,Synthetic microbleeds generation for classifier training without ground truth,"Momeni S., Fazlollahi A., Yates P., Rowe C., Gao Y., Liew A.W.-C., Salvado O.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106973314&doi=10.1016%2fj.cmpb.2021.106127&partnerID=40&md5=6a28c19a1290240e2479406a06bdb927,10.1016/j.cmpb.2021.106127,"Background and Objective: : Cerebral microbleeds (CMB) are important biomarkers of cerebrovascular diseases and cognitive dysfunctions. Susceptibility weighted imaging (SWI) is a common MRI sequence where CMB appear as small hypointense blobs. The prevalence of CMB in the population and in each scan is low, resulting in tedious and time-consuming visual assessment. Automated detection methods would be of value but are challenged by the CMB low prevalence, the presence of mimics such as blood vessels, and the difficulty to obtain sufficient ground truth for training and testing. In this paper, synthetic CMB (sCMB) generation using an analytical model is proposed for training and testing machine learning methods. The main aim is creating perfect synthetic ground truth as similar as reals, in high number, with a high diversity of shape, volume, intensity, and location to improve training of supervised methods. Method: : sCMB were modelled with a random Gaussian shape and added to healthy brain locations. We compared training on our synthetic data to standard augmentation techniques. We performed a validation experiment using sCMB and report result for whole brain detection using a 10-fold cross validation design with an ensemble of 10 neural networks. Results: : Performance was close to state of the art (~9 false positives per scan), when random forest was trained on synthetic only and tested on real lesion. Other experiments showed that top detection performance could be achieved when training on synthetic CMB only. Our dataset is made available, including a version with 37,000 synthetic lesions, that could be used for benchmarking and training. Conclusion: : Our proposed synthetic microbleeds model is a powerful data augmentation approach for CMB classification with and should be considered for training automated lesion detection system from MRI SWI. © 2021 Elsevier B.V.",Data augmentation; Gaussian modeling; Microbleeds detection; Neural network; Synthetic data generation,,,Computer Methods and Programs in Biomedicine,Article,Scopus
178,,Sub-band target alignment common spatial pattern in brain-computer interface,"Zhang X., She Q., Chen Y., Kong W., Mei C.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106940695&doi=10.1016%2fj.cmpb.2021.106150&partnerID=40&md5=9a65737b9f9682d973fab33cd0b99540,10.1016/j.cmpb.2021.106150,"Background and objective: In the brain computer interface (BCI) field, using sub-band common spatial pattern (SBCSP) and filter bank common spatial pattern (FBCSP) can improve the accuracy of classification by selection a specific frequency band. However, in the cross-subject classification, due to the individual differences between different subjects, the performance is limited. Methods: This paper introduces the idea of transfer learning and presents the sub-band target alignment common spatial pattern (SBTACSP) method and applies it to the cross-subject classification of motor imagery (MI) EEG signals. First, the EEG signals are bandpass-filtered into multiple frequency bands (sub-band filtering). Subsequently, the source domain trails are aligned into the target domain space in each frequency band. The CSP algorithm is then employed to extract features among which more representative features are selected by the minimum redundancy maximum relevance (mRMR) approach from each sub-band. Then the features of all sub-bands are fused. Finally, conventional linear discriminant analysis (LDA) algorithm is used for MI classification. Results: Our method is evaluated on Datasets Ⅱa and Ⅱb of the BCI Competition Ⅳ. Compared with six state-of-the-art algorithms, the proposed SBTACSP method performed relatively the best and achieved a mean classification accuracy of 75.15% and 66.85% in cross-subject classification of Datasets Ⅱa and Ⅱb respectively. Conclusion: Therefore, the combination of sub-band filtering and transfer learning achieves superior classification performance compared to either one. The proposed algorithms will greatly promote the practical application of MI based BCIs. © 2021",Brain-computer interface; Cross-subject classification; Sub-band filtering; Target alignment; Transfer learning,,,Computer Methods and Programs in Biomedicine,Article,Scopus
179,,Esophageal cancer detection based on classification of gastrointestinal CT images using improved Faster RCNN,"Chen K.-B., Xuan Y., Lin A.-J., Guo S.-H.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106883229&doi=10.1016%2fj.cmpb.2021.106172&partnerID=40&md5=15eafcf4c096ff9ed3a9de7bf84b3f98,10.1016/j.cmpb.2021.106172,"Purpose: Esophageal cancer is a common malignant tumor in life, which seriously affects human health. In order to reduce the work intensity of doctors and improve detection accuracy, we proposed esophageal cancer detection using deep learning. The characteristics of deep learning: association and structure, activity and experience, essence and variation, migration and application, value and evaluation. Method: The improved Faster RCNN esophageal cancer detection in this paper introduces the online hard example mining (OHEM) mechanism into the system, and the experiment used 1520 gastrointestinal CT images from 421 patients. Then, we compare the overall performance of Inception-v2, Faster RCNN, and improved Faster RCNN through F-1 measure, mean average precision (mAP), and detection time. Results: The experiment shows that the overall performance of the improved Faster RCNN is higher than the other two networks. The F-1 measure of our method reaches 95.71%, the mAP reaches 92.15%, and the detection time per CT is only 5.3s. Conclusion: Through comparative analysis on the esophageal cancer image data set, the experimental results show that the introduction of online hard example mining mechanism in the Faster RCNN algorithm can improve the detection accuracy to a certain extent. © 2021",Convolutional neural network; CT detection; Esophageal cancer; Faster RCNN; Online hard example mining,,,Computer Methods and Programs in Biomedicine,Article,Scopus
180,,Projection-Based cascaded U-Net model for MR image reconstruction,"Aghabiglou A., Eksioglu E.M.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106663612&doi=10.1016%2fj.cmpb.2021.106151&partnerID=40&md5=6645f6d4f2525cd27f313bf52ce2d54d,10.1016/j.cmpb.2021.106151,"Background and Objective: Background and Objective: Recent studies in deep learning reveal that the U-Net stands out among the diverse set of deep models as an effective network structure, especially for imaging inverse problems. Initially, the U-Net model was developed to solve segmentation problems for biomedical images while using an annotated dataset. In this paper, we will study a novel application of the U-Net structure for the important inverse problem of MRI reconstruction. Deep networks are particularly efficient for the speed-up of the MR image reconstruction process by decreasing the data acquisition time, and they can significantly reduce the aliasing artifacts caused by the undersampling in the k-space. Our aim is to develop a novel and efficient cascaded U-Net framework for reconstructing MR images from undersampled k-space data. The new framework should have improved reconstruction performance when compared to competing methodologies. Methods: In this paper, a novel cascaded framework utilizing the U-Net as a sub-block is being proposed. The introduced U-Net cascade structure is applied to the magnetic resonance image reconstruction problem. The connection between the cascaded U-Nets is realized in the form of a recently developed projection-based updated data consistency layer. The novel structure is implemented in the PyTorch environment, which is one of the standards for deep learning implementations. The recently created fastMRI dataset which forms an important benchmark for MRI reconstruction is used for training and testing purposes. Results: We present simulation results comparing the novel method with a variety of competitive deep networks. The new cascaded U-Net structures PSNR performance stands on average 1.28 dB higher than the baseline U-Net. The improvement, when compared to the standard CNN, is on average 3.32 dB. Conclusions: The proposed cascaded U-Net configuration results in an improved reconstruction performance when compared to the CNN, the cascaded CNN, and also the singular U-Net structures, where the singular U-Net forms the baseline reconstruction method from the fastMRI package. The use of the projection-based updated data consistency layer also leads to improved quantitative (including SSIM, PSNR, and NMSE results) and qualitative results when compared to the use of the conventional data consistency layer. © 2021 Elsevier B.V.",Cascaded networks; Deep learning; Image reconstruction; Magnetic resonance imaging; U-Net; Updated data consistency,,,Computer Methods and Programs in Biomedicine,Article,Scopus
181,,Ajalon: Simplifying the authoring of wearable cognitive assistants,"Pham T.A., Wang J., Iyengar R., Xiao Y., Pillai P., Klatzky R., Satyanarayanan M.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105944402&doi=10.1002%2fspe.2987&partnerID=40&md5=c42d875322fae6431b17e6a1e76d16b5,10.1002/spe.2987,"Wearable Cognitive Assistance (WCA) amplifies human cognition in real time through a wearable device and low-latency wireless access to edge computing infrastructure. It is inspired by, and broadens, the metaphor of GPS navigation tools that provide real-time step-by-step guidance, with prompt error detection and correction. WCA applications are likely to be transformative in education, health care, industrial troubleshooting, manufacturing, assisted driving, and sports training. Today, WCA application development is difficult and slow, requiring skills in areas such as machine learning and computer vision that are not widespread among software developers. This paper describes Ajalon, an authoring toolchain for WCA applications that reduces the skill and effort needed at each step of the development pipeline. Our evaluation shows that Ajalon significantly reduces the effort needed to create new WCA applications. © 2021 The Authors. Software: Practice and Experience published by John Wiley & Sons Ltd.",artificial intelligence; augmented reality; cloudlets; computer vision; edge computing; Gabriel; machine learning; mobile computing; software productivity; wearables,"1773, 1797",,Software - Practice and Experience,Article,Scopus
182,,CSSG: A cost-sensitive stacked generalization approach for software defect prediction,"Eivazpour Z., Keyvanpour M.R.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100543298&doi=10.1002%2fstvr.1761&partnerID=40&md5=a0b375bf7a6d26c1bbd613bbf5046f8d,10.1002/stvr.1761,"The prediction of software artifacts on defect-prone (DP) or non-defect-prone (NDP) classes during the testing phase helps minimize software business costs, which is a classification task in software defect prediction (SDP) field. Machine learning methods are helpful for the task, although they face the challenge of data imbalance distribution. The challenge leads to serious misclassification of artifacts, which will disrupt the predictor's performance. The previously developed stacking ensemble methods do not consider the cost issue to handle the class imbalance problem (CIP) over the training dataset in the SDP field. To bridge this research gap, in the cost-sensitive stacked generalization (CSSG) approach, we try to combine the staking ensemble learning method with cost-sensitive learning (CSL) since the CSL purpose is to reduce misclassification costs. In the cost-sensitive stacked generalization (CSSG) approach, logistic regression (LR) and extremely randomized trees classifiers in cases of CSL and cost-insensitive are used as a final classifier of stacking scheme. To evaluate the performance of CSSG, we use six performance measures. Several experiments are carried out to compare the CSSG with some cost-sensitive ensemble methods on 15 benchmark datasets with different imbalance levels. The results indicate that the CSSG can be an effective solution to the CIP than other compared methods. © 2021 John Wiley & Sons, Ltd.",class imbalance learning; cost of misclassification; cost-sensitive learning; data imbalance; ensemble learning; software defect prediction,,,Software Testing Verification and Reliability,Article,Scopus
183,,Combining formal and machine learning techniques for the generation of JML specifications,"Puccetti A., De Chalendar G., Gibello P.-Y.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113218961&doi=10.1145%2f3464971.3468425&partnerID=40&md5=d233d895c08afc45ad0c9966e3e04af6,10.1145/3464971.3468425,"Producing maintainable programs is a big challenge for the software industry as it requires solid Engineering skills and efficient CASE tools. Often, industrial programs are of a very large size (more than 1M SLOC), use high-level programming languages to their full extent (e.g. C++20, Ada 2005 or Java 16), are provided with scarce and often outdated documentation partially written in natural language. Maintenance engineers are therefore in need to understand the application at hand starting from the material left behind by the developers. The European H2020 Project DECODER (https://www.decoder-project.eu) addresses this problem by proposing to combine Natural Language Processing techniques and Formal Methods to turn as best as possible code artifacts into formal data allowing to reduce the maintenance costs and thus the total costs of ownership. In this context, we will show how to generate JML annotations using a combination of 1) automatic generation of minimal predicates, 2) Natural Language Processing (NLP) based predicates generator, and 3) manual refinement and correction, to instrument and enhance code and documentation. We will illustrate it on code samples from the MyThaiStar (https://github.com/devonfw/my-thai-star) application developed with the CASE tool devonfw by CAP GEMINI, and the Joram JMS implementation (https://gitlab.ow2.org/joram/joram) from OW2 code base. © 2021 ACM.",JML; Machine Learning; NER; OpenJML; SRL,"59, 64",,"FTfJP 2021 - Proceedings of the 23rd ACM International Workshop on Formal Techniques for Java-Like Programs, co-located with ECOOP/ISSTA 2021",Conference Paper,Scopus
184,,Semantic matching of GUI events for test reuse: Are we there yet?,"Mariani L., Mohebbi A., Pezzè M., Terragni V.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111437484&doi=10.1145%2f3460319.3464827&partnerID=40&md5=1debacaeafac81a3213965dced64a35c,10.1145/3460319.3464827,"GUI testing is an important but expensive activity. Recently, research on test reuse approaches for Android applications produced interesting results. Test reuse approaches automatically migrate human-designed GUI tests from a source app to a target app that shares similar functionalities. They achieve this by exploiting semantic similarity among textual information of GUI widgets. Semantic matching of GUI events plays a crucial role in these approaches. In this paper, we present the first empirical study on semantic matching of GUI events. Our study involves 253 configurations of the semantic matching, 337 unique queries, and 8,099 distinct GUI events. We report several key findings that indicate how to improve semantic matching of test reuse approaches, propose SemFinder a novel semantic matching algorithm that outperforms existing solutions, and identify several interesting research directions. © 2021 ACM.",Android applications; GUI testing; Mobile testing; NLP; Test reuse; Word embedding,"177, 190",,ISSTA 2021 - Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis,Conference Paper,Scopus
185,,Semantic table structure identification in spreadsheets,"Zhang Y., Lv X., Dong H., Dou W., Han S., Zhang D., Wei J., Ye D.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111437476&doi=10.1145%2f3460319.3464812&partnerID=40&md5=c745f274f3cbd2cef83be3fe2e52b7f4,10.1145/3460319.3464812,"Spreadsheets are widely used in various business tasks, and contain amounts of valuable data. However, spreadsheet tables are usually organized in a semi-structured way, and contain complicated semantic structures, e.g., header types and relations among headers. Lack of documented semantic table structures, existing data analysis and error detection tools can hardly understand spreadsheet tables. Therefore, identifying semantic table structures in spreadsheet tables is of great importance, and can greatly promote various analysis tasks on spreadsheets. In this paper, we propose Tasi (Table structure identification) to automatically identify semantic table structures in spreadsheets. Based on the contents, styles, and spatial locations in table headers, Tasi adopts a multi-classifier to predict potential header types and relations, and then integrates all header types and relations into consistent semantic table structures. We further propose TasiError, to detect spreadsheet errors based on the identified semantic table structures by Tasi. Our experiments on real-world spreadsheets show that, Tasi can precisely identify semantic table structures in spreadsheets, and TasiError can detect real-world spreadsheet errors with higher precision (75.2%) and recall (82.9%) than existing approaches. © 2021 ACM.",Error detection; Spreadsheet; Table structure,"283, 295",,ISSTA 2021 - Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis,Conference Paper,Scopus
186,,ModelDiff: Testing-based DNN similarity comparison for model reuse detection,"Li Y., Zhang Z., Liu B., Yang Z., Liu Y.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111423805&doi=10.1145%2f3460319.3464816&partnerID=40&md5=02c194761d90bc8f910932fb7575f3e7,10.1145/3460319.3464816,"The knowledge of a deep learning model may be transferred to a student model, leading to intellectual property infringement or vulnerability propagation. Detecting such knowledge reuse is nontrivial because the suspect models may not be white-box accessible and/or may serve different tasks. In this paper, we propose ModelDiff, a testing-based approach to deep learning model similarity comparison. Instead of directly comparing the weights, activations, or outputs of two models, we compare their behavioral patterns on the same set of test inputs. Specifically, the behavioral pattern of a model is represented as a decision distance vector (DDV), in which each element is the distance between the model's reactions to a pair of inputs. The knowledge similarity between two models is measured with the cosine similarity between their DDVs. To evaluate ModelDiff, we created a benchmark that contains 144 pairs of models that cover most popular model reuse methods, including transfer learning, model compression, and model stealing. Our method achieved 91.7% correctness on the benchmark, which demonstrates the effectiveness of using ModelDiff for model reuse detection. A study on mobile deep learning apps has shown the feasibility of ModelDiff on real-world models. © 2021 ACM.",Deep neural networks; Intellectual property; Model reuse; Similarity comparison; Vulnerability propagation,"139, 151",,ISSTA 2021 - Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis,Conference Paper,Scopus
187,,DialTest: Automated testing for recurrent-neural-network-driven dialogue systems,"Liu Z., Feng Y., Chen Z.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111417967&doi=10.1145%2f3460319.3464829&partnerID=40&md5=6eb0db1c719938e6ccd4612becee3055,10.1145/3460319.3464829,"With the tremendous advancement of recurrent neural networks(RNN), dialogue systems have achieved significant development. Many RNN-driven dialogue systems, such as Siri, Google Home, and Alexa, have been deployed to assist various tasks. However, accompanying this outstanding performance, RNN-driven dialogue systems, which are essentially a kind of software, could also produce erroneous behaviors and result in massive losses. Meanwhile, the complexity and intractability of RNN models that power the dialogue systems make their testing challenging. In this paper, we design and implement DialTest, the first RNN-driven dialogue system testing tool. DialTest employs a series of transformation operators to make realistic changes on seed data while preserving their oracle information properly. To improve the efficiency of detecting faults, DialTest further adopts Gini impurity to guide the test generation process. We conduct extensive experiments to validate DialTest. We first experiment it on two fundamental tasks, i.e., intent detection and slot filling, of natural language understanding. The experiment results show that DialTest can effectively detect hundreds of erroneous behaviors for different RNN-driven natural language understanding (NLU) modules of dialogue systems and improve their accuracy via retraining with the generated data. Further, we conduct a case study on an industrial dialogue system to investigate the performance of DialTest under the real usage scenario. The study shows DialTest can detect errors and improve the robustness of RNN-driven dialogue systems effectively. © 2021 ACM.",Automated testing; Deep learning testing; Dialog system testing,"115, 126",,ISSTA 2021 - Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis,Conference Paper,Scopus
188,,DeepCrime: Mutation testing of deep learning systems based on real faults,"Humbatova N., Jahangirova G., Tonella P.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111417676&doi=10.1145%2f3460319.3464825&partnerID=40&md5=1de18f24b5cac320948cee467ae2fd23,10.1145/3460319.3464825,"Deep Learning (DL) solutions are increasingly adopted, but how to test them remains a major open research problem. Existing and new testing techniques have been proposed for and adapted to DL systems, including mutation testing. However, no approach has investigated the possibility to simulate the effects of real DL faults by means of mutation operators. We have defined 35 DL mutation operators relying on 3 empirical studies about real faults in DL systems. We followed a systematic process to extract the mutation operators from the existing fault taxonomies, with a formal phase of conflict resolution in case of disagreement. We have implemented 24 of these DL mutation operators into DeepCrime, the first source-level pre-training mutation tool based on real DL faults. We have assessed our mutation operators to understand their characteristics: whether they produce interesting, i.e., killable but not trivial, mutations. Then, we have compared the sensitivity of our tool to the changes in the quality of test data with that of DeepMutation++, an existing post-training DL mutation tool. © 2021 ACM.",Deep learning; Mutation testing; Real faults,"67, 78",,ISSTA 2021 - Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis,Conference Paper,Scopus
189,,CGAN-IRB: A novel data augmentation method for apple leaf diseases,"Yuan X., Yu C., Liu B., Sun H., Zhu X.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115886351&doi=10.1109%2fCOMPSAC51774.2021.00037&partnerID=40&md5=0fa008ccc6771ccff8e7ab171f7f95bb,10.1109/COMPSAC51774.2021.00037,"At present, the identification of apple leaf diseases plays an important role in controlling apple leaf diseases and improving apple yield. CNNs(Convolutional Neural Networks) have been widely used in apple leaf diseases identification, but the training of the CNNs requires a large number of images. The lack of images would make the CNNs hard to generalize. Thus the CNNs are unable to recognize new disease images. Focusing on this problem, this paper proposes a new model named CGAN-IRB(Conditional Generative Adversarial Network with the Improved Residual Block) for data augmentation. Firstly, various improvements have been made based on CGAN to generate high-quality, robust, and specific-category images of apple leaf diseases. Among which the embedding of the residual block has been found to significantly improve the model performance. Then the interpolation algorithm is used instead of deconvolution to increase the image size. Finally, the TTUR(Two-Timescale Update Rule) training strategy is employed and all the convolutional layers of the network are spectrally normalized to stabilize the training of the network. The performance of CGAN-IRB was tested both on image generation and classification tasks. Experiment results show that the images generated by the network possess high quality and robust features, providing a novel solution for the data augmentation of apple leaf diseases. The new GAN-based data augmentation method leads to significant improvements in the classification accuracy of CNNs. In the case of all tested CNNs, the classification accuracy improvements are 11.75% and 2.17% on average over non-augmented and traditional-augmented, respectively. Among them, the classification accuracy of GoogLeNet V2 and ShuffleNet V2 is 99.34% and 99.67%, respectively. The data augmentation approach proposed in this paper can be used more widely in the field of disease identification, solving the problem of insufficient data sets, and can be extended to related fields where data sets are difficult to obtain. © 2021 IEEE.",Apple leaf disease identification; Convolutional neural networks; Data augmentation; Generative adversarial networks,"192, 200",,"Proceedings - 2021 IEEE 45th Annual Computers, Software, and Applications Conference, COMPSAC 2021",Conference Paper,Scopus
190,,Distributed big data computing for supporting predictive analytics of service requests,"Wang T., Harvey J.D., Leung C.K., Pazdor A.G.M., Chauhan A.S., Fan L., Cuzzocrea A.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115885151&doi=10.1109%2fCOMPSAC51774.2021.00257&partnerID=40&md5=9b0f807b0c75c21853dd3abb305cd80f,10.1109/COMPSAC51774.2021.00257,"In the current era of big data, huge volumes of valuable data can be easily generated and collected at a rapid velocity from a wide variety of rich data sources. In recent years, the initiates of open data also led to the willingness of many government, researchers, and organizations to share their data and make them publicly accessible. An example of open big data is service request data. Analyzing these open big data can be for social good. For instance, by analyzing and mining data on non-emergency city service requests, the city could get an insight about its residents’ demand for services. By taking appropriate actions (e.g., adding more staff and/or services, providing more information regarding city services) could enhance the living condition of city residents. In this paper, we present a distributed big data mining system to analyze and mine big data on these non-emergency city service requests. Evaluation on an open big data from a North American city shows the effectiveness and practicality of our distributed big data system in mining these requests for city services and in supporting predictive analytics. © 2021 IEEE.",311; Big data; Data management; Data mining; Distributed data; Location-based recommender system (LBRS); Non-emergency municipal services; Open data; Service requests,"1723, 1728",,"Proceedings - 2021 IEEE 45th Annual Computers, Software, and Applications Conference, COMPSAC 2021",Conference Paper,Scopus
191,,Breast mass detection and classification using deep convolutional neural networks for radiologist diagnosis assistance,"Mahmood T., Li J., Pei Y., Akhtar F., Jia Y., Khand Z.H.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115883236&doi=10.1109%2fCOMPSAC51774.2021.00291&partnerID=40&md5=4abb399b6427261746c42333a3781cc2,10.1109/COMPSAC51774.2021.00291,"Several developments in computational image processing methods assist the radiologist in detecting abnormal breast tissue in recent years. Consequently, deep learning-based models have become crucial for early screening and interpretation of mammographic images for breast masses diagnosis, helping for successful treatment. Breast masses and calcification is an essential parameter for the prognosis of breast cancer. However, the mammographic image’s mass detection needs a deeper investigation due to the breast masses’ heterogeneity and anomalies’ characteristics that are easily confused with other objects present in the image. Hence, this study proposed a deep learning-based convolutional neural network (ConvNet) that will incorporate both mammography and clinical variables to predict and classify breast masses to assist the expert’s decision-making processes. We trained our proposed model with 322 scanned digital mammographic images of the MIAS (Mammogram Image Analysis Society) dataset and 580 images of the private dataset to evaluate the performance, which is highly imbalanced. This study aimed to perform an automatic and comprehensive characterization of breast masses using appropriate layers deep ConvNet model with high accuracy true-positive rate, decreased error rate and applying data-augmentation techniques. We obtained a classification accuracy of 97% applying the filtered deep features, which is the best performance from the existing approaches. © 2021 IEEE.",Breast masses classification; Computer aid diagnosis; Data-augmentation; Deep convolutional neural network; Image classification,"1918, 1923",,"Proceedings - 2021 IEEE 45th Annual Computers, Software, and Applications Conference, COMPSAC 2021",Conference Paper,Scopus
192,,Visual gefect getection of metal screws using a geep convolutional neural network,"Sauter D., Atik C., Schenk C., Buettner R., Baumgartl H.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115879021&doi=10.1109%2fCOMPSAC51774.2021.00050&partnerID=40&md5=53c3ae131acae68af4bedcd2820259df,10.1109/COMPSAC51774.2021.00050,"In the production of screws, manual methods are often still used to detect defects. This paper aims to use a convolutional neural network-based technique to detect whether defects in screws are caused during production. Our experimental results show that a detection accuracy of 96.67% can be achieved with the proposed technique. Among the defects considered are defects on the objects' surface (e.g., scratches, dents), structural defects like distorted object parts, or defects that manifest themselves by the absence of certain object parts. Our more efficient method can be used in the future for quality control in the manufacture of screws. © 2021 IEEE.",Convolutional neural network; Deep learning; Defect detection; Metal screw,"303, 311",,"Proceedings - 2021 IEEE 45th Annual Computers, Software, and Applications Conference, COMPSAC 2021",Conference Paper,Scopus
193,,Learning to match workers and tasks via a multi-view graph attention network,"Cui N., Chen C., Shen B., Chen Y.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115876376&doi=10.1109%2fCOMPSAC51774.2021.00035&partnerID=40&md5=998a352ef33f3cb9ab97b81a5978a539,10.1109/COMPSAC51774.2021.00035,"The worker-task matching problem brings up unique characteristics that are not present in traditional matching scenarios, i.e., the huge flow of tasks with short lifespans, the importance of workers’ capabilities, and the quality of the completed tasks. These characteristics further pose significant challenges of data sparsity and comprehensive modeling. To address the two challenges, this paper proposes MvkGAN, a multi-view attention network on a bi-collaborative knowledge graph (BicKG). The core ideas of our work are 1) building BicKG from the data of workers, tasks, their interactions, and domain knowledge, and then leveraging it to reveal the latent interactions between workers and tasks to mitigate the data sparsity challenge; and 2) designing a multi-view knowledge graph attention network (MvkGAN) which learns to match workers and tasks, to meet the comprehensive modeling challenge. In this network, different features are organized as multiple views and these views are further connected by the attention mechanism. We have implemented MvkGAN and evaluated it against five state-of-the-art approaches (Wide&Deep, DeepFM, KGAT, CrowdRex and PJFNN) on two real-world datasets. The evaluation results show that MvkGAN improves the accuracy by 6.90% and F1-score by 5.52% on average, and also has the ability of generating reasonable explanations. © 2021 IEEE.",Graph neural network; Knowledge graph; Multi-view learning; Worker-task matching,"176, 185",,"Proceedings - 2021 IEEE 45th Annual Computers, Software, and Applications Conference, COMPSAC 2021",Conference Paper,Scopus
194,,Defect detection of metal nuts applying convolutional neural networks,"Sauter D., Schmitz A., Dikici F., Baumgartl H., Buettner R.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115870437&doi=10.1109%2fCOMPSAC51774.2021.00043&partnerID=40&md5=6fa24fbda6d4b43a5cdc3efe4b5aafa8,10.1109/COMPSAC51774.2021.00043,"Since the human inspection of small metal parts is complex, time-consuming and prone to human error, a convolutional neural network for the detection of defects on metal nuts was developed in order to grant fast and robust quality controls. For this approach, we built an image classification algorithm based on the Xception architecture. The evaluation of the trained model is robust and achieves reliable results after applying a hold-out 5-fold cross-validation. Implementing this algorithm on the MVTec Anomaly Detection dataset outperforms the existing benchmark on defect detection for metal nuts with a balanced accuracy of 90.00% and a value of 0.99 for the area under the curve. © 2021 IEEE.",CNN; Deep learning; Quality assessment; Xception,"248, 257",,"Proceedings - 2021 IEEE 45th Annual Computers, Software, and Applications Conference, COMPSAC 2021",Conference Paper,Scopus
195,,Local and global feature based explainable feature envy detection,"Yin X., Shi C., Zhao S.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115867718&doi=10.1109%2fCOMPSAC51774.2021.00127&partnerID=40&md5=c624b1f47c0e7c081426765ff66c86cd,10.1109/COMPSAC51774.2021.00127,"Code smell detection can help developers identify position of code smell in projects and enhance the quality of software system. Usually codes with similar semantic relationships have greater code dependencies, and most code smell detection methods ignore dependencies relationships within the source code. Thus, their detection results may be heavily influenced by inadequate code feature, which can lead to some code smell not being detected. In addition, existing methods cannot explain the correlation between detection results and code information. However, an explainable result can help developers make better judgments on code smell reconstruction. Accordingly, in this paper, we propose a local and global feature based explainable approach to detecting feature envy, one of the most common code smells. For the model to make the most of code information, we design different representation models for global code and local code respectively to extract different feature envy features, and automatically combine these features that are beneficial in terms of detection accuracy. We further design a code semantic dependency (CSD) to make the detection result easy to explain. The evaluation results of seven manual building code smell projects and three real projects show that the proposed approach improves on the state-of-the-art in detecting feature envy and boosting the explainability of results. © 2021 IEEE.",Deep learning; Feature envy; Software refactoring,"942, 951",,"Proceedings - 2021 IEEE 45th Annual Computers, Software, and Applications Conference, COMPSAC 2021",Conference Paper,Scopus
196,,Mixing machine learning and optimization for the tactical capacity planning in last-mile delivery,"Fadda E., Fedorov S., Perboli G., Barbosa I.D.C.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115866042&doi=10.1109%2fCOMPSAC51774.2021.00180&partnerID=40&md5=d31a31df5e8b39826bd439390ef126d1,10.1109/COMPSAC51774.2021.00180,"Tactical Capacity Planning (TCP) is becoming a crucial part of logistics in the current environment of demand-driven economics. This paper proposes an innovative approach in the TCP setting, consisting of using the collected historical data of the geographical position and the volume of the orders to plan the capacity requirements for the next day. To this end, the clustering of the city to microzones is introduced using K-means clustering. Then, four different methods (Gaussian Process regression, ARIMA model, Neural Network regression, and Long Short Term Memory network) are used to forecast the next day order volume for each of the clusters. Finally, the Variable Cost and Size Bin Packing problem solved with the predicted demand to outline the usage of a heterogeneous fleet required to serve the next time period. Through experiments on the real data, we conclude, that the proposed algorithm is satisfying the decision safety framework with completely unknown demand and could also be used for other demand forecast applications. © 2021 IEEE.",Capacity planning; Last-mile delivery; Machine learning; Variable sized and cost bin packing,"1291, 1296",,"Proceedings - 2021 IEEE 45th Annual Computers, Software, and Applications Conference, COMPSAC 2021",Conference Paper,Scopus
197,,Few shot learning of covid-19 classification based on sequential and pretrained models: A thick data approach,"Sawyer D., Fiaidhi J., Mohammed S.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115865348&doi=10.1109%2fCOMPSAC51774.2021.00276&partnerID=40&md5=e796f77a9e9a83fd84374ead8ca57fe6,10.1109/COMPSAC51774.2021.00276,"Classification tasks face several issues when applied to complex data sets and sophisticated images such as CT scans. Long training times are needed to properly train traditional networks to classify images, as well as the need for large amounts of data for these networks to draw accurate conclusions. Even when supplied with large datasets, popular neural networks like VGG and ResNet fail to classify images accurately and consistently for sensitive tasks like identifying COVID-19 in a CT lung scan. To overcome these challenges, we apply Siamese neural network architecture, which has been reported to reduce training times and required training data, to a sequential network. To further empower this network, we incorporate thick data heuristics into the CT image dataset, specifically, we annotate areas of interest in the images that a radiologist would be looking for to make a diagnosis, such as ground glass opacities. Our network outperforms five leading image classification neural networks by about 3% when classifying the same CT lung scan images as positive or negative for COVID-19. By applying data thickening heuristics, we have shown that accuracy is improved, and suspect that the accuracy will continue to increase as more heuristics based on more radiologists and imaging experts are to be added on top of what we have considered in this paper. © 2021 IEEE.",Classification; Learning from few shots; Machine learning; Siamese neural network; Thick data analytics,"1832, 1836",,"Proceedings - 2021 IEEE 45th Annual Computers, Software, and Applications Conference, COMPSAC 2021",Conference Paper,Scopus
198,,Vision-based hand gesture recognition for human-computer interaction using MobileNetV2,"Baumgartl H., Sauter D., Schenk C., Atik C., Buettner R.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115864701&doi=10.1109%2fCOMPSAC51774.2021.00249&partnerID=40&md5=6db0885d7b29c2631d05ec203b6ba159,10.1109/COMPSAC51774.2021.00249,"In recent years, the demand for gesture recognition has increased enormously due to many applications such as computer games, human-robot interaction, assistance systems, sports, sign language interpreters, and e-commerce. The recognition of hand gestures is one of the most important gesture recognition methods. With simple hand gestures, devices in the smart home area (TV, radio, vacuum cleaner robots, etc.) should be easier to operate. Our method is based on a convolutional neural network, or more precisely on MobileNetV2. With this lean and fast network, we have been able to achieve an accuracy of 99.96 percent in recognition of hand gestures, so that in the future, we will be able to offer an application in the field of HumanComputer Interaction to interact more easily with the ever-increasing number of technologies in everyday life. © 2021 IEEE.",Convolutional neural network; Hand gesture recognition; Human-computer interaction; Image classification; Mobilenet,"1667, 1674",,"Proceedings - 2021 IEEE 45th Annual Computers, Software, and Applications Conference, COMPSAC 2021",Conference Paper,Scopus
199,,Medical named entity recognition of Chinese electronic medical records based on stacked bidirectional long short-term memory,"Zhu Z., Li J., Zhao Q., Wei Y.-C., Jia Y.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115862321&doi=10.1109%2fCOMPSAC51774.2021.00293&partnerID=40&md5=4498a93ac236049f7dca4cb0ebb1d4ed,10.1109/COMPSAC51774.2021.00293,"The wide adoption of electronic medical record (EMR) systems causes rapid growth of medical and clinical data. It makes the medical named entity recognition (NER) technologies become critical to find useful patient information in the medical dataset. However, the medical terminologies usually have the characteristics of inherent complexity and ambiguity, it is difficult to capture context-dependency representations by supervision signal from a simple single layer structure model. In order to address this problem, this paper proposes a hybrid model based on stacked Bidirectional Long Short-Term Memory (BILSTM) for medical named entity recognition, which we call BSBC (BERT combined with stacked BILSTM and CRF). First, we use Bidirectional Encoder Representation from Transformers (BERT) to perform unsupervised learning on an unlabeled dataset to obtain character-level embeddings. Then, stacked BILSTM is utilized to obtain context-dependency representations through the multi hidden layers structure. Finally, Conditional Random Field (CRF) is used to predict sequence tags. The experiment results show that our method significantly outperforms the baseline methods, it serves as a strong alternative approach compared with traditional methods. © 2021 IEEE.",Bidirectional encoder representation from transformers (BERT); Electronic medical record (EMR); Named entity recognition (NER); Stacked bidirectional long short-term memory (BILSTM),"1930, 1935",,"Proceedings - 2021 IEEE 45th Annual Computers, Software, and Applications Conference, COMPSAC 2021",Conference Paper,Scopus
200,,Abnormal gait recognition based on integrated gait features in machine learning,"Kim W., Kim Y.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115861051&doi=10.1109%2fCOMPSAC51774.2021.00251&partnerID=40&md5=2021eb8c73b3a5e71e7b87a1fc291e2b,10.1109/COMPSAC51774.2021.00251,"Human gait has various movements by individuals and enables normal and abnormal gaits to be recognized by their characteristics. Gait abnormalities are the most common symptoms caused by various neurological disorders such as hemiparetic, myopathic, sciatic neuralgia, and Parkinson’s disease. Abnormal gaits from functional disorders have different types of characteristics. Based on the characteristics, we propose a gait abnormality recognition method that uses integrated gait features extracted from the individual’s walking movement using the Kinect depth camera. Diagnosing a detailed pathological level of the disorders requires a high knowledge of the physiological and pathological gait characteristics. Furthermore, it commonly requires a much complex environment and wearable sensors. However, using our proposed method, the initial characteristics of the gait disorders can be detected without the requirements. Thus, we build a k-NN classifier and an SVM classifier to classify abnormal gait from a walking person. In the result of our experiment, our proposed method with the gait features shows the potentiality for classifying gait abnormalities. © 2021 IEEE.",Depth camera; Gait abnormality; Gait analysis; Gait classification; Human gait; K-NN classifier; Kinect; Machine learning; SVM classifier; Time normalization,"1683, 1688",,"Proceedings - 2021 IEEE 45th Annual Computers, Software, and Applications Conference, COMPSAC 2021",Conference Paper,Scopus
201,,Joint extraction of events in Chinese electronic medical records,"Wang J., Li J., Zhu Z., Zhao Q., Yu Y., Yang L., Xu C.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115860327&doi=10.1109%2fCOMPSAC51774.2021.00292&partnerID=40&md5=122bd7d558f26a6ee65ad3ceb688e347,10.1109/COMPSAC51774.2021.00292,"The widely deployed of hospital information systems causes an explosive growth of the electronic medical records (EMRs). It makes the medical structured processing technologies become critical to find researchable data in the large medical dataset. However, the high quality structured processing is a challenging task, in particular due to the inherent complexity and polysemy of medical terminology. In this paper, we propose a novel approach to achieve the joint extraction of events in Chinese electronic medical records, which solves the problem of cascading error transmission in traditional models and the ambiguity of Chinese characters. We first use the Bi-directional Encoder Representation from Transformers(BERT) model to mine features from the preprocessed medical data; then based on the characteristics of Chinese, we use the Bi-directional Long Short-Term Memory(BILSTM) model to capture the semantic information of the context. The experiments were conducted on a real dataset. The F1 score of our model in the identification and classification tasks of event triggers and arguments is the highest, reaching 71.6, 68.1, 55.4 and 46.9, respectively, which proves the effectiveness of the proposed method. © 2021 IEEE.",BERT; BILSTM; Chinese electronic medical records; Event extraction,"1924, 1929",,"Proceedings - 2021 IEEE 45th Annual Computers, Software, and Applications Conference, COMPSAC 2021",Conference Paper,Scopus
202,,Multi-relational EHR representation learning with infusing information of diagnosis and medication,"Shi Y., Guo Y., Wu H., Li J., Li X.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115859013&doi=10.1109%2fCOMPSAC51774.2021.00241&partnerID=40&md5=bb5d93a459cde2e06a2ae3d246186232,10.1109/COMPSAC51774.2021.00241,"Medical concept embedding which aims at learning interpretable low-dimensional representations of medical codes has become one of the key technologies to enable the machine (deep) learning models to imitate the doctor’s cognitive reasoning process in a variety of clinical tasks. Most existing works focus on leveraging the medical ontology to get the representations but remains ineffective in dealing with 1) the inconsistency between the knowledge of the medical ontology and the observations in health records, and 2) the deficiency of discovering the relations among multi-types of medical concepts. To address these challenges, this paper proposes MrER(Multi-relational EHR representation learning method). It’s a heterogeneous graph convolutional network with a self-adaptive adjacency matrix, to infer the multi-relations among different types of medical concepts and align them in the same subspace for the complex knowledge inference. Moreover, an temporal convolutional network is introduced to capture the dependency patterns in the sequence of medical records. The entire framework is trained in an end-to-end fashion. The experimental results show that MrER achieves competitive performance advantages in sequential diagnosis prediction task in comparison with state-of-the-art methods and the learned embeddings have good interpretability regarding the relationship between medical codes. © 2021 IEEE.",Diagnosis prediction; Electronic health record; Medical concept embedding; Representation learning,"1617, 1622",,"Proceedings - 2021 IEEE 45th Annual Computers, Software, and Applications Conference, COMPSAC 2021",Conference Paper,Scopus
203,,A transfer learning approach to surface detection for accessible routing for wheelchair users,"Mokrenko V., Yu H., Raychoudhury V., Edinger J., Smith R.O., Gani M.O.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115854935&doi=10.1109%2fCOMPSAC51774.2021.00112&partnerID=40&md5=9634fdf4a84e2f1ba71369ab17247101,10.1109/COMPSAC51774.2021.00112,"The nature of the surface has a significant effect on how wheelchair users experience locomotion. The preferred surfaces for wheeled mobility must be even, firm and smooth while generating adequate friction. The development of accessible road maps that include ground conditions is therefore of utmost importance. Our prior work has shown how such maps can be created using surface-induced vibration data collected by motion sensors embedded in smartphones and then classifying them with machine learning algorithms. To make data collection scalable, participatory crowd-sensing can be used, where users collect and transmit sensor data while traveling on wheelchairs. The complexity here is that wheelchairs widely vary in type (manual, power-assist, power), weight, number and nature of wheels, therefore the sensor data generated by different wheelchairs varies greatly. Collecting training data on each individual wheelchair type to develop classification models is not feasible. To address this problem, in this paper we explore the possibility of transferring knowledge from known wheelchairs to unknown types. We develop a transfer learning algorithm to classify 15 surfaces with minimal training data from different wheelchairs. Our experiments with 47 subjects show that surface classification knowledge, learned from sensor data generated by manual wheelchairs, can be transferred to a power wheelchair with up to 90.02% accuracy. This allows crowd-sensing to be used effectively for data collection for generating accessible route maps. We integrate our transfer learning approach into our system for accessible routing, which we developed in previous work. © 2021 IEEE.",Accessible routing; Manual wheelchair; Power wheelchair; Surface classification; Transfer learning,"794, 803",,"Proceedings - 2021 IEEE 45th Annual Computers, Software, and Applications Conference, COMPSAC 2021",Conference Paper,Scopus
204,,Exploiting multi-aspect interactions for god class detection with dataset fine-tuning,"Ren S., Shi C., Zhao S.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115854216&doi=10.1109%2fCOMPSAC51774.2021.00119&partnerID=40&md5=65b630e6166df9d93f73f7de821291d7,10.1109/COMPSAC51774.2021.00119,"God class refers to a class that undertakes too many responsibilities for tasks that should more appropriately be handled by multiple classes. The existence of god classes seriously affects the maintainability and understandability of software. To eliminate god class, we first need to identify them. Researchers have proposed traditional methods using code metrics and deep learning methods using code metrics and text information to detect god classes. However, the relationship existing in metrics and text information is often ignored; moreover, deep learning methods require a large number of reliable datasets, while authentic god class datasets are scarce. To solve the above problems, we propose a novel god class detection method based on multi-aspect interactions and dataset fine-tuning. First, we use proposed model to extract multi-aspect interaction information, including three parts: (i) the interaction information existing in code metrics; (ii) the interaction information existing in texts; (iii) the interaction information existing in texts and code metrics. In this way, we can not only make use of code metrics and text information, but also fully exploit the multi-aspect interaction information. Second, we train with large-scale synthetic datasets to obtain a pre-trained model, then fine-tune the pre-trained model parameters with high-quality authentic datasets. Using the training method of pre-training and fine-tuning, we can solve the problem of low-reliability synthetic datasets and scarce authentic datasets. Finally, evaluation results on open-source applications suggest that the proposed approach improves on the state-of-the-art. © 2021 IEEE.",Code smells; Feature interactions; Fine-tuning; God class; Pre-training,"864, 873",,"Proceedings - 2021 IEEE 45th Annual Computers, Software, and Applications Conference, COMPSAC 2021",Conference Paper,Scopus
205,,Prediction of covid-19 from chest X-ray images using multiresolution texture classification with robust local features,"Oraibi Z.A., Albasri S.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115846941&doi=10.1109%2fCOMPSAC51774.2021.00096&partnerID=40&md5=3a00f030d02592167d301f6da4de305b,10.1109/COMPSAC51774.2021.00096,"The COVID-19 contagious disease that spread around the world, have a huge risk on people and already caused millions of deaths forcing a global pandemic in 2020. Diagnosing patients with this disease is very critical allowing fast care response and to isolate them from public. As the virus spread widely to millions of people, the fastest way to detect it is by analyzing radiology images. Early studies showed irregularity in the chest X-ray images of patients with high clinical belief of COVID-19 infection. Hence, these studies motivated us to investigate the use of machine learning techniques to help diagnosing COVID-19 patients from chest CT scans. In this paper, we propose to use a robust feature extraction descriptor and to apply a Random Forests classifier to predict COVID-19 disease in a dataset of 5000 images. First, 408 texture features are extracted using a powerful variation of Local Binary Patterns descriptor called Rotation Invariant Co-occurrence among Local Binary Patterns. Then, Random Forests classifier is applied with 250 trees to perform the classification task. Moreover, the performance of our approach was improved by using a multiresolution scheme where features are extracted from both the original input image and the subsampled image. Two metrics were used to evaluate our approach, sensitivity and specificity. We achieved 99.0% and 91.3% for both metrics, respectively. Our results are close to the state-of-the-art deep learning methods on the same dataset. © 2021 IEEE.",COVID-19; Random forests; Texture features; X-ray imaging,"663, 668",,"Proceedings - 2021 IEEE 45th Annual Computers, Software, and Applications Conference, COMPSAC 2021",Conference Paper,Scopus
206,,From teaching books to educational videos and vice versa: A cross-media content retrieval experience,"Canale L., Farinetti L., Cagliero L.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115846020&doi=10.1109%2fCOMPSAC51774.2021.00027&partnerID=40&md5=b3c9488808c311463649b849c49dc151,10.1109/COMPSAC51774.2021.00027,"Due to the rapid growth of multimedia data and the diffusion of remote and mixed learning, teaching sessions are becoming more and more multi-modal. To deepen the knowledge of specific topics, learners can be interested in retrieving educational videos that complement the textual content of teaching books. However, retrieving educational videos can be particularly challenging when there is a lack of metadata information. To tackle the aforesaid issue, this paper explores the joint use of Deep Learning and Natural Language Processing techniques to retrieve cross-media educational resources (i.e., from text snippets to videos and vice versa). It applies NLP techniques to both the audio transcript of the videos and to the text snippets in the books in order to quantify the semantic relationships between pairs of educational resources of different media types. Then, it trains a Deep Learning model on top of the NLP-based features. The probabilities returned by the Deep Learning model are used to rank the candidate resources based on their relevance to a given query. The results achieved on a real collection of educational multimodal data show that the proposed approach performs better than state-of-the-art solutions. Furthermore, a preliminary attempt to apply the same approach to address a similar retrieval task (i.e., from text to image and vice versa) has shown promising results. © 2021 IEEE.",Cross-media retrieval; Deep learning; Educational data mining; Learning analytics,"115, 120",,"Proceedings - 2021 IEEE 45th Annual Computers, Software, and Applications Conference, COMPSAC 2021",Conference Paper,Scopus
207,,Dew intelligence: Federated learning perspective,"Guberović E., Lipić T., Čavrak I.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115843928&doi=10.1109%2fCOMPSAC51774.2021.00274&partnerID=40&md5=61066c23a224bce4bc2b2ceb968c17cf,10.1109/COMPSAC51774.2021.00274,"Newly emerging and evolving technologies such as Cloud, Fog and Edge Computing, as well as Internet of Things, Cyber-Physical Systems and Distributed Ledger Technology (such as blockchain) together with advances in Artificial Intelligence (AI) research are increasingly becoming a common and pervasive phenomenon in our everyday lives. Their co-evolution with society is driving the emergence of future socio-technical systems, which further promote ubiquitous entanglement between humans and machines. Fog, Edge and Dew computing as post-Cloud computing paradigms aim to relocate computing resources closer to end users in order to mitigate cloud-specific issues of highly centralized computation. Dew computing as the youngest of the post-cloud paradigms promotes human centered independence and collaboration between devices within scalable distributed computing infrastructures. Meanwhile, the field of artificial intelligence is adapting to recent challenges posed by user data privacy regulations as well as opportunities for applications on mobile devices based on their growing computational abilities. The usage of artificial intelligence in pervasive and scalable distributed computing systems is a natural step towards ubiquitous intelligent infrastructures and collaborative human and machine environments. Federated learning is an artificial intelligence technique enabling collaborative learning in distributed devices environment without sharing the training data sets, which are often private. This paper provides the overview of the federated learning paradigm showing that it inherently leverages both independence and collaboration, thus exemplifying implementation of dew intelligence within scalable distributed computing hierarchy. © 2021 IEEE.",Collaborative learning; Dew computing; Distributed computing; Federated learning; Machine learning,"1819, 1824",,"Proceedings - 2021 IEEE 45th Annual Computers, Software, and Applications Conference, COMPSAC 2021",Conference Paper,Scopus
208,,Auto-grading OCT images diagnostic tool for retinal diseases,"Tian S., Charfi N., Tumpa J.F., Mudiam N., Medic V., Kim J.E., Ahamed S.I.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115840073&doi=10.1109%2fCOMPSAC51774.2021.00090&partnerID=40&md5=91ad639079fc07ef7c6a0cff4d68fc37,10.1109/COMPSAC51774.2021.00090,"Retinal eye disease is the most common reason for visual deterioration. Long-term management and follow-up are critical to detect the changes in symptoms. Optical Coherence Tomography (OCT) is a non-invasive diagnostic tool for diagnosing and managing various retinal eye diseases. With the increasing desire for OCT image, the clinicians are suffering from the burden of time on the diagnostic and the treatment. In this study, an auto-grading diagnostic tool is proposed to divide the OCT image for the retinal disease classification. In this tool, the classification model implements convolutional neural networks (CNNs), and the model training is based on denoised OCT images. The tool can detect the uploaded OCT image and automatically generate a result of classification in the categories of Choroidal neovascularization (CNV), Diabetic macular edema (DME), multiple drusen, and Normal. The system will definitely improve the performance of retinal eye disease diagnosis and alleviate the burden on the medical system. © 2021 IEEE.",Choroidal neovascularization (CNV); Diabetic macular edema (DME); Drusen; Optical coherence tomography (OCT),"618, 625",,"Proceedings - 2021 IEEE 45th Annual Computers, Software, and Applications Conference, COMPSAC 2021",Conference Paper,Scopus
209,,ROCT: Radius-based class overlap cleaning technique to alleviate the class overlap problem in software defect prediction,"Feng S., Keung J., Liu J., Xiao Y., Yu X., Zhang M.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115839204&doi=10.1109%2fCOMPSAC51774.2021.00041&partnerID=40&md5=7d21014c16173cf76df5c6c6a416f9bf,10.1109/COMPSAC51774.2021.00041,"The training data commonly used in software defect prediction (SDP) usually contains some instances that have similar values on features but are in different classes, which significantly degrades the performance of prediction models trained using these instances. This is referred to as the class overlap problem (COP). Previous studies have concluded that COP has a more negative impact on the performance of prediction models than the class imbalance problem (CIP). However, less research has been conducted on COP than CIP. Moreover, the performance of the existing class overlap cleaning techniques heavily relies on the settings of hyperparameters such as the value of K in the K-nearest neighbor algorithm or the K-means algorithm, but how to find those optimal hyperparameters is still a challenge. In this study, we propose a novel technique named the radius-based class overlap cleaning technique (ROCT) to better alleviate COP without tuning hyperparameters in SDP. The basic idea of ROCT is to take each instance as the center of a hypersphere and directly optimize the radius of the hypersphere. Then ROCT identifies those instances with the opposite label of the center instance as the overlapping instance and removes them. To investigate the performance of ROCT, we conduct the empirical experiment across 29 datasets collected from various software repositories on the K-nearest neighbor, random forest, logistic regression, and naive Bayes classifiers measured by AUC, balance, pd, and pf. The experimental results show that ROCT performs the best and significantly improves the performance of prediction models by as much as 15.2% and 29.9% in terms of AUC and balance compared with the existing class overlap cleaning techniques. The superior performance of ROCT indicates that ROCT should be recommended as an efficient alternative to alleviate COP in SDP. © 2021 IEEE.",Class imbalance; Class overlap; Data preprocessing; Software defect prediction,"228, 237",,"Proceedings - 2021 IEEE 45th Annual Computers, Software, and Applications Conference, COMPSAC 2021",Conference Paper,Scopus
210,,A fast training method using bounded continual learning in image classification,"Jang S., Kim Y.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115827380&doi=10.1109%2fCOMPSAC51774.2021.00036&partnerID=40&md5=876afa9627cf36bc74319482c105afa2,10.1109/COMPSAC51774.2021.00036,"These days, Deep neural networks (DNNs) are showing good performance in image classification. They bring sufficient performance not only in specific fields such as medical images and meteorological observation images, but also in fields necessary for daily life. However, for them to be more useful practically, they should be able to add new tasks to suit a changing environment. This is the same as saying that they should be able to learn by adding new data. Unfortunately, since catastrophic forgetting is an inevitable feature of connectionist model, it is very difficult to capture both accuracy and computational efficiency in continuous task learning. In this paper, we propose Bounded Continual Learning (BCL) based on inductive transfer learning. BCL extracts feature values from sub-models created by separating base datasets and train a new classifier. BCL showed very good performance in training time efficiency to learn tasks in a sequential. We demonstrate our approach is flexible and efficient by various classification tasks based on the CIFAR dataset. © 2021 IEEE.",Continual learning; Deep neural networks; Image classification; Lifelong learning; Transfer learning,"186, 191",,"Proceedings - 2021 IEEE 45th Annual Computers, Software, and Applications Conference, COMPSAC 2021",Conference Paper,Scopus
211,,MLNer: Exploiting multi-source lexicon information fusion for named entity recognition in Chinese medical text,"Xiao Y., Zhao Q., Li J., Chen J., Cheng Z.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115824802&doi=10.1109%2fCOMPSAC51774.2021.00147&partnerID=40&md5=8b5d0a60164e86a7330778deca020b72,10.1109/COMPSAC51774.2021.00147,"The integration of lexicon information into character-based models is a hot topic in Chinese Named Entity Recognition(NER) research. Most methods only utilize information from a single lexicon which is usually a general lexicon. However, In the Chinese medical text scenario, due to the large amount of medical terminology, a single lexicon, especially a general lexicon, offers little performance improvement to the Chinese NER. In this paper, we propose a Multi-source Lexicon Information Fusion method for Named Entity Recognition in Chinese Medical Text(MLNER) which can utilize information from both general and medical lexicons. Considering the small medical annotated corpus, we combine the model with the pre-trained model to improve the performance of the model on small datasets by exploiting the rich representation capability of the pre-trained model. Experiments show that our method can effectively improve the performance of NER in Chinese medical text. Our model is also applicable to Chinese NER tasks in other domain specific fields, with good scalability and application value. © 2021 IEEE.",Deep learning; Information extraction; Lexicon-based NER; Medical text; Pre-trained model,"1079, 1084",,"Proceedings - 2021 IEEE 45th Annual Computers, Software, and Applications Conference, COMPSAC 2021",Conference Paper,Scopus
212,,Screening of viral pneumonia and COVID-19 in chest X-ray using classical machine learning,"Fonseca A.U., Vieira G.S., Soares F.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115821537&doi=10.1109%2fCOMPSAC51774.2021.00294&partnerID=40&md5=4ab729c5bc40f23bc2b9f284def98983,10.1109/COMPSAC51774.2021.00294,"Governments, civil society, health professionals, and scientists have been facing a relentless fight against the pandemic of the COVID-19 disease; however, there are already about 150 million people infected worldwide and more than 3 million lives claimed, and numbers keep rising. One of the ways to combat this disease is the effective screening of infected patients. However, COVID-19 provides a similar pattern with diseases, such as pneumonia, and can misguide even very well-trained physicians. In this sense, a chest X-ray (CXR) is an effective alternative due to its low cost, accessibility, and quick response. Thus, inspired by research on the use of CXR for the diagnosis of COVID-19 pneumonia, we investigate classical machine learning methods to assist in this task. The main goal of this work is to present a robust, lightweight, and fast technique for the automatic detection of COVID-19 from CXR images. We extracted radiomic features from CXR images and trained classical machine learning models for two different classification schemes: i) COVID-19 pneumonia vs. Normal ii) COVID-19 vs. Normal vs. Viral pneumonia. Several evaluation metrics were used and comparison with many studies is presented. Our experimental results are equivalent to the state-of-the-art for both classification schemes. The solution’s high performance makes it a viable option as a computer-aided diagnostic tool, which can represent a significant gain in the speed and accuracy of the COVID-19 diagnosis. © 2021 IEEE.",Chest radiography; Classification; COVID-19; Feature extraction; Machine learning; Viral pneumonia,"1936, 1941",,"Proceedings - 2021 IEEE 45th Annual Computers, Software, and Applications Conference, COMPSAC 2021",Conference Paper,Scopus
213,,redBERT: A topic discovery and deep sentiment classification model on COVID-19 online discussions using BERT NLP model,Pandey C.,2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113173748&doi=10.4018%2fIJOSSP.2021070103&partnerID=40&md5=72ff260dc92b9a69945cbe590586ac0d,10.4018/IJOSSP.2021070103,"A natural language processing (NLP) method was used to uncover various issues and sentiments surrounding COVID-19 from social media and get a deeper understanding of fluctuating public opinion in situations of wide-scale panic to guide improved decision making with the help of a sentiment analyser created for the automated extraction of COVID-19-related discussions based on topic modelling. Moreover, the BERT model was used for the sentiment classification of COVID-19 Reddit comments. These findings shed light on the importance of studying trends and using computational techniques to assess the human psyche in times of distress. Copyright © 2021, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited.",COVID-19; Deep Learning; Natural Language Processing; Sentiment Analysis; Topic Modelling,"33, 47",,International Journal of Open Source Software and Processes,Article,Scopus
214,,Monitoring social distancing using artificial intelligence for fighting COVID-19 virus spread,"Alyami H., Alosaimi W., Krichen M., Alroobaea R.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113152310&doi=10.4018%2fIJOSSP.2021070104&partnerID=40&md5=24ed46ddbc20902ab8e20aaa9a662a7b,10.4018/IJOSSP.2021070104,"To restrict COVID-19, individuals must remain two meters away from one another in public since public health authorities find this a healthy distance. In this way, the incidence of “social distancing” keeps pace with COVID-19 spread. For this purpose, the proposed solution consists of the development of a tool based on AI technologies which takes as input videos (in real time) from streets and public spaces and gives as output the places where social distancing is not respected. Detected persons who are not respecting social distancing are surrounded with red rectangles and those who respect social distancing with green rectangles. The solution has been tested for the case of videos from the two Holy Mosques in Saudi Arabia: Makkah and Madinah. As a novel contribution compared to existent approaches in the literature, the solution allows the detection of the age, class, and sex of persons not respecting social distancing. Person detection is performed using the Faster RCNN with ResNet-50 as it is the backbone network that is pre-trained with the open source COCO dataset. The obtained results are satisfactory and may be improved by considering more sophisticated cameras, material, and techniques. Copyright © 2021, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited.",Artificial Intelligence; Coronavirus; COVID-19; Holy Mosques; Madinah; Makkah; Open Source; Sex and Age Identification; Social Distancing,"48, 63",,International Journal of Open Source Software and Processes,Article,Scopus
215,,How Far Have We Progressed in Identifying Self-admitted Technical Debts? A Comprehensive Empirical Study,"Guo Z., Liu S., Liu J., Li Y., Chen L., Lu H., Zhou Y.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112052871&doi=10.1145%2f3447247&partnerID=40&md5=64e9a5d0a33bf2cdbda8593b4b2d2ac4,10.1145/3447247,"Background. Self-admitted technical debt (SATD) is a special kind of technical debt that is intentionally introduced and remarked by code comments. Those technical debts reduce the quality of software and increase the cost of subsequent software maintenance. Therefore, it is necessary to find out and resolve these debts in time. Recently, many automatic approaches have been proposed to identify SATD. Problem. Popular IDEs support a number of predefined task annotation tags for indicating SATD in comments, which have been used in many projects. However, such clear prior knowledge is neglected by existing SATD identification approaches when identifying SATD. Objective. We aim to investigate how far we have really progressed in the field of SATD identification by comparing existing approaches with a simple approach that leverages the predefined task tags to identify SATD. Method. We first propose a simple heuristic approach that fuzzily Matches task Annotation Tags (MAT) in comments to identify SATD. In nature, MAT is an unsupervised approach, which does not need any data to train a prediction model and has a good understandability. Then, we examine the real progress in SATD identification by comparing MAT against existing approaches. Result. The experimental results reveal that: (1) MAT has a similar or even superior performance for SATD identification compared with existing approaches, regardless of whether non-effort-aware or effort-aware evaluation indicators are considered; (2) the SATDs (or non-SATDs) correctly identified by existing approaches are highly overlapped with those identified by MAT; and (3) supervised approaches misclassify many SATDs marked with task tags as non-SATDs, which can be easily corrected by their combinations with MAT. Conclusion. It appears that the problem of SATD identification has been (unintentionally) complicated by our community, i.e., the real progress in SATD comments identification is not being achieved as it might have been envisaged. We hence suggest that, when many task tags are used in the comments of a target project, future SATD identification studies should use MAT as an easy-to-implement baseline to demonstrate the usefulness of any newly proposed approach. © 2021 ACM.",baseline; code comment; match; Self-admitted technical debt; task annotation tag,,,ACM Transactions on Software Engineering and Methodology,Article,Scopus
216,,Deep Transfer Bug Localization,"Huo X., Thung F., Li M., Lo D., Shi S.-T.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110618055&doi=10.1109%2fTSE.2019.2920771&partnerID=40&md5=4b6f176bc265b6382c2e3aa695f79af7,10.1109/TSE.2019.2920771,"Many projects often receive more bug reports than what they can handle. To help debug and close bug reports, a number of bug localization techniques have been proposed. These techniques analyze a bug report and return a ranked list of potentially buggy source code files. Recent development on bug localization has resulted in the construction of effective supervised approaches that use historical data of manually localized bugs to boost performance. Unfortunately, as highlighted by Zimmermann et al., sufficient bug data is often unavailable for many projects and companies. This raises the need for cross-project bug localization - the use of data from a project to help locate bugs in another project. To fill this need, we propose a deep transfer learning approach for cross-project bug localization. Our proposed approach named TRANP-CNN extracts transferable semantic features from source project and fully exploits labeled data from target project for effective cross-project bug localization. We have evaluated TRANP-CNN on curated high-quality bug datasets and our experimental results show that TRANP-CNN can locate buggy files correctly at top 1, top 5, and top 10 positions for 29.9, 51.7, 61.3 percent of the bugs respectively, which significantly outperform state-of-the-art bug localization solution based on deep learning and several other advanced alternative solutions considering various standard evaluation metrics. © 1976-2012 IEEE.",Cross-project bug localization; deep learning; transfer learning,"1368, 1380",,IEEE Transactions on Software Engineering,Article,Scopus
217,,Feature Representation Method for Heterogeneous Defect Prediction Based on Variational Autoencoders [基于变分自编码器的异构缺陷预测特征表示方法],"Jia X.-Y., Zhang W.-Z., Li W.-W., Huang Z.-Q.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109032009&doi=10.13328%2fj.cnki.jos.006257&partnerID=40&md5=a2e72fc1138035215d3f1d30e9417f5a,10.13328/j.cnki.jos.006257,"Cross-project defect prediction technology can use the existing labeled defect data to predict new unlabeled data, but it needs to have the same metric features for two projects, which is difficult to be applied in actual development. Heterogeneous defect prediction can perform prediction without requiring the source and target project to have the same set of metrics and thus has attracted great interest. Existing heterogeneous defect prediction models use naive or traditional machine learning methods to learn feature representations between source and target projects, and perform prediction based on it. The feature representation learned by previous studies is weak, causing poor performance in predicting defect-prone instances. In view of the powerful feature extraction and representation capabilities of deep neural networks, this study proposes a feature representation method for heterogeneous defect prediction based on variational autoencoders. By combining the variational autoencoder and maximum mean discrepancy, this method can effectively learn the common feature representation of the source and target projects. Then, an effective defect prediction model can be trained based on it. The validity of the proposed method is verified by comparing it with traditional cross-project defect prediction methods and heterogeneous defect prediction methods on various datasets. © Copyright 2021, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Feature representation; Heterogeneous defect prediction; Variational autoencoders,"2204, 2218",,Ruan Jian Xue Bao/Journal of Software,Article,Scopus
218,,Automated psoriasis lesion segmentation from unconstrained environment using residual U-Net with transfer learning,"Raj R., Londhe N.D., Sonawane R.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108302946&doi=10.1016%2fj.cmpb.2021.106123&partnerID=40&md5=65e622795cafa153434b93e98aa4f102,10.1016/j.cmpb.2021.106123,"Background and objective: The automatic segmentation of psoriasis lesions from digital images is a challenging task due to the unconstrained imaging environment and non-uniform background. Existing conventional or machine learning-based image processing methods for automatic psoriasis lesion segmentation have several limitations, such as dependency on manual features, human intervention, less and unreliable performance with an increase in data, manual pre-processing steps for removal of background or other artifacts, etc. Methods: In this paper, we propose a fully automatic approach based on a deep learning model using the transfer learning paradigm for the segmentation of psoriasis lesions from the digital images of different body regions of the psoriasis patients. The proposed model is based on U-Net architecture whose encoder path utilizes a pre-trained residual network model as a backbone. The proposed model is retrained with a self-prepared psoriasis dataset and corresponding segmentation annotation of the lesion. Results: The performance of the proposed method is evaluated using a five-fold cross-validation technique. The proposed method achieves an average Dice Similarity Index of 0.948 and Jaccard Index of 0.901 for the intended task. The transfer learning provides an improvement in the segmentation performance of about 4.4% and 7.6% in Dice Similarity Index and Jaccard Index metric respectively, as compared to the training of the proposed model from scratch. Conclusions: An extensive comparative analysis with the state-of-the-art segmentation models and existing literature validates the promising performance of the proposed framework. Hence, our proposed method will provide a basis for an objective area assessment of psoriasis lesions. © 2021 Elsevier B.V.",Deep learning; Image segmentation; Psoriasis; Residual network; Transfer learning; U-Net model,,,Computer Methods and Programs in Biomedicine,Article,Scopus
219,,Assessment of off-the-shelf SE-specific sentiment analysis tools: An extended replication study,"Novielli N., Calefato F., Lanubile F., Serebrenik A.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107186199&doi=10.1007%2fs10664-021-09960-w&partnerID=40&md5=9a780cf03057cbb0c13649b2ed0f2a2b,10.1007/s10664-021-09960-w,"Sentiment analysis methods have become popular for investigating human communication, including discussions related to software projects. Since general-purpose sentiment analysis tools do not fit well with the information exchanged by software developers, new tools, specific for software engineering (SE), have been developed. We investigate to what extent off-the-shelf SE-specific tools for sentiment analysis mitigate the threats to conclusion validity of empirical studies in software engineering, highlighted by previous research. First, we replicate two studies addressing the role of sentiment in security discussions on GitHub and in question-writing on Stack Overflow. Then, we extend the previous studies by assessing to what extent the tools agree with each other and with the manual annotation on a gold standard of 600 documents. We find that different SE-specific sentiment analysis tools might lead to contradictory results at a fine-grain level, when used off-the-shelf. Conversely, platform-specific tuning or retraining might be needed to take into account differences in platform conventions, jargon, or document lengths. © 2021, The Author(s).",Human aspects of software engineering; Replication study; Sentiment analysis,,,Empirical Software Engineering,Article,Scopus
220,,Diabetic retinopathy detection through convolutional neural networks with synaptic metaplasticity,"Vives-Boix V., Ruiz-Fernández D.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106311342&doi=10.1016%2fj.cmpb.2021.106094&partnerID=40&md5=2f68bdfe8152e5145c8772727117a945,10.1016/j.cmpb.2021.106094,"Background and objectives: Diabetic retinopathy is a type of diabetes that causes vascular changes that can lead to blindness. The ravages of this disease cannot be reversed, so early detection is essential. This work presents an automated method for early detection of this disease using fundus colored images. Methods: A bio-inspired approach is proposed on synaptic metaplasticity in convolutional neural networks. This biological phenomenon is known to directly interfere in both learning and memory by reinforcing less common occurrences during the learning process. Synaptic metaplasticity has been included in the backpropagation stage of a convolution operation for every convolutional layer. Results: The proposed method has been evaluated by using a public small diabetic retinopathy dataset from Kaggle with four award-winning convolutional neural network architectures. Results show that convolutional neural network architectures including synaptic metaplasticity improve both learning rate and accuracy. Furthermore, obtained results outperform other methods in current literature, even using smaller datasets for training. Best results have been obtained for the InceptionV3 architecture with synaptic metaplasticity with a 95.56% accuracy, 94.24% F1-score, 98.9% precision and 90% recall, using 3662 images for training. Conclusions: Convolutional neural networks with synaptic metaplasticity are suitable for early detection of diabetic retinopathy due to their fast convergence rate, training simplicity and high performance. © 2021 Elsevier B.V.",Convolutional neural networks; Deep learning; Diabetic retinopathy; Image processing; Metaplasticity,,,Computer Methods and Programs in Biomedicine,Article,Scopus
221,,Data augmentation using generative adversarial neural networks on brain structural connectivity in multiple sclerosis,"Barile B., Marzullo A., Stamile C., Durand-Dubief F., Sappey-Marinier D.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106300678&doi=10.1016%2fj.cmpb.2021.106113&partnerID=40&md5=6f2621781bdb8ec3edb742cece11efd2,10.1016/j.cmpb.2021.106113,"Background and objective:Machine learning frameworks have demonstrated their potentials in dealing with complex data structures, achieving remarkable results in many areas, including brain imaging. However, a large collection of data is needed to train these models. This is particularly challenging in the biomedical domain since, due to acquisition accessibility, costs and pathology related variability, available datasets are limited and usually imbalanced. To overcome this challenge, generative models can be used to generate new data. Methods: In this study, a framework based on generative adversarial network is proposed to create synthetic structural brain networks in Multiple Sclerosis (MS). The dataset consists of 29 relapsing-remitting and 19 secondary-progressive MS patients. T1 and diffusion tensor imaging (DTI) acquisitions were used to obtain the structural brain network for each subject. Evaluation of the quality of newly generated brain networks is performed by (i) analysing their structural properties and (ii) studying their impact on classification performance. Results: We demonstrate that advanced generative models could be directly applied to the structural brain networks. We quantitatively and qualitatively show that newly generated data do not present significant differences compared to the real ones. In addition, augmenting the existing dataset with generated samples leads to an improvement of the classification performance (F1score 81%) with respect to the baseline approach (F1score 66%). Conclusions: Our approach defines a new tool for biomedical application when connectome-based data augmentation is needed, providing a valid alternative to usual image-based data augmentation techniques. © 2021 Elsevier B.V.",Brain connectivity; Data augmentation; Generative adversarial networks; Multiple sclerosis,,,Computer Methods and Programs in Biomedicine,Article,Scopus
222,,Automatic colonic polyp detection using integration of modified deep residual convolutional neural network and ensemble learning approaches,"Liew W.S., Tang T.B., Lin C.-H., Lu C.-K.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105593805&doi=10.1016%2fj.cmpb.2021.106114&partnerID=40&md5=9c0357281484fc0b8733c207d0ba55df,10.1016/j.cmpb.2021.106114,"Background and Objective: The increased incidence of colorectal cancer (CRC) and its mortality rate have attracted interest in the use of artificial intelligence (AI) based computer-aided diagnosis (CAD) tools to detect polyps at an early stage. Although these CAD tools have thus far achieved a good accuracy level to detect polyps, they still have room to improve further (e.g. sensitivity). Therefore, a new CAD tool is developed in this study to detect colonic polyps accurately. Methods: In this paper, we propose a novel approach to distinguish colonic polyps by integrating several techniques, including a modified deep residual network, principal component analysis and AdaBoost ensemble learning. A powerful deep residual network architecture, ResNet-50, was investigated to reduce the computational time by altering its architecture. To keep the interference to a minimum, median filter, image thresholding, contrast enhancement, and normalisation techniques were exploited on the endoscopic images to train the classification model. Three publicly available datasets, i.e., Kvasir, ETIS-LaribPolypDB, and CVC-ClinicDB, were merged to train the model, which included images with and without polyps. Results: The proposed approach trained with a combination of three datasets achieved Matthews Correlation Coefficient (MCC) of 0.9819 with accuracy, sensitivity, precision, and specificity of 99.10%, 98.82%, 99.37%, and 99.38%, respectively. Conclusions: These results show that our method could repeatedly classify endoscopic images automatically and could be used to effectively develop computer-aided diagnostic tools for early CRC detection. © 2021",AdaBoost ensemble learning; colorectal cancer (CRC); deep residual network; polyps; principal component analysis,,,Computer Methods and Programs in Biomedicine,Article,Scopus
223,,Generating API tags for tutorial fragments from Stack Overflow,"Wu D., Jing X.-Y., Zhang H., Li B., Xie Y., Xu B.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105563512&doi=10.1007%2fs10664-021-09962-8&partnerID=40&md5=005a4bb2667bd998edf626e04506d4a4,10.1007/s10664-021-09962-8,"API tutorials are important learning resources as they explain how to use certain APIs in a given programming context. An API tutorial can be split into a number of units. Consecutive units that describe a same topic are often called a tutorial fragment. We consider the API explained by a tutorial fragment as an API tag. Generating API tags for a tutorial fragment can help understand, navigate, and retrieve the fragment. Existing approaches often do not perform well on API tag generation due to high manual effort and low accuracy. Like API tutorials, Stack Overflow (SO) is also an important learning resource that provides the explanations of APIs. Thus, SO posts also contain API tags. Besides, API tags of SO posts are abundant and can be extracted easily. In this paper, we propose a novel approach ATTACK (stands for A PI T ag for T utorial frA gments using C rowd K nowledge), which can automatically generate API tags for tutorial fragments from SO posts. ATTACK first constructs 〈Q&Apair,tagset〉 pairs by extracting API tags of SO posts. Then, it trains a deep neural network with the attention mechanism to learn the semantic relatedness between Q&A pairs and the associated API tags, taking into consideration both textual descriptions and code in a Q&A pair. Finally, the trained model is used to generate API tags for tutorial fragments. We evaluate ATTACK on public Java and Android datasets containing 43,132 〈Q&Apair,tagset〉 pairs. Experimental results show that ATTACK is effective and outperforms the state-of-the-art approaches in terms of F-Measure. Our user study further confirms the effectiveness of ATTACK in generating API tags for tutorial fragments. We also apply ATTACK to document linking and the results confirm the usefulness of API tags generated by ATTACK. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",API tags; Document linking; Stack Overflow; Tutorial fragments,,,Empirical Software Engineering,Article,Scopus
224,,GloBug: Using global data in Fault Localization,"Miryeganeh N., Hashtroudi S., Hemmati H.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103941334&doi=10.1016%2fj.jss.2021.110961&partnerID=40&md5=26f37892938906153b3c76c8d9914907,10.1016/j.jss.2021.110961,"Fault Localization (FL) is an important first step in software debugging and is mostly manual in the current practice. Many methods have been proposed over years to automate the FL process, including information retrieval (IR)-based techniques. These methods localize the fault based on the similarity of the reported bug report and the source code. Newer variations of IR-based FL (IRFL) techniques also look into the history of bug reports and leverage them during the localization. However, all existing IRFL techniques limit themselves to the current project's data (local data). In this study, we introduce Globug, which is an IRFL framework consisting of methods that use models pre-trained on the global data (extracted from open-source benchmark projects). In Globug, we investigate two heuristics: (a) the effect of global data on a state-of-the-art IR-FL technique, namely BugLocator, and (b) the application of a Word Embedding technique (Doc2Vec) together with global data. Our large scale experiment on 51 software projects shows that using global data improves BugLocator on average 6.6% and 4.8% in terms of MRR (Mean Reciprocal Rank) and MAP (Mean Average Precision), with over 14% in a majority (64% and 54% in terms of MRR and MAP, respectively) of the cases. This amount of improvement is significant compared to the improvement rates that five other state-of-the-art IRFL tools provide over BugLocator. In addition, training the models globally is a one-time offline task with no overhead on BugLocator's run-time fault localization. Our study, however, shows that a Word Embedding-based global solution did not further improve the results. © 2021 Elsevier Inc.",Automated Fault Localization; Doc2Vec; Global training; Information Retrieval; TF.IDF; Word Embedding,,,Journal of Systems and Software,Article,Scopus
225,,Augmenting commit classification by using fine-grained source code changes and a pre-trained deep neural language model,"Ghadhab L., Jenhani I., Mkaouer M.W., Ben Messaoud M.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102442891&doi=10.1016%2fj.infsof.2021.106566&partnerID=40&md5=e0dd5cce934cdfe03723ba84c01460a0,10.1016/j.infsof.2021.106566,"Context: Analyzing software maintenance activities is very helpful in ensuring cost-effective evolution and development activities. The categorization of commits into maintenance tasks supports practitioners in making decisions about resource allocation and managing technical debt. Objective: In this paper, we propose to use a pre-trained language neural model, namely BERT (Bidirectional Encoder Representations from Transformers) for the classification of commits into three categories of maintenance tasks — corrective, perfective and adaptive. The proposed commit classification approach will help the classifier better understand the context of each word in the commit message. Methods: We built a balanced dataset of 1793 labeled commits that we collected from publicly available datasets. We used several popular code change distillers to extract fine-grained code changes that we have incorporated into our dataset as additional features to BERT's word representation features. In our study, a deep neural network (DNN) classifier has been used as an additional layer to fine-tune the BERT model on the task of commit classification. Several models have been evaluated to come up with a deep analysis of the impact of code changes on the classification performance of each commit category. Results and conclusions: Experimental results have shown that the DNN model trained on BERT's word representations and Fixminer code changes (DNN@BERT+Fix_cc) provided the best performance and achieved 79.66% accuracy and a macro-average f1 score of 0.8. Comparison with the state-of-the-art model that combines keywords and code changes (RF@KW+CD_cc) has shown that our model achieved approximately 8% improvement in accuracy. Results have also shown that a DNN model using only BERT's word representation features achieved an improvement of 5% in accuracy compared to the RF@KW+CD_cc model. © 2021",Code changes; Commit classification; Deep neural networks; Pre-trained neural language model; Software maintenance,,,Information and Software Technology,Article,Scopus
226,,Impact of Discretization Noise of the Dependent Variable on Machine Learning Classifiers in Software Engineering,"Rajbahadur G.K., Wang S., Kamei Y., Hassan A.E.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068161118&doi=10.1109%2fTSE.2019.2924371&partnerID=40&md5=6551e4baa851818fd5a2050058bd36f0,10.1109/TSE.2019.2924371,"Researchers usually discretize a continuous dependent variable into two target classes by introducing an artificial discretization threshold (e.g., median). However, such discretization may introduce noise (i.e., discretization noise) due to ambiguous class loyalty of data points that are close to the artificial threshold. Previous studies do not provide a clear directive on the impact of discretization noise on the classifiers and how to handle such noise. In this paper, we propose a framework to help researchers and practitioners systematically estimate the impact of discretization noise on classifiers in terms of its impact on various performance measures and the interpretation of classifiers. Through a case study of 7 software engineering datasets, we find that: 1) discretization noise affects the different performance measures of a classifier differently for different datasets; 2) Though the interpretation of the classifiers are impacted by the discretization noise on the whole, the top 3 most important features are not affected by the discretization noise. Therefore, we suggest that practitioners and researchers use our framework to understand the impact of discretization noise on the performance of their built classifiers and estimate the exact amount of discretization noise to be discarded from the dataset to avoid the negative impact of such noise. © 1976-2012 IEEE.",classifiers; decision trees; discretization; Discretization noise; feature importance analysis; KNN; logistic regression; performance; random forest,"1414, 1430",,IEEE Transactions on Software Engineering,Article,Scopus
227,,SEPBO: Trash separator bot VR game,"Theethum T., Iamcharoen S., Arpornrat A., Vittayakorn S.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112390495&doi=10.1109%2fJCSSE53117.2021.9493830&partnerID=40&md5=3ecac356f00a7b7330a6c7d28ab0a111,10.1109/JCSSE53117.2021.9493830,"Everyday hundreds of thousands of tons of waste goes to landfills, although more than half of it could be either recycled or composted. The waste management situation tends to get worse when there are many poorly-designed landfill sites that could either leak hazardous chemicals which can contaminate groundwater or emit harmful gases into the atmosphere. These health hazards or environmental chain problems could be diminished if everyone sorted their waste precisely. To mitigate this problems, we aim to tackle it from the beginning. Thus, in this work, we propose a computer-based VR game called SEPBO. SEPBO is an educational game which aims not only for enjoyment, but also to improve the players' waste sorting skills. The experimental results confirm that SEPBO is a better tool to improve the players' waste sorting skill compared to the interactive web-based baseline, ReCollect: The Waste Sorting Game, in ALL aspects: with 6.67% higher performance in improving players' waste sorting skill and a 48% greater degree of satisfaction than the baseline in terms of enjoyment. © 2021 IEEE.",Gamification; Oculus rift; Virtual Reality game; Waste sorting,,,JCSSE 2021 - 18th International Joint Conference on Computer Science and Software Engineering: Cybernetics for Human Beings,Conference Paper,Scopus
228,,COVID-19 Classification using DCNNs and Exploration Correlation using Canonical Correlation Analysis,"Jullapak R., Yampaka T.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112359996&doi=10.1109%2fJCSSE53117.2021.9493846&partnerID=40&md5=f86ea8e03e7cd8c1beb2186f032c38f2,10.1109/JCSSE53117.2021.9493846,"Coronavirus disease (COVID-19) has rapidly spread among people living in many countries. Chest radiography (CXR) image is an alternative diagnosis option to observe COVID-19. However, CXR usually requires an expert radiologist to distinguish the lesion from viral pneumonia and COVID-19 because the symptoms of COVID-19 pneumonia may be similar to other types of viral pneumonia. In this study, three different convolutional neural network based models (VGG19, ResNet50, and InceptionV3) have been proposed for the detection of coronavirus pneumonia infected patient using chest X-ray. In addition, this studies can potentially find the correlation between COVID-19 pneumonia and viral pneumonia using canonical correlation analysis. Considering the performance results obtained the best performance as an accuracy of 0.97, sensitivity of 0.97, specificity of 0.93, and F1-score value of 0.97 for VGG19 pre-trained model. The experiment results also show that the viral lesion of Viral pneumonia and COVID-19 is less similarity. © 2021 IEEE.",canonical correlation analysis; COVID-19 classification; deep convolution neuron networks,,,JCSSE 2021 - 18th International Joint Conference on Computer Science and Software Engineering: Cybernetics for Human Beings,Conference Paper,Scopus
229,,Detecting Facial Images in Public with and without Masks Using VGG and FR-TSVM Models,"Wang H., Lursinsap C.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112353752&doi=10.1109%2fJCSSE53117.2021.9493848&partnerID=40&md5=b34a25d932922a1bc70234c299c76622,10.1109/JCSSE53117.2021.9493848,"Since 2019, Covid-19 has become a common problem affecting all mankind. The disease has successfully spread all over the world. Wearing a mask can practically protect the infection. Thus, detecting people wearing and not wearing masks in public is essential. However, there is still some room to improve detection accuracy of the present methods. In this paper, the transfer learning model and FR-TSVM model are used to study the latest data of pneumonia epidemic situation in Covid-19. First, a data set of 12,000 facial images wearing masks and not wearing masks in public was collected for training, testing, and validation. The pictures will be put into the improved VGG model. Then the structure of VGG model was used to extract the features of images. These features were trained by FR-TSVM with fuzzy concept included. This approach can achieve 95.5% accuracy, and it is also higher than the detection results of other methods. © 2021 IEEE.",COVID-19; FR-TSVM; Public Places; TSVM; VGG-16; Wear Mask,,,JCSSE 2021 - 18th International Joint Conference on Computer Science and Software Engineering: Cybernetics for Human Beings,Conference Paper,Scopus
230,,Classification of Abusive Thai Language Content in Social Media Using Deep Learning,"Wanasukapunt R., Phimoltares S.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112351034&doi=10.1109%2fJCSSE53117.2021.9493829&partnerID=40&md5=c5980c5b8ea2120417472b4d47c26dbc,10.1109/JCSSE53117.2021.9493829,"This paper presents binomial and multinomial models for Thai language abusive speech classification in social media. While previous similar research focused on using traditional machine learning models for binomial classification, we showed that deep learning models have better performance. Our binomial and multinomial models achieved F1 scores of 0.8510 and 0.9067, respectively. These scores were significantly better than the machine learning models' respective best F1 scores of 0.7452 and 0.8090. While the bidirectional LSTM performed well, the DistilBERT had higher accuracy and recall. Moreover, the recall was especially higher for the 'figurative' class where certain words were more likely to have different meanings depending on context. © 2021 IEEE.",Abusive language detection; Large scale social networks); Thai natural language processing,,,JCSSE 2021 - 18th International Joint Conference on Computer Science and Software Engineering: Cybernetics for Human Beings,Conference Paper,Scopus
231,,A Preprocessing Method of Facial Expression Image under Different Illumination,"Hu Y., Zeng X., Huang Z., Dong X.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114520466&doi=10.1109%2fICCSN52437.2021.9463605&partnerID=40&md5=38f4fb67c5b7f1cef66a8387c7401c3e,10.1109/ICCSN52437.2021.9463605,"In this work, we propose an image processing method which combines the limited contrast adaptive histogram equalization (CLAHE) with Gamma transform to solve the illumination problem in facial expression recognition. We apply this algorithm to professional illumination datasets (Extended Yale B) and get better visual results, compared with using CLAHE and Gamma correction separately. Moreover, we use a convolution neural network (CNN) that pre-trained on FER2013 datasets to evaluate the effect of this method in facial expression recognition. We use this preprocessing algorithm to enhance the CK+ and Oulu expression datasets, and get accuracy of 89.24% and 70.24% respectively. Compared with the datasets that have not been pre-processed, it has provided an increase in classification accuracy of 7% on the Oulu datasets. © 2021 IEEE.",CLAHE; facial expression recognition; illumination,"318, 322",,"2021 13th International Conference on Communication Software and Networks, ICCSN 2021",Conference Paper,Scopus
232,,Pedestrian Traffic Lights Classification Using Transfer Learning in Smart City Application,"Khan S., Teng Y., Cui J.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114515845&doi=10.1109%2fICCSN52437.2021.9463615&partnerID=40&md5=b8663cbce66c9e4f0fd0714e0d445eb1,10.1109/ICCSN52437.2021.9463615,"Traffic accidents have become a serious issue in cities. Millions of people die in traffic accidents annually and among them the major cause is the pedestrian jaywalking. To solve this traffic issue and ensure efficient traffic monitoring, we introduced the surveillance system using AI powered UAVs in Internet of flying things based smart city scenario. To accurately classify the pedestrian traffic lights, we use the computer vision technology. We have created our own local dataset containing 809 images where 441 images belong to red signal class while 368 images belong to green signal class. We explore the power of transfer learning based on DNNs to overcome the limitation of dataset for pedestrian traffic lights classification. In this approach, we use the pre-trained MobileNetV2 model and freeze the weights. By leveraging the pre-trained convolutional base, we add our own fully connected layers on top of the model for classification. To handle the problem of limited data, we also perform the data augmentation. The task is formulated as binary classification problem. By using the MobileNetV2 on challenging and very diverse dataset, we achieve the accuracy of 94.92%, 91.84% specificity and 97.10% sensitivity. © 2021 IEEE.",computer vision; IoFT; pedestrian traffic lights classification; transfer learning,"352, 356",,"2021 13th International Conference on Communication Software and Networks, ICCSN 2021",Conference Paper,Scopus
233,,A Novel Sparse Subspace Correlation Analysis-Based Domain Adaptation Method for Sensor Drift Suppression in E-nose,"Liang Z., Yang L., Guo T., Li J.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114492717&doi=10.1109%2fICCSN52437.2021.9463598&partnerID=40&md5=26fec96d8133bf50ceb067752f92aa82,10.1109/ICCSN52437.2021.9463598,"Sensor drift caused by the sensor aging and environmental factors is an urgent problem that seriously affects the detection performance and service life of electronic nose (E-nose). It is necessary to research the sensor drift suppression methods to realize the long-term and stable detection of E-nose. In this paper, a highly efficient sparse subspace correlation analysis-based domain adaptation(SSCA-DA) method is proposed to suppress the sensor drift. This method is to find the optimal subspace for each dataset, and the transformed data after transforming to the optimal subspace is sparsely reconstructed, which can realize the knowledge transfer in the data domains with and without drift information. From the experiment results, it can be found that the sensor drift can be satisfactorily solved by the proposed method. © 2021 IEEE.",domain adaptation; electronic nose; sensor drift; sparse reconstruction,"242, 246",,"2021 13th International Conference on Communication Software and Networks, ICCSN 2021",Conference Paper,Scopus
234,,Security and Machine Learning Adoption in IoT: A Preliminary Study of IoT Developer Discussions,Uddin G.,2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115175564&doi=10.1109%2fSERP4IoT52556.2021.00013&partnerID=40&md5=bbb21d2cfac325edc4da19cc57ee6cd7,10.1109/SERP4IoT52556.2021.00013,"Internet of Things (IoT) is defined as the connection between places and physical objects (i.e., things) over the internet/network via smart computing devices. IoT is a rapidly emerging paradigm that now encompasses almost every aspect of our modern life. As such, it is crucial to ensure IoT devices follow strict security requirements. At the same time, the prevalence of IoT devices offers developers a chance to design and develop Machine Learning (ML)-based intelligent software systems using their IoT devices. However, given the diversity of IoT devices, IoT developers may find it challenging to introduce appropriate security and ML techniques into their devices. Traditionally, we learn about the IoT ecosystem/problems by conducting surveys of IoT developers/practitioners. Another way to learn is by analyzing IoT developer discussions in popular online developer forums like Stack Overflow (SO). However, we are aware of no such studies that focused on IoT developers' security and ML-related discussions in SO. This paper offers the results of preliminary study of IoT developer discussions in SO. First, we collect around 53K IoT posts (questions + accepted answers) from SO. Second, we tokenize each post into sentences. Third, we automatically identify sentences containing security and ML-related discussions. We find around 12% of sentences contain security discussions, while around 0.12% sentences contain ML-related discussions. There is no overlap between security and ML-related discussions, i.e., IoT developers discussing security requirements did not discuss ML requirements and vice versa. We find that IoT developers discussing security issues frequently inquired about how the shared data can be stored, shared, and transferred securely across IoT devices and users. We also find that IoT developers are interested to adopt deep neural network-based ML models into their IoT devices, but they find it challenging to accommodate those into their resource-constrained IoT devices. Our findings offer implications for IoT vendors and researchers to develop and design novel techniques for improved security and ML adoption into IoT devices. © 2021 IEEE.",Developer Discussions; IoT; Machine Learning; Security,"36, 43",,"Proceedings - 2021 IEEE/ACM 3rd International Workshop on Software Engineering Research and Practices for the IoT, SERP4IoT 2021",Conference Paper,Scopus
235,,Towards a question answering assistant for software development using a transformer-based language model,"Do Nascimento Vale L., De Almeida Maia M.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112868617&doi=10.1109%2fBotSE52550.2021.00016&partnerID=40&md5=47cd6e272e7e97d4846f35e109280046,10.1109/BotSE52550.2021.00016,"Question answering platforms, such as Stack Overflow, have impacted substantially how developers search for solutions for their programming problems. The crowd knowledge content available from such platforms has also been used to leverage software development tools. The recent advances on Natural Language Processing, specifically on more powerful language models, have demonstrated ability to enhance text understanding and generation. In this context, we aim at investigating the factors that can influence on the application of such models for understanding source code related data and produce more interactive and intelligent assistants for software development. In this preliminary study, we particularly investigate if a how-to question filter and the level of context in the question may impact the results of a question answering transformer-based model. We suggest that fine-tuning models with corpus based on how-to questions can impact positively in the model and more contextualized questions also induce more objective answers. © 2021 IEEE.",GPT 2 language model; Question and answer (Q&A); Stack Overflow posts,"39, 42",,"Proceedings - 2021 IEEE/ACM 3rd International Workshop on Bots in Software Engineering, BotSE 2021",Conference Paper,Scopus
236,,Identifying bot activity in GitHub pull request and issue comments,"Golzadeh M., Decan A., Constantinou E., Mens T.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112863592&doi=10.1109%2fBotSE52550.2021.00012&partnerID=40&md5=40c3fcbafe9ca5fb2b8bd9816c8112f2,10.1109/BotSE52550.2021.00012,"Development bots are used on Github to automate repetitive activities. Such bots communicate with human actors via issue comments and pull request comments. Identifying such bot comments allows to prevent bias in socio-technical studies related to software development. To automate their identification, we propose a classification model based on natural language processing. Starting from a balanced ground-truth dataset of 19,282 PR and issue comments, we encode the comments as vectors using a combination of the bag of words and TF-IDF techniques. We train a range of binary classifiers to predict the type of comment (human or bot) based on this vector representation. A multinomial Naive Bayes classifier provides the best results. Its performance on a test set containing 50% of the data achieves an average precision, recall, and F1 score of 0.88. Although the model shows a promising result on the pull request and issue comments, further work is required to generalize the model on other types of activities, like commit messages and code reviews. © 2021 IEEE.",automated comments; classification model; distributed software development; empirical analysis; GitHub,"21, 25",,"Proceedings - 2021 IEEE/ACM 3rd International Workshop on Bots in Software Engineering, BotSE 2021",Conference Paper,Scopus
237,,Exploring Plausible Patches Using Source Code Embeddings in JavaScript,"Csuvik V., Horváth D., Lajkó M., Vidács L.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111159271&doi=10.1109%2fAPR52552.2021.00010&partnerID=40&md5=bf9b3c9164c5769f14cb9e3be7550210,10.1109/APR52552.2021.00010,"Despite the immense popularity of the Automated Program Repair (APR) field, the question of patch validation is still open. Most of the present-day approaches follow the so-called Generate-and-Validate approach, where first a candidate solution is being generated and after validated against an oracle. The latter, however, might not give a reliable result, because of the imperfections in such oracles; one of which is usually the test suite. Although (re-) running the test suite is right under one's nose, in real life applications the problem of over-and underfitting often occurs, resulting in inadequate patches. Efforts that have been made to tackle with this problem include patch filtering, test suite expansion, careful patch producing and many more. Most approaches to date use post-filtering relying either on test execution traces or make use of some similarity concept measured on the generated patches. Our goal is to investigate the nature of these similarity-based approaches. To do so, we trained a Doc2Vec model on an open-source JavaScript project and generated 465 patches for 10 bugs in it. These plausible patches alongside with the developer fix are then ranked based on their similarity to the original program. We analyzed these similarity lists and found that plain document embeddings may lead to misclassification-it fails to capture nuanced code semantics. Nevertheless, in some cases it also provided useful information, thus helping to better understand the area of Automated Program Repair. © 2021 IEEE.",Automatic Program Repair; Code Embeddings; Doc2vec; Machine learning; Patch Correctness,"11, 18",,"Proceedings - 2021 IEEE/ACM International Workshop on Automated Program Repair, APR 2021",Conference Paper,Scopus
238,,Semi-supervised Heterogeneous Defect Prediction with Open-source Projects on GitHub,"Sun Y., Jing X.-Y., Wu F., Dong X., Sun Y., Wang R.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108572282&doi=10.1142%2fS0218194021500273&partnerID=40&md5=bd090dc9bb3f45af318a8ad27b9b5684,10.1142/S0218194021500273,"The heterogeneous defect prediction (HDP) technique can predict defects in a target company using heterogeneous metric data from external company, which has received substantial research attention. However, existing HDP methods assume that source data is labeled but labeling data is expensive. Semi-supervised defect prediction technique can perform defect prediction with few labeled data. In this paper, we investigate a new problem - semi-supervised HDP (SHDP). To solve this problem, we propose a new approach named cost-sensitive kernel semi-supervised correlation analysis (CKSCA) as a solution of SHDP problem. It introduces unified metric representation and canonical correlation analysis to make the data distributions of different company projects more similar. CKSCA also designs a cost-sensitive kernel semi-supervised discriminant analysis mechanism to utilize the limited labeled data and sufficient real-life unlabeled data from different companies. Besides we collect lots of open-source projects from GitHub website to construct a new large-scale unlabeled dataset called GITHUB dataset. It contains 26,407 modules and is greater than each public project dataset. It has been public online and can be extended continuously. Experiments on the GITHUB dataset and other public datasets indicate that unlabeled GITHUB data can help prediction model improve prediction performance, and CKSCA is effective and efficient for solving SHDP problem. © 2021 World Scientific Publishing Company.",canonical correlation analysis; Heterogeneous defect prediction; open-source projects; semi-supervised discriminant analysis,"889, 916",,International Journal of Software Engineering and Knowledge Engineering,Article,Scopus
239,,Deep Understanding of Runtime Configuration Intention,"Zhou C., Liu H., Zhang Y., Xue Z., Liao Q., Zhao J., Wang J.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108524072&doi=10.1142%2fS0218194021500236&partnerID=40&md5=1c716688f360ca45ef8d45ab86a35b48,10.1142/S0218194021500236,"The runtime environment and workload of software are constantly changing, requiring users to make appropriate adjustments to accommodate these changes. The runtime configuration, however, as the interface for users to manipulate software behavior often requires domain-specific knowledge to understand. This usually results in users spending a considerable amount of time wading through document and user manuals trying to understand the runtime configuration. In this paper, we study the possibility of understanding the intention of runtime configuration options through their documents, even sometimes it is difficult for users to understand. Based on these studies, we classify the runtime configuration option's intention into six categories. Accordingly, we design runtime Configuration Intention Classifier (CIC), a supervised approach based on CNN to classify the runtime configuration option's intention according to its document. CIC integrates the features of runtime configuration names and descriptions according to different levels of granularity and predicts the intention of runtime configuration options accordingly. Extensive experiments show that our approach can achieve an accuracy of 85.6% and outperform nine comparative approaches by up to 16.6% over the dataset we customized. © 2021 World Scientific Publishing Company.",deep learning; Runtime configuration intention; text classification,"775, 802",,International Journal of Software Engineering and Knowledge Engineering,Article,Scopus
240,,Automatic Generation of Large-Granularity Pull Request Description [大粒度Pull Request描述自动生成],"Kuang L., Shi R.-Y., Zhao L.-H., Zhang H., Gao H.-H.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107407089&doi=10.13328%2fj.cnki.jos.006239&partnerID=40&md5=08eaeaa222c8f256b8bfdbe84174bc74,10.13328/j.cnki.jos.006239,"In GitHub platform, many project contributors often ignore the descriptions of pull requests (PRs) when submitting PRs, making their PRs easily neglected or rejected by reviewers. Therefore, it is necessary to generate PR descriptions automatically to help increase PR pass rate. The performances of existing PR description generation methods are usually affected by PR granularity, so it is difficult to generate descriptions for large-granularity PRs effectively. For such reasons, this work focuses on generating descriptions for large-granularity PRs. The text information is first preprocessed in PR and word-sentence heterogeneous graphs are constructed where the words are used as secondary nodes, so as to establish the connections between PR sentences. Subsequently, feature extraction is performed on the heterogeneous graphs, and then the features are input into graph neural network for further graph representation learning, from which the sentence nodes can learn more abundant content information through message delivery between nodes. Finally, the sentences with key information are selected to form a PR description. In addition, the supervised learning method cannot be used for training due to the lack of manually labeled tags in the dataset, therefore, reinforcement learning is used to guide the generation of PR descriptions. The goal of model training is minimizing the negative expectation of rewards, which does not require the ground truth and directly improves the performance of the results. The experiments are conducted on real dataset and the experimental results show that the proposed method is superior to existing methods in F1 and readability. © Copyright 2021, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Heterogeneous graph neural network; Pull Request description; Reinforcement learning; Summarization generation; Unstructured document,"1597, 1611",,Ruan Jian Xue Bao/Journal of Software,Article,Scopus
241,,Introducing frequency representation into convolution neural networks for medical image segmentation via twin-Kernel Fourier convolution,"Tang X., Peng J., Zhong B., Li J., Yan Z.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105689974&doi=10.1016%2fj.cmpb.2021.106110&partnerID=40&md5=a1ef6233e72b5d7b4b932bad1702278c,10.1016/j.cmpb.2021.106110,"Background and objective: For medical image segmentation, deep learning-based methods have achieved state-of-the-art performance. However, the powerful spectral representation in the field of image processing is rarely considered in these models. Methods: In this work, we propose to introduce frequency representation into convolution neural networks (CNNs) and design a novel model, tKFC-Net, to combine powerful feature representation in both frequency and spatial domains. Through the Fast Fourier Transform (FFT) operation, frequency representation is employed on pooling, upsampling, and convolution without any adjustments to the network architecture. Furthermore, we replace original convolution with twin-Kernel Fourier Convolution (t-KFC), a new designed convolution layer, to specify the convolution kernels for particular functions and extract features from different frequency components. Results: We experimentally show that our method has an edge over other models in the task of medical image segmentation. Evaluated on four datasets—skin lesion segmentation (ISIC 2018), retinal blood vessel segmentation (DRIVE), lung segmentation (COVID-19-CT-Seg), and brain tumor segmentation (BraTS 2019), the proposed model achieves outstanding results: the metric F1-Score is 0.878 for ISIC 2018, 0.8185 for DRIVE, 0.9830 for COVID-19-CT-Seg, and 0.8457 for BraTS 2019. Conclusion: The introduction of spectral representation retains spectral features which result in more accurate segmentation. The proposed method is orthogonal to other topology improvement methods and very convenient to be combined. © 2021 Elsevier B.V.",Convolution neural networks; Frequency representation; Medical image segmentation; U-Net,,,Computer Methods and Programs in Biomedicine,Article,Scopus
242,,Automatic segmentation for ultrasound image of carotid intimal-media based on improved superpixel generation algorithm and fractal theory,"Zhuang S., Li F., Raj A.N.J., Ding W., Zhou W., Zhuang Z.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105593184&doi=10.1016%2fj.cmpb.2021.106084&partnerID=40&md5=bd89bc13e1aad54170953cfec18c02a6,10.1016/j.cmpb.2021.106084,"Objective: Carotid atherosclerosis (CAS) is the main reason leading to cardiovascular conditions such as coronary heart disease and cerebrovascular diseases. In the carotid ultrasound images, the carotid intima-media structure can be observed in an annular narrow strip, which its inner contour corresponds to the carotid intima, and the outer contour corresponds to the carotid extima. With the development of carotid atherosclerosis, the carotid intima-media will gradually thicken. Therefore, doctors can observe the carotid intima-media so as to obtain the pathological changes of the internal structure of the patient's carotid arteries. However, due to the presence of artifacts and noises the quality of the ultrasound images are degraded, making it difficult to obtain accurate carotid intima-media structures. This article presents a novel self-adaptive method to enable obtaining the carotid intima-media through carotid intima/extima segmentation. Method: After preprocessing the ultrasound images by homomorphic filtering and median filtering, we propose an improved superpixel generation algorithm that employs the fusion of gray-level and luminosity-based information to decompose the image into numerous superpixels and later presents the carotid intima. Meanwhile, based on the features of the carotid artery, the initial position of the carotid extima is located by the normalized cut algorithm and later the fractal theory is employed to segment the carotid extima. Results: The proposed method for segmenting carotid intima obtained mean values of the DICE true positive ratio (TPR), false positive ratio (FPR), precision scores of 97.797%, 99.126%, 0.540%, 97.202%, respectively. Further from the segmentation method of the carotid extima the performance measures such as mean DICE, TPR, accuracy, F-score obtained are 95.00%, 92.265%, 97.689%, 94.997%, respectively. Conclusion: Comparing with traditional methods, the proposed method performed better. The experimental results indicated that the proposed method obtained the carotid intima-media both automatically and accurately thus effectively assist doctors in the diagnosis of CAS. © 2021",Carotid atherosclerosis diagnosis; Carotid ultrasound image; Fractal theory; Normalized cut; Superpixel,,,Computer Methods and Programs in Biomedicine,Article,Scopus
243,,MHCPDP: multi-source heterogeneous cross-project defect prediction via multi-source transfer learning and autoencoder,"Wu J., Wu Y., Niu N., Zhou M.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105416319&doi=10.1007%2fs11219-021-09553-2&partnerID=40&md5=7dfb4efd1c2e3b096dbb1b303b7a73d5,10.1007/s11219-021-09553-2,"Heterogeneous cross-project defect prediction (HCPDP) is aimed at building a defect prediction model for the target project by reusing datasets from source projects, where the source project datasets and target project dataset have different features. Most existing HCPDP methods only remove redundant or unrelated features without exploring the underlying features of cross-project datasets. Additionally, when the transfer learning method is used in HCPDP, these methods ignore the negative effect of transfer learning. In this paper, we propose a novel HCPDP method called multi-source heterogeneous cross-project defect prediction (MHCPDP). To reduce the gap between the target datasets and the source datasets, MHCPDP uses the autoencoder to extract the intermediate features from the original datasets instead of simply removing redundant and unrelated features and adopts a modified autoencoder algorithm to make instance selection for eliminating irrelevant instances from the source domain datasets. Furthermore, by incorporating multiple source projects to increase the number of source datasets, MHCPDP develops a multi-source transfer learning algorithm to reduce the impact of negative transfers and upgrade the performance of the classifier. We comprehensively evaluate MHCPDP on five open source datasets; our experimental results show that MHCPDP not only has significant improvement in two performance metrics but also overcomes the shortcomings of the conventional HCPDP methods. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",Autoencoder; Heterogeneous cross-project defect prediction; Modified autoencoder; Multi-source transfer learning,"405, 430",,Software Quality Journal,Article,Scopus
244,,Deep neural network for automated simultaneous intervertebral disc (IVDs) identification and segmentation of multi-modal MR images,"Das P., Pal C., Acharyya A., Chakrabarti A., Basu S.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104786088&doi=10.1016%2fj.cmpb.2021.106074&partnerID=40&md5=f276d13ca44a40751550bbed821f1e03,10.1016/j.cmpb.2021.106074,"Background and objective: Lower back pain in humans has become a major risk. Classical approaches follow a non-invasive imaging technique for the assessment of spinal intervertebral disc (IVDs) abnormalities, where identification and segmentation of discs are done separately, making it a time-consuming phenomenon. This necessitates designing a robust automated and simultaneous IVDs identification and segmentation of multi-modality MRI images. Methods: We introduced a novel deep neural network architecture coined as ‘RIMNet’, a Region-to-Image Matching Network model, capable of performing an automated and simultaneous IVDs identification and segmentation of MRI images. The multi-modal input data is being fed to the network with a dropout strategy, by randomly disabling modalities in mini-batches. The performance accuracy as a function of the testing dataset was determined. The execution of the deep neural network model was evaluated by computing the IVDs Identification Accuracy, Dice coefficient, MDOC, Average Symmetric Surface Distance, Jaccard Coefficient, Hausdorff Distance and F1 Score. Results:Proposed model has attained 94% identification accuracy, dice coefficient value of 91.7±1% in segmentation and MDOC 90.2±1%. Our model also achieved 0.87±0.02 for Jaccard Coefficient, 0.54±0.04 for ASD and 0.62±0.02 mm Hausdorff Distance. The results have been validated and compared with other methodologies on dataset of MICCAI IVD 2018 challenge. Conclusions: Our proposed deep-learning methodology is capable of performing simultaneous identification and segmentation on IVDs MRI images of the human spine with high accuracy. © 2021 Elsevier B.V.",Convolutional neural networks; Deep learning; Identification; Intervertebral disc; Region-to-image matching (RIM); Segmentation,,,Computer Methods and Programs in Biomedicine,Article,Scopus
245,,Computer-aided diagnosis system for the classification of multi-class kidney abnormalities in the noisy ultrasound images,"Sudharson S., Kokil P.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104342062&doi=10.1016%2fj.cmpb.2021.106071&partnerID=40&md5=c4c9e51102b84f593802916ce73747b5,10.1016/j.cmpb.2021.106071,"Background and Objective: The primary causes of kidney failure are chronic and polycystic kidney diseases. Cyst, stone, and tumor development lead to chronic kidney diseases that commonly impair kidney functions. The kidney diseases are asymptomatic and do not show any significant symptoms at its initial stage. Therefore, diagnosing the kidney diseases at their earlier stage is required to prevent the loss of kidney function and kidney failure. Methods: This paper proposes a computer-aided diagnosis (CAD) system for detecting multi-class kidney abnormalities from ultrasound images. The presented CAD system uses a pre-trained ResNet-101 model for extracting the features and support vector machine (SVM) classifier for the classification purpose. Ultrasound images usually gets affected by speckle noise that degrades the image quality and performance of the CAD system. Hence, it is necessary to remove speckle noise from the ultrasound images. Therefore, a CAD based system is proposed with the despeckling module using a deep residual learning network (RLN) to reduce speckle noise. Pre-processing of ultrasound images using deep RLN helps to drastically improve the classification performance of the CAD system. The proposed CAD system achieved better prediction results when compared to the existing state-of-the-art methods. Results: To validate the proposed CAD system performance, the experiments have been carried out in the noisy kidney ultrasound images. The designed system framework achieved the maximum classification accuracy when compared to the existing approaches. The SVM classifier is selected for the CAD system based on performance comparison with various classifiers like K-nearest neighbour, tree, discriminant, Naive Bayes, and linear. Conclusions: The proposed CAD system outperforms in classifying the noisy kidney ultrasound images precisely as compared to the existing state-of-the-art methods. Further, the CAD system is evaluated in terms of selectivity and sensitivity scores. The presented CAD system with the pre-processing module would serve as a real-time supporting tool for diagnosing multi-class kidney abnormalities from the ultrasound images. © 2021",Classification; Deep neural networks; Despeckling; Residual learning network,,,Computer Methods and Programs in Biomedicine,Article,Scopus
246,,A new deep learning method for blood vessel segmentation in retinal images based on convolutional kernels and modified U-Net model,"Gegundez-Arias M.E., Marin-Santos D., Perez-Borrero I., Vasallo-Vazquez M.J.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104300453&doi=10.1016%2fj.cmpb.2021.106081&partnerID=40&md5=d7966d203623f66182ba8e9cc6de45b1,10.1016/j.cmpb.2021.106081,"Background and Objective: Automatic monitoring of retinal blood vessels proves very useful for the clinical assessment of ocular vascular anomalies or retinopathies. This paper presents an efficient and accurate deep learning-based method for vessel segmentation in eye fundus images. Methods: The approach consists of a convolutional neural network based on a simplified version of the U-Net architecture that combines residual blocks and batch normalization in the up- and downscaling phases. The network receives patches extracted from the original image as input and is trained with a novel loss function that considers the distance of each pixel to the vascular tree. At its output, it generates the probability of each pixel of the input patch belonging to the vascular structure. The application of the network to the patches in which a retinal image can be divided allows obtaining the pixel-wise probability map of the complete image. This probability map is then binarized with a certain threshold to generate the blood vessel segmentation provided by the method. Results: The method has been developed and evaluated in the DRIVE, STARE and CHASE_Db1 databases, which offer a manual segmentation of the vascular tree by each of its images. Using this set of images as ground truth, the accuracy of the vessel segmentations obtained for an operating point proposal (established by a single threshold value for each database) was quantified. The overall performance was measured using the area of its receiver operating characteristic curve. The method demonstrated robustness in the face of the variability of the fundus images of diverse origin, being capable of working with the highest level of accuracy in the entire set of possible points of operation, compared to those provided by the most accurate methods found in literature. Conclusions: The analysis of results concludes that the proposed method reaches better performance than the rest of state-of-art methods and can be considered the most promising for integration into a real tool for vascular structure segmentation. © 2021 Elsevier B.V.",Blood vessel; Convolutional neural networks; Deep learning; Fundus images; Segmentation,,,Computer Methods and Programs in Biomedicine,Article,Scopus
247,,Detecting cerebral microbleeds via deep learning with features enhancement by reusing ground truth,"Li T., Zou Y., Bai P., Li S., Wang H., Chen X., Meng Z., Kang Z., Zhou G.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103962043&doi=10.1016%2fj.cmpb.2021.106051&partnerID=40&md5=e8efa7cb87fa93fb34c7a5f162fc9fdb,10.1016/j.cmpb.2021.106051,"Background and objectives: Cerebral microbleeds (CMBs) are cerebral small vascular diseases and are often used to diagnose symptoms such as stroke and dementia. Manual detection of cerebral microbleeds is a time-consuming and error-prone task, so the application of microbleed detection algorithms based on deep learning is of great significance. This study presents the feature enhancement technology applying to improve the performances of detecting CMBs. The primary purpose of the feature enhancement is emphasizing the meaningful features, leading deep learning network easier and correctly to optimize. Method: In this study, we applied feature enhancement in detecting CMBs from brain MRI images. Feature enhancement enhanced specific intervals and suppressed the useless intervals of the feature map. This method was applied in SSD-512 and SSD-300 algorithm, using VGG architecture pre-trained in the ImageNet dataset. Results: The proposed method was applied in SSD-512. Moreover, the model was trained and tested on the sequence of SWAN images of brain MRI images. The results of the experiment demonstrate that our method effectively improves the detection performance of the SSD network in detecting CMBs. We train SSD-512 120000 iterations and test results on the test datasets, by applying the feature enhancement layer, improving the precision with 3.3% and the mAP of 2.3%. In the same way, we trained SSD-300, improving the mAP of 2.0%. 2.8% and 7.4% precision are improved by applying feature enhancement layer In ResNet-34 and MobileNet. Conclusions: The proposed method achieved more effective performance, demonstrated that feature enhancement can be a helpful algorithm to enhance the deep learning model. © 2021",Cerebral microbleeds; Convolutional neural network; Deep learning; Feature enhancement; SSD,,,Computer Methods and Programs in Biomedicine,Article,Scopus
248,,Adopting low-shot deep learning for the detection of conjunctival melanoma using ocular surface images,"Yoo T.K., Choi J.Y., Kim H.K., Ryu I.H., Kim J.K.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103960457&doi=10.1016%2fj.cmpb.2021.106086&partnerID=40&md5=1c530f2f7db0893d11ec7889f52d2b20,10.1016/j.cmpb.2021.106086,"Background and Objective: The purpose of the present study was to investigate low-shot deep learning models applied to conjunctival melanoma detection using a small dataset with ocular surface images. Methods: A dataset was composed of anonymized images of four classes; conjunctival melanoma (136), nevus or melanosis (93), pterygium (75), and normal conjunctiva (94). Before training involving conventional deep learning models, two generative adversarial networks (GANs) were constructed to augment the training dataset for low-shot learning. The collected data were randomly divided into training (70%), validation (10%), and test (20%) datasets. Moreover, 3D melanoma phantoms were designed to build an external validation set using a smartphone. The GoogleNet, InceptionV3, NASNet, ResNet50, and MobileNetV2 architectures were trained through transfer learning and validated using the test and external validation datasets. Results: The deep learning model demonstrated a significant improvement in the classification accuracy of conjunctival lesions using synthetic images generated by the GAN models. MobileNetV2 with GAN-based augmentation displayed the highest accuracy of 87.5% in the four-class classification and 97.2% in the binary classification for the detection of conjunctival melanoma. It showed an accuracy of 94.0% using 3D melanoma phantom images captured using a smartphone camera. Conclusions: The present study described a low-shot deep learning model that can detect conjunctival melanomas using ocular surface images. To the best of our knowledge, this study is the first to develop a deep learning model to detect conjunctival melanoma using a digital imaging device such as smartphone camera. © 2021",Conjunctival melanoma; Conjunctival nevus; Deep learning; Low-shot learning; Melanosis,,,Computer Methods and Programs in Biomedicine,Article,Scopus
249,,Breast mass detection in digital mammography based on anchor-free architecture,"Cao H., Pu S., Tan W., Tong J.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103784913&doi=10.1016%2fj.cmpb.2021.106033&partnerID=40&md5=ca6960b7b476ef399dabd7cd19a7c9f5,10.1016/j.cmpb.2021.106033,"Background and objective: Accurate detection of breast masses in mammography images is critical to diagnose early breast cancer, which can greatly improve the patients’ survival rate. However, it is still a big challenge due to the heterogeneity of breast masses and the complexity of their surrounding environment. Therefore, how to develop a robust breast mass detection framework in clinical practical applications to improve patient survival is a topic that researchers need to continue to explore. Methods: To address these problems, we propose a one-stage object detection architecture, called Breast Mass Detection Network (BMassDNet), based on anchor-free and feature pyramid which makes the detection of breast masses of different sizes well adapted. We introduce a truncation normalization method and combine it with adaptive histogram equalization to enhance the contrast between the breast mass and the surrounding environment. Meanwhile, to solve the overfitting problem caused by small data size, we propose a natural deformation data augmentation method and mend the train data dynamic updating method based on the data complexity to effectively utilize the limited data. Finally, we use transfer learning to assist the training process and to improve the robustness of the model ulteriorly. Results: On the INbreast dataset, each image has an average of 0.495 false positives whilst the recall rate is 0.930; On the DDSM dataset, when each image has 0.599 false positives, the recall rate reaches 0.943. Conclusions: The experimental results on datasets INbreast and DDSM show that the proposed BMassDNet can obtain competitive detection performance over the current top ranked methods. © 2021",Anchor-free architecture; Breast mass detection; Data augmentation method; Image enhancement method; Training method,,,Computer Methods and Programs in Biomedicine,Article,Scopus
250,,Extreme Learning Machine based Differentiation of Pulmonary Tuberculosis in Chest Radiographs using Integrated Local Feature Descriptors,"Govindarajan S., Swaminathan R.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103371935&doi=10.1016%2fj.cmpb.2021.106058&partnerID=40&md5=6da2f481b89710536f15b43f2a301d01,10.1016/j.cmpb.2021.106058,"Background and Objective: Computer aided diagnostics of Pulmonary Tuberculosis in chest radiographs relies on the differentiation of subtle and non-specific alterations in the images. In this study, an attempt has been made to identify and classify Tuberculosis conditions from healthy subjects in chest radiographs using integrated local feature descriptors and variants of extreme learning machine. Methods: Lung fields in the chest images are segmented using Reaction Diffusion Level Set method. Local feature descriptors such as Median Robust Extended Local Binary Patterns and Gradient Local Ternary Patterns are extracted. Extreme Learning Machine (ELM) and Online Sequential ELM (OSELM) classifiers are employed to identify Tuberculosis conditions and, their performances are analysed using standard metrics. Results: Results show that the adopted segmentation method is able to delineate lung fields in both healthy and Tuberculosis images. Extracted features are statistically significant even in images with inter and intra subject variability. Sigmoid activation function yields accuracy and sensitivity values greater than 98% for both the classifiers. Highest sensitivity is observed with OSELM for minimal significant features in detecting Tuberculosis images. Conclusion: As ELM based method is able to differentiate the subtle changes in inter and intra subject variations of chest X-ray images, the proposed methodology seems to be useful for computer-based detection of Pulmonary Tuberculosis. © 2021",Chest radiographs; Extreme learning machine; Local texture descriptor; Pulmonary tuberculosis; Reaction diffusion level set,,,Computer Methods and Programs in Biomedicine,Article,Scopus
251,,Code smell detection by deep direct-learning and transfer-learning,"Sharma T., Efstathiou V., Louridas P., Spinellis D.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102119719&doi=10.1016%2fj.jss.2021.110936&partnerID=40&md5=802a1c364cae9feb102101198284bab7,10.1016/j.jss.2021.110936,"Context: An excessive number of code smells make a software system hard to evolve and maintain. Machine learning methods, in addition to metric-based and heuristic-based methods, have been recently applied to detect code smells; however, current methods are considered far from mature. Objective: First, explore the feasibility of applying deep learning models to detect smells without extensive feature engineering. Second, investigate the possibility of applying transfer-learning in the context of detecting code smells. Methods: We train smell detection models based on Convolution Neural Networks and Recurrent Neural Networks as their principal hidden layers along with autoencoder models. For the first objective, we perform training and evaluation on C# samples, whereas for the second objective, we train the models from C# code and evaluate the models over Java code samples and vice-versa. Results: We find it feasible to detect smells using deep learning methods though the models’ performance is smell-specific. Our experiments show that transfer-learning is definitely feasible for implementation smells with performance comparable to that of direct-learning. This work opens up a new paradigm to detect code smells by transfer-learning especially for the programming languages where the comprehensive code smell detection tools are not available. © 2021 Elsevier Inc.",Code smells; Deep learning; Smell detection tools; Transfer-learning,,,Journal of Systems and Software,Article,Scopus
252,,Self-Attention Networks for Code Search,"Fang S., Tan Y.-S., Zhang T., Liu Y.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100726176&doi=10.1016%2fj.infsof.2021.106542&partnerID=40&md5=362114e3aa6e8c9821e7f86e9b9e704a,10.1016/j.infsof.2021.106542,"Context: Developers tend to search and reuse code snippets from a large-scale codebase when they want to implement some functions that exist in the previous projects, which can enhance the efficiency of software development. Objective: As the first deep learning-based code search model, DeepCS outperforms prior models such as Sourcere and CodeHow. However, it utilizes two separate LSTM to represent code snippets and natural language descriptions respectively, which ignores semantic relations between code snippets and their descriptions. Consequently, the performance of DeepCS falls into the bottleneck, and thus our objective is to break this bottleneck. Method: We propose a self-attention joint representation learning model, named SAN-CS (Self-Attention Network for Code Search). Comparing with DeepCS, we directly utilize the self-attention network to construct our code search model. By a weighted average operation, self-attention networks can fully capture the contextual information of code snippets and their descriptions. We first utilize two individual self-attention networks to represent code snippets and their descriptions, respectively, and then we utilize the self-attention network to conduct an extra joint representation network for code snippets and their descriptions, which can build semantic relationships between code snippets and their descriptions. Therefore, SAN-CS can break the performance bottleneck of DeepCS. Results: We evaluate SAN-CS on the dataset shared by Gu et al. and choose two baseline models, DeepCS and CARLCS-CNN. Experimental results demonstrate that SAN-CS achieves significantly better performance than DeepCS and CARLCS-CNN. In addition, SAN-CS has better execution efficiency than DeepCS at the training and testing phase. Conclusion: This paper proposes a code search model, SAN-CS. It utilizes the self-attention network to perform the joint attention representations for code snippets and their descriptions, respectively. Experimental results verify the effectiveness and efficiency of SAN-CS. © 2021 Elsevier B.V.",Code search; Joint embedding; Self-attention mechanism,,,Information and Software Technology,Article,Scopus
253,,ICE-Talk 2: Interface for Controllable Expressive TTS with perceptual assessment tool[Formula presented],"Tits N., El Haddad K., Dutoit T.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115872185&doi=10.1016%2fj.simpa.2021.100055&partnerID=40&md5=cd2ebce62f889ebde24e60d80d7c1d17,10.1016/j.simpa.2021.100055,"In this paper, we present open-source tools that facilitates the use of controllable TTS systems in experiments, towards the democratization of TTS systems across domains. ICE-Talk is a web-based GUI that allows the use of a TTS system with controllable parameters via a text field and a clickable 2D plot. It enables the study of latent spaces for controllable TTS. A tool to design a perceptual experiment is provided and consists of three steps: pre-synthesizing samples covering the 2D plot representing controllable dimensions, including this interface inside a template question, and integrate it in a Mechanical Turk system called turkle. © 2021 The Author(s)",Assessment; Controllable; Evaluation; Expressive; Interface; Perceptual experiment; Speech synthesis,,,Software Impacts,Article,Scopus
254,,Testing machine translation via referential transparency,"He P., Meister C., Su Z.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115701240&doi=10.1109%2fICSE43902.2021.00047&partnerID=40&md5=6e1f4a0c41f84563f06bd797e43154db,10.1109/ICSE43902.2021.00047,"Machine translation software has seen rapid progress in recent years due to the advancement of deep Neural Networks. People routinely use machine translation software in their daily lives for tasks such as ordering food in a foreign restaurant, receiving medical diagnosis and treatment from foreign doctors, and reading international political news online. However, due to the complexity and intractability of the underlying Neural Networks, modern machine translation software is still far from robust and can produce poor or incorrect translations; this can lead to misunderstanding, financial loss, threats to personal safety and health, and political conflicts. To address this problem, we introduce referentially transparent inputs (RTIs), a simple, widely applicable methodology for validating machine translation software. A referentially transparent input is a piece of text that should have similar translations when used in different contexts. Our practical implementation, Purity, detects when this property is broken by a translation. To evaluate RTI, we use Purity to test Google Translate and Bing Microsoft Translator with 200 unlabeled sentences, which detected 123 and 142 erroneous translations with high precision (79.3% and 78.3%). The translation errors are diverse, including examples of under-translation, over-translation, word/phrase mistranslation, incorrect modification, and unclear logic. © 2021 IEEE.",Machine translation; Metamorphic testing; Referential transparency; Testing,"410, 422",,Proceedings - International Conference on Software Engineering,Conference Paper,Scopus
255,,Advances in Code Summarization,"Desai U., Sridhara G., Tamilselvam S.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115693556&doi=10.1109%2fICSE-Companion52605.2021.00141&partnerID=40&md5=9461f97f900e4c4e2d00e5008e89145b,10.1109/ICSE-Companion52605.2021.00141,"Several studies have suggested that comments describing source code can help mitigate the burden of program understanding. However, software systems usually lack adequate comments and even when present, the comments may be obsolete or unhelpful. Researchers have addressed this issue by automatically generating comments from source code, a task referred to as Code Summarization. In this technical presentation, we take a deeper look at some of the significant, recent works in the area of code summarization and how each of them attempts to take a new perspective of this task including methods leveraging RNNs, Transformers, Graph neural networks and Reinforcement learning. © 2021 IEEE.",code summarization; neural networks,"330, 331",,Proceedings - International Conference on Software Engineering,Conference Paper,Scopus
256,,Testing Framework for Black-box AI Models,"Aggarwal A., Shaikh S., Hans S., Haldar S., Ananthanarayanan R., Saha D.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115677528&doi=10.1109%2fICSE-Companion52605.2021.00041&partnerID=40&md5=2d9d23416f13c083f29a8b64049b3462,10.1109/ICSE-Companion52605.2021.00041,"With widespread adoption of AI models for important decision making, ensuring reliability of such models remains an important challenge. In this paper, we present an end-to-end generic framework for testing AI Models which performs automated test generation for different modalities such as text, tabular, and time-series data and across various properties such as accuracy, fairness, and robustness. Our tool has been used for testing industrial AI models and was very effective to uncover issues present in those models. Demo video link-https://youtu.be/984UCU17YZ. © 2021 IEEE.",,"81, 84",,Proceedings - International Conference on Software Engineering,Conference Paper,Scopus
257,,Mining Software Repositories with a Collaborative Heuristic Repository,"Babii H., Prenner J.A., Stricker L., Karmakar A., Janes A., Robbes R.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115619152&doi=10.1109%2fICSE-NIER52604.2021.00030&partnerID=40&md5=38958adba17dd3dd0180e496c1bf8d72,10.1109/ICSE-NIER52604.2021.00030,"Many software engineering studies or tasks rely on categorizing software engineering artifacts. In practice, this is done either by defining simple but often imprecise heuristics, or by manual labelling of the artifacts. Unfortunately, errors in these categorizations impact the tasks that rely on them. To improve the precision of these categorizations, we propose to gather heuristics in a collaborative heuristic repository, to which researchers can contribute a large amount of diverse heuristics for a variety of tasks on a variety of SE artifacts. These heuristics are then leveraged by state-of-the-art weak supervision techniques to train high-quality classifiers, thus improving the categorizations. We present an initial version of the heuristic repository, which we applied to the concrete task of commit classification. © 2021 IEEE.",Mining Software Repositories; MSR Heuristics; Weak Supervision,"106, 110",,Proceedings - International Conference on Software Engineering,Conference Paper,Scopus
258,,Automatically matching bug reports with related app reviews,"Haering M., Stanik C., Maalej W.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114131498&doi=10.1109%2fICSE43902.2021.00092&partnerID=40&md5=d20d8e5fd11a0236abc68723b76e0cf7,10.1109/ICSE43902.2021.00092,"App stores allow users to give valuable feedback on apps, and developers to find this feedback and use it for the software evolution. However, finding user feedback that matches existing bug reports in issue trackers is challenging as users and developers often use a different language. In this work, we introduce DeepMatcher, an automatic approach using state-of-the-art deep learning methods to match problem reports in app reviews to bug reports in issue trackers. We evaluated DeepMatcher with four open-source apps quantitatively and qualitatively. On average, DeepMatcher achieved a hit ratio of 0.71 and a Mean Average Precision of 0.55. For 91 problem reports, DeepMatcher did not find any matching bug report. When manually analyzing these 91 problem reports and the issue trackers of the studied apps, we found that in 47 cases, users actually described a problem before developers discovered and documented it in the issue tracker. We discuss our findings and different use cases for DeepMatcher. © 2021 IEEE.",App store analytics; Deep learning; Mining software repositories; Natural language processing; Software evolution,"970, 981",,Proceedings - International Conference on Software Engineering,Conference Paper,Scopus
259,,Identifying key features from app user reviews,"Wu H., Deng W., Niu X., Nie C.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114129108&doi=10.1109%2fICSE43902.2021.00088&partnerID=40&md5=b372953d826aea1fad15be5db567fac1,10.1109/ICSE43902.2021.00088,"Due to the rapid growth and strong competition of mobile application (app) market, app developers should not only offer users with attractive new features, but also carefully maintain and improve existing features based on users' feedbacks. User reviews indicate a rich source of information to plan such feature maintenance activities, and it could be of great benefit for developers to evaluate and magnify the contribution of specific features to the overall success of their apps. In this study, we refer to the features that are highly correlated to app ratings as key features, and we present KEFE, a novel approach that leverages app description and user reviews to identify key features of a given app. The application of KEFE especially relies on natural language processing, deep machine learning classifier, and regression analysis technique, which involves three main steps: 1) extracting feature-describing phrases from app description; 2) matching each app feature with its relevant user reviews; and 3) building a regression model to identify features that have significant relationships with app ratings. To train and evaluate KEFE, we collect 200 app descriptions and 1,108,148 user reviews from Chinese Apple App Store. Experimental results demonstrate the effectiveness of KEFE in feature extraction, where an average F-measure of 78.13% is achieved. The key features identified are also likely to provide hints for successful app releases, as for the releases that receive higher app ratings, 70% of features improvements are related to key features. © 2021 IEEE.",App store analysis; Feature extraction; Key features; User reviews,"922, 932",,Proceedings - International Conference on Software Engineering,Conference Paper,Scopus
260,,Test Suites as a Source of Training Data for Static Analysis Alert Classifiers,"Flynn L., Snavely W., Kurtz Z.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113723244&doi=10.1109%2fAST52587.2021.00019&partnerID=40&md5=434492b4cbb6bb65052d628b8e52a8c6,10.1109/AST52587.2021.00019,"Flaw-finding static analysis tools typically generate large volumes of code flaw alerts including many false positives. To save on human effort to triage these alerts, a significant body of work attempts to use machine learning to classify and prioritize alerts. Identifying a useful set of training data, however, remains a fundamental challenge in developing such classifiers in many contexts. We propose using static analysis test suites (i.e., repositories of ""benchmark""programs that are purpose-built to test coverage and precision of static analysis tools) as a novel source of training data. In a case study, we generated a large quantity of alerts by executing various static analyzers on the Juliet C/C++ test suite, and we automatically derived ground truth labels for these alerts by referencing the Juliet test suite metadata. Finally, we used this data to train classifiers to predict whether an alert is a false positive. Our classifiers obtained high precision (90.2%) and recall (88.2%) for a large number of code flaw types on a hold-out test set. This preliminary result suggests that pre-training classifiers on test suite data could help to jumpstart static analysis alert classification in data-limited contexts. © 2021 IEEE.",alert; analysis; classification; Juliet; precise; rapid; static; test suite,"100, 108",,"Proceedings - 2021 IEEE/ACM International Conference on Automation of Software Test, AST 2021",Conference Paper,Scopus
261,,Comparative study of feature reduction techniques in software change prediction,"Malhotra R., Kapoor R., Aggarwal D., Garg P.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113691183&doi=10.1109%2fMSR52588.2021.00015&partnerID=40&md5=7b9ea3c3d4b598b0d1164e300c081b47,10.1109/MSR52588.2021.00015,"Software change prediction (SCP) is the process of identifying change-prone software classes using various structural and quality metrics by developing predictive techniques. The previous studies done in this field strongly confer the correlation between the quality of metrics and the performance of such SCP models. Past SCP studies have also applied different feature reduction (FR) techniques to address issues of high dimensionality, feature irrelevance, and feature repetition. Due to the vast variety of metric suites and FR techniques applied in SCP, there is a need to analyze and compare them. It will help in identifying the most crucial features and the most effective FR techniques. So, in this research, we conduct experiments to compare and contrast 60 Object-Oriented plus 26 Graph-based metrics and 11 state-of-the-art FR techniques previously employed for SCP over a range of 6 Java projects and 3 diverse classifiers. The AUC-ROC measures and statistical tests over experimental SCP models indicate that FR techniques are effective in SCP. Also, there exist significant differences in the performance of the different FR techniques. Furthermore, from this extensive experimentation, we were able to identify a set of the most effective FR techniques and the most crucial metrics which can be used to build effective SCP models. © 2021 IEEE.",Feature reduction; Graph-based metrics; Machine Learning; Object-Oriented metrics; Software change prediction,"18, 28",,"Proceedings - 2021 IEEE/ACM 18th International Conference on Mining Software Repositories, MSR 2021",Conference Paper,Scopus
262,,Attention-based model for predicting question relatedness on stack overflow,"Pei J., Wu Y., Qin Z., Cong Y., Guan J.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113689434&doi=10.1109%2fMSR52588.2021.00023&partnerID=40&md5=f71b3d7a821b58c5298f72b7124e3557,10.1109/MSR52588.2021.00023,"Stack Overflow is one of the most popular Programming Community-based Question Answering (PCQA) websites that has attracted more and more users in recent years. When users raise or inquire questions in Stack Overflow, providing related questions can help them solve problems. Although there are many approaches based on deep learning that can automatically predict the relatedness between questions, those approaches are limited since interaction information between two questions may be lost. In this paper, we adopt the deep learning technique, propose an Attention-based Sentence pair Interaction Model (ASIM) to predict the relatedness between questions on Stack Overflow automatically. We adopt the attention mechanism to capture the semantic interaction information between the questions. Besides, we have pre-trained and released word embeddings specific to the software engineering domain for this task, which may also help other related tasks. The experiment results demonstrate that ASIM has made significant improvement over the baseline approaches in Precision, Recall, and Micro-F1 evaluation metrics, achieving state-of-the-art performance in this task. Our model also performs well in the duplicate question detection task of AskUbuntu, which is a similar but different task, proving its generalization and robustness. © 2021 IEEE.",Attention Mechanism; Deep Learning; Question Relatedness; Stack Overflow; Word Embeddings,"97, 107",,"Proceedings - 2021 IEEE/ACM 18th International Conference on Mining Software Repositories, MSR 2021",Conference Paper,Scopus
263,,On improving deep learning trace analysis with system call arguments,"Fournier Q., Aloise D., Azhari S.V., Tetreault F.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113681899&doi=10.1109%2fMSR52588.2021.00025&partnerID=40&md5=68b794c5689a6226f5155019f9dce3b9,10.1109/MSR52588.2021.00025,"Kernel traces are sequences of low-level events comprising a name and multiple arguments, including a timestamp, a process id, and a return value, depending on the event. Their analysis helps uncover intrusions, identify bugs, and find latency causes. However, their effectiveness is hindered by omitting the event arguments. To remedy this limitation, we introduce a general approach to learning a representation of the event names along with their arguments using both embedding and encoding. The proposed method is readily applicable to most neural networks and is task-agnostic. The benefit is quantified by conducting an ablation study on three groups of arguments: call-related, process-related, and time-related. Experiments were conducted on a novel web request dataset and validated on a second dataset collected on pre-production servers by Ciena, our partnering company. By leveraging additional information, we were able to increase the performance of two widely-used neural networks, an LSTM and a Transformer, by up to 11.3% on two unsupervised language modelling tasks. Such tasks may be used to detect anomalies, pre-train neural networks to improve their performance, and extract a contextual representation of the events. © 2021 IEEE.",Deep Learning; Machine Learning; Tracing,"120, 130",,"Proceedings - 2021 IEEE/ACM 18th International Conference on Mining Software Repositories, MSR 2021",Conference Paper,Scopus
264,,The Impacts of Sentiments and Tones in Community-Generated Issue Discussions,"Sanei A., Cheng J., Adams B.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113666923&doi=10.1109%2fCHASE52884.2021.00009&partnerID=40&md5=3bc216820e030a98964d2560ad9da19d,10.1109/CHASE52884.2021.00009,"The diverse community members who contribute to the discussions on issue tracking systems of open-source software projects often exhibit complex affective states such as sentiments and tones. These affective states can significantly influence the effectiveness of the issue discussions in elaborating the initial ideas into actionable tasks that the development teams need to address. In this paper, we present an extended empirical study to investigate the impacts of sentiments and tones in community-generated issue discussions. We created and validated a large dataset of sentiments and tones in the issues posts and comments created by diverse community members in three popular open source projects. Our analysis results drew a complex picture of the relationships between, on the one hand, the sentiments and tones in the issue discussions, and on the other hand, various discussion and development-related measures such as the discussion length and the issue resolution time. We also found that when factors such as the issue poster roles and the issue types were controlled, sentiments and tones had varied associations with the measures. Insights gained from these findings can support open source community members in making and moderating effective issue discussions and guide the design of tools to better support community engagement. © 2021 IEEE.",issue tracking systems; Open source software; sentiment analysis; tone analysis,"1, 10",,"Proceedings - 2021 IEEE/ACM 13th International Workshop on Cooperative and Human Aspects of Software Engineering, CHASE 2021",Conference Paper,Scopus
265,,Search4Code: Code search intent classification using weak supervision,"Rao N., Bansal C., Guan J.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113643859&doi=10.1109%2fMSR52588.2021.00077&partnerID=40&md5=2e6be92becda0ff9d5de2a85c3c3806b,10.1109/MSR52588.2021.00077,"Developers use search for various tasks such as finding code, documentation, debugging information, etc. In particular, web search is heavily used by developers for finding code examples and snippets during the coding process. Recently, natural language based code search has been an active area of research. However, the lack of real-world large-scale datasets is a significant bottleneck. In this work, we propose a weak supervision based approach for detecting code search intent in search queries for C# and Java programming languages. We evaluate the approach against several baselines on a real-world dataset comprised of over 1 million queries mined from Bing web search engine and show that the CNN based model can achieve an accuracy of 77% and 76% for C# and Java respectively. Furthermore, we are also releasing Search4Code, the first large-scale real-world dataset of code search queries mined from Bing web search engine. We hope that the dataset will aid future research on code search. © 2021 IEEE.",Code search; Weak supervision,"575, 579",,"Proceedings - 2021 IEEE/ACM 18th International Conference on Mining Software Repositories, MSR 2021",Conference Paper,Scopus
266,,Fast and memory-efficient neural code completion,"Svyatkovskiy A., Lee S., Hadjitofi A., Riechert M., Franco J.V., Allamanis M.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113633559&doi=10.1109%2fMSR52588.2021.00045&partnerID=40&md5=245bd509ac8fa8fc60f0bcc050bcfcab,10.1109/MSR52588.2021.00045,"Code completion is one of the most widely used features of modern integrated development environments (IDEs). While deep learning has made significant progress in the statistical prediction of source code, state-of-the-art neural network models consume hundreds of megabytes of memory, bloating the development environment. We address this in two steps: first we present a modular neural framework for code completion. This allows us to explore the design space and evaluate different techniques. Second, within this framework we design a novel reranking neural completion model that combines static analysis with granular token encodings. The best neural reranking model consumes just 6 MB of RAM, - 19x less than previous models - computes a single completion in 8 ms, and achieves 90% accuracy in its top five suggestions. © 2021 IEEE.",API completion; Code completion; Deep learning,"329, 340",,"Proceedings - 2021 IEEE/ACM 18th International Conference on Mining Software Repositories, MSR 2021",Conference Paper,Scopus
267,,An exploratory study of log placement recommendation in an enterprise system,"Candido J., Haesen J., Aniche M., Van Deursen A.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113605904&doi=10.1109%2fMSR52588.2021.00027&partnerID=40&md5=bba49dd95e676126c194b1d15838736a,10.1109/MSR52588.2021.00027,"Logging is a development practice that plays an important role in the operations and monitoring of complex systems. Developers place log statements in the source code and use log data to understand how the system behaves in production. Unfortunately, anticipating where to log during development is challenging. Previous studies show the feasibility of leveraging machine learning to recommend log placement despite the data imbalance since logging is a fraction of the overall code base. However, it remains unknown how those techniques apply to an industry setting, and little is known about the effect of imbalanced data and sampling techniques. In this paper, we study the log placement problem in the code base of Adyen, a large-scale payment company. We analyze 34, 526 Java files and 309, 527 methods that sum up +2M SLOC. We systematically measure the effectiveness of five models based on code metrics, explore the effect of sampling techniques, understand which features models consider to be relevant for the prediction, and evaluate whether we can exploit 388, 086 methods from 29 Apache projects to learn where to log in an industry setting. Our best performing model achieves 79% of balanced accuracy, 81% of precision, 60% of recall. While sampling techniques improve recall, they penalize precision at a prohibitive cost. Experiments with open-source data yield under-performing models over Adyen's test set; nevertheless, they are useful due to their low rate of false positives. Our supporting scripts and tools are available to the community. © 2021 IEEE.",Log Placement; Log Recommendation; Logging Practices; Supervised Learning,"143, 154",,"Proceedings - 2021 IEEE/ACM 18th International Conference on Mining Software Repositories, MSR 2021",Conference Paper,Scopus
268,,An empirical study on the usage of BERT models for code completion,"Ciniselli M., Cooper N., Pascarella L., Poshyvanyk D., Di Penta M., Bavota G.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113573517&doi=10.1109%2fMSR52588.2021.00024&partnerID=40&md5=d4ea2a4d3d154dcf2042d0980895fa5a,10.1109/MSR52588.2021.00024,"Code completion is one of the main features of modern Integrated Development Environments (IDEs). Its objective is to speed up code writing by predicting the next code token(s) the developer is likely to write. Research in this area has substantially bolstered the predictive performance of these techniques. However, the support to developers is still limited to the prediction of the next few tokens to type. In this work, we take a step further in this direction by presenting a large-scale empirical study aimed at exploring the capabilities of state-of-the-art deep learning (DL) models in supporting code completion at different granularity levels, including single tokens, one or multiple entire statements, up to entire code blocks (e.g., the iterated block of a for loop). To this aim, we train and test several adapted variants of the recently proposed RoBERTa model, and evaluate its predictions from several perspectives, including: (i) metrics usually adopted when assessing DL generative models (i.e., BLEU score and Levenshtein distance); (ii) the percentage of perfect predictions (i.e., the predicted code snippets that match those written by developers); and (iii) the ""semantic""equivalence of the generated code as compared to the one written by developers. The achieved results show that BERT models represent a viable solution for code completion, with perfect predictions ranging from ∼7%, obtained when asking the model to guess entire blocks, up to ∼58%, reached in the simpler scenario of few tokens masked from the same code statement. © 2021 IEEE.",BERT; Code Completion,"108, 119",,"Proceedings - 2021 IEEE/ACM 18th International Conference on Mining Software Repositories, MSR 2021",Conference Paper,Scopus
269,,"NLP for Requirements Engineering: Tasks, Techniques, Tools, and Technologies","Ferrari A., Zhao L., Alhoshan W.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113560794&doi=10.1109%2fICSE-Companion52605.2021.00137&partnerID=40&md5=3d5a0098e531157b6ce1cfb389f4f464,10.1109/ICSE-Companion52605.2021.00137,"Requirements engineering (RE) is one of the most natural language-intensive fields within the software engineering area. Therefore, several works have been developed across the years to automate the analysis of natural language artifacts that are relevant for RE, including requirements documents, but also app reviews, privacy policies, and social media content related to software products. Furthermore, the recent diffusion of game-changing natural language processing (NLP) techniques and plat-forms has also boosted the interest of RE researchers. However, a reference framework to provide a holistic understanding of the field of NLP for RE is currently missing. Based on the results of a recent systematic mapping study, and stemming from a previous ICSE tutorial by one of the authors, this technical briefing gives an overview of NLP for RE tasks, available techniques, supporting tools and NLP technologies. It is oriented to both researchers and practitioners, and will gently guide the audience towards a clearer view of how NLP can empower RE, providing pointers to representative works and specialised tools. © 2021 IEEE.",Empirical Studies; Mapping Study; NLP; Requirements Engineering; Software Engineering; Survey; Transfer Learning; Tutorial,"322, 323",,Proceedings - International Conference on Software Engineering,Conference Paper,Scopus
270,,How do we Evaluate Self-adaptive Software Systems?: A Ten-Year Perspective of SEAMS,"Gerostathopoulos I., Vogel T., Weyns D., Lago P.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113500936&doi=10.1109%2fSEAMS51251.2021.00018&partnerID=40&md5=a09ddd224220f343ac7f9bec2def002f,10.1109/SEAMS51251.2021.00018,"With the increase of research in self-adaptive systems, there is a need to better understand the way research contributions are evaluated. Such insights will support researchers to better compare new findings when developing new knowledge for the community. However, so far there is no clear overview of how evaluations are performed in self-adaptive systems. To address this gap, we conduct a mapping study. The study focuses on experimental evaluations published in the last decade at the prime venue of research in software engineering for self-adaptive systems - the International Symposium on Software Engineering for Adaptive and Self-Managing Systems (SEAMS). Results point out that specifics of self-adaptive systems require special attention in the experimental process, including the distinction of the managing system (i.e., the target of evaluation) and the managed system, the presence of uncertainties that affect the system behavior and hence need to be taken into account in data analysis, and the potential of managed systems to be reused across experiments, beyond replications. To conclude, we offer a set of suggestions derived from our study that can be used as input to enhance future experiments in self-adaptive systems. © 2021 IEEE.",evaluation; mapping study; self adaptive systems,"59, 70",,"Proceedings - 2021 International Symposium on Software Engineering for Adaptive and Self-Managing Systems, SEAMS 2021",Conference Paper,Scopus
271,,Studying the usage of text-to-text transfer transformer to support code-related tasks,"Mastropaolo A., Scalabrino S., Cooper N., Nader Palacio D., Poshyvanyk D., Oliveto R., Bavota G.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113421369&doi=10.1109%2fICSE43902.2021.00041&partnerID=40&md5=8a53279ede6493be3e9eb948dc66a86d,10.1109/ICSE43902.2021.00041,"Deep learning (DL) techniques are gaining more and more attention in the software engineering community. They have been used to support several code-related tasks, such as automatic bug fixing and code comments generation. Recent studies in the Natural Language Processing (NLP) field have shown that the Text-To-Text Transfer Transformer (T5) architecture can achieve state-of-the-art performance for a variety of NLP tasks. The basic idea behind T5 is to first pre-train a model on a large and generic dataset using a self-supervised task (e.g., filling masked words in sentences). Once the model is pre-trained, it is fine-tuned on smaller and specialized datasets, each one related to a specific task (e.g., language translation, sentence classification). In this paper, we empirically investigate how the T5 model performs when pre-trained and fine-tuned to support code-related tasks. We pre-train a T5 model on a dataset composed of natural language English text and source code. Then, we fine-tune such a model by reusing datasets used in four previous works that used DL techniques to: (i) fix bugs, (ii) inject code mutants, (iii) generate assert statements, and (iv) generate code comments. We compared the performance of this single model with the results reported in the four original papers proposing DL-based solutions for those four tasks. We show that our T5 model, exploiting additional data for the self-supervised pre-training phase, can achieve performance improvements over the four baselines. © 2021 IEEE.",Deep Learning; Empirical software engineering,"336, 347",,Proceedings - International Conference on Software Engineering,Conference Paper,Scopus
272,,API2Com: On the Improvement of Automatically Generated Code Comments Using API Documentations,"Shahbazi R., Sharma R., Fard F.H.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113228933&doi=10.1109%2fICPC52881.2021.00049&partnerID=40&md5=b334642f10c29db35095ca007f9ab29f,10.1109/ICPC52881.2021.00049,"Code comments can help in program comprehension and are considered as important artifacts to help developers in software maintenance. However, the comments are mostly missing or are outdated, specially in complex software projects. As a result, several automatic comment generation models are developed as a solution. The recent models explore the integration of external knowledge resources such as Unified Modeling Language class diagrams to improve the generated comments. In this paper, we propose API2Com, a model that leverages the Application Programming Interface Documentations (API Docs) as a knowledge resource for comment generation. The API Docs include the description of the methods in more details and therefore, can provide better context in the generated comments. The API Docs are used along with the code snippets and Abstract Syntax Trees in our model.We apply the model on a large Java dataset of over 130, 000 methods and evaluate it using both Transformer and RNN-base architectures. Interestingly, when API Docs are used, the performance increase is negligible. We therefore run different experiments to reason about the results. For methods that only contain one API, adding API Docs improves the results by 4% BLEU score on average (BLEU score is an automatic evaluation metric used in machine translation). However, as the number of APIs that are used in a method increases, the performance of the model in generating comments decreases due to long documentations used in the input. Our results confirm that the API Docs can be useful in generating better comments, but, new techniques are required to identify the most informative ones in a method rather than using all documentations simultaneously. © 2021 IEEE.",API documentation; Code comment generation; External knowledge source,"411, 421",,IEEE International Conference on Program Comprehension,Conference Paper,Scopus
273,,Engineering an intelligent essay scoring and feedback system: An experience report,"Chadda A., Song K., Chandrasekar R., Gorton I.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113213778&doi=10.1109%2fWAIN52551.2021.00029&partnerID=40&md5=40ef5d26dcd3a53834e56ff4d5404eee,10.1109/WAIN52551.2021.00029,"Artificial Intelligence (AI)/Machine Learning (ML)-based systems are widely sought-after commercial solutions that can automate and augment core business services. Intelligent systems can improve the quality of services offered and support scalability through automation. In this paper we describe our experience in engineering an exploratory system for assessing the quality of essays supplied by customers of a specialized recruitment support service. The problem domain is challenging because the open-ended customer-supplied source text has considerable scope for ambiguity and error, making models for analysis hard to build. There is also a need to incorporate specialized business domain knowledge into the intelligent processing systems. To address these challenges, we experimented with and exploited a number of cloud-based machine learning models and composed them into an application-specific processing pipeline. This design allows for modification of the underlying algorithms as more data and improved techniques become available. We describe our design, and the main challenges we faced, namely keeping a check on the quality control of the models, testing the software and deploying the computationally expensive ML models on the cloud. © 2021 IEEE.",cloud-based systems; essay grading; machine learning; software architecture; software engineering,"141, 144",,"Proceedings - 2021 IEEE/ACM 1st Workshop on AI Engineering - Software Engineering for AI, WAIN 2021",Conference Paper,Scopus
274,,Emotions in Computer Vision Service QA,"Cummaudo A., Graetsch U.M., Curumsing M.K., Vasa R., Barnett S., Grundy J.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112863573&doi=10.1109%2fSEmotion52567.2021.00011&partnerID=40&md5=1ca98f10116a190c60bb7418d3f9f333,10.1109/SEmotion52567.2021.00011,"Software developers are increasingly using cloud-based services that provide machine learning capabilities to implement 'intelligent' features. Studies show that incorporating machine learning into an application increases technical debt, creates data dependencies, and introduces uncertainty due to their non-deterministic behaviour. We know very little about the emotional state of software developers who have to deal with such issues; and the impacts on productivity. This paper presents a preliminary effort to better understand the emotions of developers when experiencing issues with these services with the wider goal of discovering potential service improvements. We conducted a landscape analysis of emotions found in 1,425 Stack Overflow questions about a specific and mature subset of these cloud-based services, namely those that provide computer vision techniques. To speed up the emotion identification process, we trialled an automatic approach using a pre-trained emotion classifier that was specifically trained on Stack Overflow content, EmoTxt, and manually verified its classification results. We found that the identified emotions vary for different types of questions, and a discrepancy exists between automatic and manual emotion analysis due to subjectivity. © 2021 IEEE.",computer vision services; DevX; emotion mining; empirical study; stack overflow,"13, 18",,"Proceedings - 2021 IEEE/ACM 6th International Workshop on Emotion Awareness in Software Engineering, SEmotion 2021",Conference Paper,Scopus
275,,An Evolutionary Approach to Adapt Tests across Mobile Apps,"Mariani L., Pezze M., Terragni V., Zuddas D.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111442439&doi=10.1109%2fAST52587.2021.00016&partnerID=40&md5=094d03fc27d34d105d18f07050d7e360,10.1109/AST52587.2021.00016,"Automatic generators of GUI tests often fail to generate semantically relevant test cases, and thus miss important test scenarios. To address this issue, test adaptation techniques can be used to automatically generate semantically meaningful GUI tests from test cases of applications with similar functionalities.In this paper, we present ADAPTDROID, a technique that approaches the test adaptation problem as a search-problem, and uses evolutionary testing to adapt GUI tests (including oracles) across similar Android apps. In our evaluation with 32 popular Android apps, ADAPTDROID successfully adapted semantically relevant test cases in 11 out of 20 cross-app adaptation scenarios. © 2021 IEEE.",Android applications; GUI testing; search-based testing; test and oracle generation; test reuse,"70, 79",,"Proceedings - 2021 IEEE/ACM International Conference on Automation of Software Test, AST 2021",Conference Paper,Scopus
276,,Applying CodeBERT for Automated Program Repair of Java Simple Bugs,"Mashhadi E., Hemmati H.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111437373&doi=10.1109%2fMSR52588.2021.00063&partnerID=40&md5=c6e150df7369d12c35f368b39f58b0f2,10.1109/MSR52588.2021.00063,"Software debugging, and program repair are among the most time-consuming and labor-intensive tasks in software engineering that would benefit a lot from automation. In this paper, we propose a novel automated program repair approach based on CodeBERT, which is a transformer-based neural architecture pre-trained on large corpus of source code. We fine-tune our model on the ManySStuBs4J small and large datasets to automatically generate the fix codes. The results show that our technique accurately predicts the fixed codes implemented by the developers in 19-72% of the cases, depending on the type of datasets, in less than a second per bug. We also observe that our method can generate varied-length fixes (short and long) and can fix different types of bugs, even if only a few instances of those types of bugs exists in the training dataset. © 2021 IEEE.",CodeBERT; Deep learning; Program repair; Sequence to sequence learning; Transformers,"505, 509",,"Proceedings - 2021 IEEE/ACM 18th International Conference on Mining Software Repositories, MSR 2021",Conference Paper,Scopus
277,,CURE: Code-aware neural machine translation for automatic program repair,"Jiang N., Lutellier T., Tan L.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111217014&doi=10.1109%2fICSE43902.2021.00107&partnerID=40&md5=e44af390504ff6b73e5c87471771ff57,10.1109/ICSE43902.2021.00107,"Automatic program repair (APR) is crucial to improve software reliability. Recently, neural machine translation (NMT) techniques have been used to automatically fix software bugs. While promising, these approaches have two major limitations. Their search space often does not contain the correct fix, and their search strategy ignores software knowledge such as strict code syntax. Due to these limitations, existing NMT-based techniques underperform the best template-based approaches. We propose CURE, a new NMT-based APR technique with three major novelties. First, CURE pre-trains a programming language (PL) model on a large software codebase to learn developer-like source code before the APR task. Second, CURE designs a new code-aware search strategy that finds more correct fixes by focusing on searching for compilable patches and patches that are close in length to the buggy code. Finally, CURE uses a subword tokenization technique to generate a smaller search space that contains more correct fixes. Our evaluation on two widely-used benchmarks shows that CURE correctly fixes 57 Defects4J bugs and 26 QuixBugs bugs, outperforming all existing APR techniques on both benchmarks. © 2021 IEEE.",Automatic program repair; Software reliability,"1161, 1173",,Proceedings - International Conference on Software Engineering,Conference Paper,Scopus
278,,Automatic part-of-speech tagging for security vulnerability descriptions,"Yitagesu S., Zhang X., Feng Z., Li X., Xing Z.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111207925&doi=10.1109%2fMSR52588.2021.00016&partnerID=40&md5=a07255bcefa7b50696528f72ed76dd01,10.1109/MSR52588.2021.00016,"In this paper, we study the problem of part-of-speech (POS) tagging for security vulnerability descriptions (SVD). In contrast to newswire articles, SVD often contains a high-level natural language description of the text composed of mixed language studded with codes, domain-specific jargon, vague language, and abbreviations. Moreover, training data dedicated to security vulnerability research is not widely available. Existing neural network-based POS tagging has often relied on manually annotated training data or applying natural language processing (NLP) techniques, suffering from two significant drawbacks. The former is extremely time-consuming and requires labor-intensive feature engineering and expertise. The latter is inadequate to identify linguistically-informed words specific to the SVD domain. In this paper, we propose an automatic approach to assign POS tags to tokens in SVD. Our approach uses the character-level representation to automatically extract orthographic features and unsupervised word embeddings to capture meaningful syntactic and semantic regularities from SVD. The character level representations are then concatenated with the word embedding as a combined feature, which is then learned and used to predict the POS tagging. To deal with the issue of the poor availability of annotated security vulnerability data, we implement a finetuning approach. Our approach provides public access to a POS annotated corpus of ~8M tokens, which serves as a training dataset in this domain. Our evaluation results show a significant improvement in accuracy (17.72%-28.22%) of POS tagging in SVD over the current approaches. © 2021 IEEE.",Fine-Tuning; Part-of-Speech tagging; Security vulnerability descriptions; Unsupervised word embedding,"29, 40",,"Proceedings - 2021 IEEE/ACM 18th International Conference on Mining Software Repositories, MSR 2021",Conference Paper,Scopus
279,,Robustness of on-Device Models: Adversarial Attack to Deep Learning Models on Android Apps,"Huang Y., Hu H., Chen C.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111207102&doi=10.1109%2fICSE-SEIP52600.2021.00019&partnerID=40&md5=0a1aa53cdbd33105fe80e832111040e0,10.1109/ICSE-SEIP52600.2021.00019,"Deep learning has shown its power in many applications, including object detection in images, natural-language understanding, and speech recognition. To make it more accessible to end users, many deep learning models are now embedded in mobile apps. Compared to offloading deep learning from smartphones to the cloud, performing machine learning on-device can help improve latency, connectivity, and power consumption. However, most deep learning models within Android apps can easily be obtained via mature reverse engineering, while the models' exposure may invite adversarial attacks. In this study, we propose a simple but effective approach to hacking deep learning models using adversarial attacks by identifying highly similar pre-trained models from TensorFlow Hub. All 10 real-world Android apps in the experiment are successfully attacked by our approach. Apart from the feasibility of the model attack, we also carry out an empirical study that investigates the characteristics of deep learning models used by hundreds of Android apps on Google Play. The results show that many of them are similar to each other and widely use fine-tuning techniques to pre-trained models on the Internet. © 2021 IEEE.",Adversarial attack; Android; Deep learning; Mobile apps; Security,"101, 110",,Proceedings - International Conference on Software Engineering,Conference Paper,Scopus
280,,Neural knowledge extraction from cloud service incidents,"Shetty M., Bansal C., Kumar S., Rao N., Nagappan N., Zimmermann T.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111193341&doi=10.1109%2fICSE-SEIP52600.2021.00031&partnerID=40&md5=1d64aa67dc95dc75be771e6860429452,10.1109/ICSE-SEIP52600.2021.00031,"The move from boxed products to services and the widespread adoption of cloud computing has had a huge impact on the software development life cycle and DevOps processes. Particularly, incident management has become critical for developing and operating large-scale services. Prior work on incident management has heavily focused on the challenges with incident triaging and de-duplication. In this work, we address the fundamental problem of structured knowledge extraction from service incidents. We have built SoftNER, a framework for unsupervised knowledge extraction from service incidents. We frame the knowledge extraction problem as a Named-Entity Recognition task for extracting factual information. SoftNER leverages structural patterns like key value pairs and tables for bootstrapping the training data. Further, we build a novel multitask learning based BiLSTM-CRF model which leverages not just the semantic context but also the data-types for named-entity extraction. We have deployed SoftNER at Microsoft, a major cloud service provider and have evaluated it on more than 2 months of cloud incidents. We show that the unsupervised machine learning pipeline has a high precision of 0.96. Our multi-task learning based deep learning model also outperforms the state of the art NER models. Lastly, using the knowledge extracted by SoftNER we are able to build significantly more accurate models for important downstream tasks like incident triaging. © 2021 IEEE.",,"218, 227",,Proceedings - International Conference on Software Engineering,Conference Paper,Scopus
281,,Shallow or Deep? An Empirical Study on Detecting Vulnerabilities using Deep Learning,"Mazuera-Rozo A., Mojica-Hanke A., Linares-Vasquez M., Bavota G.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111186829&doi=10.1109%2fICPC52881.2021.00034&partnerID=40&md5=4551e3fac5d358c7abbe74de9fa4ea00,10.1109/ICPC52881.2021.00034,"Deep learning (DL) techniques are on the rise in the software engineering research community. More and more approaches have been developed on top of DL models, also due to the unprecedented amount of software-related data that can be used to train these models. One of the recent applications of DL in the software engineering domain concerns the automatic detection of software vulnerabilities. While several DL models have been developed to approach this problem, there is still limited empirical evidence concerning their actual effectiveness especially when compared with shallow machine learning techniques. In this paper, we partially fill this gap by presenting a large-scale empirical study using three vulnerability datasets and five different source code representations (i.e., the format in which the code is provided to the classifiers to assess whether it is vulnerable or not) to compare the effectiveness of two widely used DL-based models and of one shallow machine learning model in (i) classifying code functions as vulnerable or non-vulnerable (i.e., binary classification), and (ii) classifying code functions based on the specific type of vulnerability they contain (or ""clean"", if no vulnerability is there). As a baseline we include in our study the AutoML utility provided by the Google Cloud Platform. Our results show that the experimented models are still far from ensuring reliable vulnerability detection, and that a shallow learning classifier represents a competitive baseline for the newest DL-based models. © 2021 IEEE.",empirical study; Vulnerability detection,"276, 287",,IEEE International Conference on Program Comprehension,Conference Paper,Scopus
282,,Improving Code Summarization with Block-wise Abstract Syntax Tree Splitting,"Lin C., Ouyang Z., Zhuang J., Chen J., Li H., Wu R.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110910127&doi=10.1109%2fICPC52881.2021.00026&partnerID=40&md5=42e751acb3b3790dfc8a7ba770e9165f,10.1109/ICPC52881.2021.00026,"Automatic code summarization frees software developers from the heavy burden of manual commenting and benefits software development and maintenance. Abstract Syntax Tree (AST), which depicts the source code's syntactic structure, has been incorporated to guide the generation of code summaries. However, existing AST based methods suffer from the difficulty of training and generate inadequate code summaries. In this paper, we present the Block-wise Abstract Syntax Tree Splitting method (BASTS for short), which fully utilizes the rich tree-form syntax structure in ASTs, for improving code summarization. BASTS splits the code of a method based on the blocks in the dominator tree of the Control Flow Graph, and generates a split AST for each code split. Each split AST is then modeled by a Tree-LSTM using a pre-Training strategy to capture local non-linear syntax encoding. The learned syntax encoding is combined with code encoding, and fed into Transformer to generate high-quality code summaries. Comprehensive experiments on benchmarks have demonstrated that BASTS significantly outperforms state-of-The-Art approaches in terms of various evaluation metrics. To facilitate reproducibility, our implementation is available at https://github.com/XMUDM/BASTS. © 2021 IEEE.",Abstract Syntax Tree; Code Splitting; Code Summarization,"184, 195",,IEEE International Conference on Program Comprehension,Conference Paper,Scopus
283,,Exploiting Method Names to Improve Code Summarization: A Deliberation Multi-Task Learning Approach,"Xie R., Ye W., Sun J., Zhang S.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110853507&doi=10.1109%2fICPC52881.2021.00022&partnerID=40&md5=fee886b95533ea15b17c670c0293319d,10.1109/ICPC52881.2021.00022,"Code summaries are brief natural language descriptions of source code pieces. The main purpose of code summarization is to assist developers in understanding code and to reduce documentation workload. In this paper, we design a novel multi-Task learning (MTL) approach for code summarization through mining the relationship between method code summaries and method names. More specifically, since a method's name can be considered as a shorter version of its code summary, we first introduce the tasks of generation and informativeness prediction of method names as two auxiliary training objectives for code summarization. A novel two-pass deliberation mechanism is then incorporated into our MTL architecture to generate more consistent intermediate states fed into a summary decoder, especially when informative method names do not exist. To evaluate our deliberation MTL approach, we carried out a large-scale experiment on two existing datasets for Java and Python. The experiment results show that our technique can be easily applied to many state-of-The-Art neural models for code summarization and improve their performance. Meanwhile, our approach shows significant superiority when generating summaries for methods with non-informative names. © 2021 IEEE.",code summarization; deliberation network; method name prediction; multi-Task learning,"138, 148",,IEEE International Conference on Program Comprehension,Conference Paper,Scopus
284,,InferCode: Self-supervised learning of code representations by predicting subtrees,"Bui N.D.Q., Yu Y., Jiang L.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109572803&doi=10.1109%2fICSE43902.2021.00109&partnerID=40&md5=10fa54b0ff11a47d7fb00cdf6fe83245,10.1109/ICSE43902.2021.00109,"Learning code representations has found many uses in software engineering, such as code classification, code search, comment generation, and bug prediction, etc. Although representations of code in tokens, syntax trees, dependency graphs, paths in trees, or the combinations of their variants have been proposed, existing learning techniques have a major limitation that these models are often trained on datasets labeled for specific downstream tasks, and as such the code representations may not be suitable for other tasks. Even though some techniques generate representations from unlabeled code, they are far from being satisfactory when applied to the downstream tasks. To overcome the limitation, this paper proposes InferCode, which adapts the self-supervised learning idea from natural language processing to the abstract syntax trees (ASTs) of code. The novelty lies in the training of code representations by predicting subtrees automatically identified from the contexts of ASTs. With InferCode, subtrees in ASTs are treated as the labels for training the code representations without any human labelling effort or the overhead of expensive graph construction, and the trained representations are no longer tied to any specific downstream tasks or code units. We have trained an instance of InferCode model using Tree-Based Convolutional Neural Network (TBCNN) as the encoder of a large set of Java code. This pre-trained model can then be applied to downstream unsupervised tasks such as code clustering, code clone detection, cross-language code search, or be reused under a transfer learning scheme to continue training the model weights for supervised tasks such as code classification and method name prediction. Compared to prior techniques applied to the same downstream tasks, such as code2vec, code2seq, ASTNN, using our pre-trained InferCode model higher performance is achieved with a significant margin for most of the tasks, including those involving different programming languages. The implementation of InferCode and the trained embeddings are available at the link: https://github.com/bdqnghi/infercode. © 2021 IEEE.",Code clone detection; Code retrieval; Code search; Cross language; Fine tuning; Self supervised; Unlabel data; Unlabelled data,"1186, 1197",,Proceedings - International Conference on Software Engineering,Conference Paper,Scopus
285,,Early life cycle software defect prediction. Why? How?,"Shrikanth N.C., Majumder S., Menzies T.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108134539&doi=10.1109%2fICSE43902.2021.00050&partnerID=40&md5=6835e8b42f1a50e2e90f247587a98338,10.1109/ICSE43902.2021.00050,"Many researchers assume that, for software analytics, 'more data is better.' We write to show that, at least for learning defect predictors, this may not be true. To demonstrate this, we analyzed hundreds of popular GitHub projects. These projects ran for 84 months and contained 3,728 commits (median values). Across these projects, most of the defects occur very early in their life cycle. Hence, defect predictors learned from the first 150 commits and four months perform just as well as anything else. This means that, at least for the projects studied here, after the first few months, we need not continually update our defect prediction models. We hope these results inspire other researchers to adopt a 'simplicity-first' approach to their work. Some domains require a complex and data-hungry analysis. But before assuming complexity, it is prudent to check the raw data looking for 'short cuts' that can simplify the analysis. © 2021 IEEE.",Analytics; Defect prediction; Early; Sampling,"448, 459",,Proceedings - International Conference on Software Engineering,Conference Paper,Scopus
286,,Spark NLP: Natural Language Understanding at Scale[Formula presented],"Kocaman V., Talby D.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107671084&doi=10.1016%2fj.simpa.2021.100058&partnerID=40&md5=030df1b13a8abac599bcb13c8a389411,10.1016/j.simpa.2021.100058,"Spark NLP is a Natural Language Processing (NLP) library built on top of Apache Spark ML. It provides simple, performant & accurate NLP annotations for machine learning pipelines that can scale easily in a distributed environment. Spark NLP comes with 1100+ pretrained pipelines and models in more than 192+ languages. It supports nearly all the NLP tasks and modules that can be used seamlessly in a cluster. Downloaded more than 2.7 million times and experiencing 9x growth since January 2020, Spark NLP is used by 54% of healthcare organizations as the world's most widely used NLP library in the enterprise. © 2021 The Author(s)",Cluster; Deep learning; Natural language processing; Spark; Tensorflow,,,Software Impacts,Article,Scopus
287,,Code prediction by feeding trees to transformers,"Kim S., Zhao J., Tian Y., Chandra S.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106620688&doi=10.1109%2fICSE43902.2021.00026&partnerID=40&md5=56b1591370bf8a627704491d14b2e049,10.1109/ICSE43902.2021.00026,"Code prediction, more specifically autocomplete, has become an essential feature in modern IDEs. Autocomplete is more effective when the desired next token is at (or close to) the top of the list of potential completions offered by the IDE at cursor position. This is where the strength of the underlying machine learning system that produces a ranked order of potential completions comes into play. We advance the state-of-the-art in the accuracy of code prediction (next token prediction) used in autocomplete systems. Our work uses Transformers as the base neural architecture. We show that by making the Transformer architecture aware of the syntactic structure of code, we increase the margin by which a Transformer-based system outperforms previous systems. With this, it outperforms the accuracy of several state-of-the-art next token prediction systems by margins ranging from 14% to 18%. We present in the paper several ways of communicating the code structure to the Transformer, which is fundamentally built for processing sequence data. We provide a comprehensive experimental evaluation of our proposal, along with alternative design choices, on a standard Python dataset, as well as on Facebook internal Python corpus. Our code and data preparation pipeline will be available in open source. © 2021 IEEE.",Autocomplete; Code embedding; Code prediction,"150, 162",,Proceedings - International Conference on Software Engineering,Conference Paper,Scopus
288,,Automatic API Usage Scenario Documentation from Technical Q&A Sites,"Uddin G., Khomh F., Roy C.K.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105705200&doi=10.1145%2f3439769&partnerID=40&md5=6d954767e952c73c6ecf28b4474e6ee8,10.1145/3439769,"The online technical Q&A site Stack Overflow (SO) is popular among developers to support their coding and diverse development needs. To address shortcomings in API official documentation resources, several research works have thus focused on augmenting official API documentation with insights (e.g., code examples) from SO. The techniques propose to add code examples/insights about APIs into its official documentation. Recently, surveys of software developers find that developers in SO consider the combination of code examples and reviews about APIs as a form of API documentation, and that they consider such a combination to be more useful than official API documentation when the official resources can be incomplete, ambiguous, incorrect, and outdated. Reviews are opinionated sentences with positive/negative sentiments. However, we are aware of no previous research that attempts to automatically produce API documentation from SO by considering both API code examples and reviews. In this article, we present two novel algorithms that can be used to automatically produce API documentation from SO by combining code examples and reviews towards those examples. The first algorithm is called statistical documentation, which shows the distribution of positivity and negativity around the code examples of an API using different metrics (e.g., star ratings). The second algorithm is called concept-based documentation, which clusters similar and conceptually relevant usage scenarios. An API usage scenario contains a code example, a textual description of the underlying task addressed by the code example, and the reviews (i.e., opinions with positive and negative sentiments) from other developers towards the code example. We deployed the algorithms in Opiner, a web-based platform to aggregate information about APIs from online forums. We evaluated the algorithms by mining all Java JSON-based posts in SO and by conducting three user studies based on produced documentation from the posts. The first study is a survey, where we asked the participants to compare our proposed algorithms against a Javadoc-syle documentation format (called as Type-based documentation in Opiner). The participants were asked to compare along four development scenarios (e.g., selection, documentation). The participants preferred our proposed two algorithms over type-based documentation. In our second user study, we asked the participants to complete four coding tasks using Opiner and the API official and informal documentation resources. The participants were more effective and accurate while using Opiner. In a subsequent survey, more than 80% of participants asked the Opiner documentation platform to be integrated into the formal API documentation to complement and improve the API official documentation. © 2021 ACM.",API; crowd-sourced developer forum; documentation; usage scenario,,,ACM Transactions on Software Engineering and Methodology,Article,Scopus
289,,Overview and Prospect of Deep Learning for Image Segmentation in Digital Pathology [基于深度学习的数字病理图像分割综述与展望],"Song J., Xiao L., Lian Z.-C., Cai Z.-Y., Jiang G.-P.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105584671&doi=10.13328%2fj.cnki.jos.006205&partnerID=40&md5=f455c2a29ff822c8e759a77b560016f1,10.13328/j.cnki.jos.006205,"The quantitative analysis of digital pathology images plays a significant role in the diagnosis of benign and malignant diseases such as breast cancer and prostate cancer, in which the morphology measurements of histologic primitives serve as a basis of quantitative analyses. However, the complex nature of digital pathology data, such as diversity, present significant challenges for such segmentation task, which might lead to difficulties in feature extraction and instance segmentation. By converting complex pathology data into minable image features using artificial intelligence assisted pathologist's analysis, it becomes possible to automatically extract quantitative information of individual primitives. Machine learning algorithms, in particular deep models, are emerging as leading tools in quantitative analyses of digital pathology. It has exhibited great power in feature learning with producing improved accuracy of various tasks. This survey provides a comprehensive review of this fast-growing field. Popular deep models are briefly introduced, including convolutional neural networks, fully convolutional networks, encoder-decoder architectures, recurrent neural networks, and generative adversarial networks, and current deep learning achievements in various tasks are summarized, such as detection and segmentation. This study also presents the mathematical theory, key steps, main advantages and disadvantages, and performance analysis of deep learning algorithms, and interprets their formulations or modelings for specific tasks. In addition, the open challenges and potential trends of future research are discussed in pathology image segmentation using deep learning. © Copyright 2021, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Deep model; Digital pathology; Feature representation learning; Histologic primitive; Instance segmentation,"1427, 1460",,Ruan Jian Xue Bao/Journal of Software,Review,Scopus
290,,Survey on Trajectory Representation Learning Techniques [轨迹表示学习技术研究进展],"Cao H.-L., Tang H.-N., Wang F., Xu Y.-J.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105569039&doi=10.13328%2fj.cnki.jos.006210&partnerID=40&md5=747d2990a44f83c8597fdb929a9bb6b0,10.13328/j.cnki.jos.006210,"The rapid development of location-aware applications and services poses new challenges for trajectory data mining. The raw trajectory data usually consist of ordered sequences of coordinate-timestamp tuple, while many algorithms widely used for data analysis require input data to be in vector space. Therefore, it is an important and necessary step to effectively represent trajectory data from variable-length coordinate-timestamp sequence to a fixed-length vector that maintains the spatial-temporal characteristics of the movement. Most conventional trajectory representation methods are based on feature engineering, in which trajectory representation is usually considered as part of the data preprocessing. With the prevalence of deep learning, the ability of learning from large-scale data endows deep learning-based methods for trajectory representation with more potential and vitality, which achieved better performance compared to traditional methods. This paper provides a comprehensive review of recent progress in trajectory representation and summarizes the trajectory representation methods into two categories according to the different scales: trajectory unit representation and entire trajectory representation. In each category, the methods of different principles are compared and analyzed. Among them, the methods based on trajectory point are emphasized, and also the widely used methods based on neural networks are systematically classified. Besides, the applications related to trajectory representation under each category are introduced. Finally, the future research directions are pointed out in the field of trajectory representation. © Copyright 2021, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Spatial-temporal data mining; Trajectory data mining; Trajectory representation,"1461, 1479",,Ruan Jian Xue Bao/Journal of Software,Review,Scopus
291,,Traceability transformed: Generating more accurate links with pre-trained BERT models,"Lin J., Liu Y., Zeng Q., Jiang M., Cleland-Huang J.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104575743&doi=10.1109%2fICSE43902.2021.00040&partnerID=40&md5=504f19e9250894774220c2fb6ab567c4,10.1109/ICSE43902.2021.00040,"Software traceability establishes and leverages associations between diverse development artifacts. Researchers have proposed the use of deep learning trace models to link natural language artifacts, such as requirements and issue descriptions, to source code; however, their effectiveness has been restricted by availability of labeled data and efficiency at runtime. In this study, we propose a novel framework called Trace BERT (T-BERT) to generate trace links between source code and natural language artifacts. To address data sparsity, we leverage a three-step training strategy to enable trace models to transfer knowledge from a closely related Software Engineering challenge, which has a rich dataset, to produce trace links with much higher accuracy than has previously been achieved. We then apply the T-BERT framework to recover links between issues and commits in Open Source Projects. We comparatively evaluated accuracy and efficiency of three BERT architectures. Results show that a Single-BERT architecture generated the most accurate links, while a Siamese-BERT architecture produced comparable results with significantly less execution time. Furthermore, by learning and transferring knowledge, all three models in the framework outperform classical IR trace models. On the three evaluated real-word OSS projects, the best T-BERT stably outperformed the VSM model with average improvements of 60.31% measured using Mean Average Precision (MAP). RNN severely underperformed on these projects due to insufficient training data, while T-BERT overcame this problem by using pretrained language models and transfer learning. © 2021 IEEE.",Deep learning; Langauge model; Software traceability,"324, 335",,Proceedings - International Conference on Software Engineering,Conference Paper,Scopus
292,,Restoring execution environments of jupyter notebooks,"Wang J., Li L., Zeller A.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104223345&doi=10.1109%2fICSE43902.2021.00144&partnerID=40&md5=d174bc48e6d00ea885bbdbbb4315cea4,10.1109/ICSE43902.2021.00144,"More than ninety percent of published Jupyternotebooks do not state dependencies on external packages. This makes them non-executable and thus hinders reproducibility of scientific results. We present SnifferDog, an approach that1) collects the APIs of Python packages and versions, creating a database of APIs; 2) analyzes notebooks to determine candidates for required packages and versions; and 3) checks which packages are required to make the notebook executable(and ideally, reproduce its stored results). In its evaluation, we show thatSnifferDogprecisely restores execution environments for the largest majority of notebooks, making them immediately executable for end users. © 2021 IEEE.",API; Environment; Jupyter Notebook; Python,"1622, 1633",,Proceedings - International Conference on Software Engineering,Conference Paper,Scopus
293,,Convolutional neural network based automatic screening tool for cardiovascular diseases using different intervals of ECG signals,"Dai H., Hwang H.-G., Tseng V.S.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102972408&doi=10.1016%2fj.cmpb.2021.106035&partnerID=40&md5=1075c075b09c30b1b7360440ff439dac,10.1016/j.cmpb.2021.106035,"Background and Objective: Automatic screening tools can be applied to detect cardiovascular diseases (CVDs), which are the leading cause of death worldwide. As an effective and non-invasive method, electrocardiogram (ECG) based approaches are widely used to identify CVDs. Hence, this paper proposes a deep convolutional neural network (CNN) to classify five CVDs using standard 12-lead ECG signals. Methods: The Physiobank (PTB) ECG database is used in this study. Firstly, ECG signals are segmented into different intervals (one-second, two-seconds and three-seconds), without any wave detection, and three datasets are obtained. Secondly, as an alternative to any complex preprocessing, durations of raw ECG signals have been considered as input with simple min-max normalization. Lastly, a ten-fold cross-validation method is employed for one-second ECG signals and also tested on other two datasets (two-seconds and three-seconds). Results: Comparing to the competing approaches, the proposed CNN acquires the highest performance, having an accuracy, sensitivity, and specificity of 99.59%, 99.04%, and 99.87%, respectively, with one-second ECG signals. The overall accuracy, sensitivity, and specificity obtained are 99.80%, 99.48%, and 99.93%, respectively, using two-seconds of signals with pre-trained proposed models. The accuracy, sensitivity, and specificity of segmented ECG tested by three-seconds signals are 99.84%, 99.52%, and 99.95%, respectively. Conclusion: The results of this study indicate that the proposed system accomplishes high performance and keeps the characterizations in brief with flexibility at the same time, which means that it has the potential for implementation in a practical, real-time medical environment. © 2021 Elsevier B.V.",Cardiovascular disease; Convolutional neural network; Deep learning; Electrocardiogram signals; Hypertrophic cardiomyopathy; Myocardial infarction,,,Computer Methods and Programs in Biomedicine,Article,Scopus
294,,Recurrent feature fusion learning for multi-modality pet-ct tumor segmentation,"Bi L., Fulham M., Li N., Liu Q., Song S., Dagan Feng D., Kim J.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102890275&doi=10.1016%2fj.cmpb.2021.106043&partnerID=40&md5=6aa8ef0ec932ae6575e83d25be70a436,10.1016/j.cmpb.2021.106043,"Background and objective: [18f]-fluorodeoxyglucose (fdg) positron emission tomography – computed tomography (pet-ct) is now the preferred imaging modality for staging many cancers. Pet images characterize tumoral glucose metabolism while ct depicts the complementary anatomical localization of the tumor. Automatic tumor segmentation is an important step in image analysis in computer aided diagnosis systems. Recently, fully convolutional networks (fcns), with their ability to leverage annotated datasets and extract image feature representations, have become the state-of-the-art in tumor segmentation. There are limited fcn based methods that support multi-modality images and current methods have primarily focused on the fusion of multi-modality image features at various stages, i.e., early-fusion where the multi-modality image features are fused prior to fcn, late-fusion with the resultant features fused and hyper-fusion where multi-modality image features are fused across multiple image feature scales. Early- and late-fusion methods, however, have inherent, limited freedom to fuse complementary multi-modality image features. The hyper-fusion methods learn different image features across different image feature scales that can result in inaccurate segmentations, in particular, in situations where the tumors have heterogeneous textures. Methods: we propose a recurrent fusion network (rfn), which consists of multiple recurrent fusion phases to progressively fuse the complementary multi-modality image features with intermediary segmentation results derived at individual recurrent fusion phases: (1) the recurrent fusion phases iteratively learn the image features and then refine the subsequent segmentation results; and, (2) the intermediary segmentation results allows our method to focus on learning the multi-modality image features around these intermediary segmentation results, which minimize the risk of inconsistent feature learning. Results: we evaluated our method on two pathologically proven non-small cell lung cancer pet-ct datasets. We compared our method to the commonly used fusion methods (early-fusion, late-fusion and hyper-fusion) and the state-of-the-art pet-ct tumor segmentation methods on various network backbones (resnet, densenet and 3d-unet). Our results show that the rfn provides more accurate segmentation compared to the existing methods and is generalizable to different datasets. Conclusions: we show that learning through multiple recurrent fusion phases allows the iterative re-use of multi-modality image features that refines tumor segmentation results. We also identify that our rfn produces consistent segmentation results across different network architectures. © 2021",Fully convolutional networks (fcns); Positron emission tomography–computed tomography (pet-ct); Segmentation,,,Computer Methods and Programs in Biomedicine,Article,Scopus
295,,Alzheimer's disease detection using depthwise separable convolutional neural networks,"Liu J., Li M., Luo Y., Yang S., Li W., Bi Y.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102264686&doi=10.1016%2fj.cmpb.2021.106032&partnerID=40&md5=4e67500f393071ff7ec711b6715b7574,10.1016/j.cmpb.2021.106032,"To diagnose Alzheimer's disease (AD), neuroimaging methods such as magnetic resonance imaging have been employed. Recent progress in computer vision with deep learning (DL) has further inspired research focused on machine learning algorithms. However, a few limitations of these algorithms, such as the requirement for large number of training images and the necessity for powerful computers, still hinder the extensive usage of AD diagnosis based on machine learning. In addition, large number of training parameters and heavy computation make the DL systems difficult in integrating with mobile embedded devices, for example the mobile phones. For AD detection using DL, most of the current research solely focused on improving the classification performance, while few studies have been done to obtain a more compact model with less complexity and relatively high recognition accuracy. In order to solve this problem and improve the efficiency of the DL algorithm, a deep separable convolutional neural network model is proposed for AD classification in this paper. The depthwise separable convolution (DSC) is used in this work to replace the conventional convolution. Compared to the traditional neural networks, the parameters and computing cost of the proposed neural network are found greatly reduced. The parameters and computational costs of the proposed neural network are found to be significantly reduced compared with conventional neural networks. With its low power consumption, the proposed model is particularly suitable for embedding mobile devices. Experimental findings show that the DSC algorithm, based on the OASIS magnetic resonance imaging dataset, is very successful for AD detection. Moreover, transfer learning is employed in this work to improve model performance. Two trained models with complex networks, namely AlexNet and GoogLeNet, are used for transfer learning, with average classification rates of 91.40%, 93.02% and a less power consumption. © 2021 Elsevier B.V.",Alzheimer's disease; Deep learning; Depthwise separable convolution; Transfer learning,,,Computer Methods and Programs in Biomedicine,Article,Scopus
296,,White-box analysis over machine learning: Modeling performance of configurable systems,"Velez M., Jamshidi P., Siegmund N., Apel S., Kastner C.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101950822&doi=10.1109%2fICSE43902.2021.00100&partnerID=40&md5=0f05f1f4ed13923247bb77c8750779a1,10.1109/ICSE43902.2021.00100,"Performance-influence models can help stakeholders understand how and where configuration options and their interactions influence the performance of a system. With this understanding, stakeholders can debug performance behavior and make deliberate configuration decisions. Current black-box techniques to build such models combine various sampling and learning strategies, resulting in tradeoffs between measurement effort, accuracy, and interpretability. We present Comprex, a white-box approach to build performance-influence models for configurable systems, combining insights of local measurements, dynamic taint analysis to track options in the implementation, compositionality, and compression of the configuration space, without relying on machine learning to extrapolate incomplete samples. Our evaluation on 4 widely-used, open-source projects demonstrates that Comprex builds similarly accurate performance-influence models to the most accurate and expensive black-box approach, but at a reduced cost and with additional benefits from interpretable and local models. © 2021 IEEE.",Dynamic analysis; Performance influence modeling; Software configuration,"1072, 1084",,Proceedings - International Conference on Software Engineering,Conference Paper,Scopus
297,,Improving high-impact bug report prediction with combination of interactive machine learning and active learning,"Wu X., Zheng W., Chen X., Zhao Y., Yu T., Mu D.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099608821&doi=10.1016%2fj.infsof.2021.106530&partnerID=40&md5=8e73ebfc3c2c2eac5674687b8bef289f,10.1016/j.infsof.2021.106530,"Context: Bug reports record issues found during software development and maintenance. A high-impact bug report (HBR) describes an issue that can cause severe damage once occurred after deployment. Identifying HBRs from the bug repository as early as possible is crucial for guaranteeing software quality. Objective: In recent years, many machine learning-based approaches have been proposed for HBR prediction, and most of them are based on supervised machine learning. However, the assumption of supervised machine learning is that it needs a large number of labeled data, which is often difficult to gather in practice. Method: In this paper, we propose hbrPredictor, which combines interactive machine learning and active learning to HBR prediction. On the one hand, it can dramatically reduce the number of bug reports required for prediction model training; on the other hand, it improves the diversity and generalization ability of training samples via uncertainty sampling. Result: We take security bug report (SBR) prediction as an example of HBR prediction and perform a large-scale experimental evaluation on datasets from different open-source projects. The results show: (1) hbrPredictor substantially outperforms the two baselines and obtains the maximum values of F1-score (0.7939) and AUC (0.8789); (2) with the dynamic stop criteria, hbrPredictor could reach its best performance with only 45% and 13% of the total bug reports for small-sized datasets and large-sized datasets, respectively. Conclusion: By reducing the number of required training samples, hbrPredictor could substantially save the data labeling effort without decreasing the effectiveness of the model. © 2021 Elsevier B.V.",Active learning; High-impact bug report; Interactive machine learning; Security bug report prediction; Uncertainty-sampling,,,Information and Software Technology,Article,Scopus
298,,Visualizing Change in Agile Safety-Critical Systems,"Cleland-Huang J., Agrawal A., Vierhauser M., Mayr-Dorn C.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087057863&doi=10.1109%2fMS.2020.3000104&partnerID=40&md5=8dc9024c2891597df730f31c21ae843d,10.1109/MS.2020.3000104,"Many organizations working in safety-critical domains are experimenting with agile methods, while organizations experienced in agile development are increasingly building cyberphysical systems, such as factory floor robots, unmanned aerial systems, and medical devices, often without sufficient knowledge or the necessary techniques to adopt appropriate hazard analysis and safety-assurance practices. These two trends emerging from opposite ends of the process spectrum are pointing to a new way of developing safety-critical software, which embraces the rigor of safety-critical development environments while experiencing the benefits of more incremental, faster delivery cycles made possible by agile solutions. The importance of using traceability to manage change in safety-critical software development has been broadly recognized, but has not yet been adequately addressed within agile projects.",Agility; Safety-Critical Systems; Software Traceability; Visualization,"43, 51",,IEEE Software,Article,Scopus
299,,Combinatorial testing metrics for machine learning,"Lanus E., Freeman L.J., Richard Kuhn D., Kacker R.N.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108023784&doi=10.1109%2fICSTW52544.2021.00025&partnerID=40&md5=580acbd5ad524e2d0ed1921f968e881d,10.1109/ICSTW52544.2021.00025,"This paper defines a set difference metric for comparing machine learning (ML) datasets and proposes the difference between datasets be a function of combinatorial coverage. We illustrate its utility for evaluating and predicting performance of ML models. Identifying and measuring differences between datasets is of significant value for ML problems, where the accuracy of the model is heavily dependent on the degree to which training data are sufficiently representative of data encountered in application. The method is illustrated for transfer learning without retraining, the problem of predicting performance of a model trained on one dataset and applied to another. © 2021 IEEE.",Combinatorial testing; Machine learning; Operating envelopes; Test set selection; Transfer learning,"81, 84",,"Proceedings - 2021 IEEE 14th International Conference on Software Testing, Verification and Validation Workshops, ICSTW 2021",Conference Paper,Scopus
300,,AutoKG - An automotive domain knowledge graph for software testing: A position paper,"Kesri V., Nayak A., Ponnalagu K.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108021658&doi=10.1109%2fICSTW52544.2021.00047&partnerID=40&md5=1ca3f1540a1e7c0912803defe84e593d,10.1109/ICSTW52544.2021.00047,"Industries have a significant amount of data in semi-structured and unstructured formats which are typically captured in text documents, spreadsheets, images, etc. This is especially the case with the software description documents used by domain experts in the automotive domain to perform tasks at various phases of the Software Development Life Cycle (SDLC). In this paper, we propose an end-to-end pipeline to extract an Automotive Knowledge Graph (AutoKG) from textual data using Natural Language Processing (NLP) techniques with the application of automatic test case generation. The proposed pipeline primarily consists of the following components: 1) AutoOntology, an ontology that has been derived by analyzing several industry scale automotive domain software systems, 2) AutoRE, a Relation Extraction (RE) model to extract triplets from various sentence types typically found in the automotive domain, and 3) AutoVec, a neural embedding based algorithm for triplet matching and context-based search. We demonstrate the pipeline with an application of automatic test case generation from requirements using AutoKG. © 2021 IEEE.",Automotive Domain Knowledge Graph; Natural Language Processing; Software Testing,"234, 238",,"Proceedings - 2021 IEEE 14th International Conference on Software Testing, Verification and Validation Workshops, ICSTW 2021",Conference Paper,Scopus
301,,A Search-Based Testing Framework for Deep Neural Networks of Source Code Embedding,"Pour M.V., Li Z., Ma L., Hemmati H.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107971136&doi=10.1109%2fICST49551.2021.00016&partnerID=40&md5=44a3f74a2cd0c456ce25af58e81d0e54,10.1109/ICST49551.2021.00016,"Over the past few years, deep neural networks (DNNs) have been continuously expanding their real-world applications for source code processing tasks across the software engineering domain, e.g., clone detection, code search, comment generation. Although quite a few recent works have been performed on testing of DNNs in the context of image and speech processing, limited progress has been achieved so far on DNN testing in the context of source code processing, that exhibits rather unique characteristics and challenges.In this paper, we propose a search-based testing framework for DNNs of source code embedding and its downstream processing tasks like Code Search. To generate new test inputs, we adopt popular source code refactoring tools to generate the semantically equivalent variants. For more effective testing, we leverage the DNN mutation testing to guide the testing direction. To demonstrate the usefulness of our technique, we perform a large-scale evaluation on popular DNNs of source code processing based on multiple state-of-the-art code embedding methods (i.e., Code2vec, Code2seq and CodeBERT). The testing results show that our generated adversarial samples can on average reduce the performance of these DNNs from 5.41% to 9.58%. Through retraining the DNNs with our generated adversarial samples, the robustness of DNN can improve by 23.05% on average. The evaluation results also show that our adversarial test generation strategy has the least negative impact (median of 3.56%), on the performance of the DNNs for regular test data, compared to the other methods. © 2021 IEEE.",Deep Neural Network; Source Code Processing; Testing,"36, 46",,"Proceedings - 2021 IEEE 14th International Conference on Software Testing, Verification and Validation, ICST 2021",Conference Paper,Scopus
302,,A review of image analysis techniques for adult content detection: Child protection,"Appati J.K., Lodonu K.Y., Chris-Koka R.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105064494&doi=10.4018%2fIJSI.2021040106&partnerID=40&md5=0214b1efeff5d3155484868ce6d5b8ac,10.4018/IJSI.2021040106,"The fast growth of internet access globally without boundary has led to some negative impacts among children who are exposed to pornographic contents daily. Many parental control strategies have been put in place to protect these children; however, these strategies are usually inspired by political and social interventions. With the availability of computational tools, many automated explicit content detection methods though having their flaws have been proposed to support these social interventions. In this study, a review of the current automated adult content detectors is presented with open issues for future research work. © 2021 Taru Publications. All rights reserved.",Accuracy; Classification; Convolutional Neural Network; Deep Learning; Explicit Content; Feature Extraction; Global Features; Histogram Equalization; Pornography; Precision; Region of Interest,"102, 121",,International Journal of Software Innovation,Review,Scopus
303,,Multi-granularity Metamorphic Testing for Neural Machine Translation System [面向神经机器翻译系统的多粒度蜕变测试],"Zhong W.-K., Ge J.-D., Chen X., Li C.-Y., Tang Z., Luo B.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104664315&doi=10.13328%2fj.cnki.jos.006221&partnerID=40&md5=b8cd07779da197a90e263745d0c8495a,10.13328/j.cnki.jos.006221,"Machine translation task focuses on converting one natural language into another. In recent years, neural machine translation models based on sequence-to-sequence models have achieved better performance than traditional statistical machine translation models on multiple language pairs, and have been used by many translation service providers. Although the practical application of commercial translation system shows that the neural machine translation model has great improvement, how to systematically evaluate its translation quality is still a challenging task. On the one hand, if the translation effect is evaluated based on the reference text, the acquisition cost of high-quality reference text is very high. On the other hand, compared with the statistical machine translation model, the neural machine translation model has more significant robustness problems. However, there are no relevant studies on the robustness of the neural machine translation model. This study proposes a multi-granularity test framework MGMT based on metamorphic testing, which can evaluate the robustness of neural machine translation systems without reference translations. The testing framework first replaces the source sentence on sentence-granularity, phrase-granularity, and word-granularity respectively, then compares the translation results of the source sentence and the replaced sentences based on the constituency parse tree, and finally judges whether the result satisfies the metamorphic relationship. The experiments are conducted on multi-field Chinese-English translation datasets and six industrial neural machine translation systems are evaluated, and compared with same type of metamorphic testing and methods based on reference translations. The experimental results show that the proposed method MGMT is 80% and 20% higher than similar methods in terms of Pearson's correlation coefficient and Spearman's correlation coefficient respectively. This indicates that the non-reference translation evaluation method proposed in this study has a higher positive correlation with the reference translation based evaluation method, which verifies that MGMT's evaluation accuracy is significantly better than other methods of the same type. © Copyright 2021, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Machine translation; Metamorphic test; Multi-granularity; Neural network; Quality estimation,"1051, 1066",,Ruan Jian Xue Bao/Journal of Software,Article,Scopus
304,,Language Models Based on Deep Learning: A Review [基于深度学习的语言模型研究进展],"Wang N.-Y., Ye Y.-X., Liu L., Feng L.-Z., Bao T., Peng T.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104659186&doi=10.13328%2fj.cnki.jos.006169&partnerID=40&md5=1140f18e1f20ba3effc95a059aeff0d6,10.13328/j.cnki.jos.006169,"Language model, to express implicit knowledge of language, has been widely concerned as a basic problem of natural language processing in which the current research hotspot is the language model based on deep learning. Through pre-training and fine-tuning techniques, language models show their inherently power of representation, also improve the performance of downstream tasks greatly. Around the basic principles and different application directions, this study takes the neural probability language model and the pre-training language model as a pointcut for combining deep learning and natural language processing. The application as well as challenges of neural probability and pre-training model is introduced, which is based on the basic concepts and theories of language model. Then, the existing neural probability, pre-training language model include their methods are compared and analyzed. In addition, the training methods of pre-training language model are elaborated from two aspects of new training tasks and improved network structure. Meanwhile, the current research directions of pre-training model in scale compression, knowledge fusion, multi-modality, and cross-language are summarized and evaluated. Finally, the bottleneck of language model in natural language processing application is summed up, afterwards the possible future research priorities are prospected. © Copyright 2021, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Deep learning; Language model; Natural language processing; Neural language model; Pre-training,"1082, 1115",,Ruan Jian Xue Bao/Journal of Software,Review,Scopus
305,,Does non-COVID-19 lung lesion help? investigating transferability in COVID-19 CT image segmentation,"Wang Y., Zhang Y., Liu Y., Tian J., Zhong C., Shi Z., Zhang Y., He Z.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101723173&doi=10.1016%2fj.cmpb.2021.106004&partnerID=40&md5=7e3aedace27b0b2f7e3b495ad719e6b2,10.1016/j.cmpb.2021.106004,"Background and Objective: Coronavirus disease 2019 (COVID-19) is a highly contagious virus spreading all around the world. Deep learning has been adopted as an effective technique to aid COVID-19 detection and segmentation from computed tomography (CT) images. The major challenge lies in the inadequate public COVID-19 datasets. Recently, transfer learning has become a widely used technique that leverages the knowledge gained while solving one problem and applying it to a different but related problem. However, it remains unclear whether various non-COVID19 lung lesions could contribute to segmenting COVID-19 infection areas and how to better conduct this transfer procedure. This paper provides a way to understand the transferability of non-COVID19 lung lesions and a better strategy to train a robust deep learning model for COVID-19 infection segmentation. Methods: Based on a publicly available COVID-19 CT dataset and three public non-COVID19 datasets, we evaluate four transfer learning methods using 3D U-Net as a standard encoder-decoder method. i) We introduce the multi-task learning method to get a multi-lesion pre-trained model for COVID-19 infection. ii) We propose and compare four transfer learning strategies with various performance gains and training time costs. Our proposed Hybrid-encoder Learning strategy introduces a Dedicated-encoder and an Adapted-encoder to extract COVID-19 infection features and general lung lesion features, respectively. An attention-based Selective Fusion unit is designed for dynamic feature selection and aggregation. Results: Experiments show that trained with limited data, proposed Hybrid-encoder strategy based on multi-lesion pre-trained model achieves a mean DSC, NSD, Sensitivity, F1-score, Accuracy and MCC of 0.704, 0.735, 0.682, 0.707, 0.994 and 0.716, respectively, with better genetalization and lower over-fitting risks for segmenting COVID-19 infection. Conclusions: The results reveal the benefits of transferring knowledge from non-COVID19 lung lesions, and learning from multiple lung lesion datasets can extract more general features, leading to accurate and robust pre-trained models. We further show the capability of the encoder to learn feature representations of lung lesions, which improves segmentation accuracy and facilitates training convergence. In addition, our proposed Hybrid-encoder learning method incorporates transferred lung lesion features from non-COVID19 datasets effectively and achieves significant improvement. These findings promote new insights into transfer learning for COVID-19 CT image segmentation, which can also be further generalized to other medical tasks. © 2021 Elsevier B.V.",COVID-19; CT image; Segmentation; Transfer learning,,,Computer Methods and Programs in Biomedicine,Article,Scopus
306,,EEG based Major Depressive disorder and Bipolar disorder detection using Neural Networks:A review,"Yasin S., Hussain S.A., Aslan S., Raza I., Muzammel M., Othmani A.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101633674&doi=10.1016%2fj.cmpb.2021.106007&partnerID=40&md5=44f064252c1097f5f5074ca8c3c99bfe,10.1016/j.cmpb.2021.106007,"Mental disorders represent critical public health challenges as they are leading contributors to the global burden of disease and intensely influence social and financial welfare of individuals. The present comprehensive review concentrate on the two mental disorders: Major depressive Disorder (MDD) and Bipolar Disorder (BD) with noteworthy publications during the last ten years. There is a big need nowadays for phenotypic characterization of psychiatric disorders with biomarkers. Electroencephalography (EEG) signals could offer a rich signature for MDD and BD and then they could improve understanding of pathophysiological mechanisms underling these mental disorders. In this review, we focus on the literature works adopting neural networks fed by EEG signals. Among those studies using EEG and neural networks, we have discussed a variety of EEG based protocols, biomarkers and public datasets for depression and bipolar disorder detection. We conclude with a discussion and valuable recommendations that will help to improve the reliability of developed models and for more accurate and more deterministic computational intelligence based systems in psychiatry. This review will prove to be a structured and valuable initial point for the researchers working on depression and bipolar disorders recognition by using EEG signals. © 2021 Elsevier B.V.",Artificial neural networks; biomedical informatics; Bipolar disorder(BD); Electroencephalogram(EEG); Major Depressive disorder(MDD),,,Computer Methods and Programs in Biomedicine,Review,Scopus
307,,Mass Image Synthesis in Mammogram with Contextual Information Based on GANs,"Shen T., Hao K., Gou C., Wang F.-Y.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101397555&doi=10.1016%2fj.cmpb.2021.106019&partnerID=40&md5=0e402e9ee852b24756a94bf956dbb6d1,10.1016/j.cmpb.2021.106019,"Background and Objective: In medical imaging, the scarcity of labeled lesion data has hindered the application of many deep learning algorithms. To overcome this problem, the simulation of diverse lesions in medical images is proposed. However, synthesizing labeled mass images in mammograms is still challenging due to the lack of consistent patterns in shape, margin, and contextual information. Therefore, we aim to generate various labeled medical images based on contextual information in mammograms. Methods:In this paper, we propose a novel approach based on GANs to generate various mass images and then perform contextual infilling by inserting the synthetic lesions into healthy screening mammograms. Through incorporating features of both realistic mass images and corresponding masks into the adversarial learning scheme, the generator can not only learn the distribution of the real mass images but also capture the matching shape, margin and context information. Results:To demonstrate the effectiveness of our proposed method, we conduct experiments on publicly available mammogram database of DDSM and a private database provided by Nanfang Hospital in China. Qualitative and quantitative evaluations validate the effectiveness of our approach. Additionally, through the data augmentation by image generation of the proposed method, an improvement of 5.03% in detection rate can be achieved over the same model trained on original real lesion images. Conclusions:The results show that the data augmentation based on our method increases the diversity of dataset. Our method can be viewed as one of the first steps toward generating labeled breast mass images for precise detection and can be extended in other medical imaging domains to solve similar problems. © 2021",generative adversarial network; mammogram; mass detection; medical image synthesis,,,Computer Methods and Programs in Biomedicine,Article,Scopus
308,,On the quantitative effects of compression of retinal fundus images on morphometric vascular measurements in VAMPIRE,"Mookiah M.R.K., Hogg S., MacGillivray T., Trucco E., the INSPIRED project",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101350242&doi=10.1016%2fj.cmpb.2021.105969&partnerID=40&md5=313cb1b16bb28d51d4b564a634d8a4b8,10.1016/j.cmpb.2021.105969,"Background and Objectives: This paper reports a quantitative analysis of the effects of joint photographic experts group (JPEG) image compression of retinal fundus camera images on automatic vessel segmentation and on morphometric vascular measurements derived from it, including vessel width, tortuosity and fractal dimension. Methods: Measurements are computed with vascular assessment and measurement platform for images of the retina (VAMPIRE), a specialized software application adopted in many international studies on retinal biomarkers. For reproducibility, we use three public archives of fundus images (digital retinal images for vessel extraction (DRIVE), automated retinal image analyzer (ARIA), high-resolution fundus (HRF)). We generate compressed versions of original images in a range of representative levels. Results: We compare the resulting vessel segmentations with ground truth maps and morphological measurements of the vascular network with those obtained from the original (uncompressed) images. We assess the segmentation quality with sensitivity, specificity, accuracy, area under the curve and Dice coefficient. We assess the agreement between VAMPIRE measurements from compressed and uncompressed images with correlation, intra-class correlation and Bland-Altman analysis. Conclusions: Results suggest that VAMPIRE width-related measurements (central retinal artery equivalent (CRAE), central retinal vein equivalent (CRVE), arteriolar-venular width ratio (AVR)), the fractal dimension (FD) and arteriolar tortuosity have excellent agreement with those from the original images, remaining substantially stable even for strong loss of quality (20% of the original), suggesting the suitability of VAMPIRE in association studies with compressed images. © 2021 Elsevier B.V.",JPEG Compression; Retinal biomarkers; Retinal imaging; VAMPIRE,,,Computer Methods and Programs in Biomedicine,Article,Scopus
309,,SR-Net: A sequence offset fusion net and refine net for undersampled multislice MR image reconstruction,"Xiao Z., Du N., Liu J., Zhang W.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101301382&doi=10.1016%2fj.cmpb.2021.105997&partnerID=40&md5=f2ba88d4662db7650daf8be031879eee,10.1016/j.cmpb.2021.105997,"Background and objective: The study of deep learning-based fast magnetic resonance imaging (MRI) reconstruction methods has become popular in recent years. However, there is still a challenge when MRI results undersample large acceleration factors. The objective of this study was to improve the reconstruction quality of undersampled MR images by exploring data redundancy among slices. Methods: There are two aspects of redundancy in multislice MR images including correlations inside a single slice and correlations among slices. Thus, we built two subnets for the two kinds of redundancy. For correlations among slices, we built a bidirectional recurrent convolutional neural network, named Sequence Offset Fusion Net (S-Net). In S-Net, we used a deformable convolution module to construct a neighbor slice feature extractor. For the correlation inside a single slice, we built a Refine Net (R-Net), which has 5 layers of 2D convolutions. In addition, we used a data consistency (DC) operation to maintain data fidelity in k-space. Finally, we treated the reconstruction task as a dealiasing problem in the image domain, and S-Net and R-Net are applied alternately and iteratively to generate the final reconstructions. Results: The proposed algorithm was evaluated using two online public MRI datasets. Compared with several state-of-the-art methods, the proposed method achieved better reconstruction results in terms of dealiasing and restoring tissue structure. Moreover, with over 14 slices per second reconstruction speed on 256x256 pixel images, the proposed method can meet the need for real-time processing. Conclusion: With spatial correlation among slices as additional prior information, the proposed method dramatically improves the reconstruction quality of undersampled MR images. © 2021 Elsevier B.V.",Deep learning; Deformable convolution; Image reconstruction; Magnetic resonance imaging (MRI),,,Computer Methods and Programs in Biomedicine,Article,Scopus
310,,A deep learning model (ALNet) for the diagnosis of acute leukaemia lineage using peripheral blood cell images,"Boldú L., Merino A., Acevedo A., Molina A., Rodellar J.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101126315&doi=10.1016%2fj.cmpb.2021.105999&partnerID=40&md5=b67be8ec666376ac1eae578da403950d,10.1016/j.cmpb.2021.105999,"Background and objectives: Morphological differentiation among blasts circulating in blood in acute leukaemia is challenging. Artificial intelligence decision support systems hold substantial promise as part of clinical practise in detecting haematological malignancy. This study aims to develop a deep learning-based system to predict the diagnosis of acute leukaemia using blood cell images. Methods: A set of 731 blood smears containing 16,450 single-cell images was analysed from 100 healthy controls, 191 patients with viral infections and 148 with acute leukaemia. Training and testing sets were arranged with 85% and 15% of these smears, respectively. To find the best architecture for acute leukaemia classification VGG16, ResNet101, DenseNet121 and SENet154 were evaluated. Fine-tuning was implemented to these pre-trained CNNs to adapt their layers to our data. Once the best architecture was chosen, a system with two modules working sequentially was configured (ALNet). The first module recognised abnormal promyelocytes among other mononuclear blood cell images, such as lymphocytes, monocytes, reactive lymphocytes and blasts. The second distinguished if blasts were myeloid or lymphoid lineage. The final strategy was to predict patients’ initial diagnosis of acute leukaemia lineage using the blood smear review. ALNet was assessed with smears of the testing set. Results: ALNet provided the correct diagnostic prediction of all patients with promyelocytic and myeloid leukaemia. Sensitivity, specificity and precision values of 100%, 92.3% and 93.7%, respectively, were obtained for myeloid leukaemia. Regarding lymphoid leukaemia, a sensitivity of 89% and specificity and precision values of 100% were obtained. Conclusions: ALNet is a predictive model designed with two serially connected convolutional networks. It is proposed to assist clinical pathologists in the diagnosis of acute leukaemia during the blood smear review. It has been proved to distinguish neoplastic (leukaemia) and non-neoplastic (infections) diseases, as well as recognise the leukaemia lineage. © 2021 Elsevier B.V.",Blood cell automatic recognition; Convolutional neural networks; Deep learning; Leukemia; Morphological analysis,,,Computer Methods and Programs in Biomedicine,Article,Scopus
311,,Cerebrovascular segmentation from TOF-MRA based on multiple-U-net with focal loss function,"Guo X., Xiao R., Lu Y., Chen C., Yan F., Zhou K., He W., Wang Z.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101095158&doi=10.1016%2fj.cmpb.2021.105998&partnerID=40&md5=68460adeec675cede626f422b37cfa2f,10.1016/j.cmpb.2021.105998,"Background and Objective: Accurate cerebrovascular segmentation plays an important role in the diagnosis of cerebrovascular diseases. Considering the complexity and uncertainty of doctors' manual segmentation of cerebral vessels, this paper proposed an automatic segmentation algorithm based on Multiple-U-net (M-U-net) to segment cerebral vessel structures from the Time-of-flight Magnetic Resonance Angiography (TOF-MRA) data. Methods: First, the TOF-MRA data was normalized by volume and then divided into three groups through slices of axial, coronal and sagittal directions respectively. Three single U-nets were trained by divided dataset. To solve the problem of uneven distribution of positive and negative samples, the focal loss function was adopted in training. After obtaining the prediction results of three single U-nets, the voting feature fusion and the post-processing process based on connected domain analysis would be performed. 95 volumes of TOF-MRA provided by the MIDAS platform were applied to the experiment, among which 20 volumes were treated as the training dataset, 5 volumes were used as the validation dataset and the remaining 70 volumes were divided into 10 groups to test the trained model respectively. Results: Experiments showed that the proposed M-U-net based algorithm achieved 88.60% and 87.93% Dice Similarity Coefficient (DSC) on the verification dataset and testing dataset, which performed better than any single U-net. Conclusions: Compared with other existing algorithms, our algorithm reached the state of the art level. The feature fusion of three single U-nets could effectively complement the segmentation results. © 2021",Cerebrovascular segmentation; Feature fusion; Focal loss function; U-net,,,Computer Methods and Programs in Biomedicine,Article,Scopus
312,,BloodCaps: A capsule network based model for the multiclassification of human peripheral blood cells,"Long F., Peng J.-J., Song W., Xia X., Sang J.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100703859&doi=10.1016%2fj.cmpb.2021.105972&partnerID=40&md5=52f321d4bf6ea0506c91b304c8af2fa0,10.1016/j.cmpb.2021.105972,"Background and Objective: The classification of human peripheral blood cells yields significance in the detection of inflammation, infections and blood cell disorders such as leukemia. Limitations in traditional algorithms for blood cell classification and increased computational processing power have allowed machine learning methods to be utilized for this clinically prevalent task. Methods: In the current work, we present BloodCaps, a capsule based model designed for the accurate multiclassification of a diverse and broad spectrum of blood cells. Results: Implemented on a large-scale dataset of 8 categories of human peripheral blood cells, the proposed architecture achieved an overall accuracy of 99.3%, outperforming convolutional neural networks such as AlexNet(81.5%), VGG16(97.8%), ResNet-18(95.9%) and InceptionV3(98.4%). Furthermore, we devised three new datasets(low-resolution dataset, small dataset, and low-resolution small dataset) from the original dataset, and tested BloodCaps in comparison with AlexNet, VGG16, ResNet-18, and InceptionV3. To further validate the applicability of our proposed model, we tested BloodCaps on additional public datasets such as the All IDB2, BCCD, and Cell Vision datasets. Compared with the reported results, BloodCaps showed the best performance in all three scenarios. Conclusions: The proposed method proved superior in octal classification among all three datasets. We believe the proposed method represents a promising tool to improve the diagnostic performance of clinical blood examinations. © 2021",Blood Cells; Capsule Networks; CNN; Deep Learning; Image Classification,,,Computer Methods and Programs in Biomedicine,Article,Scopus
313,,Deep sparse graph functional connectivity analysis in AD patients using fMRI data,"Ahmadi H., Fatemizadeh E., Motie-Nasrabadi A.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100521485&doi=10.1016%2fj.cmpb.2021.105954&partnerID=40&md5=700011aa35d351677ef4555629591cfb,10.1016/j.cmpb.2021.105954,"Functional magnetic resonance imaging (fMRI) is a non-invasive method that helps to analyze brain function based on BOLD signal fluctuations. Functional Connectivity (FC) catches the transient relationship between various brain regions usually measured by correlation analysis. The elements of the correlation matrix are between -1 to 1. Some of them are very small values usually related to weak and spurious correlations due to noises and artifacts. They can not be concluded as real strong correlations between brain regions and their existence could make a misconception and leads to fake results. It is crucial to make a conclusion based on reliable and informative correlations. In order to eliminate weak correlations, thresholding is a common method. In this routine, by adjusting a threshold the values below the threshold turn to zero and the rest remains. In this paper, in addition to thresholding, two other methods including spectral sparsification based on Effective Resistance (ER) and autoencoders are investigated for sparsing the correlation matrices. Autoencoders are based on deep learning neural networks and ER considers the network as a resistive circuit. The fMRI data of the study correspond to Alzheimer's patients and control subjects. Graph global measures are calculated and a non-parametric permutation test is reported. Results show that the autoencoder and spectral sparsification achieved more distinctive brain graphs between healthy and AD subjects. Also, more graph global features were significantly different from these two methods due to better elimination of weak correlations and preserve more informative ones. Regardless of the sparsification method features including average strength, clustering, local efficiency, modularity, and transitivity are significantly different (P-value=0.05). On the other hand, the measures radius, diameter, and eccentricity showed no significant differences in none of the methods. In addition, according to three different methods, the brain regions show fragile and solid FCs are determined. © 2021 Elsevier B.V.",Autoencoders; Functional connectivity; Graph sparsification; Spectral sparsification; Thresholding,,,Computer Methods and Programs in Biomedicine,Article,Scopus
314,,Software reuse analytics using integrated random forest and gradient boosting machine learning algorithm,"Sandhu A.K., Batth R.S.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093974820&doi=10.1002%2fspe.2921&partnerID=40&md5=62c6cc716c7e5ae4177a051fe85a4471,10.1002/spe.2921,"The term Cleaner Production (CP) for Production Companies is contemplated as influential to get sustainable production. CP mainly deals with three R's that is, reuse, reduce, and recycle. For software enterprise, the software reuse plays a pivotal role. Software reuse is a process of producing new products or software from the existing software by updating it. To extract useful information from the existing software data mining comes into light. The algorithms used for software reuse face issues related to maintenance cost, accuracy, and performance. Also, the currently used algorithm does not give accurate results on whether the component of software can be reused. Machine Learning gives the best results to predicate if the given software component is reusable or not. This paper introduces an integrated Random Forest and Gradient Boosting Machine Learning Algorithm (RFGBM) which test the reusability of the given software code considering the object-oriented parameters such as cohesion, coupling, cyclomatic complexity, bugs, number of children, and depth inheritance tree. Further, the proposed algorithm is compared with J48, AdaBoostM1, LogitBoost, Part, One R, LMT, JRip, DecisionStump algorithms. Performance metrices like accuracy, error rate, Relative Absolute Error, and Mean Absolute Error are improved using RFGBM. This algorithm also utilizes data preprocessing with the help of an unsupervised filter to remove the missing value for efficiency improvement. Proposed algorithm outperforms existing in term of performance parameters. © 2020 John Wiley & Sons Ltd",AdaBoostM1; confusion matrix; DecisionStump; gradient boosting machine; J48; JRip; LMT; LogitBoost; one R; part; random forest; software metrics; software reuse,"735, 747",,Software - Practice and Experience,Conference Paper,Scopus
315,,Learning to Find Bugs and Code Quality Problems-What Worked and What not?,Raychev V.,2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104600959&doi=10.1109%2fICCQ51190.2021.9392977&partnerID=40&md5=4b6f60fa7925d2b5e33a13a7b0168b05,10.1109/ICCQ51190.2021.9392977,"The recent growth of open source repositories and deep learning models brought big promises for the next generation of programming tools that can automate or significantly improve the software development process. Yet, such tools are still rare and the machine learning components in them are not always apparent to their users. The current most useful techniques in machine learning for code are also not coming from the organizations such as Microsoft, Google, DeepMind, Facebook, OpenAI or NVIDIA that invested the most in deep neural techniques such as huge neural networks. This probably means that either many of these coding problems are significantly different from other hot topics in deep learning such as image processing or that it is much more difficult to collect datasets that would result in similarly successful tools. In this work, we study the results in the literature on the topic and discuss ways to address these shortcomings. © 2021 IEEE.",,"9, 13",,"2021 International Conference on Code Quality, ICCQ 2021",Conference Paper,Scopus
316,,"2021 IEEE International Conference on Information Communication and Software Engineering, ICICSE 2021",[No author name available],2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105337077&partnerID=40&md5=31d5307a181f8476901a43c922fbdfa1,,The proceedings contain 60 papers. The topics discussed include: construction of perceptual security dataset for video selective encryption based on double-blind subjective experiment; a bearing fault diagnosis method based on L1 regularization transfer learning and LSTM deep learning; evaluation of image fusion methods applied to the ZY1-04 data of Chengdu area; signal demodulation algorithm for interference spectrum of Fabry-Perot Cavity with stainless steel diaphragm; fuzzy adaptive PI clock synchronization algorithm based on kalman filter; a compact and reliable software architecture for robotics control; research and development of attendance management system based on face recognition and RFID technology; multi-sensor network-based trajectory planning for an Internet of UAVs; and temporal model for transportation management.,,,339.0,"2021 IEEE International Conference on Information Communication and Software Engineering, ICICSE 2021",Conference Review,Scopus
317,,A bearing fault diagnosis method based on L1 regularization transfer learning and LSTM deep learning,"Zhu D., Song X., Yang J., Cong Y., Wang L.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105336049&doi=10.1109%2fICICSE52190.2021.9404081&partnerID=40&md5=5478ce2c0a027673682cf7ca461abd75,10.1109/ICICSE52190.2021.9404081,"In the practical application of rail transit, it is difficult to obtain bearing fault data and the training data of fault diagnosis model is insufficient, which leads to the low accuracy and generalization ability of fault diagnosis model. In this paper, a new bearing fault diagnosis method based on transfer learning is proposed. Based on transfer learning, we introduce L1 regularization, then adds it to the Long Short-Term Memory (LSTM) classification model, and uses a small amount of target domain data to fine tune the parameters of the model, and finally constructs a bearing fault diagnosis model. In this paper, the bearing data set of Case Western Reserve University is used to test the bearing fault diagnosis model. Compared with the conventional LSTM, Gated Recurrent Unit (GRU) and Bi-LSTM, the model proposed in this paper has higher accuracy in fault diagnosis as well as certain reliability and generalization ability. © 2021 IEEE.",Fault diagnosis; L1 regularization; LSTM; Transfer learning,"308, 312",,"2021 IEEE International Conference on Information Communication and Software Engineering, ICICSE 2021",Conference Paper,Scopus
318,,Natural language processing through BERT for identifying gender-based violence messages on social media,"Soldevilla I., Flores N.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105300543&doi=10.1109%2fICICSE52190.2021.9404127&partnerID=40&md5=da7cdc443aaf5bb1734d049c165460c5,10.1109/ICICSE52190.2021.9404127,"The purpose of this article is to provide a natural language processing model to classify messages on social media that contain gender violence. The applied methodology considers performing a fine tune process to BERT, so as the output model can be trained with labeled messages -as violence and nonviolence-on the Reddit and Twitter platforms. The performance obtained in terms of area under the curve, accuracy, sensitivity and specificity were 0.9603, 0.8909, 0.8826 and 0.8989 respectively. © 2021 IEEE.",BERT; Fine-tuning; Natural language processing; Text classification,"204, 208",,"2021 IEEE International Conference on Information Communication and Software Engineering, ICICSE 2021",Conference Paper,Scopus
319,,News text classification and recommendation technology based on wide deep-bert model,"Jing W., Bailong Y.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105266772&doi=10.1109%2fICICSE52190.2021.9404101&partnerID=40&md5=5bee85df5ea330433c1d110ffa1a85ad,10.1109/ICICSE52190.2021.9404101,"With the rapid development of Internet technology, news information on various social platforms is growing wildly, generating large amounts of data. Since short text information such as news headlines, short messages, and newsletters has a small number of words and limited content, it is often difficult to extract effective information. The sparse feature information causes difficulties in text classification. If the news system cannot efficiently and accurately realize news classification and users preference recommendation, it will inevitably affect the experience and frequency of platform users. This paper mainly studies the application of deep learning in the field of text classification and users' personal recommendation. It uses multiple English text data sets to learn text features. Based on the WideDeep model, combined with the improved BERT pretraining model, the WideDeep-BERT model is designed. In addition, the corresponding news text classification and recommendation technology process is proposed, and the Tensorflow deep learning framework is used to experimentally verify the technology, which proves the effectiveness and practicability of the design technology. © 2021 IEEE.",Best model; Text classification; Widedeep model,"209, 216",,"2021 IEEE International Conference on Information Communication and Software Engineering, ICICSE 2021",Conference Paper,Scopus
320,,Network user behavior authentication based on hidden markov model,"Wu Z., Tian L., Wang Z., Wang Y.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105248166&doi=10.1109%2fICICSE52190.2021.9404100&partnerID=40&md5=14dc98aa5aebc55bc0fcbf60de50e6b3,10.1109/ICICSE52190.2021.9404100,"Aiming at the data security problem of the system in the network environment, this paper uses the hidden Markov model to analyze the user's access behavior in the network system and identify the malicious user behavior. First, the behavior characteristics of network users are analyzed, and then according to the related characteristics of hidden Markov, the observation order symbol and hidden state in the model are determined, and the user's access behavior is mapped into a hidden Markov chain. We give detailed experiments to prove the feasibility of applying the hidden Markov model to network user behavior authentication, and compare with other methods to get the accuracy of the model. © 2021 IEEE.",Behavior characteristics; Hidden Markov model; Network information security; User behavior authentication,"76, 82",,"2021 IEEE International Conference on Information Communication and Software Engineering, ICICSE 2021",Conference Paper,Scopus
321,,An indoor positioning system using Channel State Information based on TrAdaBoost tranfer learning,"Yong Z., Chengbin W.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114050666&doi=10.1109%2fAEMCSE51986.2021.00263&partnerID=40&md5=11ae3731c004e53e3b5343720467fe9c,10.1109/AEMCSE51986.2021.00263,"With the rapidly growing demand for Location-Based Services in indoor environments, fingerprint-based indoor positioning has caused great interest due to its high positioning accuracy and low equipment cost. However, the standard signal radio map cannot provide consistent high positioning accuracy under environmental changes and new scenarios. To address this problem, we present a novel indoor positioning Transfer Learning(TL) system based on improved TrAdaBoost. We perform phase correction on the raw CSI phase, then use One-vs-Rest algorithm and One-Hot coding, which can realize the multi-classification ability of the TrAdaBoost algorithm. Meanwhile, we use a correction factor to slow down the weight of the source domain and make the fingerprint of the source domain better transfer to the target domain by the TrAdaBoost in order to form a new fingerprint database. Experimental results show that the positioning accuracy can be improved by 35% in dynamic environment conditions. The proposed method can improve the positioning accuracy by an average of 30% in new scenes and the Site Survey Overhead(SSO) is reduced by 40%. Experiments show that our proposed method has robustness in time and space, and has lower SSO under the same positioning accuracy. © 2021 IEEE.",Component; CSI; Indoor Location; Transfer Learning; WiFi,"1286, 1293",,"Proceedings - 2021 4th International Conference on Advanced Electronic Materials, Computers and Software Engineering, AEMCSE 2021",Conference Paper,Scopus
322,,Brain image conversion method based on generative adversarial network,"Qiu N., Cheng D., Li C., Li Y.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114049281&doi=10.1109%2fAEMCSE51986.2021.00094&partnerID=40&md5=16308ab245d079052094482ee430f617,10.1109/AEMCSE51986.2021.00094,"In recent years, brain image conversion technology has become popular in the field of medical imaging, and plays a very important auxiliary role in clinical diagnosis and disease detection of brain diseases, such as brain image reconstruction, brain image noise reduction and brain image segmentation. The brain image conversion technology aims to learn the mapping relationship between two different modal brain images, and can convert one modal brain image into another modal brain image. However, there are complex relationships between different modal data, and there is no unified theoretical framework to realize the conversion between different modal data. Therefore, we propose a brain image conversion method based on generative adversarial network to realize the conversion between different modal brain images, and prove the superiority of this method through experiments. © 2021 IEEE.",Brain imaging conversion; Disease detection; Generative adversarial network,"422, 426",,"Proceedings - 2021 4th International Conference on Advanced Electronic Materials, Computers and Software Engineering, AEMCSE 2021",Conference Paper,Scopus
323,,Hybrid pruning for convolutional neural network convolution kernel,"Guo C.Y., Li P.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114045465&doi=10.1109%2fAEMCSE51986.2021.00096&partnerID=40&md5=e3f21589c41b507f4610718555ddecc5,10.1109/AEMCSE51986.2021.00096,"In the model compression, to achieve the optimal pruning effect, weighted pruning or filter pruning was used to prune the network in traditional methods, in fact, the network can continue to be compressed. Therefore, a hybrid pruning method of convolution kernel weight pruning and filter pruning are proposed in this paper. First of all, the weight pruning is carried out for the convolutional neural network model, this paper adopts the method of dynamic convolution kernel for weight pruning, and the fluctuation value of each time normalized value in network update process is taken as the pruning criterion for the weight of the convolutional kernel. Because the weight pruning of the convolution kernel is dynamic, the floating-point operation (FLOP) is significantly reduced, and the parameter scale does not decrease significantly. Then, the model was pruning by convolution kernel ℓ1-norm [1] method, which is not only effectively reduce the parameter scale, but also no extra calculations are introduced. In the case of a small loss of network accuracy, the hybrid pruning of the convolution kernel can maximize the compression of the model to meet the conditions for deployment in mobile edge devices, and has significant improvements in floating-point operations and parameter compression. Therefore, mixed pruning in this paper can effectively make up for the shortage of single pruning method. Through VGG-16, ResNet and other networks, it has been proved in practice that the convolution kernel hybrid pruning method proposed in this paper can effectively reduce the model size and maintain a high level of accuracy. © 2021 IEEE.",Filter pruning; Hybrid pruning; Mobile edge devices; Weight pruning,"432, 438",,"Proceedings - 2021 4th International Conference on Advanced Electronic Materials, Computers and Software Engineering, AEMCSE 2021",Conference Paper,Scopus
324,,Design of the fire evacuation training system for underground buildings based on VR,"Lu C., Zhang Y., Liu C., Li Y.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114043941&doi=10.1109%2fAEMCSE51986.2021.00187&partnerID=40&md5=b3fd6a195fc87db5686003d0d7f3321f,10.1109/AEMCSE51986.2021.00187,"In order to improve the self-rescue awareness and self-evacuation ability of the trapped people in the burning underground building and reduce casualties, we use Unity3D to design an underground building fire evacuation training system based on virtual reality (VR), which includes four modules: pre-training module, fire scene perception module, evacuation self-rescue experience module, and post-training feedback module. Practices have shown that the system allows trainees to conduct simulated evacuation training in an environment with a good sense of interaction and strong sense of immersion, and have a good training effect. © 2021 IEEE.",Immersive training; Personnel evacuation; Underground building fire; Unity3D; VR,"921, 924",,"Proceedings - 2021 4th International Conference on Advanced Electronic Materials, Computers and Software Engineering, AEMCSE 2021",Conference Paper,Scopus
325,,A neural model for aspect-level sentiment classification of product reviews assissted by question-answering,"Fan J., Zhang X., Zhang Z., Xu C.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114039965&doi=10.1109%2fAEMCSE51986.2021.00133&partnerID=40&md5=2bb3d48e3a0ebe2c4629b129be06658b,10.1109/AEMCSE51986.2021.00133,"Aspect-level sentiment analysis targets at judging the associated polarity of the opinion holder's view towards given aspect category, and it usually contains two sub-tasks, namely aspect term extraction and aspect-related sentiment classification. However, the aspect term extraction, an auxiliary sub-task, receives too much emphasis, and it is labor-consuming. With the development of deep learning theory and language models, quite a few problems in natural language processing (NLP) domain have been solved. To address the problems mentioned above, we propose to remove the aspect term extraction component and regard aspect categories as guidelines to apply the question answering theory. Simultaneously, we utilize BERT as the base support unit to sufficiently encode the context information. For the purpose of evaluating our method, we also construct our Chinese review corpus for aspect-level sentiment analysis assignment. Experimental results show that our proposed method achieves an appreciable performance on our dataset. © 2021 IEEE.",Aspect level sentiment analysis; Product review; Question answering,"643, 647",,"Proceedings - 2021 4th International Conference on Advanced Electronic Materials, Computers and Software Engineering, AEMCSE 2021",Conference Paper,Scopus
326,,An efficient method for cross-subject EEG-based mental fatigue recognition,"Zhang K., Tang W.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114037331&doi=10.1109%2fAEMCSE51986.2021.00044&partnerID=40&md5=dd91ddab0417c046ead3bd2b29036f85,10.1109/AEMCSE51986.2021.00044,"Mental fatigue has an unignorable impact on people's daily life and work. Mental fatigue recognition methods based on EEG are commonly thought as objective standard. However, the procedure of initiating fatigue needs a long time and EEG signals vary greatly, so mental fatigue recognition based on EEG is challenging when subject-specific data are limited and imbalanced. In this paper, we explored the performance of cross-subject fatigue recognition on a general multitask learning framework with two data sampling methods for imbalanced classification. One was to synthesize some minority samples (MTLSMS) until the two classes were balanced and another was to under-sample the majority samples (MTLUMS). EEG data of 11 subjects from a public EEG fatigue dataset were selected to validate our fatigue recognition methods. The MTLSMS method improved the cross-subject mental fatigue recognition accuracies to 81.07%, and much better than other transfer learning domain adaptation methods Transfer Component Analysis (TCA) and Maximum Independence Domain Adaptation (MIDA). The experiment results showed that the MTLSMS method can effectively recognize the cross-subject mental fatigue. © 2021 IEEE.",Class imbalances; Cross-subject mental fatigue recognition; EEG; Multitask learning; Transfer learning,"176, 181",,"Proceedings - 2021 4th International Conference on Advanced Electronic Materials, Computers and Software Engineering, AEMCSE 2021",Conference Paper,Scopus
327,,Summary of research on news text classification,"Deping L., Hongjuan W.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114035409&doi=10.1109%2fAEMCSE51986.2021.00224&partnerID=40&md5=dc77d2aa1993475e51dd26dec165a201,10.1109/AEMCSE51986.2021.00224,"With the rapid development of modern technology and the rapid spread of network information, the way people obtain news information has gradually shifted from traditional paper documents to electronic text. How to efficiently classify these electronic news texts has become a current research hotspot. In this paper, we introduce the concept of text classification and its main process, then summarize the existing classification methods and challenges now, as well as forecast the future trends. © 2021 IEEE.",Graph neural network; Machine learning; News text classification,"1097, 1100",,"Proceedings - 2021 4th International Conference on Advanced Electronic Materials, Computers and Software Engineering, AEMCSE 2021",Conference Paper,Scopus
328,,Power system distributed state estimation method based on transfer learning under sparse data,"Mu Q., Qian J.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114030503&doi=10.1109%2fAEMCSE51986.2021.00053&partnerID=40&md5=40353ae2084c6cfe08bb96a652947f23,10.1109/AEMCSE51986.2021.00053,"With the development of smart grids and the continuous penetration of new energy power generation, the scale of the power grid has increased, and the structure and operation methods have become increasingly complex. In order to more accurately grasp the operating status of the power grid, it is necessary to estimate the operating status of the power system. In recent years, with the development of artificial intelligence and data mining technology, neural networks have also been widely used in the field of state estimation. However, there are few researches on artificial intelligence algorithms in active distribution network state estimation, and the small amount of data in the bad data detection link that needs to be carried out before state estimation also limits the application of many artificial intelligence algorithms that require a large number of training samples. A small sample learning method applied to the detection of bad data for power system state estimation is proposed, which successfully solves the problem of the lack of available training data in building a neural network model for bad data detection, and solves the traditional method calculation speed brought by the massive increase of grid data. The problem of serious decline, and the accuracy of the traditional state estimation method is improved by more than 47%. © 2021 IEEE.",Bad data detection; Convolutional neural network; Power system; State estimation; Transfer learning,"221, 224",,"Proceedings - 2021 4th International Conference on Advanced Electronic Materials, Computers and Software Engineering, AEMCSE 2021",Conference Paper,Scopus
329,,Distillation for text classification task based on BERT,"Sun C., Li X., Ge S., An Z., Zhang C.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114020013&doi=10.1109%2fAEMCSE51986.2021.00103&partnerID=40&md5=141416af67e999b69b733e8d263cdf43,10.1109/AEMCSE51986.2021.00103,"In recent years, with the rapid development of the Internet and the surge in the number of web texts, the demand for text classification technology has become increasingly significant. However, there are also the following problems: 1. The maximum input length of the model is 512, and some information will be lost if the longer text is directly truncated; 2. The model is large and the reasoning time is long, which is not convenient for mobile terminal deployment requirements. Aiming at problem 1: Firstly, the text with a length of more than 512 is intercepted. Considering that the end of the text usually contains more emotional information, the intercepting strategy is 170th and 340th. At the same time, another kind of text feature surface of the model is selected: The mean and maximum values are calculated respectively along the dimension of sequence length, which are spliced into column vectors as the input features of the model. Aiming at problem 2, the knowledge distillation of the model for classification tasks is carried out to achieve the reduction parameters to improve the inference efficiency to facilitate the actual deployment and application. Three groups of control experiments show that the overall classification accuracy of the improved BERT model is 97%, and the overall performance is more balanced, and the overall performance is more robust, which is slightly better than the BERT text classification model. The effect of the distilled BERT model is only 1.5% lower than that of the BERT model, but the number of parameters of the model is 92.6% less than that of the original model, and the reasoning time is nearly 4 times faster than the original model, which also shows the effectiveness of improving input features and model compression. © 2021 IEEE.",Actual deployment; BERT; Input characteristics; Knowledge distillation; Text classification,"472, 478",,"Proceedings - 2021 4th International Conference on Advanced Electronic Materials, Computers and Software Engineering, AEMCSE 2021",Conference Paper,Scopus
330,,Glaucoma detection through digital processing from fundus images using MATLAB,"Almeida-Galarraga D., Benavides-Montenegro K., Insuasti-Cruz E., Lovato-Villacis N., Suarez-Jaramillo V., Tene-Hurtado D., Tirado-Espin A., Villalba-Meneses G.F.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111460467&doi=10.1109%2fICI2ST51859.2021.00014&partnerID=40&md5=46835ac2429dc4e08795b8b16ba48a0f,10.1109/ICI2ST51859.2021.00014,"A considerable proportion of the global population has lost vision due to the late diagnosis of glaucoma, causing ocular pressure on the optic nerve, and blindness to become irreversible. A number of 60.50 million people suffer open angle (OAG) and angle closure glaucoma (ACG) in 2010, increasing to 79.60 million by 2020, and of these, 74% will have OAG. For that reason, the objective of this project was to implement a computational technique of early diagnosis of glaucoma using imaging processing in MATLAB, with the aim of preventing future more severe damages. Together with this, it also sought to improve the precision of the technique to complement the work of the specialist, and providing more accessibility to the patient. The entire ACRIMA database was used for the diagnosis and then analyzed and processed by MATLAB, by extracting the most relevant qualities from the images to reach an optimal diagnosis. The performance test was based on Matlab glaucoma detection proposed method, showing satisfactory results with an accuracy of 94.61%, sensitivity of 94.57% and specificity of 95%. These percentages showed that glaucoma detection using Matlab is feasible. © 2021 IEEE.",angle closure glaucoma; CDR; fundus images; Glaucoma; Open angle glaucoma; smoothing filter,"39, 45",,"Proceedings - 2021 2nd International Conference on Information Systems and Software Technologies, ICI2ST 2021",Conference Paper,Scopus
331,,Automatic Detection of Five API Documentation Smells: Practitioners' Perspectives,"Khan J.Y., Tawkat Islam Khondaker M., Uddin G., Iqbal A.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106658440&doi=10.1109%2fSANER50967.2021.00037&partnerID=40&md5=e9b75a7998e0a91a1b12db129717cc7f,10.1109/SANER50967.2021.00037,"The learning and usage of an API is supported by official documentation. Like source code, API documentation is itself a software product. Several research results show that bad design in API documentation can make the reuse of API features difficult. Indeed, similar to code smells or code anti-patterns, poorly designed API documentation can also exhibit 'smells'. Such documentation smells can be described as bad documentation styles that do not necessarily produce an incorrect documentation but nevertheless make the documentation difficult to properly understand and to use. Recent research on API documentation has focused on finding content inaccuracies in API documentation and to complement API documentation with external resources (e.g., crowd-shared code examples). We are aware of no research that focused on the automatic detection of API documentation smells. This paper makes two contributions. First, we produce a catalog of five API documentation smells by consulting literature on API documentation presentation problems. We create a benchmark dataset of 1, 000 API documentation units by exhaustively and manually validating the presence of the five smells in Java official API reference and instruction documentation. Second, we conduct a survey of 21 professional software developers to validate the catalog. The developers agreed that they frequently encounter all five smells in API official documentation and 95.2% of them reported that the presence of the documentation smells negatively affects their productivity. The participants wished for tool support to automatically detect and fix the smells in API official documentation. We develop a suite of rule-based, deep and shallow machine learning classifiers to automatically detect the smells. The best performing classifier BERT, a deep learning model, achieves F1-scores of 0.75 - 0.97. © 2021 IEEE.",API Documentation; Benchmark; Deep Learning; Shallow Learning; Smell; Survey,"318, 329",,"Proceedings - 2021 IEEE International Conference on Software Analysis, Evolution and Reengineering, SANER 2021",Conference Paper,Scopus
332,,Representation vs. Model: What Matters Most for Source Code Vulnerability Detection,"Zheng W., Abdallah Semasaba A.O., Wu X., Agyemang S.A., Liu T., Ge Y.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106626497&doi=10.1109%2fSANER50967.2021.00082&partnerID=40&md5=bd1664b415f65aabb34f7aa0fd69a27c,10.1109/SANER50967.2021.00082,"Vulnerabilities in the source code of software are critical issues in the realm of software engineering. Coping with vulnerabilities in software source code is becoming more challenging due to several aspects of complexity and volume. Deep learning has gained popularity throughout the years as a means of addressing such issues. In this paper, we propose an evaluation of vulnerability detection performance on source code representations and evaluate how Machine Learning (ML) strategies can improve them. The structure of our experiment consists of 3 Deep Neural Networks (DNNs) in conjunction with five different source code representations; Abstract Syntax Trees (ASTs), Code Gadgets (CGs), Semantics-based Vulnerability Candidates (SeVCs), Lexed Code Representations (LCRs), and Composite Code Representations (CCRs). Experimental results show that employing different ML strategies in conjunction with the base model structure influences the performance results to a varying degree. However, ML-based techniques suffer from poor performance on class imbalance handling when used in conjunction with source code representations for software vulnerability detection. © 2021 IEEE.",deep learning; security; software vulnerability detection,"647, 653",,"Proceedings - 2021 IEEE International Conference on Software Analysis, Evolution and Reengineering, SANER 2021",Conference Paper,Scopus
333,,Bug Question Answering with Pretrained Encoders,"Bo L., Lu J.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106625888&doi=10.1109%2fSANER50967.2021.00083&partnerID=40&md5=c455522a8beb1ea06bce81410f8d5c24,10.1109/SANER50967.2021.00083,"Bug question answering is an effective way to acquire the required bug information and to help bug comprehension. Many existing approaches use keyword matching techniques to obtain more bug information directly without understanding the semantic information of bug data, which make the returned results irrelevant to the input queries. To alleviate this problem, we present a novel bug question answering approach named BERT-BugQA that takes advantage of the Bidirectional Encoder Representations from Transformers (BERT) which can fully consider the bidirectional context of bug information. In special, we design a common paradigm to construct the bug reading comprehension dataset for this approach. Empirical study demonstrates that BERT-BugQA is effective to automatically obtain the answers, and the F1-score values of Mozilla and Eclipse project are 0.84 and 0.83, respectively, which are better than the state-of-the-art QA approaches. © 2021 IEEE.",BERT; Bug natural language reading comprehension; Bug question answering,"654, 660",,"Proceedings - 2021 IEEE International Conference on Software Analysis, Evolution and Reengineering, SANER 2021",Conference Paper,Scopus
334,,Plot2API: Recommending Graphic API from Plot via Semantic Parsing Guided Neural Network,"Wang Z., Huang S., Liu Z., Yan M., Xia X., Wang B., Yang D.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106615700&doi=10.1109%2fSANER50967.2021.00049&partnerID=40&md5=7024b126d52ff30e8d69209f37d1b95c,10.1109/SANER50967.2021.00049,"Plot-based Graphic API recommendation (Plot2API) is an unstudied but meaningful issue, which has several important applications in the context of software engineering and data visualization, such as the plotting guidance of the beginner, graphic API correlation analysis, and code conversion for plotting. Plot2API is a very challenging task, since each plot is often associated with multiple APIs and the appearances of the graphics drawn by the same API can be extremely varied due to the different settings of the parameters. Additionally, the samples of different APIs also suffer from extremely imbalanced.Considering the lack of technologies in Plot2API, we present a novel deep multi-task learning approach named Semantic Parsing Guided Neural Network (SPGNN) which translates the Plot2API issue as a multi-label image classification and an image semantic parsing tasks for the solution. In SPGNN, the recently advanced Convolutional Neural Network (CNN) named EfficientNet is employed as the backbone network for API recommendation. Meanwhile, a semantic parsing module is complemented to exploit the semantic relevant visual information in feature learning and eliminate the appearance-relevant visual information which may confuse the visual-information-based API recommendation. Moreover, the recent data augmentation technique named random erasing is also applied for alleviating the imbalance of API categories.We collect plots with the graphic APIs used to drawn them from Stack Overflow, and release three new Plot2API datasets corresponding to the graphic APIs of R and Python programming languages for evaluating the effectiveness of Plot2API techniques. Extensive experimental results not only demonstrate the superiority of our method over the recent deep learning baselines but also show the practicability of our method in the recommendation of graphic APIs. © 2021 IEEE.",API Recommendation; Data Visualization; Image Recognition,"458, 469",,"Proceedings - 2021 IEEE International Conference on Software Analysis, Evolution and Reengineering, SANER 2021",Conference Paper,Scopus
335,,DeepCon: Contribution Coverage Testing for Deep Learning Systems,"Zhou Z., Dou W., Liu J., Zhang C., Wei J., Ye D.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106601500&doi=10.1109%2fSANER50967.2021.00026&partnerID=40&md5=373ba3268594eaf7ec708b9b0c7240b2,10.1109/SANER50967.2021.00026,"Deep learning (DL) has been widely adopted in many safety-critical scenarios. Deep neural networks (DNNs) usually play the core part in these DL systems. Existing studies have shown that DNNs can suffer from various vulnerabilities, and cause severe consequences. To improve the testing adequacy of DNNs, researchers have proposed several coverage criteria, e.g., neuron coverage in DeepXplore. The prediction result of a DNN is jointly determined by the outputs of neurons and the connection weights that they connect into next-level neurons. However, existing coverage criteria use only the output of a neuron to determine the activation state of the neuron and ignore the connection weights it emits.In this paper, we propose DeepCon, a novel contribution coverage. In DeepCon, we define a term contribution as the combination of the output of a neuron and the connection weight it emits, and use the contribution coverage to gauge the testing adequacy of DNNs. DeepCon can thoroughly cover both neurons and the connection weights they emit and can scale well to large DNNs. We further propose a contribution coverage guided test generation approach, DeepCon-Gen, which can automatically generate tests and activate inactivated contributions of DNNs. We evaluate DeepCon and DeepCon-Gen on five different DNNs over two popular datasets. The experimental results show that DeepCon can well present the testing adequacy of these DNNs. DeepCon-Gen can effectively activate the inactivated contributions, and 62.6% of the generated tests can lead to mispredictions. © 2021 IEEE.",adversarial example detection; contribution coverage; coverage criteria; Deep learning; deep neural network,"189, 200",,"Proceedings - 2021 IEEE International Conference on Software Analysis, Evolution and Reengineering, SANER 2021",Conference Paper,Scopus
336,,Using Structural and Semantic Information to Identify Software Components,"Sas C., Capiluppi A.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106594551&doi=10.1109%2fSANER50967.2021.00063&partnerID=40&md5=a700a709431fb797bfa9e0045d606cb1,10.1109/SANER50967.2021.00063,"Component Based Software Engineering (CBSE) seeks to promote the reuse of software by using existing software modules into the development process. However, the availability of such a reusable component is not immediate and is costly and time consuming. As an alternative, the extraction from preexisting OO software can be considered.In this work, we evaluate two community detection algorithms for the task of software components identification. Considering 'components' as 'communities', the aim is to evaluate how independent, yet cohesive, the components are when extracted by structurally informed algorithms.We analyze 412 Java systems and evaluate the cohesion of the extracted communities using four document representation techniques. The evaluation aims to find which algorithm extracts the most semantically cohesive, yet separated communities.The results show a good performance in both algorithms, however, each has its own strengths. Leiden extracts less cohesive, but better separated, and better clustered components that depend more on similar ones. Infomap, on the other side, creates more cohesive, slightly overlapping clusters that are less likely to depend on other semantically similar components. © 2021 IEEE.",Community Detection; Components Identification; Components Semantic Analysis,"546, 550",,"Proceedings - 2021 IEEE International Conference on Software Analysis, Evolution and Reengineering, SANER 2021",Conference Paper,Scopus
337,,Hybrid Quantum Network for classification of finance and MNIST data,Hellstem G.,2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106563882&doi=10.1109%2fICSA-C52384.2021.00027&partnerID=40&md5=6debdeaab8467be22059291de37d6011,10.1109/ICSA-C52384.2021.00027,"In the ongoing era of noisy intermediate scaled quantum computers, one of the possible applications to search for an advantage of quantum computing is machine learning. Here we report about an approach, where a hybrid quantumclassical network is applied to classify non-Trivial datasets (finance and MNIST data). In comparison to a pure classical network, we find an advantage when looking at several performance measures. However, as in classical machine learning problems around overfitting the dataset arise. Therefore, we outline the path to future research, which has to be done in the field of (hybrid) quantum networks. © 2021 IEEE.",Finance; Machine Learning; MNIST; Quantum Computing; Regularization; Tensorflow,"106, 109",,"Proceedings - 2021 IEEE 18th International Conference on Software Architecture Companion, ICSA-C 2021",Conference Paper,Scopus
338,,Leveraging Stack Overflow to Detect Relevant Tutorial Fragments of APIs,"Wu D., Jing X.-Y., Zhang H., Zhou Y., Xu B.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106562266&doi=10.1109%2fSANER50967.2021.00020&partnerID=40&md5=acd889351e3e1ec632301d2431b662ba,10.1109/SANER50967.2021.00020,"Developers often use learning resources such as API tutorials and Stack Overflow (SO) to learn how to use an unfamiliar API. An API tutorial can be divided into a number of consecutive units that describe the same topic, denoted as tutorial fragments. We consider a tutorial fragment explaining the API usage knowledge as a relevant fragment of the API. Discovering relevant tutorial fragments of APIs can facilitate API understanding and learning. However, existing approaches, based on supervised or unsupervised approaches, often suffer from either high manual efforts or lack of consideration of the relevance information. In this paper, we propose a novel approach, called SO2RT, to detect relevant tutorial fragments of APIs based on SO posts. SO2RT first automatically extracts relevant and irrelevant 〈API, QA〉 pairs based on heuristic rules of SO, and constructs 〈API, FRA〉 pairs (FRA stands out fragment) by using tutorial fragments and APIs. SO2RT then trains a semi-supervised transfer learning based detection model, which can transfer the API usage knowledge in SO QA pairs to tutorial fragments by utilizing the easy-to-extract relevance of 〈API, QA〉 pairs. Finally, relevant fragments of APIs can be discovered by consulting the trained model. In this way, the effort for labeling the relevance between tutorial fragments and APIs can be reduced. We evaluate SO2RT on Java and Android datasets containing 21, 008 〈API, QA〉 pairs. Experimental results show that SO2RT improves the state-of-the-art approaches in terms of F-Measure on both datasets. Our user study further confirms the effectiveness of SO2RT in practice. © 2021 IEEE.",,"119, 130",,"Proceedings - 2021 IEEE International Conference on Software Analysis, Evolution and Reengineering, SANER 2021",Conference Paper,Scopus
339,,MulCode: A Multi-task Learning Approach for Source Code Understanding,"Wang D., Yu Y., Li S., Dong W., Wang J., Qing L.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106562192&doi=10.1109%2fSANER50967.2021.00014&partnerID=40&md5=19013a3e0211265450216dfea0cf9212,10.1109/SANER50967.2021.00014,"Recent years have witnessed the significant rise of Deep Learning (DL) techniques applied to source code. Researchers exploit DL for a multitude of tasks and achieve impressive results. However, most tasks are explored separately, resulting in a lack of generalization of the solutions. In this work, we propose MulCode, a multi-task learning approach for source code understanding that learns unified representation space for tasks, with the pre-trained BERT model for the token sequence and the Tree-LSTM model for abstract syntax trees. Furthermore, we integrate two source code views into a hybrid representation via the attention mechanism and set learnable uncertainty parameters to adjust the tasks' relationship.We train and evaluate MulCode in three downstream tasks: comment classification, author attribution, and duplicate function detection. In all tasks, MulCode outperforms the state-of-the-art techniques. Moreover, experiments on three unseen tasks demonstrate the generalization ability of MulCode compared with state-of-the-art embedding methods. © 2021 IEEE.",attention mechanism; deep learning; multi-task learning; representation learning,"48, 59",,"Proceedings - 2021 IEEE International Conference on Software Analysis, Evolution and Reengineering, SANER 2021",Conference Paper,Scopus
340,,Reducing BERT Computation by Padding Removal and Curriculum Learning,"Zhang W., Wei W., Wang W., Jin L., Cao Z.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105424239&doi=10.1109%2fISPASS51385.2021.00025&partnerID=40&md5=4d6bd88390881ab387061445dbf75224,10.1109/ISPASS51385.2021.00025,"BERT [1] is very computationally expensive, which is a hurdle for its training and deployment. This work focuses on removing the unnecessary computation due to input padding in BERT. The input of BERT consists of two concatenated sentences. If the length of the two concatenated sentences is shorter than the maximum sequence length, padding must be added to the end of the sentences to fill the empty slots in the input. Because the lengths of sentences vary greatly, there can be a large amount of padding in input. For the English Wikipedia BooksCorpus dataset, the percentage of padding among all the input tokens is 17% and 48%, respectively, when the max sequence length is set to 128 and 512. For the Chinese Wikipedia dataset, this percentage is 35% and 79%, respectively, when the max sequence length is 128 and 512. For SQuAD-v1.1 [2], padding accounts for 54% of the total input tokens when the max sequence length is 384. Thus, there is a lot of wasted computation on padding, which significantly increases the training and inference time. © 2021 IEEE.",,"90, 92",,"Proceedings - 2021 IEEE International Symposium on Performance Analysis of Systems and Software, ISPASS 2021",Conference Paper,Scopus
341,,TPUPoint: Automatic Characterization of Hardware-Accelerated Machine-Learning Behavior for Cloud Computing,"Wudenhe A., Tseng H.-W.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105410202&doi=10.1109%2fISPASS51385.2021.00048&partnerID=40&md5=145f53ce7c6d7509261ef727ea9a22de,10.1109/ISPASS51385.2021.00048,"With the share of machine learning (ML) workloads in data centers rapidly increasing, cloud providers are beginning to incorporate accelerators such as tensor processing units (TPUs) to improve the energy-efficiency of applications. However, without optimizing application parameters, users may underutilize accelerators and end up wasting energy and money. This paper presents TPUPoint to facilitate the development of efficient applications on TPU-based cloud platforms. TPUPoint automatically classifies repetitive patterns into phases and identifies the most timing-critical operations in each phase. Further, TPUPoint can associate phases with checkpoints to allow fast-forwarding in applications, thereby significantly reducing the time and money spent optimizing applications. By running TPUPoint on a wide array of representative ML workloads, we found that computation is no longer the most time-consuming operation; instead, the infeed and reshape operations, which exchange and realign data, become most significant. TPUPoints advantages significantly increase the potential for discovering optimal parameters to quickly balance the complex workload pipeline of feeding data into a system, reformatting the data, and computing results. © 2021 IEEE.",,"254, 264",,"Proceedings - 2021 IEEE International Symposium on Performance Analysis of Systems and Software, ISPASS 2021",Conference Paper,Scopus
342,,Emoji-powered Sentiment and Emotion Detection from Software Developers' Communication Data,"Chen Z., Cao Y., Yao H., Lu X., Peng X., Mei H., Liu X.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102899601&doi=10.1145%2f3424308&partnerID=40&md5=e34e520c58a37a18dbd2ebe0c168a796,10.1145/3424308,"Sentiment and emotion detection from textual communication records of developers have various application scenarios in software engineering (SE). However, commonly used off-the-shelf sentiment/emotion detection tools cannot obtain reliable results in SE tasks and misunderstanding of technical knowledge is demonstrated to be the main reason. Then researchers start to create labeled SE-related datasets manually and customize SE-specific methods. However, the scarce labeled data can cover only very limited lexicon and expressions. In this article, we employ emojis as an instrument to address this problem. Different from manual labels that are provided by annotators, emojis are self-reported labels provided by the authors themselves to intentionally convey affective states and thus are suitable indications of sentiment and emotion in texts. Since emojis have been widely adopted in online communication, a large amount of emoji-labeled texts can be easily accessed to help tackle the scarcity of the manually labeled data. Specifically, we leverage Tweets and GitHub posts containing emojis to learn representations of SE-related texts through emoji prediction. By predicting emojis containing in each text, texts that tend to surround the same emoji are represented with similar vectors, which transfers the sentiment knowledge contained in emoji usage to the representations of texts. Then we leverage the sentiment-aware representations as well as manually labeled data to learn the final sentiment/emotion classifier via transfer learning. Compared to existing approaches, our approach can achieve significant improvement on representative benchmark datasets, with an average increase of 0.036 and 0.049 in macro-F1 in sentiment and emotion detection, respectively. Further investigations reveal that the large-scale Tweets make a key contribution to the power of our approach. This finding informs future research not to unilaterally pursue the domain-specific resource but try to transform knowledge from the open domain through ubiquitous signals such as emojis. Finally, we present the open challenges of sentiment and emotion detection in SE through a qualitative analysis of texts misclassified by our approach. © 2021 ACM.",Emoji; emotion; sentiment; software engineering,,,ACM Transactions on Software Engineering and Methodology,Article,Scopus
343,,Are Comments on Stack Overflow Well Organized for Easy Retrieval by Developers?,"Zhang H., Wang S., Chen T.-H., Hassan A.E.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102863919&doi=10.1145%2f3434279&partnerID=40&md5=9ac4164280af6d88cb6433e852071a2f,10.1145/3434279,"Many Stack Overflow answers have associated informative comments that can strengthen them and assist developers. A prior study found that comments can provide additional information to point out issues in their associated answer, such as the obsolescence of an answer. By showing more informative comments (e.g., the ones with higher scores) and hiding less informative ones, developers can more effectively retrieve information from the comments that are associated with an answer. Currently, Stack Overflow prioritizes the display of comments, and, as a result, 4.4 million comments (possibly including informative comments) are hidden by default from developers. In this study, we investigate whether this mechanism effectively organizes informative comments. We find that (1) the current comment organization mechanism does not work well due to the large amount of tie-scored comments (e.g., 87% of the comments have 0-score) and (2) in 97.3% of answers with hidden comments, at least one comment that is possibly informative is hidden while another comment with the same score is shown (i.e., unfairly hidden comments). The longest unfairly hidden comment is more likely to be informative than the shortest one. Our findings highlight that Stack Overflow should consider adjusting the comment organization mechanism to help developers effectively retrieve informative comments. Furthermore, we build a classifier that can effectively distinguish informative comments from uninformative comments. We also evaluate two alternative comment organization mechanisms (i.e., the Length mechanism and the Random mechanism) based on text similarity and the prediction of our classifier. © 2021 ACM.",commenting; crowdsourced knowledge sharing; Empirical software engineering; Q8A website; stack overflow,,,ACM Transactions on Software Engineering and Methodology,Article,Scopus
344,,AIBench Training: Balanced Industry-Standard AI Training Benchmarking,"Tang F., Gao W., Zhan J., Lan C., Wen X., Wang L., Luo C., Cao Z., Xiong X., Jiang Z., Hao T., Fan F., Zhang F., Huang Y., Chen J., Du M., Ren R., Zheng C., Zheng D., Tang H., Zhan K., Wang B., Kong D., Yu M., Tan C., Huan L., Xinhui X., Li Y., Shao J., Wang Z., Wang X., Dai J., Ye H.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102548679&doi=10.1109%2fISPASS51385.2021.00014&partnerID=40&md5=9f0db5a9b28c6a795c03041bad91e62a,10.1109/ISPASS51385.2021.00014,"Earlier-stage evaluations of a new AI architecture/system need affordable AI benchmarks. Only using a few AI component benchmarks like MLPerf alone in the other stages may lead to misleading conclusions. Moreover, the learning dynamics are not well understood, and the benchmarks' shelf-life is short. This paper proposes a balanced benchmarking methodology. We use real-world benchmarks to cover the factors space that impacts the learning dynamics to the most considerable extent. After performing an exhaustive survey on Internet service AI domains, we identify and implement nineteen representative AI tasks with state-of-the-art models. For repeatable performance ranking (RPR subset) and workload characterization (WC subset), we keep two subsets to a minimum for affordability. We contribute by far the most comprehensive AI training benchmark suite. The evaluations show: (1) AIBench Training (v1.1) outperforms MLPerf Training (v0.7) in terms of diversity and representativeness of model complexity, computational cost, convergent rate, computation, and memory access patterns, and hotspot functions; (2) Against the AIBench full benchmarks, its RPR subset shortens the benchmarking cost by 64%, while maintaining the primary workload characteristics; (3) The performance ranking shows the single-purpose AI accelerator like TPU with the optimized TensorFlow framework performs better than that of GPUS while losing the latter's general support for various AI models. The specification, source code, and performance numbers are available from the AIBench homepage https://www.benchcouncil.org/aibench-training/index.HTML. © 2021 IEEE.",Benchmark; Deep Learning; Learning Dynamics; Subsetting; Training; Workload Characterization,"24, 35",,"Proceedings - 2021 IEEE International Symposium on Performance Analysis of Systems and Software, ISPASS 2021",Conference Paper,Scopus
345,,Wikifying software artifacts,"Nassif M., Robillard M.P.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102422072&doi=10.1007%2fs10664-020-09918-4&partnerID=40&md5=3762238f2bd2eb47e99fbac3c67c754d,10.1007/s10664-020-09918-4,"Context: The computational linguistics community has developed tools, called wikifiers, to identify links to Wikipedia articles from free-form text. Software engineering research can leverage wikifiers to add semantic information to software artifacts. However, no empirically-grounded basis exists to choose an effective wikifier and to configure it for the software domain, on which wikifiers were not specifically trained. Objective: We conducted a study to guide the selection of a wikifier and its configuration for applications in the software domain, and to measure what performance can be expected of wikifiers. Method: We applied six wikifiers, with multiple configurations, to a sample of 500 Stack Overflow posts. We manually annotated the 41 124 articles identified by the wikifiers as correct or not to compare their precision and recall. Results: Each wikifier, in turn, achieved the highest precision, between 13% and 82%, for different thresholds of recall, from 60% to 5%. However, filtering the wikifiers’ output with a whitelist can considerably improve the precision above 79% for recall up to 30%, and above 47% for recall up to 60%. Conclusions: Results reported in each wikifier’s original article cannot be generalized to software-specific documents. Given that no wikifier performs universally better than all others, we provide empirically grounded insights to select a wikifier for different scenarios, and suggest ways to further improve their performance for the software domain. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC part of Springer Nature.",Comparative evaluation; Knowledge-driven software engineering; Stack overflow; Wikification; Wikipedia,,,Empirical Software Engineering,Article,Scopus
346,,Graph Neural Networks for Table-based Fact Verification [用于表格事实检测的图神经网络模型],"Deng Z.-Y., Zhang M.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102400916&doi=10.13328%2fj.cnki.jos.006184&partnerID=40&md5=beeef3002d0524a9c46f558e08fbd51e,10.13328/j.cnki.jos.006184,"In the study of natural language understanding and semantic representation, the fact verification task is very important to verify whether a textual statement is based on given factual evidence. Existing research is mainly limited to dealing with textual fact verification, while verification under structured evidence has yet to be explored, such as fact verification based on forms. TabFact is the latest table-based fact verification data set, but the baseline methods do not make good use of the structural characteristics of the table. This study takes advantage of the structural characteristics of the table and designs two models, Row-GVM (Row-level GNN-based verification model) and Cell-GVM (cell-level GNN-based verification model). They have achieved performances of 2.62% and 2.77% higher than the baseline model respectively. The results prove that these two methods using table features are indeed effective. © Copyright 2021, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Fact verification; Graph neural network; Table-based,"753, 762",,Ruan Jian Xue Bao/Journal of Software,Article,Scopus
347,,Survey on Data Management Technology for Machine Learning [支撑机器学习的数据管理技术综述],"Cui J.-W., Zhao Z., Du X.-Y.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102397695&doi=10.13328%2fj.cnki.jos.006182&partnerID=40&md5=3b0f810076ebfc5a94a21cd3321b7b63,10.13328/j.cnki.jos.006182,"Applications drive innovation. The advance of database technology is achieved in support of development of mainstream applications effectively and efficiently. OLTP, OLAP, and online machine learning modeling today all follow this trend. Machine learning extracts knowledge and realizes predictive analysis by modeling data, is the main approach of artificial intelligence technology. This work studies the training process of machine learning from the perspective of data management, summarizes data management technology through data selection, data storage, data access, automatic optimization, and system implementation, and analyzes the advantages and disadvantages of these techniques. Based on the analysis, this study proposes key challenges of data management technology for online machine learning. © Copyright 2021, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Artificial intelligence; Data management; Machine learning,"604, 621",,Ruan Jian Xue Bao/Journal of Software,Review,Scopus
348,,Automated end-to-end management of the modeling lifecycle in deep learning,"Gharibi G., Walunj V., Nekadi R., Marri R., Lee Y.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101321601&doi=10.1007%2fs10664-020-09894-9&partnerID=40&md5=e08470aa6b9653a16a05ff82f96fc8ff,10.1007/s10664-020-09894-9,"Deep learning has improved the state-of-the-art results in an ever-growing number of domains. This success heavily relies on the development and training of deep learning models–an experimental, iterative process that produces tens to hundreds of models before arriving at a satisfactory result. While there has been a surge in the number of tools and frameworks that aim at facilitating deep learning, the process of managing the models and their artifacts is still surprisingly challenging and time-consuming. Existing model-management solutions are either tailored for commercial platforms or require significant code changes. Moreover, most of the existing solutions address a single phase of the modeling lifecycle, such as experiment monitoring, while ignoring other essential tasks, such as model deployment. In this paper, we present a software system to facilitate and accelerate the deep learning lifecycle, named ModelKB. ModelKB can automatically manage the modeling lifecycle end-to-end, including (1) monitoring and tracking experiments; (2) visualizing, searching for, and comparing models and experiments; (3) deploying models locally and on the cloud; and (4) sharing and publishing trained models. Moreover, our system provides a stepping-stone for enhanced reproducibility. ModelKB currently supports TensorFlow 2.0, Keras, and PyTorch, and it can be extended to other deep learning frameworks easily. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC part of Springer Nature.",Data management; Deep learning; Software automation,,,Empirical Software Engineering,Article,Scopus
349,,On construction of transfer learning for facial symmetry assessment before and after orthognathic surgery,"Lin H.-H., Chiang W.-C., Yang C.-T., Cheng C.-T., Zhang T., Lo L.-J.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100168249&doi=10.1016%2fj.cmpb.2021.105928&partnerID=40&md5=0fc79149a5e06de569eac18a187ba140,10.1016/j.cmpb.2021.105928,"Orthognathic surgery (OGS) is frequently used to correct facial deformities associated with skeletal malocclusion and facial asymmetry. An accurate evaluation of facial symmetry is a critical for precise surgical planning and the execution of OGS. However, no facial symmetry scoring standard is available. Typically, orthodontists or physicians simply judge facial symmetry. Therefore, maintaining accuracy is difficult. We propose a convolutional neural network with a transfer learning approach for facial symmetry assessment based on 3-dimensional (3D) features to assist physicians in enhancing medical treatments. We trained a new model to score facial symmetry using transfer learning. Cone-beam computed tomography scans in 3D were transformed into contour maps that preserved 3D characteristics. We used various data preprocessing and amplification methods to determine the optimal results. The original data were enlarged by 100 times. We compared the quality of the four models in our experiment, and the neural network architecture was used in the analysis to import the pretraining model. We also increased the number of layers, and the classification layer was fully connected. We input random deformation data during training and dropout to prevent the model from overfitting. In our experimental results, the Xception model and the constant data amplification approach achieved an accuracy rate of 90%. © 2021 Elsevier B.V.",CNN; Data preprocessing; Deep learning; Facial symmetry; Transfer learning,,,Computer Methods and Programs in Biomedicine,Article,Scopus
350,,Efficient network selection for computer-aided cataract diagnosis under noisy environment,"Pratap T., Kokil P.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100151851&doi=10.1016%2fj.cmpb.2021.105927&partnerID=40&md5=6ae3e54f9caac0af3eb2a85a6953cb3e,10.1016/j.cmpb.2021.105927,"Background and objective:Computer-aided cataract diagnosis (CACD) methods play a crucial role in early detection of cataract. The existing CACD methods are suffering from performance diminution due to the presence of noise in digital fundus retinal images. The lack of robustness in CACD methods against noise is a serious concern since even the presence of small noise levels may degrade the performance of cataract detection. However, noise in fundus retinal images is unavoidable due to various processes involved in the acquisition or transmission. Hence, a robust CACD method against noisy conditions is required to diagnose the cataract accurately. Methods:In this paper, an efficient network selection based robust CACD method under additive white Gaussian noise (AWGN) is proposed. The presented method consists a set of locally- and globally-trained independent support vector networks with features extracted at various noise levels. A suitable network is then selected based on the noise level present in the input image. The automatic feature extraction technique using pre-trained convolutional neural network (CNN) is adopted to extract features from input fundus retinal images. Results:A good-quality fundus retinal image dataset is obtained from EyePACS dataset with the use of natural image quality evaluator (NIQE) score. The synthetic noisy fundus retinal images are then generated artificially from good-quality fundus retinal images using AWGN model for effective analysis. The analysis is carried out with existing CNN based CACD methods at different noise levels. From results it is obvious that the proposed CACD method is superior in exhibiting robust performance against AWGN than existing CNN based CACD methods. Conclusions:From the experimental results, it is clear that the proposed method show superior performance against noise when compared with existing methods in literature. The proposed method can be useful as a starting point to continue further research on CNN based robust CACD methods. © 2021 Elsevier B.V.",AWGN; Cataract; Computer-aided diagnosis; Robustness; Support vector network; Transfer learning,,,Computer Methods and Programs in Biomedicine,Article,Scopus
351,,The realist approach for evaluation of computational intelligence in software engineering,"Althar R.R., Samanta D.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099971163&doi=10.1007%2fs11334-020-00383-2&partnerID=40&md5=dbde10a4b8d25faa1a3ed1efb4cba619,10.1007/s11334-020-00383-2,"Secured software development must employ a security mindset across software engineering practices. Software security must be considered during the requirements phase so that it is included throughout the development phase. Do the requirements gathering team get the proper input from the technical team? This paper unearths some of the data sources buried within software development phases and describes the potential approaches to understand them. Concepts such as machine learning and deep learning are explored to understand the data sources and explore how these learnings can be provided to the requirements gathering team. This knowledge system will help bring objectivity in the conversations between the requirements gathering team and the customer's business team. A literature review is also done to secure requirements management and identify the possible gaps in providing future research direction to enhance our understanding. Feature engineering in the landscape of software development is explored to understand the data sources. Experts offer their insight on the root cause of the lack of security focus in requirements gathering practices. The core theme is statistical modeling of all the software artifacts that hold information related to the software development life cycle. Strengthening of some traditional methods like threat modeling is also a key area explored. Subjectivity involved in these approaches can be made more objective. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd. part of Springer Nature.",Computational intelligence; Data science; Software engineering; Software requirements management; Threat modeling,"17, 27",,Innovations in Systems and Software Engineering,Article,Scopus
352,,Comparison of machine learning models to classify Auditory Brainstem Responses recorded from children with Auditory Processing Disorder,"Wimalarathna H., Ankmnal-Veeranna S., Allan C., Agrawal S.K., Allen P., Samarabandu J., Ladak H.M.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099787713&doi=10.1016%2fj.cmpb.2021.105942&partnerID=40&md5=c92c7fd67d496aafa68f2f765c677073,10.1016/j.cmpb.2021.105942,"Introduction: Auditory brainstem responses (ABRs) offer a unique opportunity to assess the neural integrity of the peripheral auditory nervous system in individuals presenting with listening difficulties. ABRs are typically recorded and analyzed by an audiologist who manually measures the timing and quality of the waveforms. The interpretation of ABRs requires considerable experience and training, and inappropriate interpretation can lead to incorrect judgments about the integrity of the system. Machine learning (ML) techniques may be a suitable approach to automate ABR interpretation and reduce human error. Objectives: The main objective of this paper was to identify a suitable ML technique to automate the analysis of ABR responses recorded as a part of the electrophysiological testing in the Auditory Processing Disorder clinical test battery. Methods: ABR responses recorded during routine clinical assessment from 136 children being evaluated for auditory processing difficulties were analyzed using several common ML algorithms: Support Vector Machines (SVM), Random Forests (RF), Decision Trees (DT), Gradient Boosting (GB), Extreme Gradient Boosting (Xgboost), and Neural Networks (NN). A variety of signal feature extraction techniques were used to extract features from the ABR waveforms as inputs to the ML algorithms. Statistical significance testing and confusion matrices were used to identify the most robust model capable of accurately identifying neurological abnormalities present in ABRs. Results: Clinically significant features in the time-frequency representation of the signal were identified. The ML model trained using the Xgboost algorithm was identified as the most robust model with an accuracy of 92% compared to other models. Conclusion: The findings of the present study demonstrate that it is possible to develop accurate ML models to automate the process of analyzing ABR waveforms recorded at suprathreshold levels. There is currently no ML-based application to screen children with listening difficulties. Therefore, it is expected that this work will be translated into an evaluation tool that can be used by audiologists in the clinic. Furthermore, this work may aid future researchers in exploring ML paradigms to improve clinical test batteries used by audiologists in achieving accurate diagnoses. © 2021 Elsevier B.V.",Auditory Brainstem Responses; Auditory Processing Disorder; Machine Learning; Signal feature extraction,,,Computer Methods and Programs in Biomedicine,Article,Scopus
353,,Automatic medical protocol classification using machine learning approaches,"López-Úbeda P., Díaz-Galiano M.C., Martín-Noguerol T., Luna A., Ureña-López L.A., Martín-Valdivia M.T.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099632884&doi=10.1016%2fj.cmpb.2021.105939&partnerID=40&md5=1e869affbcaa3424c3a75a0459189faf,10.1016/j.cmpb.2021.105939,"Background and objective: Assignment of medical imaging procedure protocols requires extensive knowledge about patient's data, usually included in radiological request forms and radiological reports. Assignment of protocol is required prior to radiological study acquisition, determining procedure for each patient. The automation of this protocol assignment process could improve the efficiency of patient's diagnosis. Artificial intelligence has proven to be of great help in these healthcare-related problems, and specifically the application of Natural Language Processing (NLP) techniques for extracting information from text reports has been successfully used in automatic text classification tasks. Methods: In this paper, machine learning classification models based on NLP have been developed using patient's data present in radiological reports and radiological imaging protocols. We have used a real corpus provided by the private medical center “HT medica” composed of almost 700,000 Computed Tomography (CT) and Magnetic Resonance Imaging (MRI) examinations obtained during routine clinical use. We have compared several models including traditional machine learning methods such as support vector machine and random forest, neural networks and transfer language techniques. Results: The results obtained are encouraging taking into account that the system is performing a complex text multiclass classification task. Specifically, for the best proposed system we obtain 92.2% accuracy in the CT dataset and 86.9% in the MRI dataset. Conclusions: The best machine learning system is potentially efficient, quality and cost effective. For this reason it is currently used in real scenarios by radiologists as decision support tool for assigning protocols of CT and MRI studies. © 2021",Automatic protocol assignment; Image protocol; Machine learning algorithm; Natural language processing; Spanish radiological report; Textual multiclass classification task,,,Computer Methods and Programs in Biomedicine,Article,Scopus
354,,Fully automatic detection and classification of phytoplankton specimens in digital microscopy images,"Rivas-Villar D., Rouco J., Carballeira R., Penedo M.G., Novo J.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099619711&doi=10.1016%2fj.cmpb.2020.105923&partnerID=40&md5=3930c37755cc9a8d3b597a7d0b204679,10.1016/j.cmpb.2020.105923,"Background and objective: The proliferation of toxin-producing phytoplankton species can compromise the quality of the water sources. This contamination is difficult to detect, and consequently to be neutralised, since normal water purification techniques are ineffective. Currently, the water analyses about phytoplankton are commonly performed by the specialists with manual routine analyses, which represents a major limitation. The adequate identification and classification of phytoplankton specimens requires intensive training and expertise. Additionally, the performed analysis involves a lengthy process that exhibits serious problems of reliability and repeatability as inter-expert agreement is not always reached. Considering all those factors, the automatization of these analyses is, therefore, highly desirable to reduce the workload of the specialists and facilitate the process. Methods: This manuscript proposes a novel fully automatic methodology to perform phytoplankton analyses in digital microscopy images of water samples taken with a regular light microscope. In particular, we propose a method capable of analysing multi-specimen images acquired using a simplified systematic protocol. In contrast with prior approaches, this enables its use without the necessity of an expert taxonomist operating the microscope. The system is able to detect and segment the different existing phytoplankton specimens, with high variability in terms of visual appearances, and to merge them into colonies and sparse specimens when necessary. Moreover, the system is capable of differentiating them from other similar objects like zooplankton, detritus or mineral particles, among others, and then classify the specimens into defined target species of interest using a machine learning-based approach. Results: The proposed system provided satisfactory and accurate results in every step. The detection step provided a FNR of 0.4%. Phytoplankton detection, that is, differentiating true phytoplankton from similar objects (zooplankton, minerals, etc.), provided a result of 84.07% of precision at 90% of recall. The target species classification, reported an overall accuracy of 87.50%. The recall levels for each species are, 81.82% for W. naegeliana, 57.15% for A. spiroides, 85.71% for D. sociale and 95% for the ”Other” group, a set of relevant toxic and interesting species widely spread over the samples. Conclusions: The proposed methodology provided accurate results in all the designed steps given the complexity of the problem, particularly in terms of specimen identification, phytoplankton differentiation as well as the classification of the defined target species. Therefore, this fully automatic system represents a robust and consistent tool to aid the specialists in the analysis of the quality of the water sources and potability. © 2021 Elsevier B.V.",Bag of visual words; Colony merging; Deep features; Gabor filters; Microscope images; Phytoplankton detection,,,Computer Methods and Programs in Biomedicine,Article,Scopus
355,,Automated detection of conduct disorder and attention deficit hyperactivity disorder using decomposition and nonlinear techniques with EEG signals,"Tor H.T., Ooi C.P., Lim-Ashworth N.S., Wei J.K.E., Jahmunah V., Oh S.L., Acharya U.R., Fung D.S.S.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099615028&doi=10.1016%2fj.cmpb.2021.105941&partnerID=40&md5=be182f5ba2f9cd1a27831fe3a7ded6dc,10.1016/j.cmpb.2021.105941,"Background and objectives: Attention deficit hyperactivity disorder (ADHD) is often presented with conduct disorder (CD). There is currently no objective laboratory test or diagnostic method to discern between ADHD and CD, and diagnosis is further made difficult as ADHD is a common neuro-developmental disorder often presenting with other co-morbid difficulties; and in particular with conduct disorder which has a high degree of associated behavioural challenges. A novel automated system (AS) is proposed as a convenient supplementary tool to support clinicians in their diagnostic decisions. To the best of our knowledge, we are the first group to develop an automated classification system to classify ADHD, CD and ADHD+CD classes using brain signals. Methods: The empirical mode decomposition (EMD) and discrete wavelet transform (DWT) methods were employed to decompose the electroencephalogram (EEG) signals. Autoregressive modelling coefficients and relative wavelet energy were then computed on the signals. Various nonlinear features were extracted from the decomposed coefficients. Adaptive synthetic sampling (ADASYN) was then employed to balance the dataset. The significant features were selected using sequential forward selection method. The highly discriminatory features were subsequently fed to an array of classifiers. Results: The highest accuracy of 97.88% was achieved with the K-Nearest Neighbour (KNN) classifier. The proposed system was developed using ten-fold validation strategy on EEG data from 123 children. To the best of our knowledge this is the first study to develop an AS for the classification of ADHD, CD and ADHD+CD classes using EEG signals. Potential application: Our AS can potentially be used as a web-based application with cloud system to aid the clinical diagnosis of ADHD and/or CD, thus supporting faster and accurate treatment for the children. It is important to note that testing with larger data is required before the AS can be employed for clinical applications. © 2021",Attention deficit hyperactive disorder; Classifiers; Conduct disorder; K-fold validation; Machine learning; Nonlinear features; Sequential forward selection,,,Computer Methods and Programs in Biomedicine,Article,Scopus
356,,The Future of Sensitivity Analysis: An essential discipline for systems modeling and policy support,"Razavi S., Jakeman A., Saltelli A., Prieur C., Iooss B., Borgonovo E., Plischke E., Lo Piano S., Iwanaga T., Becker W., Tarantola S., Guillaume J.H.A., Jakeman J., Gupta H., Melillo N., Rabitti G., Chabridon V., Duan Q., Sun X., Smith S., Sheikholeslami R., Hosseini N., Asadzadeh M., Puy A., Kucherenko S., Maier H.R.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099475423&doi=10.1016%2fj.envsoft.2020.104954&partnerID=40&md5=c5b1b7fe003debfae8849b9e2d6895fc,10.1016/j.envsoft.2020.104954,"Sensitivity analysis (SA) is en route to becoming an integral part of mathematical modeling. The tremendous potential benefits of SA are, however, yet to be fully realized, both for advancing mechanistic and data-driven modeling of human and natural systems, and in support of decision making. In this perspective paper, a multidisciplinary group of researchers and practitioners revisit the current status of SA, and outline research challenges in regard to both theoretical frameworks and their applications to solve real-world problems. Six areas are discussed that warrant further attention, including (1) structuring and standardizing SA as a discipline, (2) realizing the untapped potential of SA for systems modeling, (3) addressing the computational burden of SA, (4) progressing SA in the context of machine learning, (5) clarifying the relationship and role of SA to uncertainty quantification, and (6) evolving the use of SA in support of decision making. An outlook for the future of SA is provided that underlines how SA must underpin a wide variety of activities to better serve science and society. © 2021 The Authors",Decision making; Machine learning; Mathematical modeling; Model robustness; Model validation and verification; Policy support; Sensitivity analysis; Uncertainty quantification,,,Environmental Modelling and Software,Article,Scopus
357,,Kashin-Beck disease diagnosis based on deep learning from hand X-ray images,"Dang J., Li H., Niu K., Xu Z., Lin J., He Z.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099252752&doi=10.1016%2fj.cmpb.2020.105919&partnerID=40&md5=8a816cbac09863798c60624bd060455e,10.1016/j.cmpb.2020.105919,"Background and objective: Kashin-Beck Disease (KBD) is a serious endemic bone disease leading to short stature. The early radiological examinations are crucial for potential patients. However, many children in rural China cannot be diagnosed in time due to the shortage of professional orthopedists. In this paper, an algorithm is developed to automatically screening KBD based on hand X-ray images of subjects, which can help the government reducing human resources investment and assisting the poor precisely. Methods: The KBD diagnosis method focuses on multi-feature fusion for classification. Two kinds of features presented in X-ray images are extracted by a deep convolutional neural network (DCNN). One is the global features that represent shapes and structures of the whole hand bone. The other is local features that represent edge and texture information from critical regions of the metaphysis. The global features tend to sketch the major informative parts, whereas other fine local features can provide supplementary information. Then both kinds of features are combined and fed into the KBD classifier of a fully connected neural network (FCNN) to obtain diagnostic results. Result: Our research team collected 960 samples in KBD endemic areas of Tibet from 2017 to 2018. The dataset contains 219 KBD positive images and 741 negative images. Experiments indicate that the method based on multi-feature achieves the best average accuracy and sensitivity rate of of 98.5% and 97.6% for diagnosis, which is 4.0% and 7.6% higher than the method with only the global features respectively. Conclusions: The KBD diagnosis method shows that our proposed multi-feature fusion helps to achieve higher diagnosis performance and stability compared with only using global features for detection. The automated KBD diagnosis algorithm provides substantial benefits to reduce large-scale screening costs and missed diagnosis rate. © 2020",Deep convolutional neural network; Deep learning; Image processing; Kashin-Beck disease,,,Computer Methods and Programs in Biomedicine,Article,Scopus
358,,Integrating segmentation information into CNN for breast cancer diagnosis of mammographic masses,"Tsochatzidis L., Koutla P., Costaridou L., Pratikakis I.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099018774&doi=10.1016%2fj.cmpb.2020.105913&partnerID=40&md5=208739e1055459b25de1581374bd3d68,10.1016/j.cmpb.2020.105913,"Background and ObjectivesSegmentation of mammographic lesions has been proven to be a valuable source of information, as it can assist in both extracting shape-related features and providing accurate localization of the lesion. In this work, a methodology is proposed for integrating mammographic mass segmentation information into a convolutional neural network (CNN), aiming to improve the diagnosis of breast cancer in mammograms. MethodsThe proposed methodology involves modification of each convolutional layer of a CNN, so that information of not only the input image but also the corresponding segmentation map is considered. Furthermore, a new loss function is introduced, which adds an extra term to the standard cross-entropy, aiming to steer the attention of the network to the mass region, penalizing strong feature activations based on their location. The segmentation maps are acquired either from the provided ground-truth or from an automatic segmentation stage. ResultsPerformance evaluation in diagnosis is conducted on two mammographic mass datasets, namely DDSM-400 and CBIS-DDSM, with differences in quality of the corresponding ground-truth segmentation maps. The proposed method achieves diagnosis performance of 0.898 and 0.862 in terms AUC when using ground-truth segmentation maps and a maximum of 0.880 and 0.860 when a U-Net-based automatic segmentation stage is employed, for DDSM-400 and CBIS-DDSM, respectively. ConclusionsThe experimental results demonstrate that integrating segmentation information into a CNN leads to improved performance in breast cancer diagnosis of mammographic masses. © 2020 Elsevier B.V.",Convolutional neural networks; Deep learning; Diagnosis; Mammography; Segmentation,,,Computer Methods and Programs in Biomedicine,Article,Scopus
359,,Coronary angiography image segmentation based on PSPNet,"Zhu X., Cheng Z., Wang S., Chen X., Lu G.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097749571&doi=10.1016%2fj.cmpb.2020.105897&partnerID=40&md5=ce0dfa61bcdf36f2d36a3f11675bb5d2,10.1016/j.cmpb.2020.105897,"Purpose: Coronary artery disease (CAD) is known to have high prevalence, high disability and mortality. The incidence and mortality of cardiovascular disease are also gradually increasing worldwide. Therefore, our paper proposes to use a more efficient image processing method to extract accurate vascular structures from vascular images by combining computer vision and deep learning. Method: Our proposed segmentation of coronary angiography images based on PSPNet network was compared with FCN, and analyzed and discussed the experimental results using three evaluation indicators of precision, recall and Fl-score. Aiming at the complex and changeable structure of coronary angiography images and over-fitting or parameter structure destruction, we implemented the parallel multi-scale convolutional neural network model using PSPNet, using small sample transfer learning that limits parameter learning method. Results: The accuracy of our technique proposed in this paper is 0.957. The accuracy of PSPNet is 26.75% higher than the traditional algorithm and 4.59% higher than U-Net. The average segmentation accuracy of the PSPNet model using transfer learning on the test set increased from 0.926 to 0.936, the sensitivity increased from 0.846 to 0.865, and the specificity increased from 0.921 to 0.949. The segmentation effect in this paper is closest to the segmentation result of the human expert, and is smoother than that of U-Net segmentation. Conclusion: The PSPNet network reduces manual interaction in diagnosis, reduces dependence on medical personnel, improves the efficiency of disease diagnosis, and provides auxiliary strategies for subsequent medical diagnosis systems based on cardiac coronary angiography. © 2020",blood vessel segmentation; Coronary angiography images; deep learning; multi-scale convolutional neural network; transfer learning,,,Computer Methods and Programs in Biomedicine,Article,Scopus
360,,A novel extended Kalman filter with support vector machine based method for the automatic diagnosis and segmentation of brain tumors,"Chen B., Zhang L., Chen H., Liang K., Chen X.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097679500&doi=10.1016%2fj.cmpb.2020.105797&partnerID=40&md5=1396705125b34df7f6cd1dde32e84c65,10.1016/j.cmpb.2020.105797,"Background: Brain tumors are life-threatening, and their early detection is crucial for improving survival rates. Conventionally, brain tumors are detected by radiologists based on their clinical experience. However, this process is inefficient. This paper proposes a machine learning-based method to 1) determine the presence of a tumor, 2) automatically segment the tumor, and 3) classify it as benign or malignant. Methods: We implemented an Extended Kalman Filter with Support Vector Machine (EKF-SVM), an image analysis platform based on an SVM for automated brain tumor detection. A development dataset of 120 patients which supported by Tiantan Hospital was used for algorithm training. Our machine learning algorithm has 5 components as follows. Firstly, image standardization is applied to all the images. This is followed by noise removal with a non-local means filter, and contrast enhancement with improved dynamic histogram equalization. Secondly, a gray-level co-occurrence matrix is utilized for feature extraction to get the image features. Thirdly, the extracted features are fed into a SVM for classify the MRI initially, and an EKF is used to classify brain tumors in the brain MRIs. Fourthly, cross-validation is used to verify the accuracy of the classifier. Finally, an automatic segmentation method based on the combination of k-means clustering and region growth is used for detecting brain tumors. Results: With regard to the diagnostic performance, the EKF-SVM had a 96.05% accuracy for automatically classifying brain tumors. Segmentation based on k-means clustering was capable of identifying the tumor boundaries and extracting the whole tumor. Conclusion: The proposed EKF-SVM based method has better classification performance for positive brain tumor images, which was mainly due to the dearth of negative examples in our dataset. Therefore, future work should obtain more negative examples and investigate the performance of deep learning algorithms such as the convolutional neural networks for automatic diagnosis and segmentation of brain tumors. © 2020",Automatic segmentation; Brain MRI diagnosis; Brain tumor segmentation; EKF-SVM; Image standardization,,,Computer Methods and Programs in Biomedicine,Article,Scopus
361,,Removal of manually induced artifacts in ultrasound images of thyroid nodules based on edge-connection and Criminisi image restoration algorithm,"Sun M., Meng Q., Wang T., Liu T., Zhu Y., Qiu J., Lu W.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097058994&doi=10.1016%2fj.cmpb.2020.105868&partnerID=40&md5=bf5ee8eabf27b42447e89182bf0d9ff9,10.1016/j.cmpb.2020.105868,"Background and Objective: There are various artificial markers in ultrasound images of thyroid nodules, which have impact on subsequent processing and computer-aided diagnosis. The purpose of this study was to develop an approach to automatically remove artifacts and restore ultrasound images of thyroid nodules. Methods: Fifty ultrasound images with manually induced artifacts were selected from publicly available and self-collected datasets. A combined approach was developed which consisted of two steps, artifacts detection and removal of the detected artifacts. Specifically, a novel edge-connection algorithm was used for artifact detection, detection accuracy and false discovery rate were used to evaluate the performance of artifact detection approaches. Criminisi algorithm was used for image restoration with peak signal-to-noise ratio (PSNR) and mean gradient difference to evaluate its performance. In addition, computation complexity was evaluated by execution time of relevant algorithms. Results: Results revealed that the proposed joint approach with edge-connection and Criminisi algorithm could achieve automatic artifacts removal. Mean detection accuracy and mean false discovery rate of the proposed edge-connection algorithm for the 50 ultrasound images were 0.86 and 1.50. Mean PSNR of the 50 restored images by Criminisi algorithm was 36.64 dB, and mean gradient difference of the restored images was -0.002 compared with the original images. Conclusions: The proposed combined approach had a good detection accuracy for different types of manually induced artifacts, and could significantly improve PSNR of the ultrasound images. The proposed combined approach may have potential use for the repair of ultrasound images with artifacts. © 2020",artifacts removal; image recovery; thyroid nodule; ultrasound image,,,Computer Methods and Programs in Biomedicine,Article,Scopus
362,,Automatic detection and segmentation of lumbar vertebrae from X-ray images for compression fracture evaluation,"Kim K.C., Cho H.C., Jang T.J., Choi J.M., Seo J.K.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096912264&doi=10.1016%2fj.cmpb.2020.105833&partnerID=40&md5=96686e9710a9e359f4f13ae34a8cb712,10.1016/j.cmpb.2020.105833,"For compression fracture detection and evaluation, an automatic X-ray image segmentation technique that combines deep-learning and level-set methods is proposed. Automatic segmentation is much more difficult for X-ray images than for CT or MRI images because they contain overlapping shadows of thoracoabdominal structures including lungs, bowel gases, and other bony structures such as ribs. Additional difficulties include unclear object boundaries, the complex shape of the vertebra, inter-patient variability, and variations in image contrast. Accordingly, a structured hierarchical segmentation method is presented that combines the advantages of two deep-learning methods. Pose-driven learning is used to selectively identify the five lumbar vertebrae in an accurate and robust manner. With knowledge of the vertebral positions, M-net is employed to segment the individual vertebra. Finally, fine-tuning segmentation is applied by combining the level-set method with the previously obtained segmentation results. The performance of the proposed method was validated by 160 lumbar X-ray images, resulting in a mean Dice similarity metric of 91.60±2.22%. The results show that the proposed method achieves accurate and robust identification of each lumbar vertebra and fine segmentation of individual vertebra. © 2020",Deep learning; Level-set; Lumbar X-ray; Vertebra detection; Vertebra segmentation,,,Computer Methods and Programs in Biomedicine,Article,Scopus
363,,Feature selection and embedding based cross project framework for identifying crashing fault residence,"Xu Z., Zhang T., Keung J., Yan M., Luo X., Zhang X., Xu L., Tang Y.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096664521&doi=10.1016%2fj.infsof.2020.106452&partnerID=40&md5=c1a393d8f47de07151956128deff0675,10.1016/j.infsof.2020.106452,"Context: The automatically produced crash reports are able to analyze the root of fault causing the crash (crashing fault for short) which is a critical activity for software quality assurance. Objective: Correctly predicting the existence of crashing fault residence in stack traces of crash report can speed up program debugging process and optimize debugging efforts. Existing work focused on the collected label information from bug-fixing logs, and the extracted features of crash instances from stack traces and source code for Identification of Crashing Fault Residence (ICFR) of newly-submitted crashes. This work develops a novel cross project ICFR framework to address the data scarcity problem by using labeled crash data of other project for the ICFR task of the project at hand. This framework removes irrelevant features, reduces distribution differences, and eases the class imbalance issue of cross project data since these factors may negatively impact the ICFR performance. Method: The proposed framework, called FSE, combines Feature Selection and feature Embedding techniques. The FSE framework first uses an information gain ratio based feature ranking method to select a relevant feature subset for cross project data, and then employs a state-of-the-art Weighted Balanced Distribution Adaptation (WBDA) method to map features of cross project data into a common space. WBDA considers both marginal and conditional distributions as well as their weights to reduce data distribution discrepancies. Besides, WBDA balances the class proportion of each project data to alleviate the class imbalance issue. Results: We conduct experiments on 7 projects to evaluate the performance of our FSE framework. The results show that FSE outperforms 25 methods under comparison. Conclusion: This work proposes a cross project learning framework for ICFR, which uses feature selection and embedding to remove irrelevant features and reduce distribution differences, respectively. The results illustrate the performance superiority of our FSE framework. © 2020",Crashing fault; Cross project framework; Feature embedding; Feature selection; Stack trace,,,Information and Software Technology,Article,Scopus
364,,YOLO Based Breast Masses Detection and Classification in Full-Field Digital Mammograms,"Aly G.H., Marey M., El-Sayed S.A., Tolba M.F.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096024503&doi=10.1016%2fj.cmpb.2020.105823&partnerID=40&md5=69ec2f72d037f4514ede5557033145de,10.1016/j.cmpb.2020.105823,"Background and Objective: With the recent development in deep learning since 2012, the use of Convolutional Neural Networks (CNNs) in bioinformatics, especially medical imaging, achieved tremendous success. Besides that, breast masses detection and classifications in mammograms and their pathology classification are considered a critical challenge. Till now, the evaluation process of the screening mammograms is held by human readers which is considered very monotonous, tiring, lengthy, costly, and significantly prone to errors. Methods: We propose an end to end computer-aided diagnosis system based on You Only Look Once (YOLO). The proposed system first preprocesses the mammograms from their DICOM format to images without losing data. Then, it detects masses in full-field digital mammograms and distinguishes between the malignant and benign lesions without any human intervention. YOLO has three different architectures, and, in this paper, the three versions are used for mass detection and classification in the mammograms to compare their performance. The use of anchors in YOLO-V3 on the original form of data and its augmented version is proved to improve the detection accuracy especially when the k-means clustering is applied to generate anchors corresponding to the used dataset. Finally, ResNet and Inception are used as feature extractors to compare their classification performance against YOLO. Results: Mammograms with different resolutions are used and based on YOLO-V3, the best results are obtained through detecting 89.4% of the masses in the INbreast mammograms with an average precision of 94.2% and 84.6% for classifying the masses as benign and malignant respectively. YOLO's classification network is replaced with ResNet and InceptionV3 to get overall accuracy of 91.0% and 95.5%, respectively. Conclusion: The proposed system showed using the experimental results the YOLO impact on the breast masses detection and classification. Especially using the anchor boxes concept in YOLO-V3 that are generated by applying k-means clustering on the dataset, we can detect most of the challenging cases of masses and classify them correctly. Also, by augmenting the dataset using different approaches and comparing with other recent YOLO based studies, it is found that augmenting the training set only is the fairest and accurate to be applied in the realistic scenarios. © 2020 Elsevier B.V.",Anchor boxes; Breast masses classification; Full-field digital mammograms; K-means clustering; YOLO based breast mass detection,,,Computer Methods and Programs in Biomedicine,Article,Scopus
365,,Author classification using transfer learning and predicting stars in co-author networks,"Abbasi R., Kashif Bashir A., Chen J., Mateen A., Piran J., Amin F., Luo B.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091604480&doi=10.1002%2fspe.2884&partnerID=40&md5=943c2eacbbe348f7c13ec632539b3f9f,10.1002/spe.2884,"The vast amount of data is key challenge to mine a new scholar that is plausible to be star in the upcoming period. The enormous amount of unstructured data raise every year is infeasible for traditional learning; consequently, we need a high quality of preprocessing technique to expand the performance of traditional learning. We have persuaded a novel approach, Authors classification algorithm using Transfer Learning (ACTL) to learn new task on target area to mine the external knowledge from the source domain. Comprehensive experimental outcomes on real-world networks showed that ACTL, Node-based Influence Predicting Stars, Corresponding Authors Mutual Influence based on Predicting Stars, and Specific Topic Domain-based Predicting Stars enhanced the node classification accuracy as well as predicting rising stars to compared with contemporary baseline methods. © 2020 John Wiley & Sons Ltd",author classification; semantic web; social network; transfer learning,"645, 669",,Software - Practice and Experience,Conference Paper,Scopus
366,,Correlation feature and instance weights transfer learning for cross project software defect prediction,"Zou Q., Lu L., Qiu S., Gu X., Cai Z.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108906184&doi=10.1049%2fsfw2.12012&partnerID=40&md5=9a06deaee0233e0855b74c84ee602908,10.1049/sfw2.12012,"Due to the differentiation between training and testing data in the feature space, cross-project defect prediction (CPDP) remains unaddressed within the field of traditional machine learning. Recently, transfer learning has become a research hot-spot for building classifiers in the target domain using the data from the related source domains. To implement better CPDP models, recent studies focus on either feature transferring or instance transferring to weaken the impact of irrelevant cross-project data. Instead, this work proposes a dual weighting mechanism to aid the learning process, considering both feature transferring and instance transferring. In our method, a local data gravitation between source and target domains determines instance weight, while features that are highly correlated with the learning task, uncorrelated with other features and minimizing the difference between the domains are rewarded with a higher feature weight. Experiments on 25 real-world datasets indicate that the proposed approach outperforms the existing CPDP methods in most cases. By assigning weights based on the different contribution of features and instances to the predictor, the proposed approach is able to build a better CPDP model and demonstrates substantial improvements over the state-of-the-art CPDP models. © 2021 The Authors. IET Software published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology.",,"55, 74",,IET Software,Article,Scopus
367,,Positive Influence Maximization in the Signed Social Networks Considering Polarity Relationship and Propagation Probability,"Qiu L., Zhang S., Yu J.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101998445&doi=10.1142%2fS0218194021500078&partnerID=40&md5=cc1b01663b1de1e9c814ed3b0e5edd31,10.1142/S0218194021500078,"The purpose of influence maximization problem is to select a small seed set to maximize the number of nodes influenced by the seed set. For viral marketing, the problem of influence maximization plays a vital role. Current works mainly focus on the unsigned social networks, which include only positive relationship between users. However, the influence maximization in the signed social networks including positive and negative relationships between users is still a challenging issue. Moreover, the existing works pay more attention to the positive influence. Therefore, this paper first analyzes the positive maximization influence in the signed social networks. The purpose of this problem is to select the seed set with the most positive influence in the signed social networks. Afterwards, this paper proposes a model that incorporates the state of node, the preference of individual and polarity relationship, called Independent Cascade with the Negative and Polarity (ICWNP) propagation model. On the basis of the ICWNP model, this paper proposes a Greedy with ICWNP algorithm. Finally, on four real social networks, experimental results manifest that the proposed algorithm has higher accuracy and efficiency than the related methods. © 2021 World Scientific Publishing Company.",Independent Cascade model; influence maximization; polarity relationship; Social networks,"249, 267",,International Journal of Software Engineering and Knowledge Engineering,Article,Scopus
368,,Survey on Multimodal Visual Language Representation Learning [多模态视觉语言表征学习研究综述],"Du P.-F., Li X.-Y., Gao Y.-L.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101406444&doi=10.13328%2fj.cnki.jos.006125&partnerID=40&md5=86593e9a4b4a088222bd5aff09626fef,10.13328/j.cnki.jos.006125,"A multimedia world in which human beings live is built from a large number of different modal contents. The information between different modalities is highly correlated and complementary. The main purpose of multi-modal representation learning is to mine the different modalities. Commonness and characteristics produce implicit vectors that can represent multimodal information. This article mainly introduces the corresponding research work of the currently widely used visual language representation, including traditional research methods based on similarity models and current mainstream pre-training methods based on language models. The current better ideas and solutions are to semanticize visual features and then generate representations with textual features through a powerful feature extractor. Transformer is currently used in various tasks of representation learning as the mainstream network architecture. This article elaborates from several different angles of research background, division of different studies, evaluation methods, future development trends, etc. © Copyright 2021, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Deep learning; Multimodal machine learning; Multimodal representation learning; Representation learning,"327, 348",,Ruan Jian Xue Bao/Journal of Software,Review,Scopus
369,,Survey on Knowledge-based Zero-shot Visual Recognition [基于知识的零样本视觉识别综述],"Feng Y.-G., Yu J., Sang J.-T., Yang P.-B.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101397354&doi=10.13328%2fj.cnki.jos.006146&partnerID=40&md5=f6242c76a13f564ffcc64e963ee43bdc,10.13328/j.cnki.jos.006146,"Zero-shot learning aims to recognize the unseen classes by using the knowledge of the seen classes that has been learned. In recent years, 'knowledge+data driven' has become a new trend but lacking of unified definition of 'knowledge' in the current zero-shot tasks of computer vision. This study tries to define the 'knowledge' in this field and divided it into three categories, which are primary knowledge, abstract knowledge, and external knowledge. In addition, based on the definition and classification of knowledge, the current works on zero-shot learning (mainly in image classification task) are sorted out, they are divided into zero-shot models based on primary knowledge, zero-shot models based on abstract knowledge, and zero-shot models based on external knowledge. This study also introduces the problems which are domain shift and hubness in this field, and further summarizes existing works based on the problems. Finally, the paper summarizes the datasets and knowledge bases that commonly used in image classification tasks, the evaluation criteria of image classification experiment and the experimental results of representative models. The future works are also summarized and prospected. © Copyright 2021, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Abstract knowledge; External knowledge; Image classification; Primary knowledge; Zero-shot learning,"370, 405",,Ruan Jian Xue Bao/Journal of Software,Review,Scopus
370,,Suvery of Medical Image Segmentation Technology Based on U-Net Structure Improvement [基于U-Net结构改进的医学影像分割技术综述],"Yin X.-H., Wang Y.-C., Li D.-Y.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101387455&doi=10.13328%2fj.cnki.jos.006104&partnerID=40&md5=cc1d1f69d931644adc90df69cdb6b787,10.13328/j.cnki.jos.006104,"The application of deep learning in the field of medical image segmentation has attracted great attentions, among which the U-Net proposed in 2015 has been widely concerned because of its good segmentation effect and scalable structure. In recent years, with the improvement of the performance requirements of medical image segmentation, many scholars are improving and expanding the U-Net structure, such as the improvement of encoder-decoder, or the external feature pyramid, and so on. In this study, the medical image segmentation technology based on U-Net structure improvement is summarized from the aspects of performance-oriented optimization and structure-oriented improvement. Related methods are reviewed, classified and summarized. The paper evaluates the parameters and modules, and then summarizes the ideas and methods for improving the U-Net structure for different goals, which provides references for related research. © Copyright 2021, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Deep neural network; Medical image segmentation; Structural improvement; Technology survey; U-Net,"519, 550",,Ruan Jian Xue Bao/Journal of Software,Review,Scopus
371,,Survey on Few-shot Learning [小样本学习研究综述],"Zhao K.-L., Jin X.-L., Wang Y.-Z.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101370732&doi=10.13328%2fj.cnki.jos.006138&partnerID=40&md5=aeff173394c0e7dbd7f948e6e9e83464,10.13328/j.cnki.jos.006138,"Few-shot learning is defined as learning models to solve problems from small samples. In recent years, under the trend of training model with big data, machine learning and deep learning have achieved success in many fields. However, in many application scenarios in the real world, there is not a large amount of data or labeled data for model training, and labeling a large number of unlabeled samples will cost a lot of manpower. Therefore, how to use a small number of samples for learning has become a problem that needs to be paid attention to at present. This paper systematically combs the current approaches of few-shot learning. It introduces each kind of corresponding model from the three categories: fine-tune based, data augmentation based, and transfer learning based. Then, the data augmentation based approaches are subdivided into unlabeled data based, data generation based, and feature augmentation based approaches. The transfer learning based approaches are subdivided into metric learning based, meta-learning based, and graph neural network based methods. In the following, the paper summarizes the few-shot datasets and the results in the experiments of the aforementioned models. Next, the paper summarizes the current situation and challenges in few-shot learning. Finally, the future technological development of few-shot learning is prospected. © Copyright 2021, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Data augmentation; Few-shot learning; Fine-tune; Meta-learning; Metric learning; Transfer learning,"349, 369",,Ruan Jian Xue Bao/Journal of Software,Review,Scopus
372,,Review of Image Steganalysis Based on Deep Learning [基于深度学习的图像隐写分析综述],"Chen J.-F., Fu Z.-J., Zhang W.-M., Cheng X., Sun X.-M.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101364102&doi=10.13328%2fj.cnki.jos.006135&partnerID=40&md5=4a6cbcc53c0b9ad1e77e6c8df43655f3,10.13328/j.cnki.jos.006135,"Steganography and steganalysis are one of the research hotspots in the field of information security. The abuse of steganography has caused many potential safety hazards. For example, illegal elements use steganography for covert communications to carry out terrorist attacks. The design of traditional steganalysis methods requires a large amount of prior knowledge, and the steganalysis methods based on deep learning use the powerful representation learning ability of the network to autonomously extract abnormal image features, which greatly reduces human participation and achieves good results. To promote the research of steganalysis technology based on deep learning, this study analyzes and summarizes the main methods and work in the field of steganalysis. Firstly, this study analyzes and compares the differences between traditional steganalysis and deep learning-based steganalysis. Furthermore, according to the different training methods, the steganalysis models based on deep learning are divided into two categories: semi-learning steganalysis model and full-learningsteganalysis model. The network structure and detection effect of various types of steganalysis based on deep learning are introduced in detail. In addition, the challenges that the adversarial samples pose to deep learning security are analyzed and summarized, the detection method of adversarial samples is expounded based on steganalysis. Finally, this study summarizes the pros and cons of existing steganalysis models based on deep learning and discusses its development trends. © Copyright 2021, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Adversarial examples; Convolution neural networks; Deep learning; Steganalysis; Steganography,"551, 578",,Ruan Jian Xue Bao/Journal of Software,Review,Scopus
373,,Pathological myopia classification with simultaneous lesion segmentation using deep learning,"Hemelings R., Elen B., Blaschko M.B., Jacob J., Stalmans I., De Boever P.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098953902&doi=10.1016%2fj.cmpb.2020.105920&partnerID=40&md5=326a1d5d1b13192f103f5ac3e4d1d006,10.1016/j.cmpb.2020.105920,"Background and Objectives: Pathological myopia (PM) is the seventh leading cause of blindness, with a reported global prevalence up to 3%. Early and automated PM detection from fundus images could aid to prevent blindness in a world population that is characterized by a rising myopia prevalence. We aim to assess the use of convolutional neural networks (CNNs) for the detection of PM and semantic segmentation of myopia-induced lesions from fundus images on a recently introduced reference data set. Methods: This investigation reports on the results of CNNs developed for the recently introduced Pathological Myopia (PALM) dataset, which consists of 1200 images. Our CNN bundles lesion segmentation and PM classification, as the two tasks are heavily intertwined. Domain knowledge is also inserted through the introduction of a new Optic Nerve Head (ONH)-based prediction enhancement for the segmentation of atrophy and fovea localization. Finally, we are the first to approach fovea localization using segmentation instead of detection or regression models. Evaluation metrics include area under the receiver operating characteristic curve (AUC) for PM detection, Euclidean distance for fovea localization, and Dice and F1 metrics for the semantic segmentation tasks (optic disc, retinal atrophy and retinal detachment). Results: Models trained with 400 available training images achieved an AUC of 0.9867 for PM detection, and a Euclidean distance of 58.27 pixels on the fovea localization task, evaluated on a test set of 400 images. Dice and F1 metrics for semantic segmentation of lesions scored 0.9303 and 0.9869 on optic disc, 0.8001 and 0.9135 on retinal atrophy, and 0.8073 and 0.7059 on retinal detachment, respectively. Conclusions: We report a successful approach for a simultaneous classification of pathological myopia and segmentation of associated lesions. Our work was acknowledged with an award in the context of the “Pathological Myopia detection from retinal images” challenge held during the IEEE International Symposium on Biomedical Imaging (April 2019). Considering that (pathological) myopia cases are often identified as false positives and negatives in glaucoma deep learning models, we envisage that the current work could aid in future research to discriminate between glaucomatous and highly-myopic eyes, complemented by the localization and segmentation of landmarks such as fovea, optic disc and atrophy. © 2020",convolutional neural network; fovea localization; fundus image; glaucoma; Pathological myopia; peripapillary atrophy; retinal detachment,,,Computer Methods and Programs in Biomedicine,Article,Scopus
374,,Adversarial multi-source transfer learning in healthcare: Application to glucose prediction for diabetic people,"De Bois M., El Yacoubi M.A., Ammi M.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097742294&doi=10.1016%2fj.cmpb.2020.105874&partnerID=40&md5=17683c632bbf6f1ce8f15418c79048cb,10.1016/j.cmpb.2020.105874,"Background and objectives: Deep learning has yet to revolutionize general practices in healthcare, despite promising results for some specific tasks. This is partly due to data being in insufficient quantities hurting the training of the models. To address this issue, data from multiple health actors or patients could be combined by capitalizing on their heterogeneity through the use of transfer learning. Methods: To improve the quality of the transfer between multiple sources of data, we propose a multi-source adversarial transfer learning framework that enables the learning of a feature representation that is similar across the sources, and thus more general and more easily transferable. We apply this idea to glucose forecasting for diabetic people using a fully convolutional neural network. The evaluation is done by exploring various transfer scenarios with three datasets characterized by their high inter and intra variability. Results: While transferring knowledge is beneficial in general, we show that the statistical and clinical accuracies can be further improved by using of the adversarial training methodology, surpassing the current state-of-the-art results. In particular, it shines when using data from different datasets, or when there is too little data in an intra-dataset situation. To understand the behavior of the models, we analyze the learnt feature representations and propose a new metric in this regard. Contrary to a standard transfer, the adversarial transfer does not discriminate the patients and datasets, helping the learning of a more general feature representation. Conclusion: The adversarial training framework improves the learning of a general feature representation in a multi-source environment, enhancing the knowledge transfer to an unseen target. The proposed method can help improve the efficiency of data shared by different health actors in the training of deep models. © 2020 Elsevier B.V.",Artificial intelligence; Deep learning; Diabetes; Neural networks; Personalized medicine; Transfer learning,,,Computer Methods and Programs in Biomedicine,Article,Scopus
375,,Adverse Drug Reaction extraction: Tolerance to entity recognition errors and sub-domain variants,"Santiso S., Pérez A., Casillas A.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097719923&doi=10.1016%2fj.cmpb.2020.105891&partnerID=40&md5=1326ebce1f7efd49a41083110ad714b3,10.1016/j.cmpb.2020.105891,"• Background and Objective:This work tackles the Adverse Drug Reaction (ADR) extraction in Electronic Health Records (EHRs) written in Spanish. This task is within the framework of natural language processing. It consists of extracting relations between drug-disease pairs, with the drug as the causing agent of the reaction. To this end, a pipeline is employed: first, relevant clinical entities are recognized (e.g. drugs, active ingredients, findings, symptoms); next, drug-disease candidate pairs are judged as either ADR or non-ADR. To develop this task, it is necessary to tackle some challenges. First, EHRs show high lexical variability. Second, EHRs are scarce due to their sensitive information. Third, the ADR detection stage has to cope with errors derived from the entity recognition. • Methods:To develop the ADR detection we decided to employ a deep neural network approach. In order to asses the tolerance to external variations, the system was exposed to different levels of noise. First, with three corpora that contain documents from different hospitals, size and class imbalance ratio. Furthermore, it was exposed to cross-corpus relation extraction. Second, we assessed the sensitivity of the ADR detection stage to noise introduced by the automatic Medical Entity Recognition (MER). • Results:The system can cope with cross-hospital predictions provided that it was trained with a large corpus. In the most challenging situation an f-measure of 75.2 was achieved. With respect to the tolerance to errors derived from the entity recognition step, with a medical entity recognizer that missed 20% of the entities, the f-measure in the ADR detection stage decreased to 68.6. • Conclusions:The ADR extraction is tackled as a cause-effect relation extraction task between drugs and diseases. It is advisable to employ as many EHRs as possible in order to make more robust the ADR extraction. Despite the entities missed in the MER step, the drop in the performance is not high with the proposed system. © 2020 Elsevier B.V.",Adverse drug reaction extraction; Deep learning; Electronic health records; Natural language processing,,,Computer Methods and Programs in Biomedicine,Article,Scopus
376,,A comprehensive comparative study of clustering-based unsupervised defect prediction models,"Xu Z., Li L., Yan M., Liu J., Luo X., Grundy J., Zhang Y., Zhang X.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096687812&doi=10.1016%2fj.jss.2020.110862&partnerID=40&md5=926219c0e671a39dbbdd1870083904d1,10.1016/j.jss.2020.110862,"Software defect prediction recommends the most defect-prone software modules for optimization of the test resource allocation. The limitation of the extensively-studied supervised defect prediction methods is that they require labeled software modules which are not always available. An alternative solution is to apply clustering-based unsupervised models to the unlabeled defect data, called Clustering-based Unsupervised Defect Prediction (CUDP). However, there are few studies to explore the impacts of clustering-based models on defect prediction performance. In this work, we performed a large-scale empirical study on 40 unsupervised models to fill this gap. We chose an open-source dataset including 27 project versions with 3 types of features. The experimental results show that (1) different clustering-based models have significant performance differences and the performance of models in the instance-violation-score-based clustering family is obviously superior to that of models in hierarchy-based, density-based, grid-based, sequence-based, and hybrid-based clustering families; (2) the models in the instance-violation-score-based clustering family achieves competitive performance compared with typical supervised models; (3) the impacts of feature types on the performance of the models are related to the indicators used; and (4)the clustering-based unsupervised models do not always achieve better performance on defect data with the combination of the 3 types of features. © 2020 Elsevier Inc.",Clustering-based unsupervised models; Data analytics for defect prediction; Empirical study,,,Journal of Systems and Software,Article,Scopus
377,,Convolutional neural networks for enhanced classification mechanisms of metamodels,"Nguyen P.T., Di Ruscio D., Pierantonio A., Di Rocco J., Iovino L.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096121119&doi=10.1016%2fj.jss.2020.110860&partnerID=40&md5=ca6285b8b8ed222076d39aa568b2626d,10.1016/j.jss.2020.110860,"Conventional wisdom on Model-Driven Engineering suggests that metamodels are crucial elements for modeling environments consisting of graphical editors, transformations, code generators, and analysis tools. Software repositories are commonly used in practice for locating existing artifacts provided that a classification procedure is available. However, the manual classification of metamodel in repositories produces results that are influenced by the subjectivity of human perception besides being tedious and prone to errors. Therefore, automated techniques for classifying metamodels stored in repositories are highly desirable and stringent. In this work, we propose MEMOCNN as a novel approach to classification of metamodels. In particular, we consider metamodels as data points and classify them using supervised learning techniques. A convolutional neural network has been built to learn from labeled data, and use the trained weights to group unlabeled metamodels. A comprehensive experimental evaluation proves that the proposal effectively categorizes input data and outperforms a state-of-the-art baseline. © 2020 Elsevier Inc.",,,,Journal of Systems and Software,Article,Scopus
378,,Revisiting heterogeneous defect prediction methods: How far are we?,"Chen X., Mu Y., Liu K., Cui Z., Ni C.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092358550&doi=10.1016%2fj.infsof.2020.106441&partnerID=40&md5=1e62671be372bad83e2145a6e1efc450,10.1016/j.infsof.2020.106441,"Context: Cross-project defect prediction applies to the scenarios that the target projects are new projects. Most of the previous studies tried to utilize the training data from other projects (i.e., the source projects). However, metrics used by practitioners to measure the extracted program modules from different projects may not be the same, and performing heterogeneous defect prediction (HDP) is challenging. Objective: Researchers have proposed many novel HDP methods with promising performance until now. Recently, unsupervised defect prediction (UDP) methods have received more attention and show competitive performance. However, to our best knowledge, whether HDP methods can perform significantly better than UDP methods has not yet been thoroughly investigated. Method: In this article, we perform a comparative study to have a holistic look at this issue. Specifically, we compare five HDP methods with four UDP methods on 34 projects in five groups under the same experimental setup from three different perspectives: non-effort-aware performance indicators (NPIs), effort-aware performance indicators (EPIs) and diversity analysis on identifying defective modules. Result: We have the following findings: (1) HDP methods do not perform significantly better than some of UDP methods in terms of two NPIs and four EPIs. (2) According to two satisfactory criteria recommended by previous studies, the satisfactory ratio of existing HDP methods is pessimistic. (3) The diversity of prediction for defective modules across HDP vs. UDP methods is more than that within HDP methods or UDP methods. Conclusion: The above findings implicate there is still a long way for the HDP issue to go. Given this, we present some observations about the road ahead for HDP. © 2020 Elsevier B.V.",Diversity analysis; Effort-aware performance indicators; Empirical studies; Heterogeneous defect prediction; Non-effort-aware performance indicators; Software defect prediction; Unsupervised defect prediction,,,Information and Software Technology,Article,Scopus
379,,An Empirical Study on Code Comment Completion,"Mastropaolo A., Aghajani E., Pascarella L., Bavota G.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123383283&doi=10.1109%2fICSME52107.2021.00021&partnerID=40&md5=624c6ff1eb1b536a253f1c00c4826f3c,10.1109/ICSME52107.2021.00021,"Code comments play a prominent role in program comprehension activities. However, source code is not always documented and code and comments not always co-evolve. To deal with these issues, researchers have proposed techniques to automatically generate comments documenting a given code at hand. The most recent works in the area applied deep learning (DL) techniques to support such a task. Despite the achieved advances, the empirical evaluations of these approaches show that they are still far from a performance level that would make them valuable for developers. We tackle a simpler and related problem: Code comment completion. Instead of generating a comment for a given code from scratch, we investigate the extent to which state-of-the-art techniques can help developers in writing comments faster. We present a large-scale study in which we empirically assess how a simple n-gram model and the recently proposed Text-To-Text Transfer Transformer (T5) architecture can perform in autocompleting a code comment the developer is typing. The achieved results show the superiority of the T5 model, despite the n-gram model being a competitive solution. © 2021 IEEE.",Code Comments; Empirical Study,"159, 170",,"Proceedings - 2021 IEEE International Conference on Software Maintenance and Evolution, ICSME 2021",Conference Paper,Scopus
380,,Task-Oriented API Usage Examples Prompting Powered by Programming Task Knowledge Graph,"Sun J., Xing Z., Peng X., Xu X., Zhu L.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123377868&doi=10.1109%2fICSME52107.2021.00046&partnerID=40&md5=477bc79e5ad47665a858ccc6fac9669b,10.1109/ICSME52107.2021.00046,"Programming tutorials demonstrate programming tasks with code examples. However, our study of Stack Overflow questions reveals the low utilization of high-quality programming tutorials, which is caused task description mismatch and code information overload. Neither document search nor recently proposed activity-centric search can address these two barriers. In this work, we enrich the programming task knowledge graph with actions extracted from comments in code examples and more forms of activity sentences. To overcome the task description mismatch problem, we use code matching based task search method to find relevant programming tasks and code examples to the code under development. We integrate our knowledge graph and task search method in the IDE, and develop an observe-push based tool to prompt developers with API usage examples in explicit task contexts. To alleviate the code information overload problem, our tool highlights programming task and API information in the prompted tutorial excerpts and code examples based on the underlying knowledge graph. Our evaluation confirms the high quality of the constructed knowledge graph, and show that our code matching based task search can recommend effective code solutions to programming issues asked on Stack Overflow. Through an user study, we demonstrate that our tool is useful for assisting developers in finding and using relevant programming tutorials in their programming tasks. © 2021 IEEE.",,"448, 459",,"Proceedings - 2021 IEEE International Conference on Software Maintenance and Evolution, ICSME 2021",Conference Paper,Scopus
381,,Duplicate Bug Report Detection by Using Sentence Embedding and Fine-tuning,"Isotani H., Washizaki H., Fukazawa Y., Nomoto T., Ouji S., Saito S.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123377197&doi=10.1109%2fICSME52107.2021.00054&partnerID=40&md5=f8c287e0ae6a37bc514cdcc738903d3f,10.1109/ICSME52107.2021.00054,"Industrial software maintenance devotes much time and effort to find duplicate bug reports. In this paper, we propose an automated duplicate bug report detection system to improve software maintenance efficiency. Our system detects duplicate reports by vectorizing the contents of each report item by deep-learning-based sentence embedding and calculating the similarity of the whole report from those of the item vectors. The Sentence-BERT fine-tuned with report texts is used for sentence embedding. Finally, we verify that the combination of processing separately by item and Sentence-BERT fine-tuned with reports effectively detects duplicate bug reports in industrial experiments that compare the performance of existing methods. © 2021 IEEE.",BERT; Bug reports; duplicate detection; information retrieval; natural language processing; sentence embedding,"535, 544",,"Proceedings - 2021 IEEE International Conference on Software Maintenance and Evolution, ICSME 2021",Conference Paper,Scopus
382,,SmartGift: Learning to Generate Practical Inputs for Testing Smart Contracts,"Zhou T., Liu K., Li L., Liu Z., Klein J., Bissyande T.F.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123371929&doi=10.1109%2fICSME52107.2021.00009&partnerID=40&md5=6f6688700e05c9ca4f93c25f7d485c78,10.1109/ICSME52107.2021.00009,"With the boom of Initial Coin Offerings (ICO) in the financial markets, smart contracts have gained rapid popularity among consumers. Smart contract vulnerabilities however made them a prime target to malicious attacks that are leading to huge losses. The research community is thus applying various software engineering technologies to smart contracts to address them. In general, to detect vulnerabilities in smart contracts, mutation and fuzz based testing approaches have been widely studied and indeed achieved promising performance on benchmark datasets. Generating test inputs with mutation approaches essentially relies on the available test cases in a smart contract program. In our preliminary study, however, we observed that 56.4% of 218 identified open-source smart contract project repositories do not provide any test case for validation. Fuzzing test inputs leads to random values and lacks practical usefulness. Our work addresses this problem: we propose an approach, Smartgift, which generates practical inputs for testing smart contracts by learning from the transaction records of real-world smart contracts. Leveraging a collected set of over 60 thousand transaction records, Smartgift is able to generate relevant test inputs for 77% smart contract functions, largely outperforming the traditional fuzzing approach (successful for only 60% functions). We further demonstrate the practicality of the test inputs by using them to replace the test inputs of the ContractFuzzer state of the art smart contract vulnerability detector: with inputs by Smartgift, ContractFuzzer can now detect 131 of the 154 vulnerabilities in its benchmark. © 2021 IEEE.",Deep Learning; Smart Contract; Test Input Generation,"23, 34",,"Proceedings - 2021 IEEE International Conference on Software Maintenance and Evolution, ICSME 2021",Conference Paper,Scopus
383,,Disambiguating Mentions of API Methods in Stack Overflow via Type Scoping,"Luong K., Thung F., Lo D.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123371631&doi=10.1109%2fICSME52107.2021.00080&partnerID=40&md5=43f9482cff8c017ddaec3e9a60dee59b,10.1109/ICSME52107.2021.00080,"Stack Overflow is one of the most popular venues for developers to find answers to their API-related questions. However, API mentions in informal text content of Stack Overflow are often ambiguous and thus it could be difficult to find the APIs and learn their usages. Disambiguating these API mentions is not trivial, as an API mention can match with names of APIs from different libraries or even the same one. In this paper, we propose an approach called DATYS to disambiguate API mentions in informal text content of Stack Overflow using type scoping. With type scoping, we consider API methods whose type (i.e. class or interface) appear in more parts (i.e., scopes) of a Stack Overflow thread as more likely to be the API method that the mention refers to. We have evaluated our approach on a dataset of 807 API mentions from 380 threads containing discussions of API methods from four popular third-party Java libraries. Our experiment shows that our approach beats the state-of-the-art by 42.86% in terms of F1-score. © 2021 IEEE.",API linking; Disambiguation; Mining,"679, 683",,"Proceedings - 2021 IEEE International Conference on Software Maintenance and Evolution, ICSME 2021",Conference Paper,Scopus
384,,Toward Less Hidden Cost of Code Completion with Acceptance and Ranking Models,"Li J., Huang R., Li W., Yao K., Tan W.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123366098&doi=10.1109%2fICSME52107.2021.00024&partnerID=40&md5=5fb026527f8992d754d45303fa442fd2,10.1109/ICSME52107.2021.00024,"Code completion is widely used by software developers to provide coding suggestions given a partially written code snippet. Apart from the traditional code completion methods, which only support single token completion at minimal positions, recent studies show the ability to provide longer code completion at more flexible positions. However, such frequently triggered and longer completion results reduce the overall precision as they generate more invalid results. Moreover, different studies are mostly incompatible with each other. Thus, it is vital to develop an ensemble framework that can combine results from multiple models to draw merits and offset defects of each model. This paper conducts a coding simulation to collect data from code context and different code completion models and then apply the data in two tasks. First, we introduce an acceptance model which can dynamically control whether to display completion results to the developer. It uses simulation features to predict whether correct results exist in the output of these models. Our best model reduces the percentage of false-positive completion from 55.09% to 17.44%. Second, we design a fusion ranking scheme that can automatically identify the priority of the completion results and reorder the candidates from multiple code completion models. This scheme is flexible in dealing with various models, regardless of the type or the length of their completion results. We integrate this ranking scheme with two frequency models and a GPT-2 styled language model, along with the acceptance model to yield 27.80% and 37.64% increase in TOP1 and TOP5 accuracy, respectively. In addition, we propose a new code completion evaluation metric, Benefit-Cost Ratio(BCR), taking into account the benefit of keystrokes saving and hidden cost of completion list browsing, which is closer to real coder experience scenario. © 2021 IEEE.",acceptance model; Code completion; evaluation metrics; neural networks; ranking model,"195, 205",,"Proceedings - 2021 IEEE International Conference on Software Maintenance and Evolution, ICSME 2021",Conference Paper,Scopus
385,,FeaRS: Recommending Complete Android Method Implementations,"Wen F., Ferrari V., Aghajani E., Nagy C., Lanza M., Bavota G.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123365740&doi=10.1109%2fICSME52107.2021.00062&partnerID=40&md5=6b9a27c8b7e74d45d3c4b03719be0701,10.1109/ICSME52107.2021.00062,"Several techniques have been proposed in the literature to support code completion, showing excellent results in predicting the next few tokens a developer is likely to type given the current context. Only recently, approaches pushing the boundaries of code completion (e.g., by presenting entire code statements) have been proposed. In this line of research, we present FeaRS, a recommender system that, given the current code a developer is writing in the IDE, recommends the next complete method to be implemented. FeaRS has been deployed to learn 'implementation patterns' (i.e., groups of methods usually implemented within the same task) by continuously mining open-source Android projects. Such knowledge is leveraged to provide method recommendations when the code written by the developer in the IDE matches an 'implementation pattern'. Preliminary results of FeaRS' accuracy show its potential as well as some open challenges to overcome. © 2021 IEEE.",Code completion; Source code recommender,"589, 593",,"Proceedings - 2021 IEEE International Conference on Software Maintenance and Evolution, ICSME 2021",Conference Paper,Scopus
386,,Leveraging Intermediate Artifacts to Improve Automated Trace Link Retrieval,"Rodriguez A.D., Cleland-Huang J., Falessi D.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123363464&doi=10.1109%2fICSME52107.2021.00014&partnerID=40&md5=1f5f8a0e372a4c8f6d5a048906143682,10.1109/ICSME52107.2021.00014,"Software traceability establishes a network of connections between diverse artifacts such as requirements, design, and code. However, given the cost and effort of creating and maintaining trace links manually, researchers have proposed automated approaches using information retrieval techniques. Current approaches focus almost entirely upon generating links between pairs of artifacts and have not leveraged the broader network of interconnected artifacts. In this paper we investigate the use of intermediate artifacts to enhance the accuracy of the generated trace links-focusing on paths consisting of source, target, and intermediate artifacts. We propose and evaluate combinations of techniques for computing semantic similarity, scaling scores across multiple paths, and aggregating results from multiple paths. We report results from five projects, including one large industrial project. We find that leveraging intermediate artifacts improves the accuracy of end-to-end trace retrieval across all datasets and accuracy metrics. After further analysis, we discover that leveraging intermediate artifacts is only helpful when a project's artifacts share a common vocabulary, which tends to occur in refinement and decomposition hierarchies of artifacts. Given our hybrid approach that integrates both direct and transitive links, we observed little to no loss of accuracy when intermediate artifacts lacked a shared vocabulary with source or target artifacts. © 2021 IEEE.",links; software traceability; traceability network,"81, 92",,"Proceedings - 2021 IEEE International Conference on Software Maintenance and Evolution, ICSME 2021",Conference Paper,Scopus
387,,Improving Traceability Link Recovery Using Fine-grained Requirements-to-Code Relations,"Hey T., Chen F., Weigelt S., Tichy W.F.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123363228&doi=10.1109%2fICSME52107.2021.00008&partnerID=40&md5=d337fb97bda2678078134231930e7f6d,10.1109/ICSME52107.2021.00008,"Traceability information is a fundamental prerequisite for many essential software maintenance and evolution tasks, such as change impact and software reusability analyses. However, manually generating traceability information is costly and error-prone. Therefore, researchers have developed automated approaches that utilize textual similarities between artifacts to establish trace links. These approaches tend to achieve low precision at reasonable recall levels, as they are not able to bridge the semantic gap between high-level natural language requirements and code. We propose to overcome this limitation by leveraging fine-grained, method and sentence level, similarities between the artifacts for traceability link recovery. Our approach uses word embeddings and a Word Mover's Distance-based similarity to bridge the semantic gap. The fine-grained similarities are aggregated according to the artifacts structure and participate in a majority vote to retrieve coarse-grained, requirement-to-class, trace links. In a comprehensive empirical evaluation, we show that our approach is able to outperform state-of-the-art unsupervised traceability link recovery approaches. Additionally, we illustrate the benefits of fine-grained structural analyses to word embedding-based trace link generation. © 2021 IEEE.",Natural Language Processing; Requirements Engineering; Traceability; Traceability Link Recovery; Word Embeddings; Word Movers Distance,"12, 22",,"Proceedings - 2021 IEEE International Conference on Software Maintenance and Evolution, ICSME 2021",Conference Paper,Scopus
388,,Assessing Generalizability of CodeBERT,"Zhou X., Han D., Lo D.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123348976&doi=10.1109%2fICSME52107.2021.00044&partnerID=40&md5=ba9c67e84ec1cb5fd8fea9b40aa5344b,10.1109/ICSME52107.2021.00044,"Pre-trained models like BERT have achieved strong improvements on many natural language processing (NLP) tasks, showing their great generalizability. The success of pre-trained models in NLP inspires pre-trained models for programming language. Recently, CodeBERT, a model for both natural language (NL) and programming language (PL), pre-trained on code search dataset, is proposed. Although promising, CodeBERT has not been evaluated beyond its pre-trained dataset for NL-PL tasks. Also, it has only been shown effective on two tasks that are close in nature to its pre-trained data. This raises two questions: Can CodeBERT generalize beyond its pre-trained data? Can it generalize to various software engineering tasks involving NL and PL? Our work answers these questions by performing an empirical investigation into the generalizability of CodeBERT. First, we assess the generalizability of CodeBERT to datasets other than its pre-training data. Specifically, considering the code search task, we conduct experiments on another dataset containing Python code snippets and their corresponding documentation. We also consider yet another dataset of questions and answers collected from Stack Overflow about Python programming. Second, to assess the generalizability of CodeBERT to various software engineering tasks, we apply CodeBERT to the just-in-time defect prediction task. Our empirical results support the generalizability of CodeBERT on the additional data and task. CodeBERT-based solutions can achieve higher or comparable performance than specialized solutions designed for the code search and just-in-time defect prediction tasks. However, the superior performance of the CodeBERT requires a tradeoff; for example, it requires much more computation resources as compared to specialized code search approaches. © 2021 IEEE.",CodeBERT; generalizability; pre-trained model,"425, 436",,"Proceedings - 2021 IEEE International Conference on Software Maintenance and Evolution, ICSME 2021",Conference Paper,Scopus
389,,D-REX: Static Detection of Relevant Runtime Exceptions with Location Aware Transformer,"Farmahinifarahani F., Lu Y., Saini V., Baldi P., Lopes C.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123283435&doi=10.1109%2fSCAM52516.2021.00032&partnerID=40&md5=28e1fd1e886e23e757ba9cb940ba6eff,10.1109/SCAM52516.2021.00032,"Runtime exceptions are inevitable parts of software systems. While developers often write exception handling code to avoid the severe outcomes of these exceptions, such code is most effective if accompanied by accurate runtime exception types. Predicting the runtime exceptions that may occur in a program, however, is difficult as the situations that lead to these exceptions are complex. We propose D-REX (Deep Runtime EXception detector), as an approach for predicting runtime exceptions of Java methods based on the static properties of code.The core of D-REX is a machine learning model that leverages the representation learning ability of neural networks to infer a set of signals from code to predict the related runtime exception types. This model, which we call Location Aware Transformer, adapts a state-of-The-Art language model, Transformer, to provide accurate predictions for the exception types, as well as interpretable recommendations for the exception prone elements of code. We curate a benchmark dataset of 200,000 Java projects from GitHub to train and evaluate D-REX. Experiments demonstrate that D-REX predicts runtime exception types with 81% of Top 1 accuracy, outperforming multiple non-Transformer baselines by a margin of at least 12%. Furthermore, it can predict the exception prone elements of code with 75% Top 1 precision. © 2021 IEEE.",exception prediction; runtime exceptions; software code modeling,"198, 208",,"Proceedings - IEEE 21st International Working Conference on Source Code Analysis and Manipulation, SCAM 2021",Conference Paper,Scopus
390,,CodemixedNLP: An Extensible and Open NLP Toolkit for Code-Mixing,"Jayanthi S.M., Nerella K., Chandu K.R., Black A.W.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123221821&doi=10.26615%2f978-954-452-056-4_014&partnerID=40&md5=57a4c825b79d98ca35ddb88503357a5d,10.26615/978-954-452-056-4_014,"The NLP community has witnessed steep progress in a variety of tasks across the realms of monolingual and multilingual language processing recently. These successes, in conjunction with the proliferating mixed language interactions on social media have boosted interest in modeling code-mixed texts. In this work, we present CODEMIXEDNLP, an open-source library with the goals of bringing together the advances in code-mixed NLP and opening it up to a wider machine learning community. The library consists of tools to develop and benchmark versatile model architectures that are tailored for mixed texts, methods to expand training sets, techniques to quantify mixing styles, and fine-tuned state-of-the-art models for 7 tasks in Hinglish1. We believe this work has a potential to foster a distributed yet collaborative and sustainable ecosystem in an otherwise dispersed space of code-mixing research. The toolkit is designed to be simple, easily extensible, and resourceful to both researchers as well as practitioners2 © 2021 Association for Computational Linguistics.",,"113, 118",,"Computational Approaches to Linguistic Code-Switching, CALCS 2021 - Proceedings of the 5th Workshop",Conference Paper,Scopus
391,,Translate and Classify: Improving Sequence Level Classification for English-Hindi Code-Mixed Data,"Gautam D., Gupta K., Shrivastava M.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123207843&doi=10.26615%2f978-954-452-056-4_003&partnerID=40&md5=ebf7e7e45809ea64540a962d5fa8cbe1,10.26615/978-954-452-056-4_003,"Code-mixing is a common phenomenon in multilingual societies around the world and is especially common in social media texts. Traditional NLP systems, usually trained on monolingual corpora, do not perform well on code-mixed texts. Training specialized models for code-switched texts is difficult due to the lack of large-scale datasets. Translating code-mixed data into standard languages like English could improve performance on various code-mixed tasks since we can use transfer learning from state-of-the-art English models for processing the translated data. This paper focuses on two sequence-level classification tasks for English-Hindi code mixed texts, which are part of the GLUECoS benchmark - Natural Language Inference and Sentiment Analysis. We propose using various pre-trained models that have been fine-tuned for similar English-only tasks and have shown state-of-the-art performance. We further fine-tune these models on the translated code-mixed datasets and achieve state-of-the-art performance in both tasks. To translate English-Hindi code-mixed data to English, we use mBART, a pre-trained multilingual sequence-to-sequence model that has shown competitive performance on various low-resource machine translation pairs and has also shown performance gains in languages that were not in its pre-training corpus. ©2021 Association for Computational Linguistics",,"15, 25",,"Computational Approaches to Linguistic Code-Switching, CALCS 2021 - Proceedings of the 5th Workshop",Conference Paper,Scopus
392,,Much Gracias: Semi-supervised Code-switch Detection for Spanish-English: How far can we get?,"Iliescu D.-M., Grand R., Qirko S., van der Goot R.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123201444&doi=10.26615%2f978-954-452-056-4_009&partnerID=40&md5=879dae6f149111cf3cf1a88d4d8b01dd,10.26615/978-954-452-056-4_009,"Because of globalization, it is becoming more and more common to use multiple languages in a single utterance, also called code-switching. This results in special linguistic structures and, therefore, poses many challenges for Natural Language Processing. Existing models for language identification in code-switched data are all supervised, requiring annotated training data which is only available for a limited number of language pairs. In this paper, we explore semi-supervised approaches, that exploit out-of-domain monolingual training data. We experiment with word uni-grams, word n-grams, character ngrams, Viterbi Decoding, Latent Dirichlet Allocation, Support Vector Machine and Logistic Regression. The Viterbi model was the best semi-supervised model, scoring a weighted F1 score of 92.23%, whereas a fully supervised state-of-the-art BERT-based model scored 98.43%.1 ©2021 Association for Computational Linguistics",,"65, 71",,"Computational Approaches to Linguistic Code-Switching, CALCS 2021 - Proceedings of the 5th Workshop",Conference Paper,Scopus
393,,Unsupervised Self-Training for Sentiment Analysis of Code-Switched Data,"Gupta A., Menghani S., Rallabandi S.K., Black A.W.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123192604&doi=10.26615%2f978-954-452-056-4_013&partnerID=40&md5=fe3df73947dd1012eb3bf7c3eb0c15ca,10.26615/978-954-452-056-4_013,"Sentiment analysis is an important task in understanding social media content like customer reviews, Twitter and Facebook feeds etc. In multilingual communities around the world, a large amount of social media text is characterized by the presence of code-switching. Thus, it has become important to build models that can handle code-switched data. However, annotated code-switched data is scarce and there is a need for unsupervised models and algorithms. We propose a general framework called Unsupervised Self-Training and show its applications for the specific use case of sentiment analysis of code-switched data. We use the power of pre-trained BERT models for initialization and fine-tune them in an unsupervised manner, only using pseudo labels produced by zero-shot transfer. We test our algorithm on multiple code-switched languages and provide a detailed analysis of the learning dynamics of the algorithm with the aim of answering the question - ‘Does our unsupervised model understand the Code-Switched languages or does it just learn its representations?’. Our unsupervised models compete well with their supervised counterparts, with their performance reaching within 1-7% (weighted F1 scores) when compared to supervised models trained for a two class problem. ©2021 Association for Computational Linguistics",,"103, 112",,"Computational Approaches to Linguistic Code-Switching, CALCS 2021 - Proceedings of the 5th Workshop",Conference Paper,Scopus
394,,On the logistical difficulties and findings of Jopara Sentiment Analysis,"Agüero-Torales M.M., Vilares D., López-Herrera A.G.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123186350&doi=10.26615%2f978-954-452-056-4_012&partnerID=40&md5=70b6dffd25bd41689cb589f0eebbdfd9,10.26615/978-954-452-056-4_012,"This paper addresses the problem of sentiment analysis for Jopara, a code-switching language between Guarani and Spanish. We first collect a corpus of Guarani-dominant tweets and discuss on the difficulties of finding quality data for even relatively easy-to-annotate tasks, such as sentiment analysis. Then, we train a set of neural models, including pre-trained language models, and explore whether they perform better than traditional machine learning ones in this low-resource setup. Transformer architectures obtain the best results, despite not considering Guarani during pre-training, but traditional machine learning models perform close due to the low-resource nature of the problem. ©2021 Association for Computational Linguistics",,"95, 102",,"Computational Approaches to Linguistic Code-Switching, CALCS 2021 - Proceedings of the 5th Workshop",Conference Paper,Scopus
395,,Investigating Code-Mixed Modern Standard Arabic-Egyptian to English Machine Translation,"Billah Nagoudi E.M., Elmadany A., Abdul-Mageed M.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123185983&doi=10.26615%2f978-954-452-056-4_008&partnerID=40&md5=457dba3e96dfc40d6cb635b22e64c4c1,10.26615/978-954-452-056-4_008,"Recent progress in neural machine translation (NMT) has made it possible to translate successfully between monolingual language pairs where large parallel data exist, with pre-trained models improving performance even further. Although there exists work on translating in code-mixed settings (where one of the pairs includes text from two or more languages), it is still unclear what recent success in NMT and language modeling exactly means for translating code-mixed text. We investigate one such context, namely MT from code-mixed Modern Standard Arabic and Egyptian Arabic (MSAEA) into English. We develop models under different conditions, employing both (i) standard end-to-end sequence-to-sequence (S2S) Transformers trained from scratch and (ii) pre-trained S2S language models (LMs). We are able to acquire reasonable performance using only MSA-EN parallel data with S2S models trained from scratch. We also find LMs fine-tuned on data from various Arabic dialects to help the MSAEA-EN task. Our work is in the context of the Shared Task on Machine Translation in Code-Switching. Our best model achieves 25.72 BLEU, placing us first on the official shared task evaluation for MSAEA-EN. ©2021 Association for Computational Linguistics",,"56, 64",,"Computational Approaches to Linguistic Code-Switching, CALCS 2021 - Proceedings of the 5th Workshop",Conference Paper,Scopus
396,,IITP-MT at CALCS2021: English to Hinglish Neural Machine Translation using Unsupervised Synthetic Code-Mixed Parallel Corpus,"Appicharla R., Gupta K.K., Ekbal A., Bhattacharyya P.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123176842&doi=10.26615%2f978-954-452-056-4_005&partnerID=40&md5=b39cc2b36e7f51726fd541f8b7542dd1,10.26615/978-954-452-056-4_005,This paper describes the system submitted by IITP-MT team to Computational Approaches to Linguistic Code-Switching (CALCS 2021) shared task on MT for English → Hinglish. We submit a neural machine translation (NMT) system which is trained on the synthetic code-mixed (cm) English-Hinglish parallel corpus. We propose an approach to create code-mixed parallel corpus from a clean parallel corpus in an unsupervised manner. It is an alignment based approach and we do not use any linguistic resources for explicitly marking any token for code-switching. We also train NMT model on the gold corpus provided by the workshop organizers augmented with the generated synthetic code-mixed parallel corpus. The model trained over the generated synthetic cm data achieves 10.09 BLEU points over the given test set. ©2021 Association for Computational Linguistics,,"31, 35",,"Computational Approaches to Linguistic Code-Switching, CALCS 2021 - Proceedings of the 5th Workshop",Conference Paper,Scopus
397,,A Language-aware Approach to Code-switched Morphological Tagging,"Özateş Ş.B., Çetinoglu Ö.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123173407&doi=10.26615%2f978-954-452-056-4_010&partnerID=40&md5=fb7e5bc49290b7e2dfa03c5271e4dab2,10.26615/978-954-452-056-4_010,"Morphological tagging of code-switching (CS) data becomes more challenging especially when language pairs composing the CS data have different morphological representations. In this paper, we explore a number of ways of implementing a language-aware morphological tagging method and present our approach for integrating language IDs into a transformer-based framework for CS morphological tagging. We perform our set of experiments on the Turkish-German SAGT Treebank. Experimental results show that including language IDs to the learning model significantly improves accuracy over other approaches. ©2021 Association for Computational Linguistics",,"72, 83",,"Computational Approaches to Linguistic Code-Switching, CALCS 2021 - Proceedings of the 5th Workshop",Conference Paper,Scopus
398,,Automatic Construction of a Domain-specific Knowledge Graph for Chinese Patent Based on Information Extraction,"Wang M., Hu X., Xie P., Du Y.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123163213&doi=10.1109%2fICMSSE53595.2021.00008&partnerID=40&md5=5f1ce3071345153e7aa8ba460a0428f7,10.1109/ICMSSE53595.2021.00008,"Knowledge graph has been proved as an effective tool in diverse domains including patent service. However, despite partial structure of data, most patent information lies in unstructured data like abstract text, from which it is relatively difficult to build a knowledge graph, since traditional methods rely on predefined human-Annotated resources of entities and their relationships. Furthermore, the difficulty in Chinese language processing worsens the problem. This paper proposed an unsupervised method to automatically construct a Chinese patent knowledge graph without pre-built dataset. This research first builds a basic graph frame using structured patent data. Then the proposed method mainly utilizes keyphrase extraction algorithm to find patent properties and semantic role labeling method to dig deeper relations. The experimental result proves the effectiveness of the proposed method. © 2021 IEEE.",automatic construction; Chinese patent; keyphrase extraction; knowledge graph; relation extraction,"1, 8",,"Proceedings - 2021 International Conference on Management Science and Software Engineering, ICMSSE 2021",Conference Paper,Scopus
399,,Multimodal Representation for Neural Code Search,"Gu J., Chen Z., Monperrus M.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123058224&doi=10.1109%2fICSME52107.2021.00049&partnerID=40&md5=6acd144a4abb960de0f0f9d0d2802dd5,10.1109/ICSME52107.2021.00049,"Semantic code search is about finding semantically relevant code snippets for a given natural language query. In the state-of-the-art approaches, the semantic similarity between code and query is quantified as the distance of their representation in the shared vector space. In this paper, to improve the vector space, we introduce tree-serialization methods on a simplified form of AST and build the multimodal representation for the code data. We conduct extensive experiments using a single corpus that is large-scale and multi-language: CodeSearchNet. Our results show that both our tree-serialized representations and multimodal learning model improve the performance of code search. Last, we define intuitive quantification metrics oriented to the completeness of semantic and syntactic information of the code data, to help understand the experimental findings. © 2021 IEEE.",code search; information completeness; multimodal learning; program representation; tree serialization,"483, 494",,"Proceedings - 2021 IEEE International Conference on Software Maintenance and Evolution, ICSME 2021",Conference Paper,Scopus
400,,Multi-view learning for software defect prediction,"Kiyak E.O., Birant D., Birant K.U.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122765350&doi=10.37190%2fe-Inf210108&partnerID=40&md5=477633851224d701bdc79b8f4cf51646,10.37190/e-Inf210108,"Background: Traditionally, machine learning algorithms have been simply applied for software defect prediction by considering single-view data, meaning the input data contains a single feature vector. Nevertheless, different software engineering data sources may include multiple and partially independent information, which makes the standard single-view approaches ineffective. Objective: In order to overcome the single-view limitation in the current studies, this article proposes the usage of a multi-view learning method for software defect classification problems. Method: The Multi-View k-Nearest Neighbors (MVKNN) method was used in the software engineering field. In this method, first, base classifiers are constructed to learn from each view, and then classifiers are combined to create a robust multi-view model. Results: In the experimental studies, our algorithm (MVKNN) is compared with the standard k-nearest neighbors (KNN) algorithm on 50 datasets obtained from different software bug repositories. The experimental results demonstrate that the MVKNN method outperformed KNN on most of the datasets in terms of accuracy. The average accuracy values of MVKNN are 86.59%, 88.09%, and 83.10% for the NASA MDP, Softlab, and OSSP datasets, respectively. Conclusion: The results show that using multiple views (MVKNN) can usually improve classification accuracy compared to a single-view strategy (KNN) for software defect prediction. © 2021 Wroclaw University of Science and Technology. All rights reserved.",K-nearest neighbors; Machine learning; Multi-view learning; Software defect prediction,"163, 184",,E-Informatica Software Engineering Journal,Article,Scopus
401,,Can Differential Testing Improve Automatic Speech Recognition Systems?,"Asyrofi M.H., Yang Z., Shi J., Quan C.W., Lo D.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122718858&doi=10.1109%2fICSME52107.2021.00079&partnerID=40&md5=40da81d102a3148c8d8e3e6de05ffc48,10.1109/ICSME52107.2021.00079,"Due to the widespread adoption of Automatic Speech Recognition (ASR) systems in many critical domains, ensuring the quality of recognized transcriptions is of great importance. A recent work, CrossASR++, can automatically uncover many failures in ASR systems by taking advantage of the differential testing technique. It employs a Text-To-Speech (TTS) system to synthesize audios from texts and then reveals failed test cases by feeding them to multiple ASR systems for cross-referencing. However, no prior work tries to utilize the generated test cases to enhance the quality of ASR systems. In this paper, we explore the subsequent improvements brought by leveraging these test cases from two aspects, which we collectively refer to as a novel idea, evolutionary differential testing. On the one hand, we fine-tune a target ASR system on the corresponding test cases generated for it. On the other hand, we fine-tune a cross-referenced ASR system inside CrossASR++, with the hope to boost CrossASR++'s performance in uncovering more failed test cases. Our experiment results empirically show that the above methods to leverage the test cases can substantially improve both the target ASR system and CrossASR++ itself. After fine-tuning, the number of failed test cases uncovered decreases by 25.81% and the word error rate of the improved target ASR system drops by 45.81%. Moreover, by evolving just one cross-referenced ASR system, CrossASR++ can find 5.70%, 7.25%, 3.93%, and 1.52% more failed test cases for 4 target ASR systems, respectively. © 2021 IEEE.",Automatic Speech Recognition; Differential Testing; Test Case Generation,"674, 678",,"Proceedings - 2021 IEEE International Conference on Software Maintenance and Evolution, ICSME 2021",Conference Paper,Scopus
402,,BiasHeal: On-the-Fly Black-Box Healing of Bias in Sentiment Analysis Systems,"Yang Z., Jain H., Shi J., Asyrofi M.H., Lo D.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122718516&doi=10.1109%2fICSME52107.2021.00073&partnerID=40&md5=53bb796da9df01e4bd7af6bf20204027,10.1109/ICSME52107.2021.00073,"Although Sentiment Analysis (SA) is widely applied in many domains, existing research has revealed that the unfairness in SA systems can be harmful to the welfare of less privileged people. Several works propose pre-processing and in-processing methods to eliminate bias in SA systems, but little attention is paid to utilizing post-processing methods to heal bias. Postprocessing methods are particularly important for systems that use third-party SA services. Systems that use such services have no access to the SA engine or its training data and thus cannot apply pre-processing nor in-processing methods. Therefore, this paper proposes a black-box post-processing method to make an SA system heal bias and construct fair results when bias is detected. We propose and investigate six self-healing strategies. Our evaluation results on two datasets show that the best strategy can construct fair results and improve accuracy on the two datasets by 2.76% and 2.85%, respectively. To the best of our knowledge, our work is the first self-healing method that can be deployed to ensure SA fairness without requiring access to the SA engine or its training data. © 2021 IEEE.",Bias Healing; Sentiment Analysis; Software Fairness,"644, 648",,"Proceedings - 2021 IEEE International Conference on Software Maintenance and Evolution, ICSME 2021",Conference Paper,Scopus
403,,Making the most of small Software Engineering datasets with modern machine learning,"Prenner J.A.A., Robbes R.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121844776&doi=10.1109%2fTSE.2021.3135465&partnerID=40&md5=1410d9efd8f71992be0a957c0c044f54,10.1109/TSE.2021.3135465,"This paper provides a starting point for Software Engineering (SE) researchers and practitioners faced with the problem of training machine learning models on small datasets. Due to the high costs associated with labeling data, in Software Engineering, there exist many small (&lt; 1 000 samples) and medium-sized (&lt; 100 000 samples) datasets. While deep learning has set the state of the art in many machine learning tasks, it is only recently that it has proven effective on small-sized datasets, primarily thanks to pre-training, a semi-supervised learning technique that leverages abundant unlabelled data alongside scarce labelled data. In this work, we evaluate pre-trained Transformer models on a selection of 13 smaller datasets from the SE literature, covering both, source code and natural language. Our results suggest that pre-trained Transformers are competitive and in some cases superior to previous models, especially for tasks involving natural language; whereas for source code tasks, in particular for very small datasets, traditional machine learning methods often has the edge. In addition, we experiment with several techniques that ought to aid training on small datasets, including active learning, data augmentation, soft labels, self-training and intermediate-task fine-tuning, and issue recommendations on when they are effective. We also release all the data, scripts, and most importantly pre-trained models for the community to reuse on their own datasets. IEEE",Active Learning; Back Translation; BERT; Codes; Data Augmentation; Fine-Tuning; Machine learning; Pre-training; RoBERTA; Small Datasets; Soft Labels; Software; Support vector machines; Task analysis; Training; Transformer; Transformers,,,IEEE Transactions on Software Engineering,Article,Scopus
404,,Highlighted document image classification,"Mao Y., Sun Y., Bauer P., Harris T., Shaw M., Li L., Allebach J.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121233557&doi=10.2352%2fissn.2169-2629.2021.29.154&partnerID=40&md5=66ab7c31712a41850916dd72a5b993fd,10.2352/issn.2169-2629.2021.29.154,"There are many existing document image classification researches, but most of them are not designed for use in constrained computer resources, like printers, or focused on documents with highlighter pen marks. To enable printers to better discriminate highlighted documents, we designed a set of features in CIE Lch(a∗b∗) space to use along with the support vector machine. The features include two gamut-based features and six low-level color features. By first identifying the highlight pixels, and then computing the distance from the highlight pixels to the boundary of the printer gamut, the gamut-based features can be obtained. The low-level color features are built upon the color distribution information of the image blocks. The best feature subset of the existing and new features is constructed by sequential forward floating selection (SFFS) feature selection. Leave-one-out cross-validation is performed on a dataset with 400 document images to evaluate the effectiveness of the classification model. The cross-validation results indicate significant improvements over the baseline highlighted document classification model. © 2021 Society for Imaging Science and Technology.",,"154, 159",,Final Program and Proceedings - IS and T/SID Color Imaging Conference,Conference Paper,Scopus
405,,New encoder learning for captioning heavy rain images via semantic visual feature matching,"Son C.-H., Ye P.-H.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121210084&doi=10.2352%2fJ.ImagingSci.Technol.2021.65.5.050402&partnerID=40&md5=7555a96bd8b6e6a2c8d882fdb258cf06,10.2352/J.ImagingSci.Technol.2021.65.5.050402,"Image captioning generates text that describes scenes from input images. It has been developed for high-quality images taken in clear weather. However, in bad weather conditions, such as heavy rain, snow, and dense fog, poor visibility as a result of rain streaks, rain accumulation, and snowflakes causes a serious degradation of image quality. This hinders the extraction of useful visual features and results in deteriorated image captioning performance. To address practical issues, this study introduces a new encoder for captioning heavy rain images. The central idea is to transform output features extracted from heavy rain input images into semantic visual features associated with words and sentence context. To achieve this, a target encoder is initially trained in an encoder–decoder framework to associate visual features with semantic words. Subsequently, the objects in a heavy rain image are rendered visible by using an initial reconstruction subnetwork (IRS) based on a heavy rain model. The IRS is then combined with another semantic visual feature matching subnetwork (SVFMS) to match the output features of the IRS with the semantic visual features of the pretrained target encoder. The proposed encoder is based on the joint learning of the IRS and SVFMS. It is trained in an end-to-end manner, and then connected to the pretrained decoder for image captioning. It is experimentally demonstrated that the proposed encoder can generate semantic visual features associated with words even from heavy rain images, thereby increasing the accuracy of the generated captions. © Society for Imaging Science and Technology 2021.",,"207, 218",,Final Program and Proceedings - IS and T/SID Color Imaging Conference,Conference Paper,Scopus
406,,Study on the use of standard 12-lead ECG data for rhythm-type ECG classification problems,"Park J., An J., Kim J., Jung S., Gil Y., Jang Y., Lee K., Oh I.-Y.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121010410&doi=10.1016%2fj.cmpb.2021.106521&partnerID=40&md5=f784e6b460658e9705b9728c4c8c618d,10.1016/j.cmpb.2021.106521,"Background and objectives: Most deep-learning-related methodologies for electrocardiogram (ECG) classification are focused on finding an optimal deep-learning architecture to improve classification performance. However, in this study, we proposed a methodology for fusion of various single-lead ECG data as training data in the single-lead ECG classification problem. Methods: We used a squeeze-and-excitation residual network (SE-ResNet) with 152 layers as the baseline model. We compared the performance of a 152-layer SE-ResNet trained on ECG signals from various leads of a standard 12-lead ECG system to that of a 152-layer SE-ResNet trained on only single-lead ECG data with the same lead information as the test set. The experiments were performed using five different types of rhythm-type single-lead ECG data obtained from Konkuk University Hospital in South Korea. Results: Experiment results based on the combination from the relationship experiments of the leads showed that lead –aVR or II revealed the best classification performance. In case of -aVR, this model achieved a high F1 score for normal (98.7%), AF (98.2%), APC (95.1%), and VPC (97.4%), indicating its potential for practical use in the medical field. Conclusion: We concluded that the 152-layer SE-ResNet trained by fusion of single-lead ECGs had better classification performance than the 152-layer SE-ResNet trained on only single-lead ECG data, regardless of the single-lead ECG signal type. We also found that the best performance directions for single-lead ECG classification are Lead -aVR and II. © 2021",12 Single-lead ECG; Convolutional neural network; Deep learning; Heterogeneous single-lead ECG; SE-ResNet; Single-lead ECG classification; Standard 12-lead ECG,,,Computer Methods and Programs in Biomedicine,Article,Scopus
407,,Semantic detection of targeted attacks using doc2vec embedding,"El-Rahmany M.S., Mohamed E.H., Haggag M.H.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120880230&doi=10.24138%2fjcomss-2021-0113&partnerID=40&md5=f9b822d163526486012b156715ee0348,10.24138/jcomss-2021-0113,"The targeted attack is one of the social engineering attacks. The detection of this type of attack is considered a challenge as it depends on semantic extraction of the intent of the attacker. However, previous research has primarily relies on the Natural Language Processing or Word Embedding techniques that lack the context of the attacker's text message. Based on Sentence Embedding and machine learning approaches, this paper introduces a model for semantic detection of targeted attacks. This model has the advantage of encoding relevant information, which helps to improve the performance of the multi-class classification process. Messages will be categorized based on the type of security rule that the attacker has violated. The suggested model was tested using a dialogue dataset taken from phone calls, which was manually categorized into four categories. The text is pre-processed using natural language processing techniques, and the semantic features are extracted as Sentence Embedding vectors that are augmented with security policy sentences. Machine Learning algorithms are applied to classify text messages. The experimental results show that sentence embeddings with doc2vec achieved high prediction accuracy 96.8%. So, it outperformed the method applied to the same dialog dataset. © 2021 University of Split. All rights reserved.",Doc2vec; Multi-class text classification; Pretexting; Sentence Embedding; Targeted Attacks Detection,"334, 341",,Journal of Communications Software and Systems,Article,Scopus
408,,MRI radiogenomics for intelligent diagnosis of breast tumors and accurate prediction of neoadjuvant chemotherapy responses-a review,"Yin X.-X., Hadjiloucas S., Zhang Y., Tian Z.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120351212&doi=10.1016%2fj.cmpb.2021.106510&partnerID=40&md5=adf0003c5a12fa62e9bce078e1fa16d6,10.1016/j.cmpb.2021.106510,"Background and objective: This paper aims to overview multidimensional mining algorithms in relation to Magnetic Resonance Imaging (MRI) radiogenomics for computer aided detection and diagnosis of breast tumours. The work also aims to address a new problem in radiogenomics mining: how to combine structural radiomics information with non-structural genomics information for improving the accuracy and efficacy of Neoadjuvant Chemotherapy (NAC). Methods: This requires the automated extraction of parameters from non-structural breast radiomics data, and finding feature vectors with diagnostic value, which then are combined with genomics data. In order to address the problem of weakly labelled tumour images, a Generative Adiversarial Networks (GAN) based deep learning strategy is proposed for the classification of tumour types; this has significant potential for providing accurate real-time identification of tumorous regions from MRI scans. In order to efficiently integrate in a deep learning framework different features from radiogenomics datasets at multiple spatio-temporal resolutions, pyramid structured and multi-scale densely connected U-Nets are proposed. A bidirectional gated recurrent unit (BiGRU) combined with an attention based deep learning approach is also proposed. Results: The aim is to accurately predict NAC responses by combining imaging and genomic datasets. The approaches discussed incorporate some of the latest developments in of current signal processing and artificial intelligence and have significant potential in advancing and provide a development platform for future cutting-edge biomedical radiogenomics analysis. Conclusions: The association of genotypic and phenotypic features is at the core of the emergent field of Precision Medicine. It makes use of advances in biomedical big data analysis, which enables the correlation between disease-associated phenotypic characteristics, genetics polymorphism and gene activation to be revealed. © 2021",BI-RADS; Breast cancer; Breast density; Computer aided classification; Data mining; Dynamic contrast enhanced magnetic resonance imaging (DCE-MRI); Multi-channel reconstruction; Neoadjuvant Chemotherapy (NCT); Precision medicine; Radiogenomics; Self-supervised & semi-supervised deep learning; Support vector machine,,,Computer Methods and Programs in Biomedicine,Review,Scopus
409,,Classification of renal biopsy direct immunofluorescence image using multiple attention convolutional neural network,"Zhang L., Li M., Wu Y., Hao F., Wang C., Han W., Niu D., Zheng W.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120310252&doi=10.1016%2fj.cmpb.2021.106532&partnerID=40&md5=cac3efc3a1fe11631e9f26811633cd56,10.1016/j.cmpb.2021.106532,"Background and objectives: Direct immunofluorescence (DIF) is an important medical evaluation tool for renal pathology. In the DIF images, the deposition appearances and locations of immunoglobulin on glomeruli involve immunological characteristics of glomerulonephritis and thus can be used to aid in the identification of glomerulonephritis disease. Manual classification to such deposition patterns is time consuming and may lead to significant inter and intra operator variances. We wanted to automate the identification and fusion of deposition location and deposition appearance to assist physicians in achieving immunofluorescence reporting. Methods: In this paper, we propose a framework that consists of a pre-segmentation module and a classification module for automatically segmenting glomerulus object and classifying the deposition pattern of immunoglobulin on glomerulus object. For the pre-segmentation module, the glomerulus object is segmented out from the acquired DIF images using a segmentation network, which excludes other tissues and makes the classification module focus on the glomerulus. For the classification module, two branches of classifying deposition region and appearance, respectively, are formed by using multiple attentions convolutional neural network (MANet) based on the segmented images, and the classification results of the two pre-trained classification networks are fused with labels. Results: Experimental results show that the proposed framework achieves a high classification performance with an accuracy of 98% and 95% in terms of deposition region and appearance, respectively. The label fusion of deposition appearance and deposition classification is achieved with high accuracy based on well-trained classification. Conclusions: The data show that automated and accurate patterned immunofluorescence report generation is achieved, which can effectively help improve the diagnosis of autoimmune kidney disease. © 2021",Direct immunofluorescence images; Image classification; Kidney pathology; Multiple attention CNN,,,Computer Methods and Programs in Biomedicine,Article,Scopus
410,,Deep State Inference: Toward Behavioral Model Inference of Black-box Software Systems,"Ataiefard F., Mashhadi M.J., Hemmati H., Walkinshaw N.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120072447&doi=10.1109%2fTSE.2021.3128820&partnerID=40&md5=6042efaa7b8ff29b371f7c96d3c970d4,10.1109/TSE.2021.3128820,"Many software engineering tasks, such as testing, debugging, and anomaly detection can benefit from the ability to infer a behavioral model of the software. Most existing inference approaches assume access to code to collect execution sequences. In this paper, we investigate a black-box scenario, where the system under analysis cannot be instrumented, in this fashion. This scenario is particularly prevalent with control systems log analysis in the form of continuous signals. In this situation, an execution trace amounts to a multivariate time-series of input and output signals, where different states of the system correspond to different phases in the time-series. From an inference perspective, the challenge is to detect when these phase changes take place. Unfortunately, most existing solutions are either univariate, make assumptions on the data distribution, or have limited learning power. Therefore, we propose a hybrid deep neural network that accepts as input a multivariate time series and applies a set of convolutional and recurrent layers to learn the non-linear correlations between signals and the patterns over time. We show how this approach can be used to accurately detect state changes, and how the inferred {\color{blue}machine learning} models can be successfully applied to transfer-learning scenarios, to accurately process traces from different products with similar execution characteristics. Our experimental results on two UAV autopilot case studies (one industrial and one open-source) indicate that our approach is highly accurate (over 90\% F1 score for state classification) and significantly improves baselines (by up to 102\% for change point detection). Using transfer learning we also show that up to 90\% of the maximum achievable F1 scores in the open-source case study can be achieved by reusing the trained models from the industrial case and only fine tuning them using as low as 5 labeled samples, which reduces the manual labeling effort by 98\%. IEEE",Autopilot; Black-box Model Inference; Codes; Convolutional Neural Network; Deep Learning; Deep learning; Monitoring; Recurrent Neural Network; Specification Mining; Task analysis; Time series; Time series analysis; Transfer learning; Transfer Learning; UAV AutoPilot,,,IEEE Transactions on Software Engineering,Article,Scopus
411,,MARS: Detecting brain class/method code smell based on metric–attention mechanism and residual network,"Zhang Y., Dong C.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118480562&doi=10.1002%2fsmr.2403&partnerID=40&md5=fb91084fc9433272a6b313fb4540da2b,10.1002/smr.2403,"Code smell is the structural design defect that makes programs difficult to understand, maintain, and evolve. Existing works of code smell detection mainly focus on prevalent code smells, such as feature envy, god class, and long method. Few works have been done on detecting brain class/method. Furthermore, existing deep-learning-based approaches leverage the CNN model to improve accuracy by barely increasing the number of layers, which may cause a problem of gradient degradation. To this end, this paper proposes a novel approach called MARS to detect brain class/method. MARS improves the gradient degradation by employing an improved residual network. It increases the weight value of those important code metrics to label smelly samples by introducing a metric–attention mechanism. To support the training of MARS, a dataset called BrainCode is generated by extracting more than 270,000 samples from 20 real-world applications. MARS is evaluated on BrainCode and compared to other machine-learning-based and deep-learning-based approaches. The experimental results demonstrate that the average accuracy of MARS is 2.01 % higher than that of the existing approaches, which improves state-of-the-art. © 2021 John Wiley & Sons, Ltd.",brain class/method; code smell; deep learning; metric–attention mechanism; residual network,,,Journal of Software: Evolution and Process,Article,Scopus
412,,A systematic mapping study on the employment of neural networks on software engineering projects: Where to go next?,"dos Santos R.A., Vieira D., Bravo A., Suzuki L., Qudah F.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118352217&doi=10.1002%2fsmr.2402&partnerID=40&md5=b7ab954e54a097e19323bc7fc157f743,10.1002/smr.2402,"Deep learning has recently experienced explosive growth in use, largely due to advances in neural networks and the availability of large corpora of domain data. Project management activities generate and handle large volumes of data. Software engineering closely relates to project management, so software engineering projects must be prone to the use of neural networks. We seek to obtain an accurate vision of how neural networks are being used in software engineering projects through a systematic mapping study. We confirm that neural networks have already made their way into these projects; however, we show that their current uses are limited to certain repetitive and legacy tasks. Given uncovered ample room for expansion, we point out a few directions the industry and academy can lean toward to in the next years for taking better advantage of neural networks in software engineering projects and immediately advancing the field. We investigate if, how, and to what extent have neural networks been employed to the advancement of software engineering projects. As such, a systematic mapping study was conducted, which led to the conclusion that even though these algorithms have indeed been employed on several software engineering tasks, this employment so far has been shy, mostly relying on legacy types of neural networks. More modern variants, namely, deep learning algorithms, are slowly gaining momentum and should be the trend going forward. © 2021 John Wiley & Sons, Ltd.",deep learning; machine learning; neural networks; project management; software engineering,,,Journal of Software: Evolution and Process,Review,Scopus
413,,ModelSet: a dataset for machine learning in model-driven engineering,"López J.A.H., Cánovas Izquierdo J.L., Cuadrado J.S.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117576971&doi=10.1007%2fs10270-021-00929-3&partnerID=40&md5=4dfe6f4506c21606c462b383c383b789,10.1007/s10270-021-00929-3,"The application of machine learning (ML) algorithms to address problems related to model-driven engineering (MDE) is currently hindered by the lack of curated datasets of software models. There are several reasons for this, including the lack of large collections of good quality models, the difficulty to label models due to the required domain expertise, and the relative immaturity of the application of ML to MDE. In this work, we present ModelSet, a labelled dataset of software models intended to enable the application of ML to address software modelling problems. To create it we have devised a method designed to facilitate the exploration and labelling of model datasets by interactively grouping similar models using off-the-shelf technologies like a search engine. We have built an Eclipse plug-in to support the labelling process, which we have used to label 5,466 Ecore meta-models and 5,120 UML models with its category as the main label plus additional secondary labels of interest. We have evaluated the ability of our labelling method to create meaningful groups of models in order to speed up the process, improving the effectiveness of classical clustering methods. We showcase the usefulness of the dataset by applying it in a real scenario: enhancing the MAR search engine. We use ModelSet to train models able to infer useful metadata to navigate search results. The dataset and the tooling are available at https://figshare.com/s/5a6c02fa8ed20782935c and a live version at http://modelset.github.io. © 2021, The Author(s).",Dataset; Machine learning; Model-driven engineering,,,Software and Systems Modeling,Article,Scopus
414,,Transfer Learning Across Variants and Versions : The Case of Linux Kernel Size,"Martin H., Acher M., Lesoil L., Jezequel J.M., Khelladi D.E., Pereira J.A.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116928784&doi=10.1109%2fTSE.2021.3116768&partnerID=40&md5=33b32ee60d7cb3eb0d1c94f3989fca76,10.1109/TSE.2021.3116768,"With large scale and complex configurable systems, it is hardfor users to choose the right combination of options (i.e., configurations)in order to obtain the wanted trade-off between functionality and per-formance goals such as speed or size. Machine learning can help inrelating these goals to the configurable system options, and thus, predictthe effect of options on the outcome, typically after a costly training step.However, many configurable systems evolve at such a rapid pace that itis impractical to retrain a new model from scratch for each new version.In this paper, we propose a new method to enable transfer learningof binary size predictions among versions of the same configurablesystem. Taking the extreme case of the Linux kernel with its14,500configuration options, we first investigate how binary size predictionsof kernel size degrade over successive versions. We show that thedirect reuse of an accurate prediction model from 2017 quickly becomesinaccurate when Linux evolves, up to a 32% mean error by August 2020.We thus propose a new approach for transfer evolution-aware modelshifting (TEAMS). It leverages the structure of a configurable systemto transfer an initial predictive model towards its future versions witha minimal amount of extra processing for each version. We show thatTEAMS vastly outperforms state of the art approaches over the 3 yearshistory of Linux kernels, from 4.13 to 5.8. IEEE",Codes; Kernel; Linux; Machine Learning; Performance Prediction; Predictive models; Software Evolution; Software Product Line; Software systems; Training; Transfer Learning; Transfer learning,,,IEEE Transactions on Software Engineering,Article,Scopus
415,,aCHAT-WF: Generating conversational agents for teaching business process models,"Rooein D., Bianchini D., Leotta F., Mecella M., Paolini P., Pernici B.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116784129&doi=10.1007%2fs10270-021-00925-7&partnerID=40&md5=fe8214af39017d580db3a28af5ae4e6a,10.1007/s10270-021-00925-7,"This paper proposes a general approach for using conversational interfaces such as chatbots to offer adaptive learning of business processes in an environment involving different actors. Adaptivity concerns both the content being proposed, the sequence of learning items, and the way the conversation is conducted. The original approach allows the development of sustainable chatbots and empowers various non-technical actors (authors, teachers, publishers, and learners) to control the chatbot features directly. The aCHAT-WF framework (adaptive CHATbot for WorkFlows), proposed in this paper for managing conversational interfaces, conceptually represents all the aspects related to a conversation about business processes, with different facets for the user, the conversation flow, and the conversation contents, combining them to obtain a flexible interaction with the user. The paper focuses on the different preparation phases for instructional material based on Business Process Modeling Notation (BPMN) models, separating the different roles involved in the construction of a chatbot for teaching business processes and with the possibility of defining different styles for the interaction with the users. The proposed method is configuration-driven, to facilitate the separation of the different aspects of the control of the interaction and the delivery of contents. © 2021, The Author(s).",BPMN; Business process; Chatbot; Configuration driven; Digital transformation; Educational conversational agent,,,Software and Systems Modeling,Article,Scopus
416,,Exploring Text-to-Text Transformers for English to Hinglish Machine Translation with Synthetic Code-Mixing,"Jawahar G., Billah Nagoudi E.M., Abdul-Mageed M., Lakshmanan L.V.S.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116747191&doi=10.26615%2f978-954-452-056-4_006&partnerID=40&md5=e6a37bf81429b5cc310a36ade8095c54,10.26615/978-954-452-056-4_006,"We describe models focused at the understudied problem of translating between monolingual and code-mixed language pairs. More specifically, we offer a wide range of models that convert monolingual English text into Hinglish (code-mixed Hindi and English). Given the recent success of pretrained language models, we also test the utility of two recent Transformer-based encoder-decoder models (i.e., mT5 and mBART) on the task finding both to work well. Given the paucity of training data for code-mixing, we also propose a dependency-free method for generating code-mixed texts from bilingual distributed representations that we exploit for improving language model performance. In particular, armed with this additional data, we adopt a curriculum learning approach where we first finetune the language models on synthetic data then on gold code-mixed data. We find that, although simple, our synthetic code-mixing method is competitive with (and in some cases is even superior to) several standard methods (backtranslation, method based on equivalence constraint theory) under a diverse set of conditions. Our work shows that the mT5 model, finetuned following the curriculum learning procedure, achieves best translation performance (12.67 BLEU). Our models place first in the overall ranking of the English-Hinglish official shared task. ©2021 Association for Computational Linguistics",,"36, 46",,"Computational Approaches to Linguistic Code-Switching, CALCS 2021 - Proceedings of the 5th Workshop",Conference Paper,Scopus
417,,Can You Traducir This? Machine Translation for Code-Switched Input,"Xu J., Yvon F.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115832773&doi=10.26615%2f978-954-452-056-4_011&partnerID=40&md5=78e67363227d488fc3db93243f9cff7a,10.26615/978-954-452-056-4_011,"Code-Switching (CSW) is a common phenomenon that occurs in multilingual geographic or social contexts, which raises challenging problems for natural language processing tools. We focus here on Machine Translation (MT) of CSW texts, where we aim to simultaneously disentangle and translate the two mixed languages. Due to the lack of actual translated CSW data, we generate artificial training data from regular parallel texts. Experiments show this training strategy yields MT systems that surpass multilingual systems for code-switched texts. These results are confirmed in an alternative task aimed at providing contextual translations for a L2 writing assistant. ©2021 Association for Computational Linguistics",,"84, 94",,"Computational Approaches to Linguistic Code-Switching, CALCS 2021 - Proceedings of the 5th Workshop",Conference Paper,Scopus
418,,Proactive content caching in edge computing environment: A review,"Aghazadeh R., Shahidinejad A., Ghobaei-Arani M.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115329006&doi=10.1002%2fspe.3033&partnerID=40&md5=7b175cdd40f65063e994811e14761894,10.1002/spe.3033,"Edge computing environment provides processing capability and computing at the network edge and close to users. Edge equipment includes small data centers that locally perform process and content delivery. Therefore, edge equipment management has received much attention due to the rapid growth of information and resources limitations. The content caching and proactive caching techniques are management methods of edge equipment resources. We recently witnessed the development of proactive caching mechanisms that have a crucial enabler in improving heavy traffic, energy, and bandwidth. Also, it has huge potential to increase the quick response time to users' requests that today, these services are demanded by many users and applications. This article prepares a systematic literature review content caching approach in the edge computing environment. The purpose of this study is to survey the research done on the proactive caching strategies in the edge computing environment to identify subjects that must be emphasized more in current and future research paths. This research has studied 71 articles divided into three classes: model-based, machine-learning-based, and heuristic-based. Next, we discuss content caching approaches based on critical factors such as performance metrics, case studies, utilized techniques, assessment tools, advantages, and disadvantages. Finally, open issues and challenges are presented, and the survey is concluded. © 2021 John Wiley & Sons Ltd.",edge caching; edge computing; heuristic-based; machine learning; model-based; proactive caching; systematic review,,,Software - Practice and Experience,Conference Paper,Scopus
419,,A volume-aware positional attention-based recurrent neural network for stock index prediction,"Yu X., Li D.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114284085&doi=10.18293%2fSEKE2021-191&partnerID=40&md5=b3d76186f20187bca5fba9fc004e8cad,10.18293/SEKE2021-191,"With the rapid development of deep learning, more researchers have attempted to apply nonlinear learning methods such as recurrent neural networks (RNNs) and attention mechanisms to capture the complex patterns hidden in stock market trends. Most existing approaches to this task employ an attention mechanism that primarily relies on the information extracted from input features but fails to consider the other important factors (e.g., trading volume and position), which can potentially enhance these attention-based approaches. Motivated by the observation, we extend the attention mechanism with features needed for stock performance prediction in this article. Specifically, we propose a volume-aware positional attentionbased recurrent neural network (VPA-RNN) for this task. First, we propose a generic method of adding position awareness to the attention mechanism. Next, the trading volume is incorporated into the original attention distribution to form a revised distribution. To evaluate the effectiveness of VPA-RNN, we collected real stock market data for stock indexes S&P 500 and DJIA, and the experimental results show that the proposed VPA-RNN can significantly outperform several existing highly competitive methods. © 2021 Knowledge Systems Institute Graduate School. All rights reserved.",Attention mechanism; Positional attention; Recurrent neural network; Stock index prediction; Volume-aware attention,"493, 498",,"Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE",Conference Paper,Scopus
420,,An efficient ROS package searching approach powered by knowledge graph,"Chen L., Mao X., Zhang Y., Yang S., Wang S.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114283258&doi=10.18293%2fSEKE2021-063&partnerID=40&md5=33e213145f09134302ef21743f6e61f0,10.18293/SEKE2021-063,"Over the past several years, the Robot Operating System (ROS), has grown from a small research project into the most popular framework for robotics development. It offers a core set of software for operating robots that can be extended by creating or using existing packages, making it possible to program robotic software that can be reused on different hardware platforms. With thousands of packages available per stable distribution, encapsulating algorithms, sensor drivers, etc., it is the de facto middleware for robotics. However, finding the proper ROS package is a nontrivial task because ROS packages involve different functions and even with the same function, there are different ROS packages for different tasks. So it is time-consuming for developers to find suitable ROS packages for given task, especially for newcomers. To tackle this challenge, we build a ROS package knowledge graph, ROSKG, including the basic information of ROS packages and ROS package characteristics extracted from text descriptions, to comprehensively and precisely characterize ROS packages. Based on ROSKG, we support ROS packages search with specific task description or attributes as input. A comprehensive evaluation of ROSKG shows the high accuracy of our knowledge construction approach. A user study shows that ROSKG is promising in helping developers find suitable ROS packages for robotics software development tasks. © 2021 Knowledge Systems Institute Graduate School. All rights reserved.",Knowledge graph; NLP; ROS package searching,"411, 416",,"Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE",Conference Paper,Scopus
421,,Complementary representation of ALBERT for text summarization,"Guo W., Wu B., Wang B., Li L., Sun J., Nazir M.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114282396&doi=10.18293%2fSEKE2021-084&partnerID=40&md5=8abb431b1d46c176a1447af9dd14947c,10.18293/SEKE2021-084,"Pretraining has proved to be an effective strategy to learn the parameters of the deep neural network. It captures the world knowledge that can be adapted to downstream tasks. Text summarization based on ALBERT [1] outperformed previous work by a large margin. However, they only use the final layer as a contextualized representation of the input text. Multiple studies have proven that intermediate layers also encode the rich hierarchy of linguistic information. In this paper, we propose a Fast Complementary Representation Network (FCRN), which dynamically incorporates linguistic knowledge spread across the entire ALBERT for extractive selection. Different from previous work, we measure the importance of hidden layers by all sentence representations rather than all token embeddings, which can filter nonsignificant words and takes six times less time during training. FCRN first obtains the importance of each layer by sentence embeddings and then automatically absorbs the supplementary information to ALBERT's output. We conduct experiments on CNN/DailyMail and XSum datasets. The results show that our model obtains higher ROUGE scores. © 2021 Knowledge Systems Institute Graduate School. All rights reserved.",Fast complementary representation network; Pretrained language model; Text summarization,"592, 597",,"Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE",Conference Paper,Scopus
422,,Transfer learning-based city similarity measurement: A case study on urban hotel,"Zhang G., Che X., Wei S., Na T.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114282391&doi=10.18293%2fSEKE2021-011&partnerID=40&md5=4dfdb7b4aa7a945f998fef98d16caa57,10.18293/SEKE2021-011,"With the development of modern cities, multiple types and wide distribution of urban data has been gradually collected. Effectively using urban data to solve city development and planning issues has become a research hot-spot. Currently, the data scale in modern cities is quite different, and the fitting degree of machine learning algorithm based on single city is not mature yet. This paper studies the problem with transfer learning technique, and trains the prediction model of urban hotel development scale using multi-source city data. Based on the location data and related information of 15 different cities, the relevant knowledge is transferred, and a city feature extraction and similarity measurement framework is proposed. © 2021 Knowledge Systems Institute Graduate School. All rights reserved.",City similarity; Scale prediction; Transfer learning,"447, 450",,"Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE",Conference Paper,Scopus
423,,SADA: Improved data symbolization and optimization method on HAR from microscopic perspective,"Men H., Wang B.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114281666&doi=10.18293%2fSEKE2021-125&partnerID=40&md5=590534a669ef84efb7181bd1fa991f01,10.18293/SEKE2021-125,"Nowadays, human activity recognition(HAR) becomes a hot topic with broad applications. Some researches have conducted HAR from microscopic perspective and achieved good results. In this article, two methods are proposed for further improvement. Firstly, an improved symbolization method with stacked sparse autoencoder is proposed for better data symbolization. Secondly, an improved multi-classification Adaboost is proposed to further optimize the recognition effect, and it is more suitable for the application scenario of this article. In the experiments section, firstly, e xperiments a nd a nalysis about various influencing p arameters a re c onducted, t hen comparison experiments with several new or representative methods are carried out, and finally five representative sensor activity datasets(UCI Sports and Daily dataset, Wisdm Phoneacc&Watchacc dataset, Skoda dataset, HAPT dataset) are used to prove the universal applicability and achieve satisfactory effect. © 2021 Knowledge Systems Institute Graduate School. All rights reserved.",Data symbolization; Ensemble learning; Human activity recognition; Machine learning,"481, 486",,"Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE",Conference Paper,Scopus
424,,DeepSCC: Source code classification based on fine-tuned RoBERTa,"Yang G., Zhou Y., Yu C., Chen X.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114280289&doi=10.18293%2fSEKE2021-005&partnerID=40&md5=711089093ca9c865d8a963477f604214,10.18293/SEKE2021-005,"In software engineering-related tasks (such as programming language tag prediction based on code snippets from Stack Overflow), the programming language classification for code snippets is a common task. In this study, we propose a novel method DeepSCC, which uses a fine-tuned RoBERTa model to classify the programming language type of the source code. In our empirical study, we choose a corpus collected from Stack Overflow, which contains 224,445 pairs of code snippets and corresponding language types. After comparing nine state-of-the-art baselines from the fields of source code classification and neural text classification in terms of four performance measures (i.e., Accuracy, Precision, Recall, and F1), we show the competitiveness of our proposed method DeepSCC. © 2021 Knowledge Systems Institute Graduate School. All rights reserved.",,"499, 502",,"Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE",Conference Paper,Scopus
425,,Multi-granularity code smell detection using deep learning method based on abstract syntax tree,"Xu W., Zhang X.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114279916&doi=10.18293%2fSEKE2021-014&partnerID=40&md5=4f914c5ed0bbf6d673686baa9098b413,10.18293/SEKE2021-014,"Code smell refers to poor design that is perceived to have a negative impact on readability and maintainability during software evolution, and it implies the possibility of refactoring. Therefore, the effective detection of code smell is of great importance. Many approaches including metric-based, heuristic-based, and machine learning approaches have been proposed to detect code smells. However, all these methods use manually selected features, which is highly subjective and difficult to select the most appropriate features. Recently, deep learning methods without extensive feature engineering have been proposed. Nevertheless, these token-based approaches may not achieve good results because they ignore many semantic and structural information of source code. To this end, we propose a novel deep learning approach based on abstract syntax trees(ASTs) to detect multi-granularity code smells, which captures the semantic and structural features of code fragments from the ASTs. The experimental results on four types of smells show that this approach achieves better results than the state-of-the-art approaches for detecting code smells with different granularities. © 2021 Knowledge Systems Institute Graduate School. All rights reserved.",Abstract syntax tree; Code smell; Deep learning,"503, 509",,"Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE",Conference Paper,Scopus
426,,BEHIND: A 4W-oriented method for event detection from twitter,"Zeng K., Liu Y., Song X., Zhou B.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114278862&doi=10.18293%2fSEKE2021-092&partnerID=40&md5=fa4702e4098f61aa5fd72b871b6e9e84,10.18293/SEKE2021-092,"Event detection from Twitter has attracted attention from researchers in the past decade due to the widespread use of social media. By leveraging the knowledge derived from these events, it is possible to understand what consumers are interested in and give the opportunity for organizations to make better decisions. Numerous studies have proven the advantage of burst detection methods in detecting events in Twitter streams. However, some burst detection methods mainly focus on the bursty characteristics caused by events while the elements in events are not fully utilized. In this paper, we focus on the elements in When, Where, Who, and What (4W) dimensions of events and propose a 4W-oriented event detection method called BEHIND. BEHIND jointly uses Bursty Elements and Heterogeneous Information Network(HIN) for event detection. Bursty Elements are calculated through probability distribution and they are used to select tweets with bursty elements. HIN is used to enhance relevance judgment in 4W dimensions between tweets to help cluster tweets. The tweet clusters are corresponding to events we detected. We used a benchmark dataset to evaluate our method. Experimental results demonstrate that our method achieves higher precision and less duplication rate, and detects more events than the state-of-the-art methods. © 2021 Knowledge Systems Institute Graduate School. All rights reserved.",4W; Data mining; Event detection; Event summarization; Twitter,"598, 603",,"Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE",Conference Paper,Scopus
427,,Multiclass classification of four types of UML diagrams from images using deep learning,"Shcherban S., Liang P., Li Z., Yang C.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114278842&doi=10.18293%2fSEKE2021-185&partnerID=40&md5=5a3fac04fce18f981ffc7da8df8d284a,10.18293/SEKE2021-185,"UML diagrams are a recognized standard modelling language for representing design of software systems. For academic research, large cases containing UML diagrams are needed. One of the challenges in collecting such datasets is automatically determining whether an image is a UML diagram or not and what type of UML diagram an image contains. In this study, we collected UML diagrams from open datasets and manually labeled them into four types of UML diagrams (i.e., class diagrams, activity diagrams, sequence diagrams, and use case diagrams) and non-UML images. We evaluated the performance of five popular neural network architectures using transfer learning on the dataset of 3231 images that contains 700 class diagrams, 454 activity diagrams, 651 use case diagrams, 706 sequence diagrams, and 720 non-UML images, respectively. We also proposed our neural network architecture for multiclass classification of UML diagrams. The experiment results show that our proposed neural network architecture achieved the best performance amongst the algorithms we evaluated with an accuracy of 98.65%, a precision of 96.76%, a recall of 96.48%, and an F1-score of 96.62%. Moreover, among the neural network architectures that we have evaluated, our proposed architecture has the least parameters (around 2.4 millions) and spends the least time per image (0.0135 seconds per image using GPU) for classifying UML diagrams. © 2021 Knowledge Systems Institute Graduate School. All rights reserved.",Deep learning; Multiclass classification; Neural network; UML diagrams,"57, 62",,"Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE",Conference Paper,Scopus
428,,Relation extraction model based on keywords attention,"Chen Y., Chen J., Liu C., Liu Q.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114278828&doi=10.18293%2fSEKE2021-073&partnerID=40&md5=1511c8824f57a2208c0b69ff2ab7364b,10.18293/SEKE2021-073,"Recently, most relational extraction models usually mitigate the adverse effects of noise in sentences for the prediction results, utilizing different tools of natural language processing that to capture high-level features in sentences combined. However, these attention mechanisms do not manage to exploit as much as possible the semantic information of certain keywords that have relational expressive information in the sentence. Therefore, this paper proposes a model based on the keyword's attention mechanism, which is a novel attention mechanism based on the keywords of relational expression related. In particular, the proposed attention mechanism utilizes a linear-chain conditional random field that combines entity-pair features, similarity features between entity-pair features, and its hidden vectors to compute each word's marginal distribution defined as the attention weight. Experimental results show that the method can focus on keywords with relational expression semantics in sentences without using sophisticated tools and achieves performance improvements on the SemEval-2010 Task 8 dataset. © 2021 Knowledge Systems Institute Graduate School. All rights reserved.",Attention; Bi-GRU; Hidden similarity,"582, 585",,"Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE",Conference Paper,Scopus
429,,FSSRE: Fusing semantic feature and syntactic dependencies feature for threat intelligence relation extraction,"Wang X., Xiong M., He F., Yang P., Song B., Jiang J., Jiang Z., Xiong Z.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114276333&doi=10.18293%2fSEKE2021-088&partnerID=40&md5=f54be0b61d6419634270f283e96ce455,10.18293/SEKE2021-088,"Threat intelligence relation extraction plays an important role in threat intelligence text analysis and processing. To extract the relation between two threat entities in a sentence, we develop a novel framework called FSSRE which fuses sematic feature and syntactic dependencies feature for threat intelligence relation extraction. We utilize graph convolutional networks (GCN) to extract syntactic dependencies features, and utilize Sentence-BERT to extract contextual semantic features. To keep vital information with irrelevant content removed to the most extent, we further apply a novel pruning strategy, SDP-VP, to the input trees. With retaining the shortest path and nodes that are K hops away from nodes on the shortest path, we give the edge connected to the verb nodes a weight of W times. We create an advanced persistent threat (APT) intelligence entities and intra-sentence relations dataset, APTER-SENT, for that there is no public dataset can be used for relation extraction research in the threat intelligence field. Experimental results on APTER-SENT demonstrate improved performance over competitive baselines. At the same time, we also conducted experiments on the SemEval-2010 dataset. The results of the experiment indicate that our method is still effective on this dataset. © 2021 Knowledge Systems Institute Graduate School. All rights reserved.",APTER-SENT; GCN; Relation extraction; SDP-VP; Sentence-BERT; Threat intelligence,"79, 85",,"Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE",Conference Paper,Scopus
430,,Attention guided filter for jointly extracting entities and classifying relations,"Chen S., Wang S., Hu W.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114276079&doi=10.18293%2fSEKE2021-153&partnerID=40&md5=96ab6c34a81525b63a7abae49d09ba11,10.18293/SEKE2021-153,"Jointly extracting entities and classifying relations aims to detect all possible triples from unstructured text with a single model. Tagging-based method effectively improves the performance of jointly relation extraction. However, some tagging-based approaches ignored that one entity pair may exist multiple relations and others set an empirical threshold value for selecting one or more relevant relations, which becomes the bottlenecks of the model. As a solution, we propose the attention guided filter, namely, AGFRel, which introduces transformer blocks to learn the number of relations for every entity pair to filter out irrelevant relations. Moreover, each module of the model has a multi-head attention guided layer to highlight valuable information. Extensive experimental results show that AGFRel is capable of gaining better performance on various tasks including overlapping triples extraction and multiple triples extraction. On NYT and WebNLG public datasets, our model obtains F1 score 90.8 and 91.9 respectively and achieves a new state-of-the-art performance. © 2021 Knowledge Systems Institute Graduate School. All rights reserved.",Attention mechanism; Joint extraction model; NYT; Transformer; WebNLG,"352, 358",,"Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE",Conference Paper,Scopus
431,,A technical capability evaluation model based concept and prerequisite relation in computer education,"Luo J., Wang T., Chang J., Guo X.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114273308&doi=10.18293%2fSEKE2021-187&partnerID=40&md5=955d420add8876dd3509afc85a9c269c,10.18293/SEKE2021-187,"Effectively assessing the results of users' online learning and enhancing social recognition has become a major development direction for online education platforms. For computer education, this article constructs a technical capability assessment model. This model integrates professional concepts in the field of computer science and extracts knowledge concepts from educational resources. The model first extracts candidate concepts, then uses a graph propagation algorithm to quantify candidate concepts and obtains concepts from them, and finally uses prerequisite relationships to further quantify the concepts mastered by students. The model combines the prerequisite relationship among concepts to quantify the skills that students have mastered. It can not only effectively evaluate the user's skill mastery but also lays a foundation for subsequent course recommendations and career recommendations for users. The model is tested in the real learning environment of 250 students. This model has been proved to own certain practicability and reliability by Kendall rank correlation coefficient, which is used as an evaluation index. © 2021 Knowledge Systems Institute Graduate School. All rights reserved.",Concept extraction; Online learning; Technical capability evaluation model,"381, 386",,"Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE",Conference Paper,Scopus
432,,Deep self-attention for sequential recommendation,"Zhang B., Xiao Z., Zhong S.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114271300&doi=10.18293%2fSEKE2021-035&partnerID=40&md5=b297996a605613288581abebde4b8ac9,10.18293/SEKE2021-035,"Sequential recommendation aims to recommend the next item that a user will likely interact with by capturing the useful sequential patterns from users' historical behaviors. Recently, it has become an important and popular component in various e-commerce platforms. As a successful network, Transformer has been widely used to adaptively capture the dynamics of users' historical behaviors for sequential recommendation. In recommender systems, the size of embedding is usually set to be small. Under small embedding, the dot-product in Transformer may have the limitation on calculating the complex relevance between keys and queries. To address the common but neglected issue, in this paper, we present a new model, Deep Self-Attention for Sequential Recommendation (DSASrec), which proposes a chunking deep attention to compute attention weights. The chunking deep attention has two modules: a deep module and a chunking module. The deep module is used to improve the nonlinearity of the attention function. The chunking module is used to calculate attention weights several times like the multi-head attention in Transformer. Extensive experiments on three benchmark datasets show that our model can achieve state-of-the-art results. Our implementation is available in PyTorch. © 2021 Knowledge Systems Institute Graduate School. All rights reserved.",Chunking representation; Deep learning; Dot-product; Recommender system; Transformer,"321, 326",,"Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE",Conference Paper,Scopus
433,,Are Multilingual Models Effective in Code-Switching?,"Winata G.I., Cahyawijaya S., Liu Z., Lin Z., Madotto A., Fung P.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113610040&doi=10.26615%2f978-954-452-056-4_020&partnerID=40&md5=156906e24bf7eaf03170e92fe74f5e78,10.26615/978-954-452-056-4_020,"Multilingual language models have shown decent performance in multilingual and cross-lingual natural language understanding tasks. However, the power of these multilingual models in code-switching tasks has not been fully explored. In this paper, we study the effectiveness of multilingual language models to understand their capability and adaptability to the mixed-language setting by considering the inference speed, performance, and number of parameters to measure their practicality. We conduct experiments in three language pairs on named entity recognition and part-of-speech tagging and compare them with existing methods, such as using bilingual embeddings and multilingual meta-embeddings. Our findings suggest that pre-trained multilingual models do not necessarily guarantee high-quality representations on code-switching, while using meta-embeddings achieves similar results with significantly fewer parameters. © 2021 Association for Computational Linguistics.",,"142, 153",,"Computational Approaches to Linguistic Code-Switching, CALCS 2021 - Proceedings of the 5th Workshop",Conference Paper,Scopus
434,,Learning to Find Usage of Library Functions in Optimized Binaries,"Ahmed T., Devanbu P., Sawant A.A.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113305767&doi=10.1109%2fTSE.2021.3106572&partnerID=40&md5=0055c8ac45a7ace0d88abcc23a127a8a,10.1109/TSE.2021.3106572,"Much software, whether beneficent or malevolent, is distributed only as binaries, sans source code. Absent source code, understanding binaries' behavior can be quite challenging, especially when compiled under higher levels of compiler optimization. These optimizations can transform comprehensible, ``natural"" source constructions into something entirely unrecognizable. Reverse engineering binaries, especially those suspected of being malevolent or guilty of intellectual property theft, are important and time-consuming tasks. There is a great deal of interest in tools to ``decompile"" binaries back into more natural source code to aid reverse engineering. Decompilation involves several desirable steps, including recreating source-language constructions, variable names, and perhaps even comments. One central step in creating binaries is optimizing function calls, using steps such as inlining. Recovering these (possibly inlined) function calls from optimized binaries is an essential task that most state-of-the-art decompiler tools try to do but do not perform very well. In this paper, we evaluate a supervised learning approach to the problem of recovering optimized function calls. We leverage open-source software and develop an automated labeling scheme to generate a reasonably large dataset of binaries labeled with actual function usages. We augment this large but limited labeled dataset with a pre-training step, which learns the decompiled code statistics from a much larger unlabeled dataset. Thus augmented, our learned labeling model can be combined with an existing decompilation tool, Ghidra, to achieve substantially improved performance in function call recovery, especially at higher levels of optimization. IEEE",Databases; Deep learning; Libraries; Malware; Optimization; Reverse engineering; Reverse engineering; Software modeling; Tools; Training,,,IEEE Transactions on Software Engineering,Article,Scopus
435,,Normalization and BackTransliteration for CodeSwitched Data,"Parikh D., Solorio T.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112554729&doi=10.26615%2f978-954-452-056-4_015&partnerID=40&md5=d7d42e3f29dfcd8a02dbe60258e34e9f,10.26615/978-954-452-056-4_015,"Codeswitching is an omnipresent phenomenon in multilingual communities all around the world but remains a challenge for NLP systems due to the lack of proper data and processing techniques. HindiEnglish codeswitched text on social media is often transliterated to the Roman script which prevents from utilizing monolingual resources available in the native Devanagari script. In this paper, we propose a method to normalize and backtransliterate codeswitched HindiEnglish text. In addition, we present a graphemetophoneme (G2P) conversion technique for romanized Hindi data. We also release a dataset of scriptcorrected HindiEnglish codeswitched sentences labeled for the named entity recognition and partofspeech tagging tasks to facilitate further research in this area. © 2021 Association for Computational Linguistics.",,"119, 124",,"Computational Approaches to Linguistic Code-Switching, CALCS 2021 - Proceedings of the 5th Workshop",Conference Paper,Scopus
436,,Transfer learning for just-in-time design smells prediction using temporal convolutional networks,"Ardimento P., Aversano L., Bernardi M.L., Cimitile M., Iammarino M.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111710917&doi=10.5220%2f0010602203100317&partnerID=40&md5=59b5b34f7c4b07b20ca7796e39fab919,10.5220/0010602203100317,"This paper investigates whether the adoption of a transfer learning approach that can be effective for just-in-time design smells prediction. The approach uses a variant of Temporal Convolutional Networks to predict design smells and a carefully selected fine-grained process and product metrics. The validation is performed on a dataset composed of three open-source systems and includes a comparison between transfer and direct learning. The hypothesis, which we want to verify, is that the proposed transfer learning approach is feasible to transfer the knowledge gained on mature systems to the system of interest to make reliable predictions even at the beginning of development when the available historical data is limited. The obtained results show that, when the class imbalance is high, the transfer learning provides F1-scores very close to the ones obtained by direct learning. Copyright © 2021 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved",Deep Learning; Design Smells Prediction; Software Quality; Transfer Learning,"310, 317",,"Proceedings of the 16th International Conference on Software Technologies, ICSOFT 2021",Conference Paper,Scopus
437,,CoMeT: Towards Code-Mixed Translation Using Parallel Monolingual Sentences,"Gautam D., Kodali P., Gupta K., Goel A., Shrivastava M., Kumaraguru P.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111176413&doi=10.26615%2f978-954-452-056-4_007&partnerID=40&md5=5c9f2f30fcba996abf6e7083e6929f4e,10.26615/978-954-452-056-4_007,"Code-mixed languages are very popular in multilingual societies around the world, yet the resources lag behind to enable robust systems on such languages. A major contributing factor is the informal nature of these languages which makes it difficult to collect code-mixed data. In this paper, we propose our system for Task 1 of CACLS 20211 to generate a machine translation system for English to Hinglish in a supervised setting. Translating in the given direction can help expand the set of resources for several tasks by translating valuable datasets from high resource languages. We propose to use mBART, a pre-trained multilingual sequence-to-sequence model, and fully utilize the pre-training of the model by transliterating the roman Hindi words in the code-mixed sentences to Devanagri script. We evaluate how expanding the input by concatenating Hindi translations of the English sentences improves mBART’s performance. Our system gives a BLEU score of 12.22 on test set. Further, we perform a detailed error analysis of our proposed systems and explore the limitations of the provided dataset and metrics. ©2021 Association for Computational Linguistics",,"47, 55",,"Computational Approaches to Linguistic Code-Switching, CALCS 2021 - Proceedings of the 5th Workshop",Conference Paper,Scopus
438,,Modeling and verifying NDN-based IoV using CSP,"Chen N., Zhu H., Yin J., Fei Y., Xiao L., Zhu M.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111000711&doi=10.1002%2fsmr.2371&partnerID=40&md5=c8d518d614abd05bb54dbfc85d287f84,10.1002/smr.2371,"As a crucial component of intelligent transportation system, Internet of Vehicles (IoV) plays an important role in the smart and intelligent cities. However, current Internet architectures cannot guarantee efficient data delivery and adequate data security for IoV. Therefore, Named Data Networking (NDN), a leading architecture of Information-Centric Networking (ICN), is introduced into IoV. Although problems about data distribution can be resolved effectively, the combination of NDN and IoV causes some new security issues. In this paper, we apply Communicating Sequential Processes (CSP) to formalize NDN-based IoV. We mainly focus on its data access mechanism and model this mechanism in detail. By feeding the formalized model into the model checker Process Analysis Toolkit (PAT), we verify four vital properties, namely, deadlock freedom, data reliability, PIT deletion faking, and CS caching pollution. According to verification results, the model cannot ensure the security of data with the appearance of intruders. To solve these problems, we construct a blockchain-based mechanism by creating a blockchain-based distribution trusted platform on top of NDN-based IoV. Through the analysis of the improved model, the blockchain-based mechanism can truly guarantee the security of NDN-based IoV. © 2021 John Wiley & Sons, Ltd.",blockchain; Internet of Vehicles (IoV); modeling and verification; Named Data Networking (NDN); process algebra CSP,,,Journal of Software: Evolution and Process,Article,Scopus
439,,A Study on the Issues Related to Building a Library Information System Based on Deep Learning,"Kim J.H., Lee J.H., Lee K.J.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105110707&doi=10.1109%2fSNPDWinter52325.2021.00076&partnerID=40&md5=48318651dfa011e7e043f6bce292f0b8,10.1109/SNPDWinter52325.2021.00076,"High-performance information systems are essential for libraries to carry out their work efficiently. Recently, a digital library environment has been created that allows electronic resources such as e-books, e-journal, and the Web to be linked to SNS, smartphones and tablet PCs. The environment available anytime and anywhere has increased the use of digital libraries, and the amount and scope of information search has also increased or expanded dramatically. These changes call for improved accuracy in the search for internal or external information in the library information system. This study designs library information retrieval system from the perspective of QA system, and builds a model based on text data. Through this, finally, we present a study comparing performance with various algorithms based on perplexity. © 2021 IEEE.",Digital Library; Information System; Natural Language; OPAC; QA system; Searching,"287, 289",,"Proceedings - 2021 21st ACIS International Semi-Virtual Winter Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing, SNPD-Winter 2021",Conference Paper,Scopus
440,,The Visualization of Cross-media Knowledge Graph of Tang and Song Poetry,"Jiang W., Li C., Wu C.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105083048&doi=10.1109%2fSNPDWinter52325.2021.00022&partnerID=40&md5=d461a073fbd22b43b334c409126f9cb7,10.1109/SNPDWinter52325.2021.00022,"The special grammar and abstruse rhetoric of Tang and Song poetry doomed the diversity of poetry research. In order to help scholars explore the information space of poetry, we took into account the breadth and depth of learning, combined with heterogeneous data sources such as poetry, videos, textbooks, knowledge graph and other media sources, and carried out visual modeling. We provide a visual and informative platform to demonstrate the potential power of visualization. It supports comprehensive search capabilities for multiple types of data, online generation of poets' knowledge graphs. Relationship diagram, word cloud diagram, pie chart and other visual charts are displayed on the platform, so that users can complete exploratory visual analysis by dragging, clicking and typing. © 2021 IEEE.",broad learning; heterogeneous network; knowledge graph; visualization,"69, 73",,"Proceedings - 2021 21st ACIS International Semi-Virtual Winter Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing, SNPD-Winter 2021",Conference Paper,Scopus
441,,BiLSTM-CRF with Compensation Method for Spatial Entity Recognition,"Wang C., Shang W., Huang W., Lin W.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105075813&doi=10.1109%2fSNPDWinter52325.2021.00017&partnerID=40&md5=97f8370a5ee11b7d1a2230cdc93272fd,10.1109/SNPDWinter52325.2021.00017,"As a basic task, named entity recognition (NER) plays a very important role in the field of natural language processing. In recent years, the neural network method has achieved excellent results in NER. However, the NER method is not very effective in fine-grained entity recognition tasks in the subdivision field. In order to solve this problem, we proposed the BiLSTM-CRF model with compensation method (BiLSTM-CC) by increasing the vector representing the semantic information of the word and compensating the model output. In the task of spatial entity recognition, the improved algorithm shows excellent performance. © 2021 IEEE.",BiLSTM-CRF; named entity recognitio; spatial entity recognition; word embedding,"39, 44",,"Proceedings - 2021 21st ACIS International Semi-Virtual Winter Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing, SNPD-Winter 2021",Conference Paper,Scopus
442,,An autonomous performance testing framework using self-adaptive fuzzy reinforcement learning,"Moghadam M.H., Saadatmand M., Borg M., Bohlin M., Lisper B.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102446552&doi=10.1007%2fs11219-020-09532-z&partnerID=40&md5=1309a42dffd2808603aa4b14a89d380e,10.1007/s11219-020-09532-z,"Test automation brings the potential to reduce costs and human effort, but several aspects of software testing remain challenging to automate. One such example is automated performance testing to find performance breaking points. Current approaches to tackle automated generation of performance test cases mainly involve using source code or system model analysis or use-case-based techniques. However, source code and system models might not always be available at testing time. On the other hand, if the optimal performance testing policy for the intended objective in a testing process instead could be learned by the testing system, then test automation without advanced performance models could be possible. Furthermore, the learned policy could later be reused for similar software systems under test, thus leading to higher test efficiency. We propose SaFReL, a self-adaptive fuzzy reinforcement learning-based performance testing framework. SaFReL learns the optimal policy to generate performance test cases through an initial learning phase, then reuses it during a transfer learning phase, while keeping the learning running and updating the policy in the long term. Through multiple experiments in a simulated performance testing setup, we demonstrate that our approach generates the target performance test cases for different programs more efficiently than a typical testing process and performs adaptively without access to source code and performance models. © 2021, The Author(s).",Autonomous testing; Performance testing; Reinforcement learning; Stress testing; Test case generation,,,Software Quality Journal,Article,Scopus
443,,Subwords-Only Alternatives to fastText for Morphologically Rich Languages,"Ghukasyan T., Yeshilbashyan Y., Avetisyan K.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101570469&doi=10.1134%2fS0361768821010059&partnerID=40&md5=d64c29e00beb5d8cb1ee963fb72aced2,10.1134/S0361768821010059,"Abstract: In this work, we present purely subword-based alternatives to fastText word embedding algorithm The alternatives are modifications of the original fastText model, but rely on subword information only, eliminating the reliance on word-level vectors and at the same time helping to dramatically reduce the size of embeddings. Proposed models differ in their subword information extraction method: character n-grams, suffixes, and the byte-pair encoding units. We test the models in the task of morphological analysis and lemmatization for 3 morphologically rich languages: Finnish, Russian, and German. The results are compared with other recent subword-based models, demonstrating consistently higher results. © 2021, Pleiades Publishing, Ltd.",,"56, 66",,Programming and Computer Software,Article,Scopus
444,,Improved Entity Linking for Simple Question Answering over Knowledge Graph,"Chen K., Shen G., Huang Z., Wang H.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100808958&doi=10.1142%2fS0218194021400039&partnerID=40&md5=183f21adb31431bacedfed59d4fcdd9e,10.1142/S0218194021400039,"Question Answering systems over Knowledge Graphs (KG) answer natural language questions using facts contained in a knowledge graph, and Simple Question Answering over Knowledge Graphs (KG-SimpleQA) means that the question can be answered by a single fact. Entity linking, which is a core component of KG-SimpleQA, detects the entities mentioned in questions, and links them to the actual entity in KG. However, traditional methods ignore some information of entities, especially entity types, which leads to the emergence of entity ambiguity problem. Besides, entity linking suffers from out-of-vocabulary (OOV) problem due to the limitation of pre-trained word embeddings. To address these problems, we encode questions in a novel way and encode the features contained in the entities in a multilevel way. To evaluate the enhancement of the whole KG-SimpleQA brought by our improved entity linking, we utilize a relatively simple approach for relation prediction. Besides, to reduce the impact of losing the feature during the encoding procedure, we utilize a ranking algorithm to re-rank (entity, relation) pairs. According to the experimental results, our method for entity linking achieves an accuracy of 81.8% that beats the state-of-the-art methods, and our improved entity linking brings a boost of 5.6% for the whole KG-SimpleQA. © 2021 World Scientific Publishing Company.",entity linking; Knowledge graph; neural network; Simple Question Answering,"55, 80",,International Journal of Software Engineering and Knowledge Engineering,Article,Scopus
445,,Technical Q8A Site Answer Recommendation via Question Boosting,"Gao Z., Xia X., Lo D., Grundy J.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099879031&doi=10.1145%2f3412845&partnerID=40&md5=781618622a879215a95cba19b79edb90,10.1145/3412845,"Software developers have heavily used online question-and-answer platforms to seek help to solve their technical problems. However, a major problem with these technical Q8A sites is ""answer hungriness,""i.e., a large number of questions remain unanswered or unresolved, and users have to wait for a long time or painstakingly go through the provided answers with various levels of quality. To alleviate this time-consuming problem, we propose a novel DEEPANS neural network-based approach to identify the most relevant answer among a set of answer candidates. Our approach follows a three-stage process: question boosting, label establishment, and answer recommendation. Given apost, we first generate a clarifying question as a way of question boosting. We automatically establish the positive, neutral+, neutral-, and negative training samples via label establishment. When it comes to answer recommendation, we sort answer candidates by the matching scores calculated by our neural network-based model. To evaluate the performance of our proposed model, we conducted a large-scale evaluation on four datasets, collected from the real-world technical Q8A sites (i.e., Ask Ubuntu, Super User, Stack Overflow Python, and Stack Overflow Java). Our experimental results show that our approach significantly outperforms several state-of-the-art baselines in automatic evaluation. We also conducted a user study with 50 solved/unanswered/unresolved questions. The user-study results demonstrate that our approach is effective in solving the answer-hungry problem by recommending the most relevant answers from historical archives. © 2020 ACM.",CQA; deep neural network; question answering; question boosting; sequence-to-sequence; weakly supervised learning,,,ACM Transactions on Software Engineering and Methodology,Review,Scopus
446,,PolyDL: Polyhedral Optimizations for Creation of High-performance DL Primitives,"Tavarageri S., Heinecke A., Avancha S., Kaul B., Goyal G., Upadrasta R.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099792327&doi=10.1145%2f3433103&partnerID=40&md5=7ec4a272f9329f2c56061c3c4a7b4e18,10.1145/3433103,"Deep Neural Networks (DNNs) have revolutionized many aspects of our lives. The use of DNNs is becoming ubiquitous, including in software for image recognition, speech recognition, speech synthesis, language translation, to name a few. The training of DNN architectures, however, is computationally expensive. Once the model is created, its use in the intended application - the inference task, is computationally heavy too and the inference needs to be fast for real time use. For obtaining high performance today, the code of Deep Learning (DL) primitives optimized for specific architectures by expert programmers exposed via libraries is the norm. However, given the constant emergence of new DNN architectures, creating hand optimized code is expensive, slow and is not scalable. To address this performance-productivity challenge, in this article we present compiler algorithms to automatically generate high-performance implementations of DL primitives that closely match the performance of hand optimized libraries. We develop novel data reuse analysis algorithms using the polyhedral model to derive efficient execution schedules automatically. In addition, because most DL primitives use some variant of matrix multiplication at their core, we develop a flexible framework where it is possible to plug in library implementations of the same in lieu of a subset of the loops. We show that such a hybrid compiler plus a minimal library-use approach results in state-of-the-art performance. We develop compiler algorithms to also perform operator fusions that reduce data movement through the memory hierarchy of the computer system. Using Convolution Neural Network (CNN) models and matrix multiplication operations, we demonstrate that our approach automatically creates high performing DNN building blocks whose performance matches the performance of hand-crafted kernels of Intel's oneDNN library on high end CPUs. At the same time, our techniques take only a fraction of time (1/20 or less) compared to AutoTVM, a deep learning auto-tuner to create optimized implementations. © 2021 ACM.",data reuse; loop optimization; machine learning; microkernels; Polyhedral compilation,,,ACM Transactions on Architecture and Code Optimization,Article,Scopus
447,,On the Anatomy of Predictive Models for Accelerating GPU Convolution Kernels and beyond,"Labini P.S., Cianfriglia M., Perri D., Gervasi O., Fursin G., Lokhmotov A., Nugteren C., Carpentieri B., Zollo F., Vella F.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099790155&doi=10.1145%2f3434402&partnerID=40&md5=a4c30176ed8868e02952d8c4ca69e6d8,10.1145/3434402,"Efficient HPC libraries often expose multiple tunable parameters, algorithmic implementations, or a combination of them, to provide optimized routines. The optimal parameters and algorithmic choices may depend on input properties such as the shapes of the matrices involved in the operation. Traditionally, these parameters are manually tuned or set by auto-tuners. In emerging applications such as deep learning, this approach is not effective across the wide range of inputs and architectures used in practice. In this work, we analyze different machine learning techniques and predictive models to accelerate the convolution operator and GEMM. Moreover, we address the problem of dataset generation, and we study the performance, accuracy, and generalization ability of the models. Our insights allow us to improve the performance of computationally expensive deep learning primitives on high-end GPUs as well as low-power embedded GPU architectures on three different libraries. Experimental results show significant improvement in the target applications from 50% up to 300% compared to auto-tuned and high-optimized vendor-based heuristics by using simple decision tree- and MLP-based models. © 2021 Owner/Author.",GPU computing; neural networks; performance optimization; predictive models; supervised classification; tuning,,,ACM Transactions on Architecture and Code Optimization,Article,Scopus
448,,Survey of Deep Learning Model Compression and Acceleration [深度学习模型压缩与加速综述],"Gao H., Tian Y.-L., Xu F.-Y., Zhong S.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099103735&doi=10.13328%2fj.cnki.jos.006096&partnerID=40&md5=13d54377ae6bea15bc219d349d8ea1db,10.13328/j.cnki.jos.006096,"With the development of the amount of data available for training and the processing power of new computing platform, the intelligent model based on deep learning can accomplish more and more complex tasks, and it has made major breakthroughs in the field of AI such as computer vision and natural language processing. However, the large number of parameters of these deep models bring awesome computational overhead and memory requirements, which makes the big models must face great difficulties and challenges in the deployment of computing-capable platforms (such as mobile embedded devices). Therefore, model compression and acceleration without affecting the performance have become a research hotspot. This study first analyzes the classical deep learning model compression and acceleration methods proposed by domestic and international scholars, and summarize seven aspects: Parameter pruning, parameter quantization, compact network, knowledge distillation, low-rank decomposition, parameter sharing, and hybrid methods. Secondly, the compression and acceleration performance of several mainstream representative methods is compared on multiple public models. Finally, the future research directions in the field of model compression and acceleration are discussed. © Copyright 2021, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Compact network; Deep learning; Model acceleration; Model compression; Parameter pruning; Parameter quantization,"68, 92",,Ruan Jian Xue Bao/Journal of Software,Review,Scopus
449,,Cross project defect prediction: a comprehensive survey with its SWOT analysis,"Khatri Y., Singh S.K.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098735838&doi=10.1007%2fs11334-020-00380-5&partnerID=40&md5=bdbaceed7fcdb87fcfb1ca78a374238f,10.1007/s11334-020-00380-5,"Software fault prediction (SFP) refers to the process of identifying (or predicting) faulty modules based on its characteristics/software metrics. SFP can be done either using the same project data in both the training and testing phase i.e. within project defect prediction or using a different one, as done in cross-project defect prediction (CPDP). Previous works show that contemporary research in this field is progressing towards CPDP. To present the current state of progress and the future prospects of CPDP, this article presents a comprehensive survey of CPDP considering the latest work along with its SWOT analysis. This survey is targeted to present the novice researchers, academicians, and practitioners with the alphas and omegas of this contemporary challenging field. We have also carried a qualitative and quantitative evaluation of CPDP w.r.t some of the targeted research questions. A total of 34 significant primary CPDP studies published from 2008 to 2019 were selected. Both qualitative and quantitative data are extracted from each study. The collected data is then consolidated and analyzed to present a comprehensive report showing the current state of the art, along with the answers to the targeted research questions and finally the CPDP SWOT analysis. We observed that there exists a big scope for performance improvement in CPDP. Integration of feature engineering, exploration with different process metrics, hyperparameter tuning, class imbalance handling in CPDP setting are some of the ways identified for bringing enhancement in CPDP performance. Apart from this, we would like to conclude that there is a strong need to investigate Precision over the Recall and model’s validity in terms of effort/cost-effectiveness. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd. part of Springer Nature.",Cross-project defect prediction; Software metrics; SWOT analysis; Transfer learning,,,Innovations in Systems and Software Engineering,Review,Scopus
450,,Automatic stenosis recognition from coronary angiography using convolutional neural networks,"Moon J.H., Lee D.Y., Cha W.C., Chung M.J., Lee K.-S., Cho B.H., Choi J.H.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096404243&doi=10.1016%2fj.cmpb.2020.105819&partnerID=40&md5=0d803fd72ca7b8204c7cd36607ff8387,10.1016/j.cmpb.2020.105819,"Background and objective: Coronary artery disease, which is mostly caused by atherosclerotic narrowing of the coronary artery lumen, is a leading cause of death. Coronary angiography is the standard method to estimate the severity of coronary artery stenosis, but is frequently limited by intra- and inter-observer variations. We propose a deep-learning algorithm that automatically recognizes stenosis in coronary angiographic images. Methods: The proposed method consists of key frame detection, deep learning model training for classification of stenosis on each key frame, and visualization of the possible location of the stenosis. Firstly, we propose an algorithm that automatically extracts key frames essential for diagnosis from 452 right coronary artery angiography movie clips. Our deep learning model is then trained with image-level annotations to classify the areas narrowed by over 50 %. To make the model focus on the salient features, we apply a self-attention mechanism. The stenotic locations are visualized using the activated area of feature maps with gradient-weighted class activation mapping. Results: The automatically detected key frame was very close to the manually selected key frame (average distance (1.70 ± 0.12) frame per clip). The model was trained with key frames on internal datasets, and validated with internal and external datasets. Our training method achieved high frame-wise area-under-the-curve of 0.971, frame-wise accuracy of 0.934, and clip-wise accuracy of 0.965 in the average values of cross-validation evaluations. The external validation results showed high performances with the mean frame-wise area-under-the-curve of (0.925 and 0.956) in the single and ensemble model, respectively. Heat map visualization shows the location for different types of stenosis in both internal and external data sets. With the self-attention mechanism, the stenosis could be precisely localized, which helps to accurately classify the stenosis by type. Conclusions: Our automated classification algorithm could recognize and localize coronary artery stenosis highly accurately. Our approach might provide the basis for a screening and assistant tool for the interpretation of coronary angiography. © 2020 The Authors",Automated screening; Coronary angiography; Coronary artery stenosis; Deep learning; Stenosis recognition,,,Computer Methods and Programs in Biomedicine,Article,Scopus
451,,A framework for homogeneous cross-project defect prediction,"Goel L., Sharma M., Khatri S.K., Damodaran D.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096049788&doi=10.4018%2fIJSI.2021010105&partnerID=40&md5=0b5227334c6b7f11336683188737140f,10.4018/IJSI.2021010105,"Often, the prior defect data of the same project is unavailable; researchers thought whether the defect data of the other projects can be used for prediction. This made cross project defect prediction an open research issue. In this approach, the training data often suffers from class imbalance problem. Here, the work is directed on homogeneous cross-project defect prediction. A novel ensemble model that will perform in dual fold is proposed. Firstly, it will handle the class imbalance problem of the dataset. Secondly, it will perform the prediction of the target class. For handling the imbalance problem, the training dataset is divided into data frames. Each data frame will be balanced. An ensemble model using the maximum voting of all random forest classifiers is implemented. The proposed model shows better performance in comparison to the other baseline models. Wilcoxon signed rank test is performed for validation of the proposed model. Copyright © 2021, IGI Global.",Class Imbalance Learning; Cross Project Defect Prediction (CPDP); Ensemble Model; Homogeneous; Within Project Defect Prediction (WPDP),"52, 68",,International Journal of Software Innovation,Article,Scopus
452,,DeepHistReg: Unsupervised Deep Learning Registration Framework for Differently Stained Histology Samples,"Wodzinski M., Müller H.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094325369&doi=10.1016%2fj.cmpb.2020.105799&partnerID=40&md5=777a8f7894aef13194d6a1ca02a21d99,10.1016/j.cmpb.2020.105799,"Background and objective: The use of several stains during histology sample preparation can be useful for fusing complementary information about different tissue structures. It reveals distinct tissue properties that combined may be useful for grading, classification, or 3-D reconstruction. Nevertheless, since the slide preparation is different for each stain and the procedure uses consecutive slices, the tissue undergoes complex and possibly large deformations. Therefore, a nonrigid registration is required before further processing. The nonrigid registration of differently stained histology images is a challenging task because: (i) the registration must be fully automatic, (ii) the histology images are extremely high-resolution, (iii) the registration should be as fast as possible, (iv) there are significant differences in the tissue appearance, and (v) there are not many unique features due to a repetitive texture. Methods: In this article, we propose a deep learning-based solution to the histology registration. We describe a registration framework dedicated to high-resolution histology images that can perform the registration in real-time. The framework consists of an automatic background segmentation, iterative initial rotation search and learning-based affine/nonrigid registration. Results: We evaluate our approach using an open dataset provided for the Automatic Non-rigid Histological Image Registration (ANHIR) challenge organized jointly with the IEEE ISBI 2019 conference. We compare our solution to the challenge participants using a server-side evaluation tool provided by the challenge organizers. Following the challenge evaluation criteria, we use the target registration error (TRE) as the evaluation metric. Our algorithm provides registration accuracy close to the best scoring teams (median rTRE 0.19% of the image diagonal) while being significantly faster (the average registration time is about 2 seconds). Conclusions: The proposed framework provides results, in terms of the TRE, comparable to the best-performing state-of-the-art methods. However, it is significantly faster, thus potentially more useful in clinical practice where a large number of histology images are being processed. The proposed method is of particular interest to researchers requiring an accurate, real-time, nonrigid registration of high-resolution histology images for whom the processing time of traditional, iterative methods in unacceptable. We provide free access to the software implementation of the method, including training and inference code, as well as pretrained models. Since the ANHIR dataset is open, this makes the results fully and easily reproducible. © 2020 Elsevier B.V.",ANHIR; Deep Learning; Histology; Image Registration,,,Computer Methods and Programs in Biomedicine,Article,Scopus
453,,Predicting the emergence of community smells using socio-technical metrics: A machine-learning approach,"Palomba F., Tamburri D.A.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093986587&doi=10.1016%2fj.jss.2020.110847&partnerID=40&md5=0a10ae423a46f93ffb1d25251416e0b9,10.1016/j.jss.2020.110847,"Community smells represent sub-optimal conditions appearing within software development communities (e.g., non-communicating sub-teams, deviant contributors, etc.) that may lead to the emergence of social debt and increase the overall project's cost. Previous work has studied these smells under different perspectives, investigating their nature, diffuseness, and impact on technical aspects of source code. Furthermore, it has been shown that some socio-technical metrics like, for instance, the well-known socio-technical congruence, can potentially be employed to foresee their appearance. Yet, there is still a lack of knowledge of the actual predictive power of such socio-technical metrics. In this paper, we aim at tackling this problem by empirically investigating (i) the potential value of socio-technical metrics as predictors of community smells and (ii) what is the performance of within- and cross-project community smell prediction models based on socio-technical metrics. To this aim, we exploit a dataset composed of 60 open-source projects and consider four community smells such as ORGANIZATIONAL SILO, BLACK CLOUD, LONE WOLF, and BOTTLENECK. The key results of our work report that a within-project solution can reach F-Measure and AUC-ROC of 77% and 78%, respectively, while cross-project models still require improvements, being however able to reach an F-Measure of 62% and overcome a random baseline. Among the metrics investigated, socio-technical congruence, communicability, and turnover-related metrics are the most powerful predictors of the emergence of community smells. © 2020",Community smells; Empirical software engineering; Social debt,,,Journal of Systems and Software,Article,Scopus
454,,Biomedical image classification made easier thanks to transfer and semi-supervised learning,"Inés A., Domínguez C., Heras J., Mata E., Pascual V.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092453671&doi=10.1016%2fj.cmpb.2020.105782&partnerID=40&md5=c872a54fce7931bff642bdf878096143,10.1016/j.cmpb.2020.105782,"Background and objectives: Deep learning techniques are the state-of-the-art approach to solve image classification problems in biomedicine; however, they require the acquisition and annotation of a considerable volume of images. In addition, using deep learning libraries and tuning the hyperparameters of the networks trained with them might be challenging for several users. These drawbacks prevent the adoption of these techniques outside the machine-learning community. In this work, we present an Automated Machine Learning (AutoML) method to deal with these problems. Methods: Our AutoML method combines transfer learning with a new semi-supervised learning procedure to train models when few annotated images are available. In order to facilitate the dissemination of our method, we have implemented it as an open-source tool called ATLASS. Finally, we have evaluated our method with two benchmarks of biomedical image classification datasets. Results: Our method has been thoroughly tested both with small datasets and partially annotated biomedical datasets; and, it outperforms, both in terms of speed and accuracy, the existing AutoML tools when working with small datasets; and, might improve the accuracy of models up to a 10% when working with partially annotated datasets. Conclusions: The work presented in this paper allows the use of deep learning techniques to solve an image classification problem with few resources. Namely, it is possible to train deep models with small, and partially annotated datasets of images. In addition, we have proven that our AutoML method outperforms other AutoML tools both in terms of accuracy and speed when working with small datasets. © 2020 Elsevier B.V.",AutoML; Benchmark; Image classification; Semi-Supervised learning; Transfer-learning,,,Computer Methods and Programs in Biomedicine,Article,Scopus
455,,Virus identification in electron microscopy images by residual mixed attention network,"Xiao C., Chen X., Xie Q., Li G., Xiao H., Song J., Han H.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092420005&doi=10.1016%2fj.cmpb.2020.105766&partnerID=40&md5=3343fb44d0b8ed17a0f09f9038bad0c8,10.1016/j.cmpb.2020.105766,"Background and Objective: Virus identification in electron microscopy (EM) images is considered as one of the front-line method in pathogen diagnosis and re-emerging infectious agents. However, the existing methods either focused on the detection of a single virus or required large amounts of manual labeling work to segment virus. In this work, we focus on the task of virus classification and propose an effective and simple method to identify different viruses. Methods: We put forward a residual mixed attention network (RMAN) for virus classification. The proposed network uses channel attention, bottom-up and top-down attention, and incorporates a residual architecture in an end-to-end training manner, which is suitable for dealing with EM virus images and reducing the burden of manual annotation. Results: We validate the proposed network through extensive experiments on a transmission electron microscopy virus image dataset. The top-1 error rate of our RMAN on 12 virus classes is 4.285%, which surpasses that of state-of-the-art networks and even human experts. In addition, the ablation study and the visualization of class activation mapping (CAM) further demonstrate the effectiveness of our method. Conclusions: The proposed automated method contributes to the development of medical virology, which provides virologists with a high-accuracy approach to recognize viruses as well as assist in the diagnosis of viruses. © 2020 Elsevier B.V.",attention mechanism; deep learning; transmission electron microscopy; viral morphology; Virus identification,,,Computer Methods and Programs in Biomedicine,Article,Scopus
456,,VSSC Net: Vessel Specific Skip chain Convolutional Network for blood vessel segmentation,"Samuel P.M., Veeramalai T.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092259076&doi=10.1016%2fj.cmpb.2020.105769&partnerID=40&md5=683dc1966fc6b9d648858dcf838593df,10.1016/j.cmpb.2020.105769,"Background and objective: Deep learning techniques are instrumental in developing network models that aid in the early diagnosis of life-threatening diseases. To screen and diagnose the retinal fundus and coronary blood vessel disorders, the most important step is the proper segmentation of the blood vessels. Methods: This paper aims to segment the blood vessels from both the coronary angiogram and the retinal fundus images using a single VSSC Net after performing the image-specific preprocessing. The VSSC Net uses two-vessel extraction layers with added supervision on top of the base VGG-16 network. The vessel extraction layers comprise of the vessel-specific convolutional blocks to localize the blood vessels, skip chain convolutional layers to enable rich feature propagation, and a unique feature map summation. Supervision is associated with the two-vessel extraction layers using separate loss/sigmoid function. Finally, the weighted fusion of the individual loss/sigmoid function produces the desired blood vessel probability map. It is then binary segmented and validated for performance. Results: The VSSC Net shows improved accuracy values on the standard retinal and coronary angiogram datasets respectively. The computational time required to segment the blood vessels is 0.2 seconds using GPU. Moreover, the vessel extraction layer uses a lesser parameter count of 0.4 million parameters to accurately segment the blood vessels. Conclusion: The proposed VSSC Net that segments blood vessels from both the retinal fundus images and coronary angiogram can be used for the early diagnosis of vessel disorders. Moreover, it could aid the physician to analyze the blood vessel structure of images obtained from multiple imaging sources. © 2020",Convolutional neural network; Coronary angiogram; Feature propagation; Retinal fundus; Vessel segmentation,,,Computer Methods and Programs in Biomedicine,Article,Scopus
457,,A regression framework to head-circumference delineation from US fetal images,"Fiorentino M.C., Moccia S., Capparuccini M., Giamberini S., Frontoni E.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092253830&doi=10.1016%2fj.cmpb.2020.105771&partnerID=40&md5=fead0d7f4bf6a70d3c7fe014a55f796b,10.1016/j.cmpb.2020.105771,"Background and Objectives: Measuring head-circumference (HC) length from ultrasound (US) images is a crucial clinical task to assess fetus growth. To lower intra- and inter-operator variability in HC length measuring, several computer-assisted solutions have been proposed in the years. Recently, a large number of deep-learning approaches is addressing the problem of HC delineation through the segmentation of the whole fetal head via convolutional neural networks (CNNs). Since the task is a edge-delineation problem, we propose a different strategy based on regression CNNs. Methods: The proposed framework consists of a region-proposal CNN for head localization and centering, and a regression CNN for accurately delineate the HC. The first CNN is trained exploiting transfer learning, while we propose a training strategy for the regression CNN based on distance fields. Results: The framework was tested on the HC18 Challenge dataset, which consists of 999 training and 335 testing images. A mean absolute difference of 1.90 ( ± 1.76) mm and a Dice similarity coefficient of 97.75 ( ± 1.32) % were achieved, overcoming approaches in the literature. Conclusions: The experimental results showed the effectiveness of the proposed framework, proving its potential in supporting clinicians during the clinical practice. © 2020 Elsevier B.V.",Convolutional neural networks; Fetal ultrasounds; Head circumference delineation; Regression networks,,,Computer Methods and Programs in Biomedicine,Article,Scopus
458,,COSTE: Complexity-based OverSampling TEchnique to alleviate the class imbalance problem in software defect prediction,"Feng S., Keung J., Yu X., Xiao Y., Bennin K.E., Kabir M.A., Zhang M.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091712466&doi=10.1016%2fj.infsof.2020.106432&partnerID=40&md5=8d032fbe681e9112cb9d11d3ddfbab05,10.1016/j.infsof.2020.106432,"Context: Generally, there are more non-defective instances than defective instances in the datasets used for software defect prediction (SDP), which is referred to as the class imbalance problem. Oversampling techniques are frequently adopted to alleviate the problem by generating new synthetic defective instances. Existing techniques generate either near-duplicated instances which result in overgeneralization (high probability of false alarm, pf) or overly diverse instances which hurt the prediction model's ability to find defects (resulting in low probability of detection, pd). Furthermore, when existing oversampling techniques are applied in SDP, the effort needed to inspect the instances with different complexity is not taken into consideration. Objective: In this study, we introduce Complexity-based OverSampling TEchnique (COSTE), a novel oversampling technique that can achieve low pf and high pd simultaneously. Meanwhile, COSTE also performs better in terms of Norm(popt) and ACC, two effort-aware measures that consider the testing effort. Method: COSTE combines pairs of defective instances with similar complexity to generate synthetic instances, which improves the diversity within the data, maintains the ability of prediction models to find defects, and takes the different testing effort needed for different instances into consideration. We conduct experiments to compare COSTE with Synthetic Minority Oversampling TEchnique, Borderline-SMOTE, Majority Weighted Minority Oversampling TEchnique and MAHAKIL. Results: The experimental results on 23 releases of 10 projects show that COSTE greatly improves the diversity of the synthetic instances without compromising the ability of prediction models to find defects. In addition, COSTE outperforms the other oversampling techniques under the same testing effort. The statistical analysis indicates that COSTE's ability to outperform the other oversampling techniques is significant under the statistical Wilcoxon rank sum test and Cliff's effect size. Conclusion: COSTE is recommended as an efficient alternative to address the class imbalance problem in SDP. © 2020 Elsevier B.V.",Class imbalance; Effort-aware defect prediction; MAHAKIL; Oversampling; SMOTE; Software defect prediction,,,Information and Software Technology,Article,Scopus
459,,An Exploratory Study of Machine Learning Model Stores,"Xiu M., Jiang Z.M.J., Adams B.",2021,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079619475&doi=10.1109%2fMS.2020.2975159&partnerID=40&md5=fd8d13276bd7b2400510e6b7827658ef,10.1109/MS.2020.2975159,Several organizations have introduced stores that provide public access to pretrained machine learning models and infrastructure. We examine three of them and compare the information they provide against two mobileapp stores and among themselves. © 1984-2012 IEEE.,,"114, 122",,IEEE Software,Article,Scopus
460,,Manifold embedded distribution adaptation for cross-project defect prediction,"Sun Y., Jing X.-Y., Wu F., Sun Y.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101786394&doi=10.1049%2fiet-sen.2019.0389&partnerID=40&md5=eae01a26666a147cccda2c322f2e35e3,10.1049/iet-sen.2019.0389,"Cross-project defect prediction (CPDP) technology refers to the constructing prediction model to predict the instance label of the target project by utilising labelled data from an external project. The challenge of CPDP methods is the distribution difference between the data from different projects. Transfer learning can transfer the knowledge from the source domain to the target domain with the aim to minimise the domain difference between different domains. However, most existing methods reduce the distribution discrepancy in the original feature space, where the features are high-dimensional and non-linear, which makes it hard to reduce the distribution distance between different projects. Moreover, previous works mainly consider marginal distribution or conditional distribution difference. In this study, the authors proposed a manifold embedded distribution adaptation (MDA) approach to narrow the distribution gap in manifold feature subspace. MDA maps source and target project data to manifold subspace and then joint distribution adaptation of conditional and marginal distributions is performed on manifold subspace. To evaluate the effectiveness of MDA, the authors perform extensive experiments on 20 public projects with three indicators. The experiment results show that MDA improves the average performance, but the improvement is not statistically significant in comparison to HYDRA (one of the baselines). © The Institution of Engineering and Technology 2020.",,"825, 838",,IET Software,Article,Scopus
461,,Software defect prediction using k-pca and various kernel-based extreme learning machine: An empirical study,"Pandey S.K., Rathee D., Tripathi A.K.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101742244&doi=10.1049%2fiet-sen.2020.0119&partnerID=40&md5=eb8b88ddbc45b78f69c33a1f276a597e,10.1049/iet-sen.2020.0119,"Predicting defects during software testing reduces an enormous amount of testing effort and help to deliver a high-quality software system. Owing to the skewed distribution of public datasets, software defect prediction (SDP) suffers from the class imbalance problem, which leads to unsatisfactory results. Overfitting is also one of the biggest challenges for SDP. In this study, the authors performed an empirical study of these two problems and investigated their probable solution. They have conducted 4840 experiments over five different classifiers using eight NASA projects and 14 PROMISE repository datasets. They suggested and investigated the varying kernel function of an extreme learning machine (ELM) along with kernel principal component analysis (K-PCA) and found better results compared with other classical SDP models. They used the synthetic minority oversampling technique as a sampling method to address class imbalance problems and k-fold cross-validation to avoid the overfitting problem. They found ELM-based SDP has a high receiver operating characteristic curve over 11 out of 22 datasets. The proposed model has higher precision and F-score values over ten and nine, respectively, compared with other state-of-the-art models. The Mathews correlation coefficient (MCC) of 17 datasets of the proposed model surpasses other classical models' MCC. © The Institution of Engineering and Technology 2020.",,"768, 782",,IET Software,Article,Scopus
462,,A benchmark study of the contemporary toxicity detectors on software engineering interactions,"Sarker J., Turzo A.K., Bosu A.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102369054&doi=10.1109%2fAPSEC51365.2020.00030&partnerID=40&md5=89bec66b6b21d972a13c82d1f650e308,10.1109/APSEC51365.2020.00030,"Automated filtering of toxic conversations may help an Open-source software (OSS) community to maintain healthy interactions among the project participants. Although, several general purpose tools exist to identify toxic contents, those may incorrectly flag some words commonly used in the Software Engineering (SE) context as toxic (e.g., 'junk', 'kill', and 'dump') and vice versa. To encounter this challenge, an SE specific tool has been proposed by the CMU Strudel Lab (referred as the 'STRUDEL' hereinafter) by combining the output of the Perspective API with the output from a customized version of the Stanford's Politeness detector tool. However, since STRUDEL's evaluation was very limited with only 654 SE text, its practical applicability is unclear. Therefore, this study aims to empirically evaluate the Strudel tool as well as four state-of-the-art general purpose toxicity detectors on a large scale SE dataset. On this goal, we empirically developed a rubric to manually label toxic SE interactions. Using this rubric, we manually labeled a dataset of 6, 533 code review comments and 4, 140 Gitter messages. The results of our analyses suggest significant degradation of all tools' performances on our datasets. Those degradations were significantly higher on our dataset of formal SE communication such as code review than on our dataset of informal communication such as Gitter messages. Two of the models from our study showed significant performance improvements during 10-fold cross validations after we retrained those on our SE datasets. Based on our manual investigations of the incorrectly classified text, we have identified several recommendations for developing an SE specific toxicity detector. © 2020 IEEE.",benchmark; chat; code review; developer communication; rubric; toxicity,"218, 227",,"Proceedings - Asia-Pacific Software Engineering Conference, APSEC",Conference Paper,Scopus
463,,Improving machine learning-based code smell detection via hyper-parameter optimization,"Shen L., Liu W., Chen X., Gu Q., Liu X.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102361663&doi=10.1109%2fAPSEC51365.2020.00036&partnerID=40&md5=c49ef6a8d2e7118337a543df45030969,10.1109/APSEC51365.2020.00036,"Unlike code errors, the presence of code smell often does not affect the behavior of the software system, but it will cause quality problems in terms of readability, understandability, and efficiency. To improve the software quality and reduce the maintenance costs, the developers need to detect code smells rapidly and make corresponding code refactoring. In code smell detection, recently, machine learning-based methods become more prevalent and can overcome the shortcomings of the heuristic-based methods, which mainly rely on manually designed rules. However, to our best knowledge, there is little research to analyze whether using hyper-parameter optimization can improve the performance of machine learning-based methods. In this study, we mainly focus on two classical code smells (i.e., Data Class and Feature Envy). First, we consider four optimizers for hyper-parameter optimization, and six commonly used classifiers for machine-learning-based methods. Second, we use AUC as the performance measure to evaluate the performance of constructed models. Based on final empirical results, we find that (1) Using hyper-parameter optimization can significantly improve the performance of code smell detection. (2) Differential evolution (DE) optimizer can achieve better performance than the other three optimizers when using the random forest classifier. (3) We can further improve the performance of code smell detection when performing parameter optimization on the DE optimizer. © 2020 IEEE.",code smell detection; differential evolution; hyperparameter optimization; machine learning,"276, 285",,"Proceedings - Asia-Pacific Software Engineering Conference, APSEC",Conference Paper,Scopus
464,,An attentive deep supervision based semantic matching framework for tag recommendation in software information sites,"Zheng X., Li L., Zhou D.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102356674&doi=10.1109%2fAPSEC51365.2020.00062&partnerID=40&md5=1996388e44faf42f0276dc1096eda179,10.1109/APSEC51365.2020.00062,"Tag recommendation in software information sites is a popular way to help developers classify software objects. Existing methods mostly consider tag recommendation as a multi-label classification task, which does not adequately leverage the semantic information of tags themselves. It is observed that the information granularity of tags is from abstract to specific and deep learning models have proven capable of automatically learning the features in different layers of an integrated network with different abstraction degrees. In this paper, we propose TagMatchRec, a deep semantic matching framework for tag recommendation instead of being based on classification. In our framework, multiple layers with different information granularities are directly connected to the output layer aiming at improving the quality of tag recommendation. Moreover, because the abstraction levels of semantic features learned by each layer may be different given different software objects and tags, an attentive deep supervision is introduced so that the dense connections from early layers to the output layer have directly weighted impact on loss function optimization. Comprehensive evaluations are conducted the datasets from four software information sites. The experimental results show that TagMatchRec has achieved better performance compared with the state-of-the-art approaches. © 2020 IEEE.",Multi-level feature; Semantic matching; Software information site; Tag recommdation,"490, 494",,"Proceedings - Asia-Pacific Software Engineering Conference, APSEC",Conference Paper,Scopus
465,,A Fusion of Java Domain Knowledge Base and Siamese Network for Java API Recommendation,"Li H., Li T., Zhong S., Kang Y., Chen T.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099406010&doi=10.1109%2fQRS-C51114.2020.00074&partnerID=40&md5=387fb9637f5a46f253c3603fc0facd41,10.1109/QRS-C51114.2020.00074,"APIs play an important role in modern software development. Programmers need to frequently search for the appropriate APIs according to different tasks. With the development of the information industry, API reference documents have become larger and larger. Due to redundant and erroneous information on the Internet, traditional search methods can also cause inconvenience to programmers' queries. At the same time, there is a gap in terms of vocabulary and knowledge between the natural language description of the programming task and the description in the API documentation, so it is difficult to find a suitable API. To solve these problems, this paper proposes a Java API recommendation model by fusing the Java domain knowledge base and the Siamese Network to improve the accuracy of API recommendation. Experiments on the BIKER data set show that our method has better recommendation results than the state-of-art DeepAPI and BIKER model. © 2020 IEEE.",API recommendation; BERT; deep learning; Java; Stack Overflow,"398, 405",,"Proceedings - Companion of the 2020 IEEE 20th International Conference on Software Quality, Reliability, and Security, QRS-C 2020",Conference Paper,Scopus
466,,Depth Estimation and Object Detection for Monocular Semantic SLAM Using Deep Convolutional Network,"Hou C., Zhao X., Lin Y.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099340486&doi=10.1109%2fQRS-C51114.2020.00051&partnerID=40&md5=3105eb066f8314134dd18115f4dad4e8,10.1109/QRS-C51114.2020.00051,"It is still challenging to efficiently construct semantic map with a monocular camera. In this paper, deep learning is introduced to combined with SLAM to realize semantic map production. We replace depth estimation module of SLAM with FCN which effectively solves the contradiction of triangulation. The Fc layers of FCN are modified to convolutional layers. Redundant calculation of Fc layers is avoided after optimization, and images can be input in any size. Besides, Faster RCNN, namely, a two-stage object detection network is utilized to obtain semantic information. We fine-tune RPN and Fc layers by transfer learning. The two algorithms are evaluated on official dataset. Results show that the average relative error of depth estimation is reduced by 12.6%, the accuracy of object detection is improved by 10.9%. The feasibility of the combination of deep learning and SLAM is verified. © 2020 IEEE.",deep learning; indoor object detection; monocular depth estimation; ORB-SLAM2,"256, 263",,"Proceedings - Companion of the 2020 IEEE 20th International Conference on Software Quality, Reliability, and Security, QRS-C 2020",Conference Paper,Scopus
467,,Cross-Project Dynamic Defect Prediction Model for Crowdsourced test,"Yao Y., Liu Y., Huang S., Chen H., Liu J., Yang F.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099284438&doi=10.1109%2fQRS51102.2020.00040&partnerID=40&md5=9d8511468f522924a4a02c4c666fe944,10.1109/QRS51102.2020.00040,"By comparing the predicted number of defects with the number found in crowdsourced test in real time, people can dynamically assess the progress of crowdsourced test tasks. In this paper, we propose a cross-project dynamic defect prediction model (CPDDPM) for crowdsourced test to predict the number of defects in real time. In the construction of training dataset, we use density-based clustering method to select instances from the multiple source project datasets and build the initial training dataset. In the dynamic correction, CPDDPM iteratively corrects the prediction model using crowdsourced test reports and ability attributes of the crowdsourced testers until the predicted results converge. We collected project defect datasets on the crowdsourced test platform, and evaluated prediction accuracy of CPDDPM by using relative error and prediction at level l. The results show that CPDDPM can greatly improve the prediction performance of defect number. © 2020 IEEE.",crowdsourced test; defect number prediction; dynamic correction; instance selection,"223, 230",,"Proceedings - 2020 IEEE 20th International Conference on Software Quality, Reliability, and Security, QRS 2020",Conference Paper,Scopus
468,,Recognition Method Based on Deep Learning for Chinese Textual Entailment Chunks and Labels [中文文本蕴含类型及语块识别方法研究],"Yu D., Jin T.-H., Xie W.-Y., Zhang Y., Xun E.-D.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098546742&doi=10.13328%2fj.cnki.jos.005885&partnerID=40&md5=8d9f06c095a8f2d0ad1dba99589b4feb,10.13328/j.cnki.jos.005885,"Recognizing textual entailment (RTE) is a task to recognize whether two sentences have an entailment relationship. In recent years, RTE in English had made a great progress. The current researches are mainly based on type judgment, and pay less attention to locate the language chunks that lead to the entailment relationship. More over, it leads to a low interpretability of the RTE models. This study selects 12 000 Chinese entailment sentence pairs from the Chinese Natural Language Inference (CNLI) data and labeled chunks which lead to their entailment relationship. Then 7 entailment types are summarized considering Chinese linguistic features. On the basis, two tasks are proposed. One is to recognize the seven-category of entailment type for each entailment sentence pairs, another is to recognize the boundaries of the entailment chunks in it. The proposed deep learning based method reaches an accuracy of 69.19% and 62.09% in the two tasks. The experimental results show that proposed approaches can effectively identifying different types of entailment in Chinese and find the boundaries of the entailment chunks, which demonstrate that the proposed model provides a reliable benchmark for further research. © Copyright 2020, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Chunk labeling; Deep learning; Recognizing textual entailment,"3772, 3786",,Ruan Jian Xue Bao/Journal of Software,Article,Scopus
469,,Quality Assurance for Machine Learning-an approach to function and system safeguarding,"Poth A., Meyer B., Schlicht P., Riel A.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095746350&doi=10.1109%2fQRS51102.2020.00016&partnerID=40&md5=f7588f0629abd80f26519359433c3d35,10.1109/QRS51102.2020.00016,"In an industrial context, high software quality is mandatory in order to avoid costly patching. We present a state of the art analysis of approaches to ensure that a specific Artificial Intelligence (AI) model is ready for release. We analyze the requirements a Machine Learning (ML) system has to fulfill in order to comply with the needs of an automotive OEM. The main implication for projects relying on ML is a holistic assessment of possible quality risks. These risks may stem from implemented ML models and spread into the delivery. We present a methodological quality assurance (QA) approach and its evaluation. © 2020 IEEE.",artificial intelligence; machine learning; quality assurance; quality management; risk management,"22, 29",,"Proceedings - 2020 IEEE 20th International Conference on Software Quality, Reliability, and Security, QRS 2020",Conference Paper,Scopus
470,,Developing a new radiomics-based CT image marker to detect lymph node metastasis among cervical cancer patients,"Chen X., Liu W., Thai T.C., Castellano T., Gunderson C.C., Moore K., Mannel R.S., Liu H., Zheng B., Qiu Y.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091625628&doi=10.1016%2fj.cmpb.2020.105759&partnerID=40&md5=6f4861bc59fd0c91fd42bc4ac83131e6,10.1016/j.cmpb.2020.105759,"Background and Objective: In diagnosis of cervical cancer patients, lymph node (LN) metastasis is a highly important indicator for the following treatment management. Although CT/PET (i.e., computed tomography/positron emission tomography) examination is the most effective approach for this detection, it is limited by the high cost and low accessibility, especially for the rural areas in the U.S.A. or other developing countries. To address this challenge, this investigation aims to develop and test a novel radiomics-based CT image marker to detect lymph node metastasis for cervical cancer patients. Methods: A total of 1,763 radiomics features were first computed from the segmented primary cervical tumor depicted on one CT image with the maximal tumor region. Next, a principal component analysis algorithm was applied on the initial feature pool to determine an optimal feature cluster. Then, based on this optimal cluster, the prediction models (i.e., logistic regression or support vector machine) were trained and optimized to generate an image marker to detect LN metastasis. In this study, a retrospective dataset containing 127 cervical cancer patients were established to build and test the model. The model was trained using a leave-one-case-out (LOCO) cross-validation strategy and image marker performance was evaluated using the area under receiver operation characteristic (ROC) curve (AUC). Results: The results indicate that the SVM based imaging marker achieved an AUC value of 0.841 ± 0.035. When setting an operating threshold of 0.5 on model-generated prediction scores, the imaging marker yielded a positive and negative predictive value (PPV and NPV) of 0.762 and 0.765 respectively, while the total accuracy is 76.4%. Conclusions: This study initially verified the feasibility of utilizing CT image and radiomics technology to develop a low-cost image marker to detect LN metastasis for assisting stratification of cervical cancer patients. © 2020",Computer aided detection; Lymph node metastasis; Radiomics Cervical cancer; Treatment management,,,Computer Methods and Programs in Biomedicine,Article,Scopus
471,,Inter-subject pattern analysis for multivariate group analysis of functional neuroimaging. A unifying formalization,"Wang Q., Artières T., Takerkart S.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091588263&doi=10.1016%2fj.cmpb.2020.105730&partnerID=40&md5=32538187a3b660f571f4d189a22c2325,10.1016/j.cmpb.2020.105730,"Background and objective: In medical imaging, population studies have to overcome the differences that exist between individuals to identify invariant image features that can be used for diagnosis purposes. In functional neuroimaging, an appealing solution to identify neural coding principles that hold at the population level is inter-subject pattern analysis, i.e. to learn a predictive model on data from multiple subjects and evaluate its generalization performance on new subjects. Although it has gained popularity in recent years, its widespread adoption is still hampered by the blatant lack of a formal definition in the literature. In this paper, we precisely introduce the first principled formalization of inter-subject pattern analysis targeted at multivariate group analysis of functional neuroimaging. Methods: We propose to frame inter-subject pattern analysis as a multi-source transductive transfer question, thus grounding it within several well defined machine learning settings and broadening the spectrum of usable algorithms. We describe two sets of inter-subject brain decoding experiments that use several open datasets: a magneto-encephalography study with 16 subjects and a functional magnetic resonance imaging paradigm with 100 subjects. We assess the relevance of our framework by performing model comparisons, where one brain decoding model exploits our formalization while others do not. Results: The first set of experiments demonstrates the superiority of a brain decoder that uses subject-by-subject standardization compared to state of the art models that use other standardization schemes, making the case for the interest of the transductive and the multi-source components of our formalization The second set of experiments quantitatively shows that, even after such transformation, it is more difficult for a brain decoder to generalize to new participants rather than to new data from participants available in the training phase, thus highlighting the transfer gap that needs to be overcome. Conclusion: This paper describes the first formalization of inter-subject pattern analysis as a multi-source transductive transfer learning problem. We demonstrate the added value of this formalization using proof-of-concept experiments on several complementary functional neuroimaging datasets. This work should contribute to popularize inter-subject pattern analysis for functional neuroimaging population studies and pave the road for future methodological innovations. © 2020",Functional neuroimaging; Machine learning; Neuroinformatics; Population studies,,,Computer Methods and Programs in Biomedicine,Article,Scopus
472,,"Accurate deep neural network model to detect cardiac arrhythmia on more than 10,000 individual subject ECG records","Yildirim O., Talo M., Ciaccio E.J., Tan R.S., Acharya U.R.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090584970&doi=10.1016%2fj.cmpb.2020.105740&partnerID=40&md5=2aae036d6638ac8a51a66ff445927828,10.1016/j.cmpb.2020.105740,"Background and objective: Cardiac arrhythmia, which is an abnormal heart rhythm, is a common clinical problem in cardiology. Detection of arrhythmia on an extended duration electrocardiogram (ECG) is done based on initial algorithmic software screening, with final visual validation by cardiologists. It is a time consuming and subjective process. Therefore, fully automated computer-assisted detection systems with a high degree of accuracy have an essential role in this task. In this study, we proposed an effective deep neural network (DNN) model to detect different rhythm classes from a new ECG database. Methods: Our DNN model was designed for high performance on all ECG leads. The proposed model, which included both representation learning and sequence learning tasks, showed promising results on all 12-lead inputs. Convolutional layers and sub-sampling layers were used in the representation learning phase. The sequence learning part involved a long short-term memory (LSTM) unit after representation of learning layers. Results: We performed two different class scenarios, including reduced rhythms (seven rhythm types) and merged rhythms (four rhythm types) according to the records from the database. Our trained DNN model achieved 92.24% and 96.13% accuracies for the reduced and merged rhythm classes, respectively. Conclusion: Recently, deep learning algorithms have been found to be useful because of their high performance. The main challenge is the scarcity of appropriate training and testing resources because model performance is dependent on the quality and quantity of case samples. In this study, we used a new public arrhythmia database comprising more than 10,000 records. We constructed an efficient DNN model for automated detection of arrhythmia using these records. © 2020",12-lead ECG; Arrhythmia detection; Deep neural networks; Ecg signals,,,Computer Methods and Programs in Biomedicine,Article,Scopus
473,,Diagnose ADHD disorder in children using convolutional neural network based on continuous mental task EEG,"Moghaddari M., Lighvan M.Z., Danishvar S.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090562418&doi=10.1016%2fj.cmpb.2020.105738&partnerID=40&md5=c1083c9248e2e050e47a127ec3f19889,10.1016/j.cmpb.2020.105738,"Background and objective: Attention-Deficit/Hyperactivity Disorder (ADHD) is a chronic behavioral disorder in children. Children with ADHD face many difficulties in maintaining their concentration and controlling their behaviors. Early diagnosis of this disorder is one of the most important challenges in its control and treatment. No definitive expert method has been found to detect this disorder early. Our goal in this study is to develop an assistive tool for physicians to recognize ADHD children from healthy children using electroencephalography (EEG) based on a continuous mental task. Methods: We used EEG signals recorded from 31 ADHD children and 30 healthy children. In this study, we developed a deep learning model using a convolutional neural network that have had significant performance in image processing fields. For this purpose, we first preprocessed EEG signals to eliminate noise and artifacts. Then we segmented preprocessed samples into more samples. We extracted the theta, alpha, beta, and gamma frequency bands from each segmented sample and formed a color RGB image with three channels. Eventually, we imported the resulting images into a 13-layer convolutional neural network for feature extraction and classification. Results: The proposed model was evaluated by 5-fold cross validation for train, evaluation, and test data and achieved an average accuracy of 99.06%, 97.81%, 97.47% for segmented samples. The average accuracy for subject-based test samples was 98.48%. Also, the performance of the model was evaluated using the confusion matrix with precision, recall, and f1-score metrics. The results of these metrics also confirmed the outstanding performance of the model. Conclusions: The accuracy, precision, recall, and f1-score of our model were better than all previous works for diagnosing ADHD in children. Based on these prominent and reliable results, this technique can be used as an assistive tool for the physicians in the early diagnosis of ADHD in children. © 2020 Elsevier B.V.",ADHD; Convolutional neural network; Deep learning; Electroencephalography,,,Computer Methods and Programs in Biomedicine,Article,Scopus
474,,Automatic diagnosis of multiple cardiac diseases from PCG signals using convolutional neural network,"Baghel N., Dutta M.K., Burget R.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090549125&doi=10.1016%2fj.cmpb.2020.105750&partnerID=40&md5=ce1a2664a1e4ba31dda347b78b698b65,10.1016/j.cmpb.2020.105750,"Background and objectives: Cardiovascular diseases are critical diseases and need to be diagnosed as early as possible. There is a lack of medical professionals in remote areas to diagnose these diseases. Artificial intelligence-based automatic diagnostic tools can help to diagnose cardiac diseases. This work presents an automatic classification method using machine learning to diagnose multiple cardiac diseases from phonocardiogram signals. Methods: The proposed system involves a convolutional neural network (CNN) model because of its high accuracy and robustness to automatically diagnose the cardiac disorders from the heart sounds. To improve the accuracy in a noisy environment and make the method robust, the proposed method has used data augmentation techniques for training and multi-classification of multiple cardiac diseases. Results: The model has been validated both heart sound data and augmented data using n-fold cross-validation. Results of all fold have been shown reported in this work. The model has achieved accuracy on the test set up to 98.60% to diagnose multiple cardiac diseases. Conclusions: The proposed model can be ported to any computing devices like computers, single board computing processors, android handheld devices etc. To make a stand-alone diagnostic tool that may be of help in remote primary health care centres. The proposed method is non-invasive, efficient, robust, and has low time complexity making it suitable for real-time applications. © 2020",Cardiac signals; Data augmentation; Deep neural networks; Multi-label classification; Phonocardiogram,,,Computer Methods and Programs in Biomedicine,Article,Scopus
475,,Improving near real-time precipitation estimation using a U-Net convolutional neural network and geographical information,"Sadeghi M., Nguyen P., Hsu K., Sorooshian S.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090338928&doi=10.1016%2fj.envsoft.2020.104856&partnerID=40&md5=4ceaea35dfefae463ac19c3ab772290d,10.1016/j.envsoft.2020.104856,"Reliable near real-time precipitation estimates are essential for monitoring and managing of natural disasters such as floods. Quality of inputs and capability of the retrieval algorithm are two important aspects for developing satellite-based precipitation datasets. Most retrieval algorithms utilize infrared (IR) information as their input due to its fine spatiotemporal resolution and near-instantaneous availability. However, their sole reliance on IR information limits their capability to learn different mechanisms of precipitation during training, resulting in less accurate estimates. Moreover, recent advances in the field of machine learning offer attractive opportunities to improve the precipitation retrieval algorithms. This study investigates the effectiveness of adding geographical information (i.e. latitude and longitude) to IR information and the application of a U-Net-based convolutional neural network for improving the accuracy of retrieval algorithms. This research suggests that applying an appropriate CNN architecture on geographical and IR information provides an opportunity to improve the satellite-based precipitation products. © 2020 Elsevier Ltd",Convolutional neural networks; Deep learning; Infrared information; Precipitation estimation,,,Environmental Modelling and Software,Article,Scopus
476,,The effects of skin lesion segmentation on the performance of dermatoscopic image classification,"Mahbod A., Tschandl P., Langs G., Ecker R., Ellinger I.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090043899&doi=10.1016%2fj.cmpb.2020.105725&partnerID=40&md5=cdd09df90e44e9e85a902959618aa05f,10.1016/j.cmpb.2020.105725,"Background and Objective: Malignant melanoma (MM) is one of the deadliest types of skin cancer. Analysing dermatoscopic images plays an important role in the early detection of MM and other pigmented skin lesions. Among different computer-based methods, deep learning-based approaches and in particular convolutional neural networks have shown excellent classification and segmentation performances for dermatoscopic skin lesion images. These models can be trained end-to-end without requiring any hand-crafted features. However, the effect of using lesion segmentation information on classification performance has remained an open question. Methods: In this study, we explicitly investigated the impact of using skin lesion segmentation masks on the performance of dermatoscopic image classification. To do this, first, we developed a baseline classifier as the reference model without using any segmentation masks. Then, we used either manually or automatically created segmentation masks in both training and test phases in different scenarios and investigated the classification performances. The different scenarios included approaches that exploited the segmentation masks either for cropping of skin lesion images or removing the surrounding background or using the segmentation masks as an additional input channel for model training. Results: Evaluated on the ISIC 2017 challenge dataset which contained two binary classification tasks (i.e. MM vs. all and seborrheic keratosis (SK) vs. all) and based on the derived area under the receiver operating characteristic curve scores, we observed four main outcomes. Our results show that 1) using segmentation masks did not significantly improve the MM classification performance in any scenario, 2) in one of the scenarios (using segmentation masks for dilated cropping), SK classification performance was significantly improved, 3) removing all background information by the segmentation masks significantly degraded the overall classification performance, and 4) in case of using the appropriate scenario (using segmentation for dilated cropping), there is no significant difference of using manually or automatically created segmentation masks. Conclusions: We systematically explored the effects of using image segmentation on the performance of dermatoscopic skin lesion classification. © 2020 Elsevier B.V.",deep learning; dermatoscopy; effect of segmentation on classification; medical image analysis; Skin cancer,,,Computer Methods and Programs in Biomedicine,Article,Scopus
477,,An ensemble of deep neural networks for kidney ultrasound image classification,"Sudharson S., Kokil P.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090036597&doi=10.1016%2fj.cmpb.2020.105709&partnerID=40&md5=3d4ede16629edfaf7d3e2dee20f9764a,10.1016/j.cmpb.2020.105709,"Background and objective: Chronic kidney disease is a worldwide health issue which includes not only kidney failure but also complications of reduced kidney functionality. Cyst formation, nephrolithiasis or kidney stone, and renal cell carcinoma or kidney tumor are the common kidney disorders which affects the functionality of kidneys. These disorders are typically asymptomatic, therefore early and automatic diagnosis of kidney disorders are required to avoid serious complications. Methods: This paper proposes an automatic classification of B-mode kidney ultrasound images based on the ensemble of deep neural networks (DNNs) using transfer learning. The ultrasound images are usually affected by speckle noise and quality selection in the ultrasound image is based on perception-based image quality evaluator score. Three variant datasets are given to the pre-trained DNN models for feature extraction followed by support vector machine for classification. The ensembling of different pre-trained DNNs like ResNet-101, ShuffleNet, and MobileNet-v2 are combined and final predictions are done by using the majority voting technique. By combining the predictions from multiple DNNs the ensemble model shows better classification performance than the individual models. The presented method proved its superiority when compared to the conventional and DNN based classification methods. The developed ensemble model classifies the kidney ultrasound images into four classes, namely, normal, cyst, stone, and tumor. Results: To highlight effectiveness of the proposed approach, the ensemble based approach is compared with the existing state-of-the-art methods and tested in the variants of ultrasound images like in quality and noisy conditions. The presented method resulted in maximum classification accuracy of 96.54% in testing with quality images and 95.58% in testing with noisy images. The performance of the presented approach is evaluated based on accuracy, sensitivity, and selectivity. Conclusions: From the experimental analysis, it is clear that the ensemble of DNNs classifies the majority of images correctly and results in maximum classification accuracy as compared to the existing methods. This automatic classification approach is a supporting tool for the radiologists and nephrologists for precise diagnosis of kidney diseases. © 2020",Classification; Deep neural networks; Ensemble method; Transfer learning; Ultrasound images,,,Computer Methods and Programs in Biomedicine,Article,Scopus
478,,Shoulder muscle activation pattern recognition based on sEMG and machine learning algorithms,"Jiang Y., Chen C., Zhang X., Chen C., Zhou Y., Ni G., Muh S., Lemos S.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089938422&doi=10.1016%2fj.cmpb.2020.105721&partnerID=40&md5=98474682f56bb3c7ccb9a640121383bf,10.1016/j.cmpb.2020.105721,"Background and Objective: Surface electromyography (sEMG) has been used for robotic rehabilitation engineering for volitional control of hand prostheses or elbow exoskeleton, however, using sEMG for volitional control of an upper limb exoskeleton has not been perfectly developed. The long-term goal of our study is to process shoulder muscle bio-electrical signals for rehabilitative robotic assistive device motion control. The purposes of this study included: 1) to test the feasibility of machine learning algorithms in shoulder motion pattern recognition using sEMG signals from shoulder and upper limb muscles, 2) to investigate the influence of motion speed, individual variability, EMG recording device, and the amount of EMG datasets on the shoulder motion pattern recognition accuracy. Methods: A novel convolutional neural network (CNN) structure was constructed to process EMG signals from 12 muscles for the pattern recognition of upper arm motions including resting, drinking, backward-forward motion, and abduction motion. The accuracy of the CNN models for pattern recognition under different motion speeds, among individuals, and by EMG recording devices was statistically analyzed using ANOVA, GLM Univariate analysis, and Chi-square tests. The influence of EMG dataset number used for CNN model training on recognition accuracy was studied by gradually increasing dataset number until the highest accuracy was obtained. Results: Results showed that the accuracy of the normal speed CNN model in motion pattern recognition was 97.57% for normal speed motions and 97.07% for fast speed motions. The accuracy of the cross-subjects CNN model in motion pattern recognition was 79.64%. The accuracy of the cross-device CNN model in motion pattern recognition was 88.93% for normal speed motion and 80.87% for mixed speed. There was a statistical difference in pattern recognition accuracy between different CNN models. Conclusion: The EMG signals of shoulder and upper arm muscles from the upper limb motions can be processed using CNN algorithms to recognize the identical motions of the upper limb including drinking, forward/backward, abduction, and resting. A simple CNN model trained by EMG datasets of a designated motion speed accurately detected the motion patterns of the same motion speed, yielding the highest accuracy compared with other mixed CNN models for various speeds of motion pattern recognition. Increase of the number of EMG datasets for CNN model training improved the pattern recognition accuracy. © 2020",Convolutional neural network (CNN); Electromyography (EMG); Machine learning; Motion; Pattern recognition; Shoulder,,,Computer Methods and Programs in Biomedicine,Article,Scopus
479,,Generating summaries for methods of event-driven programs: An Android case study,"Aghamohammadi A., Izadi M., Heydarnoori A.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089832552&doi=10.1016%2fj.jss.2020.110800&partnerID=40&md5=4ec764683d60204b4ae45c885249678b,10.1016/j.jss.2020.110800,"The lack of proper documentation makes program comprehension a cumbersome process for developers. Source code summarization is one of the existing solutions to this problem. Many approaches have been proposed to summarize source code in recent years. A prevalent weakness of these solutions is that they do not pay much attention to interactions among elements of software. An element is simply a callable code snippet such as a method or even a clickable button. As a result, these approaches cannot be applied to event-driven programs, such as Android applications, because they have specific features such as numerous interactions between their elements. To tackle this problem, we propose a novel approach based on deep neural networks and dynamic call graphs to generate summaries for methods of event-driven programs. First, we collect a set of comment/code pairs from Github and train a deep neural network on the set. Afterward, by exploiting a dynamic call graph, the Pagerank algorithm, and the pre-trained deep neural network, we generate summaries. An empirical evaluation with 14 real-world Android applications and 42 participants indicates 32.3% BLEU4 which is a definite improvement compared to the existing state-of-the-art techniques. We also assessed the informativeness and naturalness of our generated summaries from developers’ perspectives and showed they are sufficiently understandable and informative. © 2020 Elsevier Inc.",Deep learning; Event-driven programs; Neural machine translation; Source code summarization,,,Journal of Systems and Software,Article,Scopus
480,,Real-time segmentation and tracking of excised corneal contour by deep neural networks for DALK surgical navigation,"Pan J., Liu W., Ge P., Li F., Shi W., Jia L., Qin H.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089429963&doi=10.1016%2fj.cmpb.2020.105679&partnerID=40&md5=f2c287c9de24659002ceac48870a7bf6,10.1016/j.cmpb.2020.105679,"Objective: Corneal disease is one of the main causes of blindness for humans globally nowadays, and deep anterior lamellar keratoplasty (DALK) is a widely applied technique for corneal transplantation. However, the position of stitch points highly influences the success rate of such surgery, which would require accurate control and manipulation of surgical instruments. Methods: In this paper, we present a deep learning framework for augmented reality (AR) based surgery navigation to guide the suturing in DALK. It can robustly track the excised corneal contour by semantic segmentation and the reconstruction of occlusion. We propose a novel optical flow inpainting network to recover the missing motion caused by occlusion. The occluded regions are detected by weakly supervised segmentation of surgical instruments and reconstructed by key frame warping along the completed optical flow. Then we introduce two types of loss function to adapt the inpainting network in the optical flow space. Results: Our techniques are tested and evaluated by a number of real surgery videos from Shandong Eye Hospital in China. We compare our approaches with other typical methods in the corneal contour segmentation, optical flow inpainting and occlusion regions reconstruction. The tracking accuracy reachs 99.2% in average and PSNR reaches 25.52 for the reconstruction of the occluded frames. Conclusion: From the experimental evaluations and user study, both the qualitative and quantitative results indicate that our techniques can achieve accurate detection and tracking of corneal contour under complex disturbance in real-time surgical scenes. Our prototype AR navigation system would be highly useful in clinical practice. © 2020 Elsevier B.V.",AR-based surgical navigation; Contour tracking; DALK; Optical flow inpainting; Semantic segmentation,,,Computer Methods and Programs in Biomedicine,Article,Scopus
481,,Neural joint attention code search over structure embeddings for software Q&A sites,"Hu G., Peng M., Zhang Y., Xie Q., Yuan M.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089281710&doi=10.1016%2fj.jss.2020.110773&partnerID=40&md5=c739108203c697be3f5dd9b61402cac9,10.1016/j.jss.2020.110773,"Code search is frequently needed in software Q&A sites for software development. Over the years, various code search engines and techniques have been explored to support user query. Early approaches often utilize text retrieval models to match textual code fragments for natural query, but fail to build sufficient semantic correlations. Some recent advanced neural methods focus on restructuring bi-modal networks to measure the semantic similarity. However, they ignore potential structure information of source codes and the joint attention information from natural queries. In addition, they mostly focus on specific code structures, rather than general code fragments in software Q&A sites. In this paper, we propose NJACS, a novel two-way attention-based neural network for retrieving code fragments in software Q&A sites, which aligns and focuses the more structure informative parts of source codes to natural query. Instead of directly learning bi-modal unified vector representations, NJACS first embeds the queries and codes using a bidirectional LSTM with pre-trained structure embeddings separately, then learns an aligned joint attention matrix for query-code mappings, and finally derives the pooling-based projection vectors in different directions to guide the attention-based representations. On different benchmark search codebase collected from StackOverflow, NJACS outperforms state-of-art baselines with 7.5% to 6% higher Recall@1 and MRR, respectively. Moreover, our designed structure embeddings can be leveraged for other deep-learning-based software tasks. © 2020",Code search; Joint attention; Software Q&A sites; Structure embeddings,,,Journal of Systems and Software,Article,Scopus
482,,A Stacked Generalization U-shape network based on zoom strategy and its application in biomedical image segmentation,"Shi T., Jiang H., Zheng B.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089216996&doi=10.1016%2fj.cmpb.2020.105678&partnerID=40&md5=d700feb87b3fed4b71899a0ec71ac20a,10.1016/j.cmpb.2020.105678,"Background and objective: The deep neural network model can learn complex non-linear relationships in the data and has superior flexibility and adaptability. A downside of this flexibility is that they are sensitive to initial conditions, both in terms of the initial random weights and in terms of the statistical noise in the training dataset. And the disadvantage caused by adaptability is that deep convolutional networks usually have poor robustness or generalization when the models are trained using the extremely limited amount of labeled data, especially in the biomedical imaging informatics field. Methods: In this paper, we propose to develop and test a stacked generalization U-shape network (SG-UNet) based on the zoom strategy applying to biomedical image segmentation. SG-UNet is essentially a stacked generalization architecture consisting of multiple sub-modules, which takes multi-resolution images as input and uses hybrid features to segment regions of interest and detect diseases under the multi-supervision. The proposed new SG-UNet applies the zoom of multi-supervision to do optimization search in global feature space without pre-training. Besides, the zoom loss function can gradually enhance the focus training on a sparse set of hard samples. Results: We evaluated the proposed algorithm in comparison with several popular U-shape ensemble network architectures across multi-modal biomedical image segmentation tasks to segment malignant rectal cancers, polyps and glands from the three imaging modalities of computed tomography (CT), digital colonoscopy and histopathology images. Applying the proposed algorithm improves 3.116%, 2.676%, 2.356% on Dice coefficients, and 3.044%, 2.420%, 1.928% on F2-score for the three imaging modality datasets, respectively. The comparison results using different amounts of rectal cancer CT data show that the proposed algorithm has a slower tendency of diminishing marginal efficiency. And glands segmentation study results also support the feasibility of yielding comparable performance with other state-of-the-art methods. Conclusions: The proposed algorithm can be trained more efficiently by using the small image datasets without using additional techniques such as fine-tuning, and achieves higher accuracy with less computational complexity than other stacked ensemble networks for biomedical image segmentation. © 2020 Elsevier B.V.",Biomedical image segmentation; Convolutional neural network; Deep supervision; Stacked generalization; Zoom loss function,,,Computer Methods and Programs in Biomedicine,Article,Scopus
483,,Deep learning for risk prediction in patients with nasopharyngeal carcinoma using multi-parametric MRIs,"Jing B., Deng Y., Zhang T., Hou D., Li B., Qiang M., Liu K., Ke L., Li T., Sun Y., Lv X., Li C.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089138271&doi=10.1016%2fj.cmpb.2020.105684&partnerID=40&md5=5521f1ace9db56324a838ca7a9cc70ad,10.1016/j.cmpb.2020.105684,"Background: Magnetic resonance images (MRI) is the main diagnostic tool for risk stratification and treatment decision in nasopharyngeal carcinoma (NPC). However, the holistic feature information of multi-parametric MRIs has not been fully exploited by clinicians to accurately evaluate patients. Objective: To help clinicians fully utilize the missed information to regroup patients, we built an end-to-end deep learning model to extract feature information from multi-parametric MRIs for predicting and stratifying the risk scores of NPC patients. Methods: In this paper, we proposed an end-to-end multi-modality deep survival network (MDSN) to precisely predict the risk of disease progression of NPC patients. Extending from 3D dense net, this proposed MDSN extracted deep representation from multi-parametric MRIs (T1w, T2w, and T1c). Moreover, deep features and clinical stages were integrated through MDSN to more accurately predict the overall risk score (ORS) of individual NPC patient. Result: A total of 1,417 individuals treated between January 2012 and December 2014 were included for training and validating the end-to-end MDSN. Results were then tested in a retrospective cohort of 429 patients included in the same institution. The C-index of the proposed method with or without clinical stages was 0.672 and 0.651 on the test set, respectively, which was higher than the that of the stage grouping (0.610). Conclusions: The C-index of the model which integrated clinical stages with deep features is 0.062 higher than that of stage grouping alone (0.672 vs 0.610). We conclude that features extracted from multi-parametric MRIs based on MDSN can well assist the clinical stages in regrouping patients. © 2020 Elsevier B.V.",Deep learning; Magnetic resonance images; Nasopharyngeal carcinoma; Risk prediction; Survival analysis,,,Computer Methods and Programs in Biomedicine,Article,Scopus
484,,Detection of peripherally inserted central catheter (PICC) in chest X-ray images: A multi-task deep learning model,"Yu D., Zhang K., Huang L., Zhao B., Zhang X., Guo X., Li M., Gu Z., Fu G., Hu M., Ping Y., Sheng Y., Liu Z., Hu X., Zhao R.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089104969&doi=10.1016%2fj.cmpb.2020.105674&partnerID=40&md5=aca3565ffaa0804016b3760353b6344b,10.1016/j.cmpb.2020.105674,"Background and Objective: Peripherally inserted central catheter (PICC) is a novel drug delivery mode which has been widely used in clinical practice. However, long-term retention and some improper actions of patients may cause some severe complications of PICC, such as the drift and prolapse of its catheter. Clinically, the postoperative care of PICC is mainly completed by nurses. However, they cannot recognize the correct position of PICC from X-ray chest images as soon as the complications happen, which may lead to improper treatment. Therefore, it is necessary to identify the position of the PICC catheter as soon as these complications occur. Here we proposed a novel multi-task deep learning framework to detect PICC automatically through X-ray images, which could help nurses to solve this problem. Methods: We collected 348 X-ray chest images from 326 patients with visible PICC. Then we proposed a multi-task deep learning framework for line segmentation and tip detection of PICC catheters simultaneously. The proposed deep learning model is composed of an extraction structure and three routes, an up-sampling route for segmentation, an RPNs route, and an RoI Pooling route for detection. We further compared the effectiveness of our model with the models previously proposed. Results: In the catheter segmentation task, 300 X-ray images were utilized for training the model, then 48 images were tested. In the tip detection task, 154 X-ray images were used for retraining and 20 images were used in the test. Our model achieved generally better results among several popular deep learning models previously proposed. Conclusions: We proposed a multi-task deep learning model that could segment the catheter and detect the tip of PICC simultaneously from X-ray chest images. This model could help nurses to recognize the correct position of PICC, and therefore, to handle the potential complications properly. © 2020 Elsevier B.V.",Chest x-ray images; Deep learning; Multi-task learning; Picc; Segmentation; Tip detection,,,Computer Methods and Programs in Biomedicine,Article,Scopus
485,,ConfigCrusher: towards white-box performance analysis for configurable systems,"Velez M., Jamshidi P., Sattler F., Siegmund N., Apel S., Kästner C.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089025519&doi=10.1007%2fs10515-020-00273-8&partnerID=40&md5=960f36a60b2a69008efc50a936c88026,10.1007/s10515-020-00273-8,"Stakeholders of configurable systems are often interested in knowing how configuration options influence the performance of a system to facilitate, for example, the debugging and optimization processes of these systems. Several black-box approaches can be used to obtain this information, but they either sample a large number of configurations to make accurate predictions or miss important performance-influencing interactions when sampling few configurations. Furthermore, black-box approaches cannot pinpoint the parts of a system that are responsible for performance differences among configurations. This article proposes ConfigCrusher, a white-box performance analysis that inspects the implementation of a system to guide the performance analysis, exploiting several insights of configurable systems in the process. ConfigCrusher employs a static data-flow analysis to identify how configuration options may influence control-flow statements and instruments code regions, corresponding to these statements, to dynamically analyze the influence of configuration options on the regions’ performance. Our evaluation on 10 configurable systems shows the feasibility of our white-box approach to more efficiently build performance-influence models that are similar to or more accurate than current state of the art approaches. Overall, we showcase the benefits of white-box performance analyses and their potential to outperform black-box approaches and provide additional information for analyzing configurable systems. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.",Configurable systems; Dynamic analysis; Performance analysis; Static analysis,"265, 300",,Automated Software Engineering,Article,Scopus
486,,The effect of Bellwether analysis on software vulnerability severity prediction models,"Kudjo P.K., Chen J., Mensah S., Amankwah R., Kudjo C.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077588152&doi=10.1007%2fs11219-019-09490-1&partnerID=40&md5=1e30317eeed3cf3b60d90f3841984222,10.1007/s11219-019-09490-1,"Vulnerability severity prediction (VSP) models provide useful insight for vulnerability prioritization and software maintenance. Previous studies have proposed a variety of machine learning algorithms as an important paradigm for VSP. However, to the best of our knowledge, there are no other existing research studies focusing on investigating how a subset of features can be used to improve VSP. To address this deficiency, this paper presents a general framework for VSP using the Bellwether analysis (i.e., exemplary data). First, we apply the natural language processing techniques to the textual descriptions of software vulnerability. Next, we developed an algorithm termed Bellvul to identify and select an exemplary subset of data (referred to as Bellwether) to be considered as the training set to yield improved prediction accuracy against the growing portfolio, within-project cases, and the k-fold cross-validation subset. Finally, we assessed the performance of four machine learning algorithms, namely, deep neural network, logistic regression, k-nearest neighbor, and random forest using the sampled instances. The prediction results of the suggested models and the benchmark techniques were assessed based on the standard classification evaluation metrics such as precision, recall, and F-measure. The experimental result shows that the Bellwether approach achieves F-measure ranging from 14.3% to 97.8%, which is an improvement over the benchmark techniques. In conclusion, the proposed approach is a promising research direction for assisting software engineers when seeking to predict instances of vulnerability records that demand much attention prior to software release. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.",Bellwether; Feature selection; Machine learning algorithms; Severity; Software vulnerability,"1413, 1446",,Software Quality Journal,Article,Scopus
487,,Region of Interest Synthesis using Image-to-Image Translation for ear recognition,"Khaldi Y., Benzaoui A.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103825959&doi=10.1109%2fICAASE51408.2020.9380127&partnerID=40&md5=01b0661da16bb9a2219bfbdee094ab15,10.1109/ICAASE51408.2020.9380127,"Most ear recognition techniques use cropped ear images, as they are, with backgrounds, hair, part of the face or neck skin, and even cloths. These non-ear pixels of the image can negatively affect the classification decision. To avoid that, and to make sure that the classifier depends on ear pixels only, we propose using a tight Region-of-Interest (RoI) segmentation of the ear instead. This paper uses Image-to-Image translation to synthesize ear RoI segmentation and remove irrelevant pixels from input images. Furthermore, missing parts of the ear due to occlusion or distortion can also be synthesized. To accomplish that, we used Pix2Pix Generative Adversarial Network (GAN) trained on the AWE dataset, which is a challenging ear dataset. Experimental results show that using ear RoI segmentation positively affects the classification process, and significantly increases the recognition rate. © 2020 IEEE.",AWE dataset; Biometrics; Ear recognition; Ear segmentation; Pix2Pix GAN,,,"ICAASE 2020 - Proceedings, 4th International Conference on Advanced Aspects of Software Engineering",Conference Paper,Scopus
488,,An exploratory study on applicability of cross project defect prediction approaches to cross-company effort estimation,"Amasaki S., Aman H., Yokogawa T.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097277209&doi=10.1145%2f3416508.3417118&partnerID=40&md5=b627c775383f32a2b1be8876f89dbfe7,10.1145/3416508.3417118,"BACKGROUND: Research on software effort estimation has been active for decades, especially in developing effort estimation models. Effort estimation models need a dataset collected from completed projects similar to a project to be estimated. The similarity suffers from dataset shift, and cross-company software effort estimation (CCSEE) gets an attractive research topic. A recent study on the dataset shift problem examined the applicability and the effectiveness of cross-project defect prediction (CPDP) approaches. It was insufficient to bring a conclusion due to a limited number of examined approaches. AIMS: To investigate the characteristics of CPDP approaches that are applicable and effective for dataset shift problem in effort estimation. METHOD: We first reviewed the characteristics of 24 CPDP approaches to find applicable approaches. Next, we investigated their effectiveness in effort estimation performance with ten dataset configurations. RESULTS: 16 out of 24 CPDP approaches implemented in CrossPare framework were found to be applicable to CCSEE. However, only one approach could improve the effort estimation performance. Most of the others degraded it and were harmful. CONCLUSIONS: Most of the CPDP approaches we examined were helpless for CCSEE. © 2020 ACM.",cross-company effort estimation; cross-project defect prediction; empirical evaluation,"71, 80",,"PROMISE 2020 - Proceedings of the 16th ACM International Conference on Predictive Models and Data Analytics in Software Engineering, Co-located with ESEC/FSE 2020",Conference Paper,Scopus
489,,MTFuzz: Fuzzing with a multi-task neural network,"She D., Krishna R., Yan L., Jana S., Ray B.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097203127&doi=10.1145%2f3368089.3409723&partnerID=40&md5=6108547037fa8fa249af49d25c86237e,10.1145/3368089.3409723,"Fuzzing is a widely used technique for detecting software bugs and vulnerabilities. Most popular fuzzers generate new inputs using an evolutionary search to maximize code coverage. Essentially, these fuzzers start with a set of seed inputs, mutate them to generate new inputs, and identify the promising inputs using an evolutionary fitness function for further mutation.Despite their success, evolutionary fuzzers tend to get stuck in long sequences of unproductive mutations. In recent years, machine learning (ML) based mutation strategies have reported promising results. However, the existing ML-based fuzzers are limited by the lack of quality and diversity of the training data. As the input space of the target programs is high dimensional and sparse, it is prohibitively expensive to collect many diverse samples demonstrating successful and unsuccessful mutations to train the model.In this paper, we address these issues by using a Multi-Task Neural Network that can learn a compact embedding of the input space based on diverse training samples for multiple related tasks (i.e.,predicting for different types of coverage). The compact embedding can guide the mutation process by focusing most of the mutations on the parts of the embedding where the gradient is high. MTFuzz uncovers 11 previously unseen bugs and achieves an average of 2× more edge coverage compared with 5 state-of-the-art fuzzer on 10 real-world programs © 2020 ACM.",Fuzzing; Machine learning; Multi-task learning,"737, 749",,ESEC/FSE 2020 - Proceedings of the 28th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,Conference Paper,Scopus
490,,API method recommendation via explicit matching of functionality verb phrases,"Xie W., Peng X., Liu M., Treude C., Xing Z., Zhang X., Zhao W.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097197430&doi=10.1145%2f3368089.3409731&partnerID=40&md5=8b566c1bec92438d60c10aa270619547,10.1145/3368089.3409731,"Due to the lexical gap between functionality descriptions and user queries, documentation-based API retrieval often produces poor results.Verb phrases and their phrase patterns are essential in both describing API functionalities and interpreting user queries. Thus we hypothesize that API retrieval can be facilitated by explicitly recognizing and matching between the fine-grained structures of functionality descriptions and user queries. To verify this hypothesis, we conducted a large-scale empirical study on the functionality descriptions of 14,733 JDK and Android API methods. We identified 356 different functionality verbs from the descriptions, which were grouped into 87 functionality categories, and we extracted 523 phrase patterns from the verb phrases of the descriptions. Building on these findings, we propose an API method recommendation approach based on explicit matching of functionality verb phrases in functionality descriptions and user queries, called PreMA. Our evaluation shows that PreMA can accurately recognize the functionality categories (92.8%) and phrase patterns (90.4%) of functionality description sentences; and when used for API retrieval tasks, PreMA can help participants complete their tasks more accurately and with fewer retries compared to a baseline approach. © 2020 ACM.",API Documentation; API Retrieval; Functionality Description,"1015, 1026",,ESEC/FSE 2020 - Proceedings of the 28th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,Conference Paper,Scopus
491,,Questions for data scientists in software engineering: A replication,"Huijgens H., Rastogi A., Mulders E., Gousios G., Deursen A.V.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097188393&doi=10.1145%2f3368089.3409717&partnerID=40&md5=9ac04d3b658edef8016e698145eb4960,10.1145/3368089.3409717,"In 2014, a Microsoft study investigated the sort of questions that data science applied to software engineering should answer. This resulted in 145 questions that developers considered relevant for data scientists to answer, thus providing a research agenda to the community. Fast forward to five years, no further studies investigated whether the questions from the software engineers at Microsoft hold for other software companies, including software-intensive companies with different primary focus (to which we refer as software-defined enterprises). Furthermore, it is not evident that the problems identified five years ago are still applicable, given the technological advances in software engineering. This paper presents a study at ING, a software-defined enterprise in banking in which over 15,000 IT staff provides in-house software solutions. This paper presents a comprehensive guide of questions for data scientists selected from the previous study at Microsoft along with our current work at ING. We replicated the original Microsoft study at ING, looking for questions that impact both software companies and software-defined enterprises and continue to impact software engineering. We also add new questions that emerged from differences in the context of the two companies and the five years gap in between. Our results show that software engineering questions for data scientists in the software-defined enterprise are largely similar to the software company, albeit with exceptions. We hope that the software engineering research community builds on the new list of questions to create a useful body of knowledge. © 2020 ACM.",Data Science; Software Analytics; Software Engineering,"568, 579",,ESEC/FSE 2020 - Proceedings of the 28th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,Conference Paper,Scopus
492,,Identifying linked incidents in large-scale online service systems,"Chen Y., Yang X., Dong H., He X., Zhang H., Lin Q., Chen J., Zhao P., Kang Y., Gao F., Xu Z., Zhang D.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097187876&doi=10.1145%2f3368089.3409768&partnerID=40&md5=ae8a9d50c9196e8cb98b82e9c2c2d690,10.1145/3368089.3409768,"In large-scale online service systems, incidents occur frequently due to a variety of causes, from updates of software and hardware to changes in operation environment. These incidents could significantly degrade system's availability and customers' satisfaction. Some incidents are linked because they are duplicate or inter-related. The linked incidents can greatly help on-call engineers find mitigation solutions and identify the root causes. In this work, we investigate the incidents and their links in a representative real-world incident management (IcM) system. Based on the identified indicators of linked incidents, we further propose LiDAR (Linked Incident identification with DAta-driven Representation), a deep learning based approach to incident linking. More specifically, we incorporate the textual description of incidents and structural information extracted from historical linked incidents to identify possible links among a large number of incidents. To show the effectiveness of our method, we apply our method to a real-world IcM system and find that our method outperforms other state-of-the-art methods. © 2020 ACM.",Incident management; Link prediction; Linked incidents; Online service system,"304, 314",,ESEC/FSE 2020 - Proceedings of the 28th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,Conference Paper,Scopus
493,,Enhancing the interoperability between deep learning frameworks by model conversion,"Liu Y., Chen C., Zhang R., Qin T., Ji X., Lin H., Yang M.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097175237&doi=10.1145%2f3368089.3417051&partnerID=40&md5=84ef89bfd42e262e4d1f8d5df7ba6bc7,10.1145/3368089.3417051,"Deep learning (DL) has become one of the most successful machine learning techniques. To achieve the optimal development result, there are emerging requirements on the interoperability between DL frameworks that the trained model files and training/serving programs can be re-utilized. Faithful model conversion is a promising technology to enhance the framework interoperability in which a source model is transformed into the semantic equivalent in another target framework format. However, several major challenges need to be addressed. First, there are apparent discrepancies between DL frameworks. Second, understanding the semantics of a source model could be difficult due to the framework scheme and optimization. Lastly, there exist a large number of DL frameworks, bringing potential significant engineering efforts. In this paper, we propose MMdnn, an open-sourced, comprehensive, and faithful model conversion tool for popular DL frameworks. MMdnn adopts a novel unified intermediate representation (IR)-based methodology to systematically handle the conversion challenges. The source model is first transformed into an intermediate computation graph represented by the simple graph-based IR of MMdnn and then to the target framework format, which greatly reduces the engineering complexity. Since the model structure expressed by developers may have been changed by DL frameworks (e.g., graph optimization), MMdnn tries to recover the original high-level neural network layers for better semantic comprehension via a pattern matching similar method. In the meantime, a piece of model construction code is generated to facilitate later retraining or serving. MMdnn implements an extensible conversion architecture from the compilation point of view, which eases contribution from the community to support new DL operators and frameworks. MMdnn has reached good maturity and quality, and is applied for converting production models. © 2020 ACM.",Deep learning; Model conversion; Neural network,"1320, 1330",,ESEC/FSE 2020 - Proceedings of the 28th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,Conference Paper,Scopus
494,,Clustering test steps in natural language toward automating test automation,"Li L., Li Z., Zhang W., Zhou J., Wang P., Wu J., He G., Zeng X., Deng Y., Xie T.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097171130&doi=10.1145%2f3368089.3417067&partnerID=40&md5=49349702584906e3e367a30c4c632c0b,10.1145/3368089.3417067,"For large industrial applications, system test cases are still often described in natural language (NL), and their number can reach thousands. Test automation is to automatically execute the test cases. Achieving test automation typically requires substantial manual effort for creating executable test scripts from these NL test cases. In particular, given that each NL test case consists of a sequence of NL test steps, testers first implement a test API method for each test step and then write a test script for invoking these test API methods sequentially for test automation. Across different test cases, multiple test steps can share semantic similarities, supposedly mapped to the same API method. However, due to numerous test steps in various NL forms under manual inspection, testers may not realize those semantically similar test steps and thus waste effort to implement duplicate test API methods for them. To address this issue, in this paper, we propose a new approach based on natural language processing to cluster similar NL test steps together such that the test steps in each cluster can be mapped to the same test API method. Our approach includes domain-specific word embedding training along with measurement based on Relaxed Word Mover'sDistance to analyze the similarity of test steps. Our approach also includes a technique to combine hierarchical agglomerative clustering and K-means clustering post-refinement to derive high-quality and manually-adjustable clustering results. The evaluation results of our approach on a large industrial mobile app, WeChat, show that our approach can cluster the test steps with high accuracy, substantially reducing the number of clusters and thus reducing the downstream manual effort. In particular, compared with the baseline approach, our approach achieves 79.8% improvement on cluster quality, reducing 65.9% number of clusters, i.e., the number of test API methods to be implemented. © 2020 ACM.",Clustering; Natural language processing; Software testing,"1285, 1295",,ESEC/FSE 2020 - Proceedings of the 28th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,Conference Paper,Scopus
495,,Dimensions of software configuration: On the configuration context in modern software development,"Siegmund N., Ruckel N., Siegmund J.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097170862&doi=10.1145%2f3368089.3409675&partnerID=40&md5=2a3e0045378cde6771cc0d57fba9d7bf,10.1145/3368089.3409675,"With the rise of containerization, cloud development, and continuous integration and delivery, configuration has become an essential aspect not only to tailor software to user requirements, but also to configure a software system's environment and infrastructure. This heterogeneity of activities, domains, and processes blurs the term configuration, as it is not clear anymore what tasks, artifacts, or stakeholders are involved and intertwined. However, each re- search study and each paper involving configuration places their contributions and findings in a certain context without making the context explicit. This makes it difficult to compare findings, translate them to practice, and to generalize the results. Thus, we set out to evaluate whether these different views on configuration are really distinct or can be summarized under a common umbrella. By interviewing practitioners from different domains and in different roles about the aspects of configuration and by analyzing two qualitative studies in similar areas, we derive a model of configuration that provides terminology and context for research studies, identifies new research opportunities, and allows practitioners to spot possible challenges in their current tasks. Although our interviewees have a clear view about configuration, it substantially differs due to their personal experience and role. This indicates that the term configuration might be overloaded. However, when taking a closer look, we see the interconnections and dependencies among all views, arriving at the conclusion that we need to start considering the entire spectrum of dimensions of configuration. © 2020 ACM.",Configuration management and life cycle; Developer study; Dimensions of software configuration; Variability,"338, 349",,ESEC/FSE 2020 - Proceedings of the 28th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,Conference Paper,Scopus
496,,Estimating GPU memory consumption of deep learning models,"Gao Y., Liu Y., Zhang H., Li Z., Zhu Y., Lin H., Yang M.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097161758&doi=10.1145%2f3368089.3417050&partnerID=40&md5=2d3343deedfce6420e4ba80b5d6a15fb,10.1145/3368089.3417050,"Deep learning (DL) has been increasingly adopted by a variety of software-intensive systems. Developers mainly use GPUs to accelerate the training, testing, and deployment of DL models. However, the GPU memory consumed by a DL model is often unknown to them before the DL job executes. Therefore, an improper choice of neural architecture or hyperparameters can cause such a job to run out of the limited GPU memory and fail. Our recent empirical study has found that many DL job failures are due to the exhaustion of GPU memory. This leads to a horrendous waste of computing resources and a significant reduction in development productivity. In this paper, we propose DNNMem, an accurate estimation tool for GPU memory consumption of DL models. DNNMem employs an analytic estimation approach to systematically calculate the memory consumption of both the computation graph and the DL framework runtime. We have evaluated DNNMem on 5 real-world representative models with different hyperparameters under 3 mainstream frameworks (TensorFlow, PyTorch, and MXNet). Our extensive experiments show that DNNMem is effective in estimating GPU memory consumption. © 2020 ACM.",Deep learning; Estimation model; Memory consumption; Program analysis,"1342, 1352",,ESEC/FSE 2020 - Proceedings of the 28th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,Conference Paper,Scopus
497,,Automatically identifying performance issue reports with heuristic linguistic patterns,"Zhao Y., Xiao L., Babvey P., Sun L., Wong S., Martinez A.A., Wang X.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097161071&doi=10.1145%2f3368089.3409674&partnerID=40&md5=cbba6fbb6beda256ee604d94c5b1817f,10.1145/3368089.3409674,"Performance issues compromise the response time and resource consumption of a software system. Modern software systems use issue tracking systems to manage all kinds of issue reports, including performance issues. The problem is that performance issues are often not explicitly tagged. The tagging mechanism, if exists, is completely voluntary, depending on the project's convention and on submitters' discipline. For example, the performance tag rate in Apache's Jira system is below 1%. This paper contributes a hybrid classification approach that combines linguistic patterns and machine/deep learning techniques to automatically detect performance issue reports. We manually analyzed 980 real-life performance issue reports and derived 80 project-agnostic linguistic patterns that recur in the reports. Our approach uses these linguistic patterns to construct the sentence-level and issue-level learning features for training effective machine/deep learning classifiers. We test our approach on two separate datasets, each consisting of 980 unclassified issue reports, and compare the results with 31 baseline methods. Our approach can reach up to 83% precision and up to 59% recall. The only comparable baseline method is BERT, which is still 25% lower in the F1-score. © 2020 ACM.",Performance optimization; Software performance; Software repositories mining,"964, 975",,ESEC/FSE 2020 - Proceedings of the 28th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,Conference Paper,Scopus
498,,DiffTech: A tool for differencing similar technologies from question-and-answer discussions,"Wang H., Chen C., Xing Z., Grundy J.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097159976&doi=10.1145%2f3368089.3417931&partnerID=40&md5=ddaea85059fd0efa88156de5fcf2b006,10.1145/3368089.3417931,"Developers can use different technologies for different software development tasks in their work. However, when faced with several technologies with comparable functionalities, it can be challenging for developers to select the most appropriate one, as trial and error comparisons among such technologies are time-consuming. Instead, developers resort to expert articles, read official documents or ask questions in Q&A sites for technology comparison. However, it is still very opportunistic whether they will get a comprehensive comparison, as online information is often fragmented, contradictory and biased. To overcome these limitations, we propose the DiffTech system that exploits the crowd sourced discussions from Stack Overflow, and assists technology comparison with an informative summary of different comparison aspects. We found 19,118 comparative sentences from 2,410 pairs of comparable technologies. We released our DiffTech website for public use. Our website attracts over 1800 users and we also receive some positive comments on social media. A walkthrough video of the tool demo: https://www.youtube.com/watch?v=ixX41DXRNsI Website link: https://difftech.herokuapp.com/ © 2020 ACM.",Differencing similar technology; NLP; Stack Overflow,"1576, 1580",,ESEC/FSE 2020 - Proceedings of the 28th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,Conference Paper,Scopus
499,,Efficient customer incident triage via linking with system incidents,"Gu J., Wen J., Wang Z., Zhao P., Luo C., Kang Y., Zhou Y., Yang L., Sun J., Xu Z., Qiao B., Li L., Lin Q., Zhang D.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097159592&doi=10.1145%2f3368089.3417061&partnerID=40&md5=a00316a15073454af92310e74e3cc7ce,10.1145/3368089.3417061,"In cloud service systems, customers will report the service issues they have encountered to cloud service providers. Despite many issues can be handled by the support team, sometimes the customer issues can not be easily solved, thus raising customer incidents. Quick troubleshooting of a customer incident is critical. To this end, a customer incident should be assigned to its responsible team accurately in a timely manner. Our industrial experiences show that linking customer incidents with detected system incidents can help the customer incident triage. In particular, our empirical study on 7 real cloud service systems shows that with the additional information about the system incidents (i.e., incident reports generated by system monitors), the triage time of customer incidents can be accelerated 13.1× on average. Based on this observation, in this paper, we propose LinkCM, a learning based approach to automatically link customer incidents to monitor reported system incidents. LinkCM incorporates a novel learning-based model that effectively extracts related information from two resources, and a transfer learning strategy is proposed to help LinkCM achieve better performance without huge amount of data. The experimental results indicate that LinkCM is able to achieve accurate link prediction. Furthermore, case studies are presented to demonstrate how LinkCM can help the customer incident triage procedure in real production cloud service systems. © 2020 ACM.",Cloud Service Systems; Customer Issue Triage; Transfer Learning,"1296, 1307",,ESEC/FSE 2020 - Proceedings of the 28th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,Conference Paper,Scopus
500,,On decomposing a deep neural network into modules,"Pan R., Rajan H.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097153545&doi=10.1145%2f3368089.3409668&partnerID=40&md5=8488e929c42daf7e8fb99d75b8b51c0a,10.1145/3368089.3409668,"Deep learning is being incorporated in many modern software systems. Deep learning approaches train a deep neural network (DNN) model using training examples, and then use the DNN model for prediction. While the structure of a DNN model as layers is observable, the model is treated in its entirety as a monolithic component. To change the logic implemented by the model, e.g. to add/remove logic that recognizes inputs belonging to a certain class, or to replace the logic with an alternative, the training examples need to be changed and the DNN needs to be retrained using the new set of examples. We argue that decomposing a DNN into DNN modules - akin to decomposing a monolithic software code into modules - can bring the benefits of modularity to deep learning. In this work, we develop a methodology for decomposing DNNs for multi-class problems into DNN modules. For four canonical problems, namely MNIST, EMNIST, FMNIST, and KMNIST, we demonstrate that such decomposition enables reuse of DNN modules to create different DNNs, enables replacement of one DNN module in a DNN with another without needing to retrain. The DNN models formed by composing DNN modules are at least as good as traditional monolithic DNNs in terms of test accuracy for our problems. © 2020 ACM.",Decomposing; Deep neural networks; Modularity; Modules,"889, 900",,ESEC/FSE 2020 - Proceedings of the 28th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,Conference Paper,Scopus
501,,IntelliCode compose: Code generation using transformer,"Svyatkovskiy A., Deng S.K., Fu S., Sundaresan N.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097147673&doi=10.1145%2f3368089.3417058&partnerID=40&md5=b2b88b9e7b2f7153b2fa5a87a30f58d2,10.1145/3368089.3417058,"In software development through integrated development environments (IDEs), code completion is one of the most widely used features. Nevertheless, majority of integrated development environments only support completion of methods and APIs, or arguments. In this paper, we introduce IntelliCode Compose - a general-purpose multilingual code completion tool which is capable of predicting sequences of code tokens of arbitrary types, generating up to entire lines of syntactically correct code. It leverages state-of-the-art generative transformer model trained on 1.2 billion lines of source code in Python, C#, JavaScript and TypeScript programming languages. IntelliCode Compose is deployed as a cloud-based web service. It makes use of client-side tree-based caching, efficient parallel implementation of the beam search decoder, and compute graph optimizations to meet edit-time completion suggestion requirements in the Visual Studio Code IDE and Azure Notebook. Our best model yields an average edit similarity of 86.7% and a perplexity of 1.82 for Python programming language. © 2020 ACM.",Code completion; Naturalness of software; Neural networks,"1433, 1443",,ESEC/FSE 2020 - Proceedings of the 28th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,Conference Paper,Scopus
502,,Operational calibration: Debugging confidence errors for DNNs in the field,"Li Z., Ma X., Xu C., Xu J., Cao C., Lü J.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097145327&doi=10.1145%2f3368089.3409696&partnerID=40&md5=1546c59d84a64d6b14debbe33f8a30ef,10.1145/3368089.3409696,"Trained DNN models are increasingly adopted as integral parts of software systems, but they often perform deficiently in the field. A particularly damaging problem is that DNN models often give false predictions with high confidence, due to the unavoidable slight divergences between operation data and training data. To minimize the loss caused by inaccurate confidence, operational calibration, i.e., calibrating the confidence function of a DNN classifier against its operation domain, becomes a necessary debugging step in the engineering of the whole system. Operational calibration is difficult considering the limited budget of labeling operation data and the weak interpretability of DNN models. We propose a Bayesian approach to operational calibration that gradually corrects the confidence given by the model under calibration with a small number of labeled operation data deliberately selected from a larger set of unlabeled operation data. The approach is made effective and efficient by leveraging the locality of the learned representation of the DNN model and modeling the calibration as Gaussian Process Regression. Comprehensive experiments with various practical datasets and DNN models show that it significantly outperformed alternative methods, and in some difficult tasks it eliminated about 71% to 97% high-confidence (>0.9) errors with only about 10% of the minimal amount of labeled operation data needed for practical learning techniques to barely work © 2020 Owner/Author.",Deep Neural Networks; Gaussian Process; Operational Calibration,"901, 913",,ESEC/FSE 2020 - Proceedings of the 28th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,Conference Paper,Scopus
503,,Adapting bug prediction models to predict reverted commits at Wayfair,Suh A.,2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097145304&doi=10.1145%2f3368089.3417062&partnerID=40&md5=7de560de3b4367b19540884d667cf939,10.1145/3368089.3417062,"Researchers have proposed many algorithms to predict software bugs. Given a software entity (e.g., a file or method), these algorithms predict whether the entity is bug-prone. However, since these algorithms cannot identify specific bugs, this does not tend to be particularly useful in practice. In this work, we adapt this prior work to the related problem of predicting whether a commit is likely to be reverted. Given the batch nature of continuous integration deployment at scale, this allows developers to find time-sensitive bugs in production more quickly. The models in this paper are based on features extracted from the revision history of a codebase that are typically used in bug prediction. Our experiments, performed on the three main repositories for the Wayfair website, show that our models can rank reverted commits above 80% of non-reverted commits on average. Moreover, when given to Wayfair developers, our models reduce the amount of time needed to find certain kinds of bugs by 55%. Wayfair continues to use our findings and models today to help find bugs during software deployments. © 2020 Owner/Author.",Reverted commits; Software defect prediction; Software deployment,"1251, 1262",,ESEC/FSE 2020 - Proceedings of the 28th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,Conference Paper,Scopus
504,,DeepCommenter: A deep code comment generation tool with hybrid lexical and syntactical information,"Li B., Yan M., Xia X., Hu X., Li G., Lo D.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097143673&doi=10.1145%2f3368089.3417926&partnerID=40&md5=0db5e86b482fb8fb228dceec7f700a55,10.1145/3368089.3417926,"As the scale of software projects increases, the code comments are more and more important for program comprehension. Unfortunately, many code comments are missing, mismatched or outdated due to tight development schedule or other reasons. Automatic code comment generation is of great help for developers to comprehend source code and reduce their workload. Thus, we propose a code comment generation tool (DeepCommenter) to generate descriptive comments for Java methods. DeepCommenter formulates the comment generation task as a machine translation problem and exploits a deep neural network that combines the lexical and structural information of Java methods. We implement DeepCommenter in the form of an Integrated Development Environment (i.e., Intellij IDEA) plug-in. Such plug-in is built upon a Client/Server architecture. The client formats the code selected by the user, sends request to the server and inserts the comment generated by the server above the selected code. The server listens for client's request, analyzes the requested code using the pre-trained model and sends back the generated comment to the client. The pre-trained model learns both the lexical and syntactical information from source code tokens and Abstract Syntax Trees (AST) respectively and combines these two types of information together to generate comments. To evaluate DeepCommenter, we conduct experiments on a large corpus built from a large number of open source Java projects on GitHub. The experimental results on different metrics show that DeepCommenter outperforms the state-of-the-art approaches by a substantial margin. © 2020 ACM.",Comment Generation; Deep Learning; Program Comprehension,"1571, 1575",,ESEC/FSE 2020 - Proceedings of the 28th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,Conference Paper,Scopus
505,,Global cost/quality management across multiple applications,"Liu L., Isaacman S., Kremer U.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097142710&doi=10.1145%2f3368089.3409721&partnerID=40&md5=4ad150a125650387db8ac51e6af555f3,10.1145/3368089.3409721,"Approximation is a technique that optimizes the balance between application outcome quality and its resource usage. Trading quality for performance has been investigated for single application scenarios, but not for environments where multiple approximate applications may run concurrently on the same machine, interfering with each other by sharing machine resources. Applying existing, single application techniques to this multi-programming environment may lead to configuration space size explosion, or result in poor overall application quality outcomes. Our new RAPID-M system is the first cross-application con-figuration management framework. It reduces the problem size by clustering configurations of individual applications into local""similarity buckets"". The global cross-applications configuration selection is based on these local bucket spaces. RAPID-M dynamically assigns buckets to applications such that overall quality is maximized while respecting individual application cost budgets.Once assigned a bucket, reconfigurations within buckets may be performed locally with minimal impact on global selections. Experimental results using six configurable applications show that even large configuration spaces of complex applications can be clustered into a small number of buckets, resulting in search space size reductions of up to 9 orders of magnitude for our six applications. RAPID-M constructs performance cost models with an average prediction error of ≤3%. For our application execution traces, RAPID-M dynamically selects configurations that lower the budget violation rate by 33.9% with an average budget exceeding rate of 6.6% as compared to other possible approaches. RAPID-M successfully finishes 22.75% more executions which translates to a 1.52X global output quality increase under high system loads. Theo verhead ofRAPID-Mis within≤1% of application execution times. © 2020 ACM.",Approximate Computing; Global Configuration Management; Multi-Programming; Performance Prediction,"350, 361",,ESEC/FSE 2020 - Proceedings of the 28th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,Conference Paper,Scopus
506,,Effort-aware just-in-time defect identification in practice: A case study at Alibaba,"Yan M., Xia X., Fan Y., Lo D., Hassan A.E., Zhang X.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097138978&doi=10.1145%2f3368089.3417048&partnerID=40&md5=41973787b54adf14d6e036bde1855eeb,10.1145/3368089.3417048,"Effort-aware Just-in-Time (JIT) defect identification aims at identifying defect-introducing changes just-in-time with limited code inspection effort. Such identification has two benefits compared with traditional module-level defect identification, i.e., identifying defects in a more cost-effective and efficient manner. Recently, researchers have proposed various effort-aware JIT defect identification approaches, including supervised (e.g., CBS+, OneWay) and unsupervised approaches (e.g., LT and Code Churn). The comparison of the effectiveness between such supervised and unsupervised approaches has attracted a large amount of research interest. However, the effectiveness of the recently proposed approaches and the comparison among them have never been investigated in an industrial setting. In this paper, we investigate the effectiveness of state-of-the-art effort-aware JIT defect identification approaches in an industrial setting. To that end, we conduct a case study on 14 Alibaba projects with 196,790 changes. In our case study, we investigate three aspects: (1) The effectiveness of state-of-the-art supervised (i.e., CBS+,OneWay, EALR) and unsupervised (i.e., LT and Code Churn) effortaware JIT defect identification approaches on Alibaba projects, (2) the importance of the features used in the effort-aware JIT defect identification approach, and (3) the association between projectspecific factors and the likelihood of a defective change. Moreover, we develop a tool based on the best performing approach and investigate the tool's effectiveness in a real-life setting at Alibaba. © 2020 ACM.",Effort-aware; Industrial study; Just-in-Time defect identification,"1308, 1319",,ESEC/FSE 2020 - Proceedings of the 28th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,Conference Paper,Scopus
507,,Dynamic slicing for deep neural networks,"Zhang Z., Li Y., Guo Y., Chen X., Liu Y.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097137679&doi=10.1145%2f3368089.3409676&partnerID=40&md5=8a90014aa005b95a8ee7c77eee246b46,10.1145/3368089.3409676,"Program slicing has been widely applied in a variety of software engineering tasks. However, existing program slicing techniques only deal with traditional programs that are constructed with instructions and variables, rather than neural networks that are composed of neurons and synapses. In this paper, we introduce NNSlicer, the first approach for slicing deep neural networks based on data-flow analysis. Our method understands the reaction of each neuron to an input based on the difference between its behavior activated by the input and the average behavior over the whole dataset. Then we quantify the neuron contributions to the slicing criterion by recursively backtracking from the output neurons, and calculate the slice as the neurons and the synapses with larger contributions. We demonstrate the usefulness and effectiveness of NNSlicer with three applications, including adversarial input detection, model pruning, and selective model protection. In all applications, NNSlicer significantly outperforms other baselines that do not rely on data flow analysis. © 2020 ACM.",Data-flow analysis; Deep neural networks; Dynamic slicing; Program slicing,"838, 850",,ESEC/FSE 2020 - Proceedings of the 28th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,Conference Paper,Scopus
508,,How to mitigate the incident? an effective troubleshooting guide recommendation technique for online service systems,"Jiang J., Lu W., Chen J., Lin Q., Zhao P., Kang Y., Zhang H., Xiong Y., Gao F., Xu Z., Dang Y., Zhang D.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097136254&doi=10.1145%2f3368089.3417054&partnerID=40&md5=13bab90214525fc0f1a56f0f9c72f847,10.1145/3368089.3417054,"In recent years, more and more traditional shrink-wrapped software is provided as 7x24 online services. Incidents (events that lead to service disruptions or outages) could affect service availability and cause great financial loss. Therefore, mitigating the incidents is important and time critical. In practice, a document describing a mitigation process, called a troubleshooting guide (TSG), is usually used to reduce the Time To Mitigate (TTM). To investigate the usage of TSGs in real-world online services, we conduct the first empirical study on 18 real-world, large-scale online service systems in Microsoft. We analyze the distribution and characteristics of TSGs among all incident records in the past two years. According to our study, 27.2% incidents have TSG records and 36.2% of them occurred at least twice. Besides, on average developers spend around 36.3% of the entire mitigation time on locating the desired TSGs. Our study shows that incidents could occur repeatedly and TSGs could be reused to facilitate incident mitigation. Motivated by our empirical study, we propose an automated TSG recommendation approach, DeepRmd, by leveraging the textual similarity between incident description and its corresponding TSG using deep learning techniques. We evaluate the effectiveness of DeepRmd on 18 online service systems. The results show that DeepRmd can recommend the correct TSG as the Top 1 returned result for 80.3% incidents, which significantly outperforms two baseline approaches. © 2020 ACM.",Incident management; Incident mitigation; Online service systems; Troubleshooting guide,"1410, 1420",,ESEC/FSE 2020 - Proceedings of the 28th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,Conference Paper,Scopus
509,,Machine translation testing via pathological invariance,"Gupta S., He P., Meister C., Su Z.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097132670&doi=10.1145%2f3368089.3409756&partnerID=40&md5=1a7411cb36b1832b93598876ae5c9917,10.1145/3368089.3409756,"Machine translation software has become heavily integrated into our daily lives due to the recent improvement in the performance of deep neural networks. However, machine translation software has been shown to regularly return erroneous translations, which can lead to harmful consequences such as economic loss and political conflicts. Additionally, due to the complexity of the underlying neural models, testing machine translation systems presents new challenges. To address this problem, we introduce a novel methodology called PatInv. The main intuition behind PatInv is that sentences with different meanings should not have the same translation. Under this general idea, we provide two realizations of PatInv that given an arbitrary sentence, generate syntactically similar but semantically different sentences by: (1) replacing one word in the sentence using a masked language model or (2) removing one word or phrase from the sentence based on its constituency structure. We then test whether the returned translations are the same for the original and modified sentences. We have applied PatInv to test Google Translate and Bing Microsoft Translator using 200 English sentences. Two language settings are considered: English-Hindi (En-Hi) and English-Chinese (En-Zh). The results show that PatInv can accurately find 308 erroneous translations in Google Translate and 223 erroneous translations in Bing Microsoft Translator, most of which cannot be found by the state-of-the-art approaches. © 2020 ACM.",Machine translation; Pathological Invariance; Testing,"863, 875",,ESEC/FSE 2020 - Proceedings of the 28th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,Conference Paper,Scopus
510,,AMS: Generating AutoML search spaces from weak specifications,"Cambronero J.P., Cito J., Rinard M.C.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097130729&doi=10.1145%2f3368089.3409700&partnerID=40&md5=f362e67bca3abd38fb4c1207e324792a,10.1145/3368089.3409700,"We consider a usage model for automated machine learning (AutoML) in which users can influence the generated pipeline by providing a weak pipeline specification: an unordered set of API components from which the AutoML system draws the components it places into the generated pipeline. Such specifications allow users to express preferences over the components that appear in the pipeline, for example a desire for interpretable components to appear in the pipeline. We present AMS, an approach to automatically strengthen weak specifications to include unspecified complementary and functionally related API components, populate the space of hyperparameters and their values, and pair this configuration with a search procedure to produce a strong pipeline specification: a full description of the search space for candidate pipelines. ams uses normalized pointwise mutual information on a code corpus to identify complementary components, BM25 as a lexical similarity score over the target API's documentation to identify functionally related components, and frequency distributions in the code corpus to extract key hyperparameters and values. We show that strengthened specifications can produce pipelines that outperform the pipelines generated from the initial weak specification and an expert-annotated variant, while producing pipelines that still reflect the user preferences captured in the original weak specification. © 2020 Owner/Author.",Automated machine learning; Program mining; Search-based software engineering,"763, 774",,ESEC/FSE 2020 - Proceedings of the 28th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,Conference Paper,Scopus
511,,A multi-Task representation learning approach for source code,"Wang D., Dong W., Li S.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096999195&doi=10.1145%2f3416506.3423575&partnerID=40&md5=90838a9e9cff8ebedea3fa270f9fc249,10.1145/3416506.3423575,"Representation learning has shown impressive results for a multitude of tasks in software engineering. However, most researches still focus on a single problem. As a result, the learned representations cannot be applied to other problems and lack generalizability and interpretability. In this paper, we propose a Multi-Task learning approach for representation learning across multiple downstream tasks of software engineering. From the perspective of generalization, we build a shared sequence encoder with a pretrained BERT for the token sequence and a structure encoder with a Tree-LSTM for the abstract syntax tree of code. From the perspective of interpretability, we integrate attention mechanism to focus on different representations and set learnable parameters to adjust the relationship between tasks. We also present the early results of our model. The learning process analysis shows our model has a significant improvement over strong baselines. © 2020 Owner/Author.",attention mechanism; deep learning; multi-Task learning; representation learning,"1, 2",,"RL+SE and PL 2020 - Proceedings of the 1st ACM SIGSOFT International Workshop on Representation Learning for Software Engineering and Program Languages, Co-located with ESEC/FSE 2020",Conference Paper,Scopus
512,,Beware the evolving 'intelligent' web service! an integration architecture tactic to guard AI-first components,"Cummaudo A., Barnett S., Vasa R., Grundy J., Abdelrazek M.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095508614&doi=10.1145%2f3368089.3409688&partnerID=40&md5=1a789e8c27a01344f0c422a4b1ff6f53,10.1145/3368089.3409688,"Intelligent services provide the power of AI to developers via simple RESTful API endpoints, abstracting away many complexities of machine learning. However, most of these intelligent services - -such as computer vision - -continually learn with time. When the internals within the abstracted 'black box' become hidden and evolve, pitfalls emerge in the robustness of applications that depend on these evolving services. Without adapting the way developers plan and construct projects reliant on intelligent services, significant gaps and risks result in both project planning and development. Therefore, how can software engineers best mitigate software evolution risk moving forward, thereby ensuring that their own applications maintain quality? Our proposal is an architectural tactic designed to improve intelligent service-dependent software robustness. The tactic involves creating an application-specific benchmark dataset baselined against an intelligent service, enabling evolutionary behaviour changes to be mitigated. A technical evaluation of our implementation of this architecture demonstrates how the tactic can identify 1,054 cases of substantial confidence evolution and 2,461 cases of substantial changes to response label sets using a dataset consisting of 331 images that evolve when sent to a service. © 2020 ACM.",Intelligent web services; Software architecture; Software evolution,"269, 280",,ESEC/FSE 2020 - Proceedings of the 28th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,Conference Paper,Scopus
513,,Analyzing the Stationarity Process in Software Effort Estimation Datasets,"Bosu M.F., MacDonell S.G., Whigham P.A.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099886902&doi=10.1142%2fS0218194020400239&partnerID=40&md5=bf13a690f18bba7407074cdc326b071c,10.1142/S0218194020400239,"Software effort estimation models are typically developed based on an underlying assumption that all data points are equally relevant to the prediction of effort for future projects. The dynamic nature of several aspects of the software engineering process could mean that this assumption does not hold in at least some cases. This study employs three kernel estimator functions to test the stationarity assumption in five software engineering datasets that have been used in the construction of software effort estimation models. The kernel estimators are used in the generation of nonuniform weights which are subsequently employed in weighted linear regression modeling. In each model, older projects are assigned smaller weights while the more recently completed projects are assigned larger weights, to reflect their potentially greater relevance to present or future projects that need to be estimated. Prediction errors are compared to those obtained from uniform models. Our results indicate that, for the datasets that exhibit underlying nonstationary processes, uniform models are more accurate than the nonuniform models; that is, models based on kernel estimator functions are worse than the models where no weighting was applied. In contrast, the accuracies of uniform and nonuniform models for datasets that exhibited stationary processes were essentially equivalent. Our analysis indicates that as the heterogeneity of a dataset increases, the effect of stationarity is overridden. The results of our study also confirm prior findings that the accuracy of effort estimation models is independent of the type of kernel estimator function used in model development. © 2020 World Scientific Publishing Company.",kernel estimators; Software effort estimation; software processes; stationarity; weighted linear regression,"1607, 1640",,International Journal of Software Engineering and Knowledge Engineering,Conference Paper,Scopus
514,,Conversion-based Approach to Obtain an SNN Construction,"Shang Y., Li Y., You F., Zhao R.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099875801&doi=10.1142%2fS0218194020400318&partnerID=40&md5=e7351c490193c61cc53296775d0cc49b,10.1142/S0218194020400318,"Spiking Neuron Network (SNN) uses spike sequence for data processing, so it has an excellent characteristic of low power consumption. However, due to the immaturity of learning algorithm, the multiplayer network training has difficulty in convergence. Utilizing the mature learning algorithm and fast training speed of the back-propagation network, this paper proposes a method to converse the Convolutional Neural Network (CNN) to the SNN. First, the adjustment strategy for CNN is introduced. Then after training, the weight parameters in the model are extracted, which is the corresponding synaptic weight in the layer of the SNN. Finally, a new threshold-setting algorithm based on feedback is proposed to solve the critical problem of the threshold setting of neurons in the SNN. We evaluate our method on the CIFAR-10 datasets released by Hinton's team. The experimental results show that the image classification accuracy of the SNN is more than 98% of that of CNN, and the theoretical value of power consumption per second is 3.9mW. © 2020 World Scientific Publishing Company.",Convolutional neural network; spiking neural network; threshold setting algorithm,"1801, 1818",,International Journal of Software Engineering and Knowledge Engineering,Conference Paper,Scopus
515,,SMAUG: End-to-End Full-Stack Simulation Infrastructure for Deep Learning Workloads,"Xi S.L., Yao Y., Bhardwaj K., Whatmough P., Wei G.-Y., Brooks D.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097241177&doi=10.1145%2f3424669&partnerID=40&md5=112a03597a4e107ba0c6b3176e7d1194,10.1145/3424669,"In recent years, there has been tremendous advances in hardware acceleration of deep neural networks. However, most of the research has focused on optimizing accelerator microarchitecture for higher performance and energy efficiency on a per-layer basis. We find that for overall single-batch inference latency, the accelerator may only make up 25-40%, with the rest spent on data movement and in the deep learning software framework. Thus far, it has been very difficult to study end-to-end DNN performance during early stage design (before RTL is available), because there are no existing DNN frameworks that support end-to-end simulation with easy custom hardware accelerator integration. To address this gap in research infrastructure, we present SMAUG, the first DNN framework that is purpose-built for simulation of end-to-end deep learning applications. SMAUG offers researchers a wide range of capabilities for evaluating DNN workloads, from diverse network topologies to easy accelerator modeling and SoC integration. To demonstrate the power and value of SMAUG, we present case studies that show how we can optimize overall performance and energy efficiency for up to 1.8×-5× speedup over a baseline system, without changing any part of the accelerator microarchitecture, as well as show how SMAUG can tune an SoC for a camera-powered deep learning pipeline. © 2020 Owner/Author.",architectural simulation; Deep neural networks; hardware accelerators,,,ACM Transactions on Architecture and Code Optimization,Article,Scopus
516,,Multi-module TSK Fuzzy System Based on Training Space Reconstruction [基于训练空间重构的多模块TSK模糊系统],"Zhou T., Deng Z.-H., Jiang Y.-Z., Wang S.-T.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096704763&doi=10.13328%2fj.cnki.jos.005846&partnerID=40&md5=55d508df4108b7511145f49a0804fd61,10.13328/j.cnki.jos.005846,"A multi-training module Takagi-Sugeno-Kang (TSK) fuzzy classifier, H-TSK-FS, is proposed by means of reconstruction of training sample space. H-TSK-FS has good classification performance and high interpretability, which can solve the problems of existing hierarchical fuzzy classifiers such as the output and fuzzy rules of intermediate layer that are difficult to explain. In order to achieve enhanced classification performance, H-TSK-FS is composed of several optimized zero-order TSK fuzzy classifiers. These zero-order TSK fuzzy classifiers adopt an ingenious training method. The original training sample, part of the sample of the previous layer and part of the decision information that most approximates the real value in all the training layers are projected into the training module of the current layer and constitute its input space. In this way, the training results of the previous layers play a guiding and controlling role in the training of the current layer. This method of randomly selecting sample points and training features within a certain range can open up the manifold structure of the original input space and ensure better or equivalent classification performance. In addition, this study focuses on data sets with a small number of sample points and a small number of training features. In the design of each training unit, extreme learning machine is used to obtain the Then-part parameters of fuzzy rules. For each intermediate training layer, short rules are used to express knowledge. Each fuzzy rule determines the variable input features and Gaussian membership function by means of constraints, in order to ensure that the selected input features are highly interpretable. Experimental results of real datasets and application cases show that H-TSK-FS enhances classification performance and high interpretability. © Copyright 2020, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Extreme learning machine; Interpret ability; Multi-module training; TSK fuzzy system,"3506, 3518",,Ruan Jian Xue Bao/Journal of Software,Article,Scopus
517,,Dynamic Gene Regulatory Network Evolution Analysis [动态基因调控网演化分析],"Liu Z.-Z., Hu W.-B., Xu P.-H., Tang C.-H., Gao K., Ma F.-Y., Qiu Z.-Y.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096679553&doi=10.13328%2fj.cnki.jos.005821&partnerID=40&md5=0469cf839d928b34e1509f017d13800b,10.13328/j.cnki.jos.005821,"Dynamic gene regulatory network is a complex network representing the dynamic interactions between genes in organism. The interactions can be divided into two groups, motivation and inhibition. The researches on the evolution of dynamic gene regulatory network can be used to predict the gene regulation relationship in the future, thus playing a reference role in diagnosis and prediction of diseases, Pharma projects, and biological experiments. However, the evolution of gene regulatory network is a huge and complex system in real world, the researches about its evolutionary mechanism only focus on statics networks but ignore dynamic networks as well as ignore the types of interaction. In response to these defects, a dynamic gene regulatory network evolution analyzing method (DGNE) is proposed to extend the research to the field of dynamic signed networks. According to the link prediction algorithm based on motif transfer probability (MT) and symbol discrimination algorithm based on latent space character included in DGNE, the evolution mechanism of dynamic gene regulatory network can be dynamically captured as well as the links of gene regulatory network are predicted precisely. The experiment results showed that the proposed DGNE method performs greatly on simulated datasets and real datasets. © Copyright 2020, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Gene regulatory network; Latent space; Link prediction; Motif; Network evolution,"3334, 3350",,Ruan Jian Xue Bao/Journal of Software,Article,Scopus
518,,Feature Representation Method of Microscopic Sandstone Images Based on Convolutional Neural Network [一种基于卷积神经网络的砂岩显微图像特征表示方法],"Li N., Gu Q., Jiang F., Hao H.-Z., Yu H., Ni C.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096659264&doi=10.13328%2fj.cnki.jos.005836&partnerID=40&md5=321f294a1f886084dbc831db1231d62d,10.13328/j.cnki.jos.005836,"The classification of microscopic sandstone images is a basic work in geological research, and it has an important significance in the evaluation of oil and gas reservoirs. In the automatic classification of microscopic sandstone images, due to their complex and variable micro-structures, the hand-crafted features have limited abilities to represent them. In addition, since the collection and labeling of sandstone samples are costly, labeled microscopic sandstone images are usually few. In this study, a convolutional neural network based feature representation method for small-scale data sets, called FeRNet, is proposed to effectively capture the semantic information of microscopic sandstone images and enhance their feature representation. The FeRNet has a simple structure, which reduces the quantity requirements for labeled images, and prevents the overfitting. Aiming at the problem of insufficient labeled microscopic sandstone image, the image augmentation preprocessing and a CAE network-based weight initialization strategy are proposed, to reduce the risk of overfitting. Based on the microscopic sandstone images collected from Tibet, the experiments are designed and conducted. The results show that both image augmentation and CAE network can effectively improve the training of FeRNet network, when the labeled microscopic sandstone images are few; and the FeRNet features are more capable of the representations of microscopic sandstone images than the hand-crafted features. © Copyright 2020, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Convolutional autoencoder; Convolutional neural network; Feature representation; Image augmentation; Microscopic sandstone image,"3621, 3639",,Ruan Jian Xue Bao/Journal of Software,Article,Scopus
519,,Evaluation of Headset-based Viewing and Desktop-based Viewing of Remote Lectures in a Social VR Platform,"Yoshimura A., Borst C.W.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095822968&doi=10.1145%2f3385956.3422124&partnerID=40&md5=1aca02d8226d3bb002fca1258bfabb85,10.1145/3385956.3422124,"We study experiences of students attending classes remotely from home using a social VR platform, considering both desktop-based and headset-based viewing of remote lectures. Ratings varied widely. Headset viewing produced higher presence overall. Strong negative correlations between headset simulator sickness symptoms and overall experience ratings, and some other ratings, suggest that the headset experience was much better for comfortable users than for others. Reduced sickness symptoms, and no similar correlations, were found for desktop viewing. Desktop viewing appears to be a good alternative for students not comfortable with headsets. Future VR systems are expected to provide more stable and comfortable visuals, providing benefits to more users. © 2020 Owner/Author.",COVID-19; distance learning; educational VR; Mozilla Hubs; remote instruction; SARS-CoV-2; teleconferencing; virtual reality,,,"Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST",Conference Paper,Scopus
520,,On the feasibility of automated prediction of bug and non-bug issues,"Herbold S., Trautsch A., Trautsch F.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090988857&doi=10.1007%2fs10664-020-09885-w&partnerID=40&md5=986c40c5273cec5af6bc32afec0bac1b,10.1007/s10664-020-09885-w,"Context: Issue tracking systems are used to track and describe tasks in the development process, e.g., requested feature improvements or reported bugs. However, past research has shown that the reported issue types often do not match the description of the issue. Objective: We want to understand the overall maturity of the state of the art of issue type prediction with the goal to predict if issues are bugs and evaluate if we can improve existing models by incorporating manually specified knowledge about issues. Method: We train different models for the title and description of the issue to account for the difference in structure between these fields, e.g., the length. Moreover, we manually detect issues whose description contains a null pointer exception, as these are strong indicators that issues are bugs. Results: Our approach performs best overall, but not significantly different from an approach from the literature based on the fastText classifier from Facebook AI Research. The small improvements in prediction performance are due to structural information about the issues we used. We found that using information about the content of issues in form of null pointer exceptions is not useful. We demonstrate the usefulness of issue type prediction through the example of labelling bugfixing commits. Conclusions: Issue type prediction can be a useful tool if the use case allows either for a certain amount of missed bug reports or the prediction of too many issues as bug is acceptable. © 2020, The Author(s).",Issue tracking; Issue type prediction; Mislabeled issues,"5333, 5369",,Empirical Software Engineering,Article,Scopus
521,,On the time-based conclusion stability of cross-project defect prediction models,"Bangash A.A., Sahar H., Hindle A., Ali K.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090463838&doi=10.1007%2fs10664-020-09878-9&partnerID=40&md5=4166ed1a7e77d503354d555f39d16cbd,10.1007/s10664-020-09878-9,"Researchers in empirical software engineering often make claims based on observable data such as defect reports. Unfortunately, in many cases, these claims are generalized beyond the data sets that have been evaluated. Will the researcher’s conclusions hold a year from now for the same software projects? Perhaps not. Recent studies show that in the area of Software Analytics, conclusions over different data sets are usually inconsistent. In this article, we empirically investigate whether conclusions in the area of cross-project defect prediction truly exhibit stability throughout time or not. Our investigation applies a time-aware evaluation approach where models are trained only on the past, and evaluations are executed only on the future. Through this time-aware evaluation, we show that depending on which time period we evaluate defect predictors, their performance, in terms of F-Score, the area under the curve (AUC), and Mathews Correlation Coefficient (MCC), varies and their results are not consistent. The next release of a product, which is significantly different from its prior release, may drastically change defect prediction performance. Therefore, without knowing about the conclusion stability, empirical software engineering researchers should limit their claims of performance within the contexts of evaluation, because broad claims about defect prediction performance might be contradicted by the next upcoming release of a product under analysis. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.",Conclusion stability; Defect prediction; Time-aware evaluation,"5047, 5083",,Empirical Software Engineering,Article,Scopus
522,,CROKAGE: effective solution recommendation for programming tasks by leveraging crowd knowledge,"da Silva R.F.G., Roy C.K., Rahman M.M., Schneider K.A., Paixão K., Dantas C.E.C., Maia M.A.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090219490&doi=10.1007%2fs10664-020-09863-2&partnerID=40&md5=0d6cf326e8ad161ffcdde586f51f4c04,10.1007/s10664-020-09863-2,"Developers often search for relevant code examples on the web for their programming tasks. Unfortunately, they face three major problems. First, they frequently need to read and analyse multiple results from the search engines to obtain a satisfactory solution. Second, the search is impaired due to a lexical gap between the query (task description) and the information associated with the solution (e.g., code example). Third, the retrieved solution may not be comprehensible, i.e., the code segment might miss a succinct explanation. To address these three problems, we propose CROKAGE (CrowdKnowledge Answer Generator), a tool that takes the description of a programming task (the query) as input and delivers a comprehensible solution for the task. Our solutions contain not only relevant code examples but also their succinct explanations written by human developers. The search for code examples is modeled as an Information Retrieval (IR) problem. We first leverage the crowd knowledge stored in Stack Overflow to retrieve the candidate answers against a programming task. For this, we use a fine-tuned IR technique, chosen after comparing 11 IR techniques in terms of performance. Then we use a multi-factor relevance mechanism to mitigate the lexical gap problem, and select the top quality answers related to the task. Finally, we perform natural language processing on the top quality answers and deliver the comprehensible solutions containing both code examples and code explanations unlike earlier studies. We evaluate and compare our approach against ten baselines, including the state-of-art. We show that CROKAGE outperforms the ten baselines in suggesting relevant solutions for 902 programming tasks (i.e., queries) of three popular programming languages: Java, Python and PHP. Furthermore, we use 24 programming tasks (queries) to evaluate our solutions with 29 developers and confirm that CROKAGE outperforms the state-of-art tool in terms of relevance of the suggested code examples, benefit of the code explanations and the overall solution quality (code + explanation). © 2020, Springer Science+Business Media, LLC, part of Springer Nature.",Code search; Mining crowd knowledge; Stack overflow; Word embedding,"4707, 4758",,Empirical Software Engineering,Article,Scopus
523,,Automatic detection of acute ischemic stroke using non-contrast computed tomography and two-stage deep learning model,"Nishio M., Koyasu S., Noguchi S., Kiguchi T., Nakatsu K., Akasaka T., Yamada H., Itoh K.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089748684&doi=10.1016%2fj.cmpb.2020.105711&partnerID=40&md5=58ddbbaf3f81e84059587d952efe73d7,10.1016/j.cmpb.2020.105711,"Background and objective: Currently, it is challenging to detect acute ischemic stroke (AIS)-related changes on computed tomography (CT) images. Therefore, we aimed to develop and evaluate an automatic AIS detection system involving a two-stage deep learning model. Methods: We included 238 cases from two different institutions. AIS-related findings were annotated on each of the 238 sets of head CT images by referring to head magnetic resonance imaging (MRI) images in which an MRI examination was performed within 24 h following the CT scan. These 238 annotated cases were divided into a training set including 189 cases and test set including 49 cases. Subsequently, a two-stage deep learning detection model was constructed from the training set using the You Only Look Once v3 model and Visual Geometry Group 16 classification model. Then, the two-stage model performed the AIS detection process in the test set. To assess the detection model's results, a board-certified radiologist also evaluated the test set head CT images with and without the aid of the detection model. The sensitivity of AIS detection and number of false positives were calculated for the evaluation of the test set detection results. The sensitivity of the radiologist with and without the software detection results was compared using the McNemar test. A p-value of less than 0.05 was considered statistically significant. Results: For the two-stage model and radiologist without and with the use of the software results, the sensitivity was 37.3%, 33.3%, and 41.3%, respectively, and the number of false positives per one case was 1.265, 0.327, and 0.388, respectively. On using the two-stage detection model's results, the board-certified radiologist's detection sensitivity significantly improved (p-value = 0.0313). Conclusions: Our detection system involving the two-stage deep learning model significantly improved the radiologist's sensitivity in AIS detection. © 2020 Elsevier B.V.",Acute ischemic stroke; Computer-aided detection; Deep learning; Magnetic resonance imaging; Non-contrast computed tomography,,,Computer Methods and Programs in Biomedicine,Article,Scopus
524,,Deep learning analysis in coronary computed tomographic angiography imaging for the assessment of patients with coronary artery stenosis,"Han D., Liu J., Sun Z., Cui Y., He Y., Yang Z.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088372517&doi=10.1016%2fj.cmpb.2020.105651&partnerID=40&md5=3e4e0619d38896aa05bd6c0ace636d54,10.1016/j.cmpb.2020.105651,"Background and Objective: Recently, deep convolutional neural network has significantly improved image classification and image segmentation. If coronary artery disease (CAD) can be diagnosed through machine learning and deep learning, it will significantly reduce the burdens of the doctors and accelerate the critical patient diagnoses. The purpose of the study is to assess the practicability of utilizing deep learning approaches to process coronary computed tomographic angiography (CCTA) imaging (termed CCTA-artificial intelligence, CCTA-AI) in coronary artery stenosis. Materials and Methods: A CCTA reconstruction pipeline was built by utilizing deep learning and transfer learning approaches to generate auto-reconstructed CCTA images based on a series of two-dimensional (2D) CT images. 150 patients who underwent successively CCTA and digital subtraction angiography (DSA) from June 2017 to December 2017 were retrospectively analyzed. The dataset was divided into two parts comprising training dataset and testing dataset. The training dataset included the CCTA images of 100 patients which are trained using convolutional neural networks (CNN) in order to further identify various plaque classifications and coronary stenosis. The other 50 CAD patients acted as testing dataset that is evaluated by comparing the auto-reconstructed CCTA images with traditional CCTA images on the condition that DSA images are regarded as the reference method. Receiver operating characteristic (ROC) analysis was used for statistical analysis to compare CCTA-AI with DSA and traditional CCTA in the aspect of detecting coronary stenosis and plaque features. Results: AI significantly reduces time for post-processing and diagnosis comparing to the traditional methods. In identifying various degrees of coronary stenosis, the diagnostic accuracy of CCTA-AI is better than traditional CCTA (AUCAI = 0.870, AUCCCTA = 0.781, P &lt; 0.001). In identifying ≥ 50% stenotic vessels, the accuracy, sensitivity, specificity, positive predictive value and negative predictive value of CCTA-AI and traditional method are 86% and 83%, 88% and 59%, 85% and 94%, 73% and 84%, 94% and 83%, respectively. In the aspect of identifying plaque classification, accuracy of CCTA-AI is moderate compared to traditional CCTA (AUC = 0.750, P &lt; 0.001). Conclusion: The proposed CCTA-AI allows the generation of auto-reconstructed CCTA images from a series of 2D CT images. This approach is relatively accurate for detecting ≥50% stenosis and analyzing plaque features compared to traditional CCTA. © 2020",Convolutional neural network; Coronary atherosclerotic stenosis; Coronary computed tomographic angiography; Deep learning,,,Computer Methods and Programs in Biomedicine,Article,Scopus
525,,Learn from one data set to classify all – A multi-target domain adaptation approach for white blood cell classification,"Baydilli Y.Y., Atila U., Elen A.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088100336&doi=10.1016%2fj.cmpb.2020.105645&partnerID=40&md5=bffeb3f0dafb392cb66e06a622c92324,10.1016/j.cmpb.2020.105645,"Background and objective: Traditional machine learning methods assume that both training and test data come from the same distribution. In this way, it becomes possible to achieve high successes when modelling on the same domain. Unfortunately, in real-world problems, direct transfer between domains is adversely affected due to differences in the data collection process and the internal dynamics of the data. In order to cope with such drawbacks, researchers use a method called “domain adaptation”, which enables the successful transfer of information learned in one domain to other domains. In this study, a model that can be used in the classification of white blood cells (WBC) and is not affected by domain differences was proposed. Methods: Only one data set was used as source domain, and an adaptation process was created that made possible the learned knowledge to be used effectively in other domains (multi-target domain adaptation). While constructing the model, we employed data augmentation, data generation and fine-tuning processes, respectively. Results: The proposed model has been able to extract “domain-invariant” features and achieved high success rates in the tests performed on nine different data sets. Multi-target domain adaptation accuracy was measured as %98.09. Conclusions: At the end of the study, it has been observed that the proposed model ignores the domain differences and it can adapt in a successful way to target domains. In this way, it becomes possible to classify unlabeled samples rapidly by using only a few number of labeled ones. © 2020 Elsevier B.V.",Classification; Deep learning; Medical data analysis; Multi-target domain adaptation; White blood cells (WBC),,,Computer Methods and Programs in Biomedicine,Article,Scopus
526,,A machine learning based framework for code clone validation,"Mostaeen G., Roy B., Roy C.K., Schneider K., Svajlenko J.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087971737&doi=10.1016%2fj.jss.2020.110686&partnerID=40&md5=fae6e9e8eef5743ebfee334cfe263cbf,10.1016/j.jss.2020.110686,"A code clone is a pair of code fragments, within or between software systems that are similar. Since code clones often negatively impact the maintainability of a software system, several code clone detection techniques and tools have been proposed and studied over the last decade. However, the clone detection tools are not always perfect and their clone detection reports often contain a number of false positives or irrelevant clones from specific project management or user perspective. To detect all possible similar source code patterns in general, the clone detection tools work on the syntax level while lacking user-specific preferences. This often means the clones must be manually inspected before analysis in order to remove those false positives from consideration. This manual clone validation effort is very time-consuming and often error-prone, in particular for large-scale clone detection. In this paper, we propose a machine learning approach for automating the validation process. First, a training dataset is built by taking code clones from several clone detection tools for different subject systems and then manually validating those clones. Second, several features are extracted from those clones to train the machine learning model by the proposed approach. The trained algorithm is then used to automatically validate clones without human inspection. Thus the proposed approach can be used to remove the false positive clones from the detection results, automatically evaluate the precision of any clone detectors for any given set of datasets, evaluate existing clone benchmark datasets, or even be used to build new clone benchmarks and datasets with minimum effort. In an experiment with clones detected by several clone detectors in several different software systems, we found our approach has an accuracy of up to 87.4% when compared against the manual validation by multiple expert judges. The proposed method also shows better results in several comparative studies with the existing related approaches for clone classification. © 2020 Elsevier Inc.",Clone management; Code clones; Machine learning; Validation,,,Journal of Systems and Software,Article,Scopus
527,,Using the VQ-VAE to improve the recognition of abnormalities in short-duration 12-lead electrocardiogram records,"Liu H., Zhao Z., Chen X., Yu R., She Q.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087770145&doi=10.1016%2fj.cmpb.2020.105639&partnerID=40&md5=fd94e034e8eb2090d0c00aa0926a2e76,10.1016/j.cmpb.2020.105639,"Background and Objective: Morphological diagnosis is a basic clinical task of the short-duration 12-lead electrocardiogram (ECG). Due to the scarcity of positive samples and other factors, there is currently no algorithm that is comparable to human experts in ECG morphological recognition. Our objective is to develop an ECG specialist-level deep learning method that can accurately identify ten ECG morphological abnormalities in real scene data. Methods: We established a short-duration 12-lead ECG image dataset that consists of approximately 200,000 samples. To address the problems with small positive samples, a data augmentation method was proposed. We solved it by interpolating in the latent space of the vector quantized variational autoencoder (VQ-VAE) and generating new samples via sampling. The trained final classifier, general doctors, and ECG specialists evaluated the diagnostic performance on a test set that consisted of 1000 samples. Results: Relative to that of unaugmented data, the F1 score was improved by 0–6%. Compared with ECG specialists, the deep neural network achieved higher F1 scores and sensitivity in most categories. Conclusions: Our method can improve the classification performance of ECG data with insufficient positive samples and reach the level of ECG specialists. This approach can provide specialized reference opinions for ordinary clinicians and reduce the errors of ECG specialists. © 2020 Elsevier B.V.",Artificial intelligence; Data augmentation; Deep learning; Electrocardiogram,,,Computer Methods and Programs in Biomedicine,Article,Scopus
528,,Large-scale machine learning systems in real-world industrial settings: A review of challenges and solutions,"Lwakatare L.E., Raj A., Crnkovic I., Bosch J., Olsson H.H.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087690796&doi=10.1016%2fj.infsof.2020.106368&partnerID=40&md5=39ff261482f573a45f895642acbd9059,10.1016/j.infsof.2020.106368,"Background: Developing and maintaining large scale machine learning (ML) based software systems in an industrial setting is challenging. There are no well-established development guidelines, but the literature contains reports on how companies develop and maintain deployed ML-based software systems. Objective: This study aims to survey the literature related to development and maintenance of large scale ML-based systems in industrial settings in order to provide a synthesis of the challenges that practitioners face. In addition, we identify solutions used to address some of these challenges. Method: A systematic literature review was conducted and we identified 72 papers related to development and maintenance of large scale ML-based software systems in industrial settings. The selected articles were qualitatively analyzed by extracting challenges and solutions. The challenges and solutions were thematically synthesized into four quality attributes: adaptability, scalability, safety and privacy. The analysis was done in relation to ML workflow, i.e. data acquisition, training, evaluation, and deployment. Results: We identified a total of 23 challenges and 8 solutions related to development and maintenance of large scale ML-based software systems in industrial settings including six different domains. Challenges were most often reported in relation to adaptability and scalability. Safety and privacy challenges had the least reported solutions. Conclusion: The development and maintenance on large-scale ML-based systems in industrial settings introduce new challenges specific for ML, and for the known challenges characteristic for these types of systems, require new methods in overcoming the challenges. The identified challenges highlight important concerns in ML system development practice and the lack of solutions point to directions for future research. © 2020 Elsevier B.V.",Challenges; Industrial settings; Machine learning systems; SLR; Software engineering; Solutions,,,Information and Software Technology,Article,Scopus
529,,Knowledge-guided synthetic medical image adversarial augmentation for ultrasonography thyroid nodule classification,"Shi G., Wang J., Qiang Y., Yang X., Zhao J., Hao R., Yang W., Du Q., Kazihise N.G.-F.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087513820&doi=10.1016%2fj.cmpb.2020.105611&partnerID=40&md5=4749f7eaf21b1b3aa5d995e53ff0b452,10.1016/j.cmpb.2020.105611,"Background and objective: Image classification is an important task in many medical applications. Methods based on deep learning have made great achievements in the computer vision domain. However, they typically rely on large-scale datasets which are annotated. How to obtain such great datasets is still a serious problem in medical domain. Methods: In this paper, we propose a knowledge-guided adversarial augmentation method for synthesizing medical images. First, we design Term and Image Encoders to extract domain knowledge from radiologists, then we use domain knowledge as novel condition to constrain the Auxiliary Classifier Generative Adversarial Network (ACGAN) framework for the synthesis of high-quality thyroid nodule images. Finally, we demonstrate our method on the task of classifying ultrasonography thyroid nodule. Our method can make effective use of the high-quality diagnostic experience of advanced radiologists. In addition, we creatively choose to extract domain knowledge from standardized terms rather than ultrasound images. Results: Our novel method is demonstrated on a limited dataset of 1937 clinical thyroid ultrasound images and corresponding standardized terms. The accuracy of the proposed model for thyroid nodules is 91.46%, the sensitivity is 90.63%, the specificity is 92.65%, and the AUC is 95.32%, which is better than the current classification methods for thyroid nodules. The experimental results show the model has better generalization and robustness. Conclusions: We believe that the proposed method can alleviate the problem of insufficient data in the medical domain, and other medical problems can benefit from using synthetic augmentation. © 2020",Classification; Data augmentation; Domain knowledge; Generative adversarial network; Image synthesis; Thyroid nodule,,,Computer Methods and Programs in Biomedicine,Article,Scopus
530,,Explainable Deep Learning for Pulmonary Disease and Coronavirus COVID-19 Detection from X-rays,"Brunese L., Mercaldo F., Reginelli A., Santone A.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086819887&doi=10.1016%2fj.cmpb.2020.105608&partnerID=40&md5=24239343cee532cdf4ed72bf620e8db7,10.1016/j.cmpb.2020.105608,"Background and Objective: Coronavirus disease (COVID-19) is an infectious disease caused by a new virus never identified before in humans. This virus causes respiratory disease (for instance, flu) with symptoms such as cough, fever and, in severe cases, pneumonia. The test to detect the presence of this virus in humans is performed on sputum or blood samples and the outcome is generally available within a few hours or, at most, days. Analysing biomedical imaging the patient shows signs of pneumonia. In this paper, with the aim of providing a fully automatic and faster diagnosis, we propose the adoption of deep learning for COVID-19 detection from X-rays. Method: In particular, we propose an approach composed by three phases: the first one to detect if in a chest X-ray there is the presence of a pneumonia. The second one to discern between COVID-19 and pneumonia. The last step is aimed to localise the areas in the X-ray symptomatic of the COVID-19 presence. Results and Conclusion: Experimental analysis on 6,523 chest X-rays belonging to different institutions demonstrated the effectiveness of the proposed approach, with an average time for COVID-19 detection of approximately 2.5 seconds and an average accuracy equal to 0.97. © 2020 Elsevier B.V.",Artificial intelligence; Chest; Coronavirus; COVID-19; Deep learning; Transfer learning,,,Computer Methods and Programs in Biomedicine,Article,Scopus
531,,Evaluation of deep learning detection and classification towards computer-aided diagnosis of breast lesions in digital X-ray mammograms,"Al-antari M.A., Han S.-M., Kim T.-S.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086366096&doi=10.1016%2fj.cmpb.2020.105584&partnerID=40&md5=8b22d89f02142d5de5794c635aaacb82,10.1016/j.cmpb.2020.105584,"Background and Objective: Deep learning detection and classification from medical imagery are key components for computer-aided diagnosis (CAD) systems to efficiently support physicians leading to an accurate diagnosis of breast lesions. Methods: In this study, an integrated CAD system of deep learning detection and classification is proposed aiming to improve the diagnostic performance of breast lesions. First, a deep learning YOLO detector is adopted and evaluated for breast lesion detection from entire mammograms. Then, three deep learning classifiers, namely regular feedforward CNN, ResNet-50, and InceptionResNet-V2, are modified and evaluated for breast lesion classification. The proposed deep learning system is evaluated over 5-fold cross-validation tests using two different and widely used databases of digital X-ray mammograms: DDSM and INbreast. Results: The evaluation results of breast lesion detection show the capability of the YOLO detector to achieve overall detection accuracies of 99.17% and 97.27% and F1-scores of 99.28% and 98.02% for DDSM and INbreast datasets, respectively. Meanwhile, the YOLO detector could predict 71 frames per second (FPS) at the testing time for both DDSM and INbreast datasets. Using detected breast lesions, the classification models of CNN, ResNet-50, and InceptionResNet-V2 achieve promising average overall accuracies of 94.50%, 95.83%, and 97.50%, respectively, for the DDSM dataset and 88.74%, 92.55%, and 95.32%, respectively, for the INbreast dataset. Conclusion: The capability of the YOLO detector boosted the classification models to achieve a promising breast lesion diagnostic performance. Such prediction results should help to develop a feasible CAD system for practical breast cancer diagnosis. © 2020",Breast lesions; Classification; Computer-aided diagnosis (CAD); Deep learning; Detection; Evaluation,,,Computer Methods and Programs in Biomedicine,Article,Scopus
532,,CoroNet: A deep neural network for detection and diagnosis of COVID-19 from chest x-ray images,"Khan A.I., Shah J.L., Bhat M.M.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086140653&doi=10.1016%2fj.cmpb.2020.105581&partnerID=40&md5=a4148950a9ef491a82f561da324c0bc2,10.1016/j.cmpb.2020.105581,"Background and Objective: The novel Coronavirus also called COVID-19 originated in Wuhan, China in December 2019 and has now spread across the world. It has so far infected around 1.8 million people and claimed approximately 114,698 lives overall. As the number of cases are rapidly increasing, most of the countries are facing shortage of testing kits and resources. The limited quantity of testing kits and increasing number of daily cases encouraged us to come up with a Deep Learning model that can aid radiologists and clinicians in detecting COVID-19 cases using chest X-rays. Methods: In this study, we propose CoroNet, a Deep Convolutional Neural Network model to automatically detect COVID-19 infection from chest X-ray images. The proposed model is based on Xception architecture pre-trained on ImageNet dataset and trained end-to-end on a dataset prepared by collecting COVID-19 and other chest pneumonia X-ray images from two different publically available databases. Results: CoroNet has been trained and tested on the prepared dataset and the experimental results show that our proposed model achieved an overall accuracy of 89.6%, and more importantly the precision and recall rate for COVID-19 cases are 93% and 98.2% for 4-class cases (COVID vs Pneumonia bacterial vs pneumonia viral vs normal). For 3-class classification (COVID vs Pneumonia vs normal), the proposed model produced a classification accuracy of 95%. The preliminary results of this study look promising which can be further improved as more training data becomes available. Conclusion: CoroNet achieved promising results on a small prepared dataset which indicates that given more data, the proposed model can achieve better results with minimum pre-processing of data. Overall, the proposed model substantially advances the current radiology based methodology and during COVID-19 pandemic, it can be very helpful tool for clinical practitioners and radiologists to aid them in diagnosis, quantification and follow-up of COVID-19 cases. © 2020","Convolutional Neural Network; Coronavirus; COVID-19, Pneumonia viral; Deep learning; Pneumonia bacterial",,,Computer Methods and Programs in Biomedicine,Article,Scopus
533,,Decoding surface touch typing from hand-tracking,"Richardson M., Durasoff M., Wang R.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096996969&doi=10.1145%2f3379337.3415816&partnerID=40&md5=d91dd7fca0526090ab4a13e609b34b91,10.1145/3379337.3415816,"We propose a novel text decoding method that enables touch typing on an uninstrumented flat surface. Rather than relying on physical keyboards or capacitive touch, our method takes as input hand motion of the typist, obtained through hand-tracking, and decodes this motion directly into text. We use a temporal convolutional network to represent a motion model that maps the hand motion, represented as a sequence of hand pose features, into text characters. To enable touch typing without the haptic feedback of a physical keyboard, we had to address more erratic typing motion due to drift of the fingers. Thus, we incorporate a language model as a text prior and use beam search to efficiently combine our motion and language models to decode text from erratic or ambiguous hand motion. We collected a dataset of 20 touch typists and evaluated our model on several baselines, including contact-based text decoding and typing on a physical keyboard. Our proposed method is able to leverage continuous hand pose information to decode text more accurately than contact-based methods and an offline study shows parity (73 WPM, 2.38% UER) with typing on a physical keyboard. Our results show that hand-tracking has the potential to enable rapid text entry in mobile environments. © 2020 Owner/Author.",Augmented reality; Hand-tracking; Text input; Virtual reality,"686, 696",,UIST 2020 - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology,Conference Paper,Scopus
534,,Skyline: Interactive in-editor computational performance profiling for deep neural network training,"Yu G.X., Grossman T., Pekhimenko G.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096979813&doi=10.1145%2f3379337.3415890&partnerID=40&md5=083a747b24d9673d33072c3e0f52833d,10.1145/3379337.3415890,"Training a state-of-the-art deep neural network (DNNs) is a computationally-expensive and time-consuming process, which incentivizes deep learning developers to debug their DNNs for computational performance. However, effectively performing this debugging requires intimate knowledge about the underlying software and hardware systems-something that the typical deep learning developer may not have. To help bridge this gap, we present Skyline: a new interactive tool for DNN training that supports in-editor computational performance profiling, visualization, and debugging. Skyline's key contribution is that it leverages special computational properties of DNN training to provide (i) interactive performance predictions and visualizations, and (ii) directly manipulatable visualizations that, when dragged, mutate the batch size in the code. As an in-editor tool, Skyline allows users to leverage these diagnostic features to debug the performance of their DNNs during development. An exploratory qualitative user study of Skyline produced promising results; all the participants found Skyline to be useful and easy to use. © 2020 ACM.",Debugging; Deep neural networks; Interactive performance profiling; Machine learning; Skyline; Visualization,"126, 139",,UIST 2020 - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology,Conference Paper,Scopus
535,,MYND: Unsupervised evaluation of novel BCI control strategies on consumer hardware,"Hohmann M.R., Konieczny L., Hackl M., Wirth B., Zaman T., Enficiaud R., Grosse-Wentrup M., Schölkopf B.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096956245&doi=10.1145%2f3379337.3415844&partnerID=40&md5=4929fb3bb57da388c982934bad52c153,10.1145/3379337.3415844,"Neurophysiological laboratory studies are often constraint to immediate geographical surroundings and access to equipment may be temporally restricted. Limitations of ecological validity, scalability, and generalizability of findings pose a significant challenge for the development of brain-computer interfaces (BCIs), which ultimately need to function in any context, on consumer-grade hardware. We introduce MYND: An open-source framework that couples consumer-grade recording hardware with an easy-to-use application for the unsupervised evaluation of BCI control strategies. Subjects are guided through experiment selection, hardware fitting, recording, and data upload in order to self-administer multi-day studies that include neurophysiological recordings and questionnaires at home. As a use case, thirty subjects evaluated two BCI control strategies ""Positive memories""and ""Music imagery""by using a four-channel electroencephalogram (EEG) with MYND. Neural activity in both control strategies could be decoded with an average offline accuracy of 68.5% and 64.0% across all days. © 2020 Owner/Author.",Bci; Brain-computer interface; Eeg; Electroencephalography; Self-supervised study; Smartphone application; Unsupervised study,"1071, 1084",,UIST 2020 - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology,Conference Paper,Scopus
536,,"FLAVA: Find, Localize, Adjust and Verify to Annotate LiDAR-based Point Clouds","Wang T., He C., Wang Z., Shi J., Lin D.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095609175&doi=10.1145%2f3379350.3416176&partnerID=40&md5=18d7bc1027dbedfd915cbf978e4b7c3a,10.1145/3379350.3416176,"Recent years have witnessed the rapid progress of perception algorithms on top of LiDAR, a widely adopted sensor for autonomous driving systems. These LiDAR-based solutions are typically data hungry, requiring a large amount of data to be labeled for training and evaluation. However, annotating this kind of data is very challenging due to the sparsity and irregularity of point clouds and more complex interaction involved in this procedure. To tackle this problem, we propose FLAVA, a systematic approach to minimizing human interaction in the annotation process. Specifically, we divide the annotation pipeline into four parts: find, localize, adjust and verify. In addition, we carefully design the UI for different stages of the annotation procedure, thus keeping the annotators to focus on the aspects that are most important to each stage. Furthermore, our system also greatly reduces the amount of interaction by introducing a lightweight yet effective mechanism to propagate the annotation results. Experimental results show that our method can remarkably accelerate the procedure and improve the annotation quality. © 2020 Owner/Author.",autonomous driving; lidar; point cloud annotation; scene understanding,"31, 33",,UIST 2020 - Adjunct Publication of the 33rd Annual ACM Symposium on User Interface Software and Technology,Conference Paper,Scopus
537,,Multi-modal repairs of conversational breakdowns in task-oriented dialogs,"Li T.J.-J., Chen J., Xia H., Mitchell T.M., Myers B.A.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095581941&doi=10.1145%2f3379337.3415820&partnerID=40&md5=0cb15c921305805615772d9e7c6f64b0,10.1145/3379337.3415820,"A major problem in task-oriented conversational agents is the lack of support for the repair of conversational breakdowns. Prior studies have shown that current repair strategies for these kinds of errors are often ineffective due to: (1) the lack of transparency about the state of the system's understanding of the user's utterance; and (2) the system's limited capabilities to understand the user's verbal attempts to repair natural language understanding errors. This paper introduces SOVITE, a new multi-modal speech plus direct manipulation interface that helps users discover, identify the causes of, and recover from conversational breakdowns using the resources of existing mobile app GUIs for grounding. SOVITE displays the system's understanding of user intents using GUI screenshots, allows users to refer to third-party apps and their GUI screens in conversations as inputs for intent disambiguation, and enables users to repair breakdowns using direct manipulation on these screenshots. The results from a remote user study with 10 users using SOVITE in 7 scenarios suggested that SOVITE's approach is usable and effective. © 2020 Owner/Author.",Breakdown repair; Chatbots; Conversational breakdown; Conversational interfaces; Disambiguation; Grounding in communication; Gui semantics; Instructable agents,"1094, 1107",,UIST 2020 - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology,Conference Paper,Scopus
538,,Aspect Based Sentiment Analysis with Self-Attention and Gated Convolutional Networks,"Yang J., Yang J.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096180459&doi=10.1109%2fICSESS49938.2020.9237640&partnerID=40&md5=dc841c033b7b7d4f7319afa757720b9d,10.1109/ICSESS49938.2020.9237640,"Aspect based sentiment analysis (ABSA) is a fine-grained sentiment analysis task, whose main goal is to identify the sentiment polarity of an aspect in a sentence. A sentence may contain many different aspects, each of which may have different sentiment polarities. Based on the current researches in this area, ABSA can be divided into two subtasks: aspect-category sentiment analysis (ACSA) and aspect-term sentiment analysis (ATSA). In the past, more commonly used method is to adopt the time serial algorithm such as Long Short-Term Memory (LSTM) or Recurrent Neural Network (RNN), which usually needs more training time and has complex structures. Moreover, many previous models lack the abilities to effectively learn the internal structure features of sentences. For ABSA, sometimes the sentence structure may significantly affect the final classification results. However, we found the excellent performance of self-attention algorithm and gating mechanism in some other related researches. Therefore, to solve the problems above, we build a new model based on gating mechanism, combined with convolutional neural networks (CNN) and self-attention mechanism. First, we use self-attention to extract the structural feature of the input, and integrate it with the features of the original sentence extracted by CNN. On such basis, we further combine the aspect-category or aspect-term of the input sentence to form the final sentiment feature. Experiments on SemEval datasets show the performance of our models and the effectiveness of the model is proved. © 2020 IEEE.",aspect based sentiment analysis; convolutional neural networks; deep learning; gating mechanism; self-Attention,"146, 149",,"Proceedings of the IEEE International Conference on Software Engineering and Service Sciences, ICSESS",Conference Paper,Scopus
539,,Transfer Learning on Natural YES/NO Questions,"Yin H., Zhou F., Li X., Zheng J., Liu K.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096158659&doi=10.1109%2fICSESS49938.2020.9237654&partnerID=40&md5=dc31264b8eaad406f8f75885e7ba3717,10.1109/ICSESS49938.2020.9237654,"Although neural network approaches achieve remarkable success on QA, many of them struggle to answer questions that require retrieving relevant factual information from the given paragraph. Inferring from a given paragraph and answering the question to be true or false is an essential part of natural language understanding. To improve the model's inferring ability on Natural YES/NO Question, we provide a simple and effective method. First, we find the tasks related to main task Natural YES/NO Question to fine-tune the model by multi-task learning, then we fine-tune the model on the main task. Results on dataset BoolQ show this method is competitive with other recently published methods, which means transferring from the related datasets through multi-task learning in first stage can save more beneficial information about main task Natural YES/NO. Further analysis show that this method can not only have benefit in this task, it also can be used to other tasks. © 2020 IEEE.",component; Multi-Task Learning; Transfer Learning; YES/NO QA,"531, 536",,"Proceedings of the IEEE International Conference on Software Engineering and Service Sciences, ICSESS",Conference Paper,Scopus
540,,Beyond accuracy: ROI-driven data analytics of empirical data,"Deshpande G., Ruhe G.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095828857&doi=10.1145%2f3382494.3422159&partnerID=40&md5=6d085769086b305384a08124301423b7,10.1145/3382494.3422159,"Background: The unprecedented access to data has rendered a remarkable opportunity to analyze, understand, and optimize the investigation approaches in almost all the areas of (Empirical) Software Engineering. However, data analytics is time and effort consuming, thus, expensive, and not automatically valuable. Objective: This vision paper demonstrates that it is crucial to consider Return-on-Investment (ROI) when performing Data Analytics. Decisions on ""How much analytics is needed""? are hard to answer. ROI could guide for decision support on the What?, How?, and How Much? analytics for a given problem. Method: The proposed conceptual framework is validated through two empirical studies that focus on requirements dependencies extraction in the Mozilla Firefox project. The two case studies are (i) Evaluation of fine-tuned BERT against Naive Bayes and Random Forest machine learners for binary dependency classification and (ii) Active Learning against passive Learning (random sampling) for REQUIRES dependency extraction. For both the cases, their analysis investment (cost) is estimated, and the achievable benefit from DA is predicted, to determine a break-even point of the investigation. Results: For the first study, fine-tuned BERT performed superior to the Random Forest, provided that more than 40% of training data is available. For the second, Active Learning achieved higher F1 accuracy within fewer iterations and higher ROI compared to Baseline (Random sampling based RF classifier). In both the studies, estimate on, How much analysis likely would pay off for the invested efforts?, was indicated by the break-even point. Conclusions: Decisions for the depth and breadth of DA of empirical data should not be made solely based on the accuracy measures. Since ROI-driven Data Analytics provides a simple yet effective direction to discover when to stop further investigation while considering the cost and value of the various types of analysis, it helps to avoid over-analyzing empirical data. © 2020 IEEE Computer Society. All rights reserved.",BERT; Data Analytics; Dependency extraction; Mozilla; Requirements Engineering; Return-on-Investment,,,International Symposium on Empirical Software Engineering and Measurement,Conference Paper,Scopus
541,,Perf-AL: Performance prediction for configurable software through adversarial learning,"Shu Y., Sui Y., Zhang H., Xu G.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095809113&doi=10.1145%2f3382494.3410677&partnerID=40&md5=cc3d70a089f48e2d3ed22366663e8bd4,10.1145/3382494.3410677,"Context: Many software systems are highly configurable. Different configuration options could lead to varying performances of the system. It is difficult to measure system performance in the presence of an exponential number of possible combinations of these options. Goal: Predicting software performance by using a small configuration sample. Method: This paper proposes PERF-AL to address this problem via adversarial learning. Specifically, we use a generative network combined with several different regularization techniques (L1 regularization, L2 regularization and a dropout technique) to output predicted values as close to the ground truth labels as possible. With the use of adversarial learning, our network identifies and distinguishes the predicted values of the generator network from the ground truth value distribution. The generator and the discriminator compete with each other by refining the prediction model iteratively until its predicted values converge towards the ground truth distribution. Results:We argue that (i) the proposed method can achieve the same level of prediction accuracy, but with a smaller number of training samples. (ii) Our proposed model using seven real-world datasets show that our approach outperforms the state-of-the-art methods. This help to further promote software configurable performance. Conclusion: Experimental results on seven public real-world datasets demonstrate that PERF-AL outperforms state-of-the-art software performance prediction methods. © 2020 IEEE Computer Society. All rights reserved.",Adversarial learning; Configurable systems; Regularization; Software performance prediction,,,International Symposium on Empirical Software Engineering and Measurement,Conference Paper,Scopus
542,,Research on Mass News Classification Algorithm Based on Spark,"Wang J., Ji F., Liu B., Wang N., Yin H., Zhang F.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105408257&doi=10.1109%2fICBASE51474.2020.00093&partnerID=40&md5=b3c07107920c6d2defd2631614ad4005,10.1109/ICBASE51474.2020.00093,"In recent years, with the explosion of the number of Internet news, people pay more and more attention to how to classify the mass of news. Therefore, this paper studies the mass news classification algorithm based on Spark, aiming at the problem of how to classify mass news data quickly and efficiently. In this paper, a large amount of news text is segmented based on Jieba segmentation tool, and several versions of stop words list are combined to remove stop words. Secondly, on the basis of traditional convolutional neural network, this paper proposes a news classification algorithm based on the combination of pre-trained Word2vec and improved CNN. In addition, the classification algorithm proposed in this paper is parallelized based on Spark, which improves the speed of mass news classification. In this paper, the standard data sets are used to compare and experiment the proposed news classification algorithm. The experimental results show that compared with the traditional algorithm, the news classification optimization algorithm designed in this paper has obvious improvement in multiple evaluation indexes such as accuracy, recall and F1. In addition, after parallel design of the algorithm proposed in this paper based on Spark, compared with the serial algorithm, the speed improvement effect is also more significant. © 2020 IEEE.",Classification; CNN; Jieba; Spark; Word2Vec,"408, 414",,"Proceedings - 2020 International Conference on Big Data and Artificial Intelligence and Software Engineering, ICBASE 2020",Conference Paper,Scopus
543,,Research on Real-time and Efficient Alignment Algorithm for License Plate Detection Based on Neural Network A lightweight network for license plate detection and alignment,"Li P., Wang K., Li X., Xu M.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105385767&doi=10.1109%2fICBASE51474.2020.00082&partnerID=40&md5=1ee16fe3bd94f0d9f888d9e0c7e34a88,10.1109/ICBASE51474.2020.00082,"As one of the main identification marks of vehicles, vehicle license plate plays an important role in intelligent traffic management, and vehicle license plate detection and recognition is also a hot topic in recent years. Low accuracy of traditional method for license plate detection under the natural scene, the accuracy is higher based on the deep learning of license plate detection, but the real-time performance is poorer, for distortion correction of license plate alignment with special network return the four vertices of license plate, then the perspective transform[1] correct license plate area, the process to further increase the computational complexity. For existing detection and license plate four vertices of regression model, such as MTCNN[2], but for the big input picture, MTCNN's image pyramid has more computation, so this paper proposes a mobilenet[3] backbone network and sampling at the three different scale layer on the license plate of the network to forecast the results, the network reference yolov4[4] design ideas, so called mobilenet - yolov4, in addition to regress the category of the license plate and boundary box. Four vertices regress mission add to the license plate detection. The integration of these three tasks into a simple network improves the multiplexing of the network layer and reduces the computation of subsequent license plate alignment. Experiments show that the algorithm has higher real-time and accuracy, and has higher research and application value. © 2020 IEEE.",component; mobilenet; MTCNN; perspective transform,"359, 362",,"Proceedings - 2020 International Conference on Big Data and Artificial Intelligence and Software Engineering, ICBASE 2020",Conference Paper,Scopus
544,,Insulator defect detection based on YOLO and SPP-Net,"Zhang X., Zhang Y., Hu M., Ju X.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105376463&doi=10.1109%2fICBASE51474.2020.00092&partnerID=40&md5=ab150b2d6c093dbb2cbdb1d26901ec4f,10.1109/ICBASE51474.2020.00092,"With the continuous construction of smart grids, drones are gradually being used in routine maintenance and inspection of transmission lines. Aiming at the problem of few defect insulator samples and complex background, a detection method of defect insulator based on YOLO and SPP-Net is proposed. This paper uses the original samples to train the YOLOv5s model, and the cropped samples to fine-tune the classification network composed of the pre-trained VGG16 network and SPP-Net, and then cascade the two models. After positioning and cutting the insulators, YOLOv5s sent them to the classification network for defect detection. The final insulator detection accuracy reached 89%. © 2020 IEEE.",defect detection; fine tuning; insulator; transfer learning; YOLO,"403, 407",,"Proceedings - 2020 International Conference on Big Data and Artificial Intelligence and Software Engineering, ICBASE 2020",Conference Paper,Scopus
545,,Evaluating Deep Learning Classification Reliability in Android Malware Family Detection,"Iadarola G., Martinelli F., Mercaldo F., Santone A.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099871466&doi=10.1109%2fISSREW51248.2020.00082&partnerID=40&md5=fbcdeb62899d45511a175f359fe32e8f,10.1109/ISSREW51248.2020.00082,"Artificial intelligence techniques are nowadays widespread to perform a great number of classification tasks. One of the biggest controversies regarding the adoption of these techniques is related to their use as a 'black box' i.e., the security analyst must trust the prediction without the possibility to understand the reason why the classifier made a certain choice. In this paper we propose a malicious family detector based on deep learning, providing a mechanism aimed to assess the prediction reliability. The proposed method obtains an accuracy of 0.98 in Android family identification. Moreover, we show how the proposed method can assist the security analyst to interpret the output classification and verify the prediction reliability by exploiting activation maps. © 2020 IEEE.",Android; artificial intelligence; deep learning; explainability; interpretability; malware; reliability; security,"255, 260",,"Proceedings - 2020 IEEE 31st International Symposium on Software Reliability Engineering Workshops, ISSREW 2020",Conference Paper,Scopus
546,,Multi-label Classification of Commit Messages using Transfer Learning,"Sarwar M.U., Zafar S., Mkaouer M.W., Walia G.S., Malik M.Z.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099867731&doi=10.1109%2fISSREW51248.2020.00034&partnerID=40&md5=76172084c9f7e2b6018b200bc52d67e6,10.1109/ISSREW51248.2020.00034,"Commit messages are used in the industry by developers to annotate changes made to the code. Accurate classification of these messages can help monitor the software evolution process and enable better tracking for various industrial stakeholders. In this paper, we present a state of the art method for commit message classification into categories as per Swanson's maintenance activities i.e. 'Corrective', 'Perfective', and 'Adaptive'. This is a challenging task because not all commit messages are well written and informative. Existing approaches rely on keyword-based techniques to solve this problem. However, these approaches are oblivious to the full language model and do not recognize the contextual relationship between words. State of the art methodology in Natural Language Processing (NLP), is to train a context-aware neural network (Transformer) on a very large data set that encompasses the entire language and then fine-tunes it for a specific task. In this way, the model can learn the language, pay attention to the context, and then transfer that knowledge for better performance at the specific task. We use an off-the-shelf neural network called DistilBERT and fine-tune it for commit message classification task. This step is non-trivial because programming languages and commit messages have unique keywords, jargon, and idioms. This paper presents our effort in training this model and constructing the data set for this task. We describe the rules used to construct the data set. We validate our approach on industrial projects from GitHub, such as Kubernetes, Linux, TensorFlow, Spark, TypeScript, and PyTorch. We were able to achieve 87% F1-score for the commit message classification task, which is an order of magnitude accurate than previous studies. © 2020 IEEE.",commit message classification; software maintenance; software quality,"37, 42",,"Proceedings - 2020 IEEE 31st International Symposium on Software Reliability Engineering Workshops, ISSREW 2020",Conference Paper,Scopus
547,,Migrating Large Deep Learning Models to Serverless Architecture,"Chahal D., Ojha R., Ramesh M., Singhal R.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099848738&doi=10.1109%2fISSREW51248.2020.00047&partnerID=40&md5=ade5479a5291ae6c5742fdd38d779925,10.1109/ISSREW51248.2020.00047,"Serverless computing platform is emerging as a solution for event-driven artificial intelligence applications. Function-as-a-Service (FaaS) using serverless computing paradigm provides high performance and low cost solutions for deploying such applications on cloud while minimizing the operational logic. Using FaaS for efficient deployment of complex applications, such as natural language processing (NLP) and image processing, containing large deep learning models will be an advantage. However, constrained resources and stateless nature of FaaS offers numerous challenges while deploying such applications. In this work, we discuss the methodological suggestions and their implementation for deploying pre-trained large size machine learning and deep learning models on FaaS. We also evaluate the performance and deployment cost of an enterprise application, consisting of suite of deep vision preprocessing algorithms and models, on VM and FaaS platform. Our evaluation shows that migration from monolithic to FaaS platform significantly improves the performance of the application at a reduced cost. © 2020 IEEE.",AI; cloud; FaaS; serverless,"111, 116",,"Proceedings - 2020 IEEE 31st International Symposium on Software Reliability Engineering Workshops, ISSREW 2020",Conference Paper,Scopus
548,,Machine Translation Testing via Pathological Invariance,Gupta S.,2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098588789&doi=10.1145%2f3377812.3382162&partnerID=40&md5=e07e33be9be7ba536312822f775d7827,10.1145/3377812.3382162,"Due to the rapid development of deep neural networks, in recent years, machine translation software has been widely adopted in people's daily lives, such as communicating with foreigners or understanding political news from the neighbouring countries. However, machine translation software could return incorrect translations because of the complexity of the underlying network. To address this problem, we introduce a novel methodology called PaInv for validating machine translation software. Our key insight is that sentences of different meanings should not have the same translation (i.e., pathological invariance). Specifically, PaInv generates syntactically similar but semantically different sentences by replacing one word in the sentence and filter out unsuitable sentences based on both syntactic and semantic information. We have applied PaInv to Google Translate using 200 English sentences as input with three language settings: English?Hindi, English?Chinese, and English?German. PaInv can accurately find 331 pathological invariants in total, revealing more than 100 translation errors. © 2020 ACM.",identical translation; machine translation; pathological invariance; Testing,"107, 109",,"Proceedings - 2020 ACM/IEEE 42nd International Conference on Software Engineering: Companion, ICSE-Companion 2020",Conference Paper,Scopus
549,,Variability Aware Requirements Reuse Analysis,Abbas M.,2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098580778&doi=10.1145%2f3377812.3381399&partnerID=40&md5=a967c2009b413aeb69c69f277e065630,10.1145/3377812.3381399,"Problem: The goal of a software product line is to aid quick and quality delivery of software products, sharing common features. Effectively achieving the above-mentioned goals requires reuse analysis of the product line features. Existing requirements reuse analysis approaches are not focused on recommending product line features, that can be reused to realize new customer requirements. Hypothesis: Given that the customer requirements are linked to product line features' description satisfying them: Then the customer requirements can be clustered based on patterns and similarities, preserving the historic reuse information. New customer requirements can be evaluated against existing customer requirements and reuse of product line features can be recommended. Contributions: We treated the problem of feature reuse analysis as a text classification problem at the requirements-level. We use Natural Language Processing and clustering to recommend reuse of features based on similarities and historic reuse information. The recommendations can be used to realize new customer requirements. © 2020 ACM.",product line; requirements; similarities; software reuse; variability,"190, 193",,"Proceedings - 2020 ACM/IEEE 42nd International Conference on Software Engineering: Companion, ICSE-Companion 2020",Conference Paper,Scopus
550,,SimilarAPI: Mining Analogical APIs for Library Migration,Chen C.,2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098568735&doi=10.1145%2f3377812.3382140&partnerID=40&md5=5d732eb9aab8c2602043c41f09dc4a9d,10.1145/3377812.3382140,"Establishing API mappings between libraries is a prerequisite step for library migration tasks. Manually establishing API mappings is tedious due to the large number of APIs to be examined, and existing methods based on supervised learning requires unavailable already-ported or functionality similar applications. Therefore, we propose an unsupervised deep learning based approach to embed both API usage semantics and API description (name and document) semantics into vector space for inferring likely analogical API mappings between libraries. We implement a proof-of-concept website SimilarAPI (https://similarapi.appspot.com) which can recommend analogical APIs for 583,501 APIs of 111 pairs of analogical Java libraries with diverse functionalities. Video: Https://youtu.be/EAwD6l24vLQ © 2020 ACM.",Analogical API; Skip thoughts; Word embedding,"37, 40",,"Proceedings - 2020 ACM/IEEE 42nd International Conference on Software Engineering: Companion, ICSE-Companion 2020",Conference Paper,Scopus
551,,FuRong: Fusing Report of Automated Android Testing on Multi-Devices *,"Tian Y., Yu S., Fang C., Li P.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098559688&doi=10.1145%2f3377812.3382138&partnerID=40&md5=590c9c4a90477d5485fe77b7146841b2,10.1145/3377812.3382138,"Automated testing has been widely used to ensure the quality of Android applications. However, incomprehensible testing results make it difficult for developers to understand and fix potential bugs. This paper proposes FuRong, a novel tool, to fuse bug reports of high-readability and strong-guiding-ability via analyzing the automated testing results on multi-devices. FuRong builds a bug model with complete context information, such as screenshots, operation sequences, and logs from multi-devices, and then leverages pretrained Decision Tree classifier (with 18 bug category labels) to classify bugs. FuRong deduplicates the classified bugs via Levenshtein distance and finally generates the easy-to-understand report, not only context information of bugs, where possible causes and fix suggestions for each bug category are also provided. An empirical study of 8 open-source Android applications with automated testing on 20 devices has been conducted, the results show the effectiveness of FuRong, which has a bug classification precision of 93.4% and a bug classification accuracy of 87.9%. Video URL: Https://youtu.be/LUkFTc32B6k © 2020 ACM.",Android Testing; Automated Testing; Bug Classification; Bug Report,"49, 52",,"Proceedings - 2020 ACM/IEEE 42nd International Conference on Software Engineering: Companion, ICSE-Companion 2020",Conference Paper,Scopus
552,,EvalDNN: A Toolbox for Evaluating Deep Neural Network Models,"Tian Y., Zeng Z., Wen M., Liu Y., Kuo T.-Y., Cheung S.-C.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098536021&doi=10.1145%2f3377812.3382133&partnerID=40&md5=95cb2dd165036d2fbc8653a6176a9912,10.1145/3377812.3382133,"Recent studies have shown that the performance of deep learning models should be evaluated using various important metrics such as robustness and neuron coverage, besides the widely-used prediction accuracy metric. However, major deep learning frameworks currently only provide APIs to evaluate a model's accuracy. In order to comprehensively assess a deep learning model, framework users and researchers often need to implement new metrics by themselves, which is a tedious job. What is worse, due to the large number of hyper-parameters and inadequate documentation, evaluation results of some deep learning models are hard to reproduce, especially when the models and metrics are both new.To ease the model evaluation in deep learning systems, we have developed EvalDNN, a user-friendly and extensible toolbox supporting multiple frameworks and metrics with a set of carefully designed APIs. Using EvalDNN, evaluation of a pre-trained model with respect to different metrics can be done with a few lines of code. We have evaluated EvalDNN on 79 models from TensorFlow, Keras, GluonCV, and PyTorch. As a result of our effort made to reproduce the evaluation results of existing work, we release a performance benchmark of popular models, which can be a useful reference to facilitate future research. The tool and benchmark are available at https://github.com/yqtianust/EvalDNN and https://yqtianust.github.io/EvalDNN-benchmark/, respectively. A demo video of EvalDNN is available at: Https://youtu.be/v69bNJN2bJc. © 2020 ACM.",Deep Learning Model; Evaluation,"45, 48",,"Proceedings - 2020 ACM/IEEE 42nd International Conference on Software Engineering: Companion, ICSE-Companion 2020",Conference Paper,Scopus
553,,Generating adversarial examples for sentiment classifier of chinese sentences,"Zheng Y., Cui Z., Xu Y., Li H., Jiang Z.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098324221&doi=10.1109%2fISSSR51244.2020.00013&partnerID=40&md5=bc8af18100fdd867ed91b12afa343c0f,10.1109/ISSSR51244.2020.00013,"Studies have shown that deep learning models are vulnerable to adversarial examples, which cause incorrect predictions by adding imperceptible perturbations into normal inputs. The same characteristic goes for the sentiment orientation classification models. If the input text for the models contains perturbing information such as typos or special symbols, the inputs could misled the models. The adversarial examples reflect the diversity of text features, and the flaws of the sentiment orientation classification models can be found by adversarial attacks. The adversarial examples can be used to train the sentiment orientation classification models to improve the robustness of the model. This paper proposes to generate adversarial examples of Chinese sentences by replacing one of the characters in the word with similar Chinese characters in a black-box manner. The experimental results show that the adversarial examples generated by this method cost less and visually closer to the normal text. © 2020 IEEE.",Adversarial examples; Robustness; Sentiment tendency; Text classification,"27, 32",,"Proceedings - 2020 6th International Symposium on System and Software Reliability, ISSSR 2020",Conference Paper,Scopus
554,,Fault injection to generate failure data for failure prediction: A case study,"Campos J.R., Costa E.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097350528&doi=10.1109%2fISSRE5003.2020.00020&partnerID=40&md5=f23ca52c87d68f7f014022862f21caaa,10.1109/ISSRE5003.2020.00020,"Due to the complexity of modern software, identifying every fault before deployment is extremely difficult or even not possible. Such residual faults can ultimately lead to failures, often incurring considerable risks or costs. Online Failure Prediction (OFP) is a fault-tolerance technique that attempts to predict the occurrence of failures in the near future and thus prevent/mitigate their consequences. Combined with recent technological developments, Machine Learning (ML) has been successfully used to create predictive models for OFP. However, as failures are rare events, failure data are often not available for building accurate models. Although fault injection has been accepted as a viable solution to generate realistic failure data, fault injectors are difficult to implement/update and thus research on Operating System (OS)-level OFP has become stale, with most works using data from outdated OSs. In this paper, we conduct a comprehensive fault injection campaign on an up-to-date Linux kernel and thoroughly study its behavior in the presence of faults. We then transform the data to explore and assess the predictive performance of various ML techniques for OFP. Finally, we study the influence of different OFP parameters (i.e., lead-time, prediction-window) and compare the results with existing related work. Results suggest that the various failures observed can be grouped into categories that can then be accurately predicted and distinguished by diverse ML models. ©2020 IEEE.",Dependability; Failure Prediction; Fault Injection; Machine Learning,"115, 126",,"Proceedings - International Symposium on Software Reliability Engineering, ISSRE",Conference Paper,Scopus
555,,Logtransfer: Cross-system log anomaly detection for software systems with transfer learning,"Chen R., Zhang S., Li D., Zhang Y., Guo F., Meng W., Pei D., Zhang Y., Chen X., Liu Y.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097348980&doi=10.1109%2fISSRE5003.2020.00013&partnerID=40&md5=ccb8a1312db446e55edb205852181b17,10.1109/ISSRE5003.2020.00013,"System logs, which describe a variety of events of software systems, are becoming increasingly popular for anomaly detection. However, for a large software system, current unsupervised learning-based methods are suffering from low accuracy due to the high diversity of logs, while the supervised learning methods are nearly infeasible to be used in practice because it is time-consuming and labor-intensive to obtain sufficient labels for different types of software systems. In this paper, we propose a novel framework, LogTransfer, which applies transfer learning to transfer the anomalous knowledge of one type of software system (source system) to another (target system). We represent every template using Glove, which considers both global word co-occurrence and local context information, to address the challenge that different types of software systems are different in log syntax while the semantics of logs should be reserved. We apply an LSTM network to extract the sequential patterns of logs, and propose a novel transfer learning method sharing fully connected networks between source and target systems, to minimize the impact of noises in anomalous log sequences. Extensive experiments have been performed on switch logs of different vendors collected from a top global cloud service provider. LogTransfer achieves an averaged 0.84 F1-score and outperforms the state-of-the-art supervised and unsupervised logbased anomaly detection methods, which are consistent with the experiments conducted on the public HDFS and Hadoop application datasets. © 2020 IEEE Computer Society. All rights reserved.",Anomaly detection; LSTM; System log; Transfer learning; Word embedding,"37, 47",,"Proceedings - International Symposium on Software Reliability Engineering, ISSRE",Conference Paper,Scopus
556,,Swisslog: Robust and unified deep learning based log anomaly detection for diverse faults,"Li X., Chen P., Jing L., He Z., Yu G.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097335606&doi=10.1109%2fISSRE5003.2020.00018&partnerID=40&md5=3775779528eb81831a1ed0311f9f8e60,10.1109/ISSRE5003.2020.00018,"Log-based anomaly detection has been widely studied and achieves a satisfying performance on stable log data. But, the existing approaches still fall short meeting these challenges: 1) Log formats are changing continually in practice in those software systems under active development and maintenance. 2) Performance issues are latent causes that may not be detected by trivial monitoring tools. We thus propose SwissLog, namely a robust and unified deep learning based anomaly detection model for detecting diverse faults. SwissLog targets at those faults resulting in log sequence order changes and log time interval changes. To achieve that, an advanced log parser is introduced. Moreover, the semantic embedding and the time embedding approaches are combined to train a unified attention based Bi- LSTM model to detect anomalies. The experiments on real-world datasets and synthetic datasets show that SwissLog is robust to the changing log data and effective for diverse faults. ©2020 IEEE.",Anomaly detection; BERT; Deep learning; Log parsing,"92, 103",,"Proceedings - International Symposium on Software Reliability Engineering, ISSRE",Conference Paper,Scopus
557,,Cross-project aging-related bug prediction based on joint distribution adaptation and improved subclass discriminant analysis,"Xu B., Zhao D., Jia K., Zhou J., Tian J., Xiang J.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097329519&doi=10.1109%2fISSRE5003.2020.00038&partnerID=40&md5=ba661fa851849938d1e5e64c900b1fe8,10.1109/ISSRE5003.2020.00038,"Software aging, which is caused by Aging-Related Bugs (ARBs), refers to the phenomenon of performance degradation and eventual crash in long running systems. In order to discover and remove ARBs, ARB prediction is proposed. However, due to the low presence and reproducing difficulty of ARBs, it is usually difficult to collect sufficient ARB data within a project. Therefore, cross-project ARB prediction is proposed as a solution to build the target project's ARB predictor by using the labeled data from the source project. A key point for cross-project ARB prediction is to reduce distribution difference between source and target project. However, existing approaches mainly focus on the marginal distribution difference while somehow overlook the conditional distribution difference, and they mainly use random oversampling to alleviate the class imbalance which may lead to overfitting. To address these problems, we propose a new crossproject ARB prediction approach based on Joint Distribution Adaptation (JDA) and Improved Subclass Discriminant Analysis (ISDA), called JDA-ISDA. The key idea of JDA-ISDA is first to use JDA to reduce the marginal distribution and conditional distribution difference jointly and then apply ISDA to alleviate the severe class imbalance problem. A set of experiments are carried out on two large open-source projects with six different machine learning (ML) classifiers. The experimental results demonstrate that compared with the stateof- the-art Transfer Learning based Aging-related bug Prediction (TLAP) and Supervised Representation Learning Approach (SRLA), JDA-ISDA is much more robust to different ML classifiers than TLAP, and the average improvement in terms of the balance value can be achieved up to 31.8%, and JDA-ISDA also outperforms TLAP and SRLA on average when logistic regression is chosen as the classifier for best performance prediction. ©2020 IEEE.",Aging-related bugs; Cross-project ARB prediction; Improved subclass discriminant analysis; Joint distribution adaptation; Software aging,"325, 334",,"Proceedings - International Symposium on Software Reliability Engineering, ISSRE",Conference Paper,Scopus
558,,Autonomous Learning System Towards Mobile Intelligence [面向移动终端智能的自治学习系统],"Xu M.-W., Liu Y.-Q., Huang K., Liu X.-Z., Huang G.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092627288&doi=10.13328%2fj.cnki.jos.006064&partnerID=40&md5=87fefd375f4b7b9983329da07ec44495,10.13328/j.cnki.jos.006064,"How to efficiently deploy machine learning models on mobile devices has drawn a lot of attention in both academia and industry, among which the model training is a critical part. However, with increasingly public attention on data privacy and the recently adopted laws and regulations, it becomes harder for developers to collect training data from users and thus cannot train high-quality models. Researchers have been exploring approaches of training neural networks on decentralized data. Those efforts will be summarized and their limitations be pointed out. To this end, this work presents a novel neural network training paradigm on mobile devices, which distributes all training computations associated with private data on local devices and requires no data to be uploaded in any form. Such training paradigm autonomous learning is named. To deal with two main challenges of autonomous learning, i.e., limited data volume and insufficient computing power available on mobile devices, the first autonomous learning system AutLearn is designed and implemented. It incorporates the cloud (public data, pre-training)-client (private data, transfer learning) cooperation methodology and data augmentation techniques to ensure the model convergence on mobile devices. Furthermore, by utilizing a series of optimization techniques such as model compression, neural network compiler, and runtime cache reuse, AutLearn can significantly reduce the on-client training cost. Two classical scenarios of autonomous learning are implemented based on AutLearn and carried out a set of experiments. The results showed that AutLearn can train the neural networks with comparable or even higher accuracy compared to traditional centralized/federated training mode with privacy preserved. AutLearn can also significantly reduce the computational and energy cost of neural network training on mobile devices. © Copyright 2020, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Distributed system; Edge computing; Machine learning; Mobile computing,"3004, 3018",,Ruan Jian Xue Bao/Journal of Software,Article,Scopus
559,,Categorisation-based approach for predicting the fault-proneness of object-oriented classes in software post-releases,Al Dallal J.,2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092098847&doi=10.1049%2fiet-sen.2019.0326&partnerID=40&md5=b19229537ab6fdea61d4f1074ca9172d,10.1049/iet-sen.2019.0326,"Subsequent releases of a system have common development environments and characteristics. However, prediction models based on within-project data potentially suffer from being based on fault data reported within relatively short maintenance time intervals, which potentially decreases their prediction abilities. In this study, the authors propose an approach that improves the classification performance of models based on within-project data that are applied to predict the fault-proneness of the classes in a software post-release (PR). The proposed approach involves selecting a set of immediate pre-releases and constructing a prediction model based on each pre-release. The PR classes are categorised based on whether they are newly developed or they are reused, with or without modification, from one or more of the selected pre-releases. The prediction models are applied to the PR classes reused from selected pre-releases, and the results are used to construct a fault-proneness prediction model. After applying this prediction model to all PR classes, the fault-proneness results are adjusted by considering the relationship between the prediction results of the individual pre-release models and the actual fault data. They reported an empirical study that shows that the classification performance of the categorisation-based fault-proneness prediction models is considerably better than those constructed using existing approaches. © The Institution of Engineering and Technology 2020",,"525, 534",,IET Software,Article,Scopus
560,,Towards a Deep Learning Model for Vulnerability Detection on Web Application Variants,"Fidalgo A., Medeiros I., Antunes P., Neves N.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091804546&doi=10.1109%2fICSTW50294.2020.00083&partnerID=40&md5=dfece71de110b8d36f13978c448ee00b,10.1109/ICSTW50294.2020.00083,"Reported vulnerabilities have grown significantly over the recent years, with SQL injection (SQLi) being one of the most prominent, especially in web applications. For these, such increase can be explained by the integration of multiple software parts (e.g., various plugins and modules), often developed by different organizations, composing thus web application variants. Machine Learning has the potential to be a great ally on finding vulnerabilities, aiding experts by reducing the search space or even by classifying programs on their own. However, previous work usually does not consider SQLi or utilizes techniques hard to scale. Moreover, there is a clear gap in vulnerability detection with machine learning for PHP, the most popular server-side language for web applications. This paper presents a Deep Learning model able to classify PHP slices as vulnerable (or not) to SQLi. As slices can belong to any variant, we propose the use of an intermediate language to represent the slices and interpret them as text, resorting to well-studied Natural Language Processing (NLP) techniques. Preliminary results of the use of the model show that it can discover SQLi, helping programmers and precluding attacks that would eventually cost a lot to repair. © 2020 IEEE.",deep learning; natural language processing; software security; vulnerability detection; web application vulnerabilities,"465, 476",,"Proceedings - 2020 IEEE 13th International Conference on Software Testing, Verification and Validation Workshops, ICSTW 2020",Conference Paper,Scopus
561,,An Empirical Evaluation of Mutation Operators for Deep Learning Systems,"Jahangirova G., Tonella P.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091004295&doi=10.1109%2fICST46399.2020.00018&partnerID=40&md5=1e714c8e0997552fbb668ef4bd2f24e9,10.1109/ICST46399.2020.00018,"Deep Learning (DL) is increasingly adopted to solve complex tasks such as image recognition or autonomous driving. Companies are considering the inclusion of DL components in production systems, but one of their main concerns is how to assess the quality of such systems. Mutation testing is a technique to inject artificial faults into a system, under the assumption that the capability to expose (kilt) such artificial faults translates into the capability to expose also real faults. Researchers have proposed approaches and tools (e.g., Deep-Mutation and MuNN) that make mutation testing applicable to deep learning systems. However, existing definitions of mutation killing, based on accuracy drop, do not take into account the stochastic nature of the training process (accuracy may drop even when re-training the un-mutated system). Moreover, the same mutation operator might be effective or might be trivial/impossible to kill, depending on its hyper-parameter configuration. We conducted an empirical evaluation of existing operators, showing that mutation killing requires a stochastic definition and identifying the subset of effective mutation operators together with the associated most effective configurations. © 2020 IEEE.",deep learning; mutation; testing,"74, 84",,"Proceedings - 2020 IEEE 13th International Conference on Software Testing, Verification and Validation, ICST 2020",Conference Paper,Scopus
562,,R2D2: A scalable deep learning toolkit for medical imaging segmentation,"Guedria S., De Palma N., Renard F., Vuillerme N.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089296310&doi=10.1002%2fspe.2878&partnerID=40&md5=8c3907cd211f9a4e52b3451e084c11a0,10.1002/spe.2878,"Deep learning has gained a significant popularity in recent years thanks to its tremendous success across a wide range of relevant fields of applications, including medical image analysis domain in particular. Although convolutional neural networks (CNNs) based medical applications have been providing powerful solutions and revolutionizing medicine, efficiently training of CNNs models is a tedious and challenging task. It is a computationally intensive process taking long time and rare system resources, which represents a significant hindrance to scientific research progress. In order to address this challenge, we propose in this article, R2D2, a scalable intuitive deep learning toolkit for medical imaging semantic segmentation. To the best of our knowledge, the present work is the first that aims to tackle this issue by offering a novel distributed versions of two well-known and widely used CNN segmentation architectures [ie, fully convolutional network (FCN) and U-Net]. We introduce the design and the core building blocks of R2D2. We further present and analyze its experimental evaluation results on two different concrete medical imaging segmentation use cases. R2D2 achieves up to 17.5× and 10.4× speedup than single-node based training of U-Net and FCN, respectively, with a negligible, though still unexpected segmentation accuracy loss. R2D2 offers not only an empirical evidence and investigates in-depth the latest published works but also it facilitates and significantly reduces the effort required by researchers to quickly prototype and easily discover cutting-edge CNN configurations and architectures. © 2020 John Wiley & Sons, Ltd.",deep learning; distributed optimization; distributed systems; high-performance computing; medical imaging; semantic segmentation; software engineering,"1966, 1985",,Software - Practice and Experience,Article,Scopus
563,,Morphological autoencoders for apnea detection in respiratory gating radiotherapy,"Abreu M., Fred A., Valente J., Wang C., Plácido da Silva H.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088900129&doi=10.1016%2fj.cmpb.2020.105675&partnerID=40&md5=a5a187382446149b4a0cc50fa4387e13,10.1016/j.cmpb.2020.105675,"Background and Objective: Respiratory gating training is a common technique to increase patient proprioception, with the goal of (e.g.) minimizing the effects of organ motion during radiotherapy. In this work, we devise a system based on autoencoders for classification of regular, apnea and unconstrained breathing patterns (i.e. multiclass). Methods: Our approach is based on morphological analysis of the respiratory signals, using an autoencoder trained on regular breathing. The correlation between the input and output of the autoencoder is used to train and test several classifiers in order to select the best. Our approach is evaluated in a novel real-world respiratory gating biofeedback training dataset and on the Apnea-ECG reference dataset. Results: Accuracies of 95 ± 3.5% and 87 ± 6.6% were obtained for two different datasets, in the classification of breathing and apnea. These results suggest the viability of a generalised model to characterise the breathing patterns under study. Conclusions: Using autoencoders to learn respiratory gating training patterns allows a data-driven approach to feature extraction, by focusing only on the signal's morphology. The proposed system is prone to be used in real-time and could potentially be transferred to other domains. © 2020",Apnea detection; Artificial neural networks; Machine learning; Respiratory gating; Signal processing,,,Computer Methods and Programs in Biomedicine,Article,Scopus
564,,Similarity-based analyses on software applications: A systematic literature review,"Auch M., Weber M., Mandl P., Wolff C.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086137745&doi=10.1016%2fj.jss.2020.110669&partnerID=40&md5=1998b34bc2c21777901a5735c9247122,10.1016/j.jss.2020.110669,"In empirical studies on processes, practices, and techniques of software engineering, automation and machine learning are gaining popularity. In order to extract knowledge from existing software projects, a sort of similarity analysis is often performed using different methodologies, data and metadata. This systematic literature review focuses therefore on existing approaches of similarity-, categorization- and relevance-based analysis on software applications. In total, 136 relevant publications and patents were identified between 2002 and 2019 according to the established inclusion and exclusion criteria, which perform a calculation of software similarity in general or to support certain software engineering phases. © 2020 Elsevier Inc.",Machine learning; Secondary study; Software similarity,,,Journal of Systems and Software,Article,Scopus
565,,A GAN-based image synthesis method for skin lesion classification,"Qin Z., Liu Z., Zhu P., Xue Y.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086068778&doi=10.1016%2fj.cmpb.2020.105568&partnerID=40&md5=966fca81984b19020791b2b06ce6a42e,10.1016/j.cmpb.2020.105568,"Background and Objective: There are many types of skin cancer, and melanoma is the most lethal one. Dermoscopy is an important imaging technique to screen melanoma and other skin lesions. However, Skin lesion classification based on computer-aided diagnostic techniques is a challenging task owing to the scarcity of labeled data and class-imbalanced dataset. It is necessary to apply data augmentation technique based on generative adversarial networks (GANs) to skin lesion classification for helping dermatologists in more accurate diagnostic decisions. Methods: A whole process of using GAN-based data augmentation technology to improve the skin lesion classification performance has been established in this article. First of all, the skin lesion style-based GANs is proposed according to the basic architecture of style-based GANs. The proposed model modifies the structure of style control and noise input in the original generator, adjusts both the generator and discriminator to efficiently synthesize high-quality skin lesion images. As for image classification, the classifier is constructed on the pretrained deep neural network using transfer learning method. The synthetic images from the proposed skin lesion style-based GANs are finally added to the training set to help train the classifier for better classification performance. Results: The proposed skin lesion style-based GAN has been evaluated by Inception Score (IS), Fréchet Inception Distance (FID), Precision and Recall, and is superior to other compared GAN models in these quantitative evaluation metrics. By adding the synthesized images to the training set, the main classification indicators like accuracy, sensitivity, specificity, average precision and balanced multiclass accuracy are 95.2%, 83.2%, 74.3%, 96.6% and 83.1% on the dataset of International Skin Imaging Collaboration (ISIC) 2018 Challenge, which have been improved by 1.6%, 24.4%, 3.6%, 23.2% and 5.6% respectively compared to the CNN model. Conclusions: The proposed skin lesion style-based GANs can generate high-quality skin lesion images efficiently, leading to the performance improvement of the classification model. This work provides a valuable reference for medical image analysis based on deep learning. © 2020 Elsevier B.V.",Data augmentation; Generative adversarial networks; Image synthesis; Skin lesion classification; Transfer learning,,,Computer Methods and Programs in Biomedicine,Article,Scopus
566,,COVID-19 identification in chest X-ray images on flat and hierarchical classification scenarios,"Pereira R.M., Bertolini D., Teixeira L.O., Silla C.N., Jr., Costa Y.M.G.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085953358&doi=10.1016%2fj.cmpb.2020.105532&partnerID=40&md5=1547877932df40d6fd5bcc102fcb421c,10.1016/j.cmpb.2020.105532,"Background and Objective:The COVID-19 can cause severe pneumonia and is estimated to have a high impact on the healthcare system. Early diagnosis is crucial for correct treatment in order to possibly reduce the stress in the healthcare system. The standard image diagnosis tests for pneumonia are chest X-ray (CXR) and computed tomography (CT) scan. Although CT scan is the gold standard, CXR are still useful because it is cheaper, faster and more widespread. This study aims to identify pneumonia caused by COVID-19 from other types and also healthy lungs using only CXR images. Methods:In order to achieve the objectives, we have proposed a classification schema considering the following perspectives: i) a multi-class classification; ii) hierarchical classification, since pneumonia can be structured as a hierarchy. Given the natural data imbalance in this domain, we also proposed the use of resampling algorithms in the schema in order to re-balance the classes distribution. We observed that, texture is one of the main visual attributes of CXR images, our classification schema extract features using some well-known texture descriptors and also using a pre-trained CNN model. We also explored early and late fusion techniques in the schema in order to leverage the strength of multiple texture descriptors and base classifiers at once. To evaluate the approach, we composed a database, named RYDLS-20, containing CXR images of pneumonia caused by different pathogens as well as CXR images of healthy lungs. The classes distribution follows a real-world scenario in which some pathogens are more common than others. Results:The proposed approach tested in RYDLS-20 achieved a macro-avg F1-Score of 0.65 using a multi-class approach and a F1-Score of 0.89 for the COVID-19 identification in the hierarchical classification scenario. Conclusions:As far as we know, the top identification rate obtained in this paper is the best nominal rate obtained for COVID-19 identification in an unbalanced environment with more than three classes. We must also highlight the novel proposed hierarchical classification approach for this task, which considers the types of pneumonia caused by the different pathogens and lead us to the best COVID-19 recognition rate obtained here. © 2020 Elsevier B.V.",Chest X-ray; COVID-19; Medical image analysis; Pneumonia; Texture,,,Computer Methods and Programs in Biomedicine,Article,Scopus
567,,Despeckling of clinical ultrasound images using deep residual learning,"Kokil P., Sudharson S.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084947874&doi=10.1016%2fj.cmpb.2020.105477&partnerID=40&md5=62222e4e8703bf942b7950f4f1cb885c,10.1016/j.cmpb.2020.105477,"Background and objective Ultrasound is the non-radioactive imaging modality used in the diagnosis of various diseases related to the internal organs of the body. The presence of speckle noise in ultrasound image (UI) is inevitable and may affect resolution and contrast of the image. Existence of the speckle noise degrades the visual evaluation of the image. The despeckling of UI is a desirable pre-processing step in computer-aided UI based diagnosis systems. Methods This paper proposes a novel method for despeckling UIs using pre-trained residual learning network (RLN). Initially, RLN is trained with pristine and its corresponding noisy images in order to achieve a better performance. The developed method chooses a pre-trained RLN for despeckling UIs with less computational resources. But the training procedure of RLN from scratch is computationally demanding. The pre-trained RLN is a blind despeckling approach and does not require any fine tuning and noise level estimation. The presented approach shows superiority in the removal of speckle noise as compared to the existing state-of-art methods. Results To highlight the effectiveness of the proposed method the pristine images from the Waterloo dataset has been considered. The proposed pre-trained RLN based UI despeckling method resulted in a better peak signal to noise ratio (PSNR) and structural similarity index measure (SSIM) at different speckle noise levels. The no-reference image quality approach is adopted to ensure robustness of the established method for real time UI. From results it is obvious that, the performance of the proposed method is superior than the existing methods in terms of naturalness image quality evaluator (NIQE). Conclusions From the experimental results, it is clear that the proposed method outperforms the existing despeckling methods in terms of both artificially added and naturally occurring speckle images. © 2020",Computer-aided diagnosis; Despeckling; Pre-trained residual learning network; Speckle noise; Ultrasound image,,,Computer Methods and Programs in Biomedicine,Article,Scopus
568,,Computer assisted recognition of breast cancer in biopsy images via fusion of nucleus-guided deep convolutional features,"George K., Sankaran P., K P.J.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084562321&doi=10.1016%2fj.cmpb.2020.105531&partnerID=40&md5=7409207d0f84ea214d9e8f68af7f8289,10.1016/j.cmpb.2020.105531,"Background and objective: Breast cancer is a commonly detected cancer among women, resulting in a high number of cancer-related mortality. Biopsy performed by pathologists is the final confirmation procedure for breast cancer diagnosis. Computer-aided diagnosis systems can support the pathologist for better diagnosis and also in reducing subjective errors. Methods: In the automation of breast cancer analysis, feature extraction is a challenging task due to the structural diversity of the breast tissue images. Here, we propose a nucleus feature extraction methodology using a convolutional neural network (CNN), ‘NucDeep’, for automated breast cancer detection. Non-overlapping nuclei patches detected from the images enable the design of a low complexity CNN for feature extraction. A feature fusion approach with support vector machine classifier (FF + SVM) is used to classify breast tumor images based on the extracted CNN features. The feature fusion method transforms the local nuclei features into a compact image-level feature, thus improving the classifier performance. A patch class probability based decision scheme (NucDeep + SVM + PD) for image-level classification is also introduced in this work. Results: The proposed framework is evaluated on the publicly available BreaKHis dataset by conducting 5 random trials with 70-30 train-test data split, achieving average image level recognition rate of 96.66 ± 0.77%, 100% specificity and 96.21% sensitivity. Conclusion: It was found that the proposed NucDeep + FF + SVM model outperforms several recent existing methods and reveals a comparable state of the art performance even with low training complexity. As an effective and inexpensive model, the classification of biopsy images for breast tumor diagnosis introduced in this research will thus help to develop a reliable support tool for pathologists. © 2020 Elsevier B.V.",Breast cancer; Computer aided diagnosis (CAD); Convolutional neural network; Deep learning; Feature fusion; Histopathology; Image processing; Support vector machine,,,Computer Methods and Programs in Biomedicine,Article,Scopus
569,,Web-based fully automated cephalometric analysis by deep learning,"Kim H., Shim E., Park J., Kim Y.-J., Lee U., Kim Y.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084262931&doi=10.1016%2fj.cmpb.2020.105513&partnerID=40&md5=8aaceeffce95605c214fde8736aceb89,10.1016/j.cmpb.2020.105513,"Background and Objective: An accurate lateral cephalometric analysis is vital in orthodontic diagnosis. Identification of anatomic landmarks on lateral cephalograms is tedious, and errors may occur depending on the doctor's experience. Several attempts have been made to reduce this time-consuming process by automating the process through machine learning; however, they only dealt with a small amount of data from one institute. This study aims to develop a fully automated cephalometric analysis method using deep learning and a corresponding web-based application that can be used without high-specification hardware. Methods: We built our own dataset comprising 2,075 lateral cephalograms and ground truth positions of 23 landmarks from two institutes and trained a two-stage automated algorithm with a stacked hourglass deep learning model specialized for detecting landmarks in images. Additionally, a web-based application with the proposed algorithm for fully automated cephalometric analysis was developed for better accessibility regardless of the user's computer hardware, which is essential for a deep learning-based method. Results: The algorithm was evaluated with datasets from various devices and institutes, including a widely used open dataset and achieved 1.37 ± 1.79 mm of point-to-point errors with ground truth positions for 23 cephalometric landmarks. Based on the predicted positions, anatomical types of the subjects were automatically classified and compared with the ground truth, and the automated algorithm achieved a successful classification rate of 88.43%. Conclusions: We expect that this fully automated cephalometric analysis algorithm and the web-based application can be widely used in various medical environments to save time and effort for manual marking and diagnosis. © 2020",Automated landmark detection; Deep learning; Fully automated cephalometry; Stacked hourglass network; Web-based application,,,Computer Methods and Programs in Biomedicine,Article,Scopus
570,,Syntax-aware Neural Semantic Role Labeling for Morphologically Rich Languages,"Vasic D., Vasic M.K.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096593330&doi=10.23919%2fSoftCOM50211.2020.9238179&partnerID=40&md5=d205becfa0c8f1a64c90587592d5d5ac,10.23919/SoftCOM50211.2020.9238179,"Semantic Role Labeling (SRL) is one of the most challenging tasks in Natural Language Processing (NLP). SRL is a task that consists of predicate identification, argument identification and argument classification. In this article we present novel approach for argument classification that is based on deep neural network architecture. Traditional discrete based SRL relies heavily on feature engineering that uses syntactic structures in contrast to deep learning approaches that encode whole sentences without taking into account syntactic features. We present an approach that uses combination of syntactic features and external word representations from FastText. The advantages of using FastText embeddings is the generation of better vector representations for rare words and FastText gives better results for words that are not within the dictionary. These attributes for vector representations give good results for morphologically rich languages. Most of the SRL approaches today are trained on resource rich languages. In this article we present novel neural architecture for SRL that is suitable for resource poor morphology rich languages. Experiments on hr500k corpus shows that our syntax-aware approach shows competitive results for argument classification. We present architecture for argument classification that is based on Bidirectional Long-Short Term Memory (Bi-LSTM) and Conditional Random Field (CRF) decoding for finding optimal sequence. Our approach showed results that are very close to benchmark results with F1 score of 72%. © 2020 University of Split, FESB.",deep learning; morphologically rich languages; semantic parsing; semantic role labeling,,,"2020 28th International Conference on Software, Telecommunications and Computer Networks, SoftCOM 2020",Conference Paper,Scopus
571,,Detecting Leaf Plant Diseases Using Deep Learning: A Review,"Muresan H.B., Coroiu A.M., Calin A.D.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096516341&doi=10.23919%2fSoftCOM50211.2020.9238318&partnerID=40&md5=377abef7de5abbf55fb727b737eee690,10.23919/SoftCOM50211.2020.9238318,"This paper presents the latest research in using deep learning methods for detecting plant disease using leaf images. It is estimated that early detection and treatment can save up to 40% of the agricultural product affected by pests. Various methods used by researchers obtain results between 62% and 98% accuracy for various types of diseases and images collected. We present the state of the art of the most relevant methods utilised, the datasets used, and the results yielded. © 2020 University of Split, FESB.",deep learning; leaf disease detection; real image datasets; smart agriculture; transfer learning,,,"2020 28th International Conference on Software, Telecommunications and Computer Networks, SoftCOM 2020",Conference Paper,Scopus
572,,Styx: A Data-Oriented Mutation Framework to Improve the Robustness of DNN,"Liu M., Hong W., Pan W., Feng C., Chen Z., Wang J.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099265075&doi=10.1145%2f3324884.3418903&partnerID=40&md5=5e9004f2f5cbe0bb2927ae9a003a0a11,10.1145/3324884.3418903,"The robustness of deep neural network (DNN) is critical and challenging to ensure. In this paper, we propose a general data-oriented mutation framework, called Styx, to improve the robustness of DNN. Styx generates new training data by slightly mutating the training data. In this way, Styx ensures the DNN's accuracy on the test dataset while improving the adaptability to small perturbations, i.e., improving the robustness. We have instantiated Styx for image classification and proposed pixel-level mutation rules that are applicable to any image classification DNNs. We have applied Styx on several commonly used benchmarks and compared Styx with the representative adversarial training methods. The preliminary experimental results indicate the effectiveness of Styx. © 2020 ACM.",Adversarial examples; DNN; Mutation; Robustness,"1260, 1261",,"Proceedings - 2020 35th IEEE/ACM International Conference on Automated Software Engineering, ASE 2020",Conference Paper,Scopus
573,,Problems and Opportunities in Training Deep Learning Software Systems: An Analysis of Variance,"Pham H.V., Qian S., Wang J., Lutellier T., Rosenthal J., Tan L., Yu Y., Nagappan N.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099249887&doi=10.1145%2f3324884.3416545&partnerID=40&md5=19165f3b08c2caf9ef6e43ddb3fc9b80,10.1145/3324884.3416545,"Deep learning (DL) training algorithms utilize nondeterminism to improve models' accuracy and training efficiency. Hence, multiple identical training runs (e.g., identical training data, algorithm, and network) produce different models with different accuracies and training times. In addition to these algorithmic factors, DL libraries (e.g., TensorFlow and cuDNN) introduce additional variance (referred to as implementation-level variance) due to parallelism, optimization, and floating-point computation. This work is the first to study the variance of DL systems and the awareness of this variance among researchers and practitioners. Our experiments on three datasets with six popular networks show large overall accuracy differences among identical training runs. Even after excluding weak models, the accuracy difference is 10.8%. In addition, implementation-level factors alone cause the accuracy difference across identical training runs to be up to 2.9%, the per-class accuracy difference to be up to 52.4%, and the training time difference to be up to 145.3%. All core libraries (TensorFlow, CNTK, and Theano) and low-level libraries (e.g., cuDNN) exhibit implementation-level variance across all evaluated versions. Our researcher and practitioner survey shows that 83.8% of the 901 participants are unaware of or unsure about any implementation-level variance. In addition, our literature survey shows that only 19.5±3% of papers in recent top software engineering (SE), artificial intelligence (AI), and systems conferences use multiple identical training runs to quantify the variance of their DL approaches. This paper raises awareness of DL variance and directs SE researchers to challenging tasks such as creating deterministic DL implementations to facilitate debugging and improving the reproducibility of DL software and results. © 2020 ACM.",deep learning; nondeterminism; variance,"771, 783",,"Proceedings - 2020 35th IEEE/ACM International Conference on Automated Software Engineering, ASE 2020",Conference Paper,Scopus
574,,Identifying Software Performance Changes across Variants and Versions,"Muhlbauer S., Apel S., Siegmund N.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099241511&doi=10.1145%2f3324884.3416573&partnerID=40&md5=e2d0963e8b87922e2aad5a3dd1680bef,10.1145/3324884.3416573,"We address the problem of identifying performance changes in the evolution of configurable software systems. Finding optimal configurations and configuration options that influence performance is already difficult, but in the light of software evolution, configuration-dependent performance changes may lurk in a potentially large number of different versions of the system. In this work, we combine two perspectives-variability and time-into a novel perspective. We propose an approach to identify configuration-dependent performance changes retrospectively across the software variants and versions of a software system. In a nutshell, we iteratively sample pairs of configurations and versions and measure the respective performance, which we use to update a model of likelihoods for performance changes. Pursuing a search strategy with the goal of measuring selectively and incrementally further pairs, we increase the accuracy of identified change points related to configuration options and interactions. We have conducted a number of experiments both on controlled synthetic data sets as well as in real-world scenarios with different software systems. Our evaluation demonstrates that we can pinpoint performance shifts to individual configuration options and interactions as well as commits introducing change points with high accuracy and at scale. Experiments on three real-world systems explore the effectiveness and practicality of our approach. © 2020 ACM.",active learning; configurable software systems; machine learning; software evolution; Software performance,"611, 622",,"Proceedings - 2020 35th IEEE/ACM International Conference on Automated Software Engineering, ASE 2020",Conference Paper,Scopus
575,,Metamorphic Object Insertion for Testing Object Detection Systems,"Wang S., Su Z.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099241457&doi=10.1145%2f3324884.3416584&partnerID=40&md5=d579fdd265fa29afc47ae26fbfce0c9d,10.1145/3324884.3416584,"Recent advances in deep neural networks (DNNs) have led to object detectors (ODs) that can rapidly process pictures or videos, and recognize the objects that they contain. Despite the promising progress by industrial manufacturers such as Amazon and Google in commercializing deep learning-based ODs as a standard computer vision service, ODs - similar to traditional software - may still produce incorrect results. These errors, in turn, can lead to severe negative outcomes for the users. For instance, an autonomous driving system that fails to detect pedestrians can cause accidents or even fatalities. However, despite their importance, principled, systematic methods for testing ODs do not yet exist. To fill this critical gap, we introduce the design and realization of Metaod, a metamorphic testing system specifically designed for ODs to effectively uncover erroneous detection results. To this end, we (1) synthesize natural-looking images by inserting extra object instances into background images, and (2) design metamorphic conditions asserting the equivalence of OD results between the original and synthetic images after excluding the prediction results on the inserted objects. Metaod is designed as a streamlined workflow that performs object extraction, selection, and insertion. We develop a set of practical techniques to realize an effective workflow, and generate diverse, natural-looking images for testing. Evaluated on four commercial OD services and four pretrained models provided by the TensorFlow API, Metaod found tens of thousands of detection failures. To further demonstrate the practical usage of Metaod, we use the synthetic images that cause erroneous detection results to retrain the model. Our results show that the model performance is significantly increased, from an mAP score of 9.3 to an mAP score of 10.5. © 2020 ACM.",computer vision; deep neural networks; object detection; testing,"1053, 1065",,"Proceedings - 2020 35th IEEE/ACM International Conference on Automated Software Engineering, ASE 2020",Conference Paper,Scopus
576,,BugPecker: Locating Faulty Methods with Deep Learning on Revision Graphs,"Cao J., Yang S., Jiang W., Zeng H., Shen B., Zhong H.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099240542&doi=10.1145%2f3324884.3418934&partnerID=40&md5=65793530454080fa46dc551bca00c9df,10.1145/3324884.3418934,"Given a bug report of a project, the task of locating the faults of the bug report is called fault localization. To help programmers in the fault localization process, many approaches have been proposed, and have achieved promising results to locate faulty files. However, it is still challenging to locate faulty methods, because many methods are short and do not have sufficient details to determine whether they are faulty. In this paper, we present BugPecker, a novel approach to locate faulty methods based on its deep learning on revision graphs. Its key idea includes (1) building revision graphs and capturing the details of past fixes as much as possible, and (2) discovering relations inside our revision graphs to expand the details for methods and calculating various features to assist our ranking. We have implemented BugPecker, and evaluated it on three open source projects. The early results show that BugPecker achieves a mean average precision (MAP) of 0.263 and mean reciprocal rank (MRR) of 0.291, which improve the prior approaches significantly. For example, BugPecker improves the MAP values of all three projects by five times, compared with two recent approaches such as DNNLoc-m and BLIA 1.5. © 2020 ACM.",bug localization; deep learning; revision graph,"1214, 1218",,"Proceedings - 2020 35th IEEE/ACM International Conference on Automated Software Engineering, ASE 2020",Conference Paper,Scopus
577,,A Deep Multitask Learning Approach for Requirements Discovery and Annotation from Open Forum,"Li M., Shi L., Yang Y., Wang Q.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099225955&doi=10.1145%2f3324884.3416627&partnerID=40&md5=5215551f10579403d1e78496036b8911,10.1145/3324884.3416627,"The ability in rapidly learning and adapting to evolving user needs is key to modern business successes. Existing methods are based on text mining and machine learning techniques to analyze user comments and feedback, and often constrained by heavy reliance on manually codified rules or insufficient training data. Multitask learning (MTL) is an effective approach with many successful applications, with the potential to address these limitations associated with requirements analysis tasks. In this paper, we propose a deep MTL-based approach, DEMAR, to address these limitations when discovering requirements from massive issue reports and annotating the sentences in support of automated requirements analysis. DEMAR consists of three main phases: (1) data augmentation phase, for data preparation and allowing data sharing beyond single task learning; (2) model construction phase, for constructing the MTL-based model for requirements discovery and requirements annotation tasks; and (3) model training phase, enabling eavesdropping by shared loss function between the two related tasks. Evaluation results from eight open-source projects show that, the proposed multitask learning approach outperforms two state-of-the-art approaches (CNC and FRA) and six common machine learning algorithms, with the precision of 91 % and the recall of 83% for requirements discovery task, and the overall accuracy of 83% for requirements annotation task. The proposed approach provides a novel and effective way to jointly learn two related requirements analysis tasks. We believe that it also sheds light on further directions of exploring multitask learning in solving other software engineering problems. © 2020 ACM.",Deep learning; Multitask learning; Requirements annotation; Requirements discovery,"336, 348",,"Proceedings - 2020 35th IEEE/ACM International Conference on Automated Software Engineering, ASE 2020",Conference Paper,Scopus
578,,Multi-task Learning based Pre-trained Language Model for Code Completion,"Liu F., Li G., Zhao Y., Jin Z.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099203766&doi=10.1145%2f3324884.3416591&partnerID=40&md5=ee3e9a69fc2b05b69c2f6883e204c4fc,10.1145/3324884.3416591,"Code completion is one of the most useful features in the Integrated Development Environments (IDEs), which can accelerate software development by suggesting the next probable token based on the contextual code in real-time. Recent studies have shown that statistical language modeling techniques can improve the performance of code completion tools through learning from large-scale software repositories. However, these models suffer from two major drawbacks: a) Existing research uses static embeddings, which map a word to the same vector regardless of its context. The differences in the meaning of a token in varying contexts are lost when each token is associated with a single representation; b) Existing language model based code completion models perform poor on completing identifiers, and the type information of the identifiers is ignored in most of these models. To address these challenges, in this paper, we develop a multi-task learning based pre-trained language model for code understanding and code generation with a Transformer-based neural architecture. We pre-train it with hybrid objective functions that incorporate both code understanding and code generation tasks. Then we fine-tune the pre-trained model on code completion. During the completion, our model does not directly predict the next token. Instead, we adopt multi-task learning to predict the token and its type jointly and utilize the predicted type to assist the token prediction. Experiments results on two real-world datasets demonstrate the effectiveness of our model when compared with state-of-the-art methods. © 2020 ACM.",code completion; multi-task learning; pre-trained language model; transformer networks,"473, 485",,"Proceedings - 2020 35th IEEE/ACM International Conference on Automated Software Engineering, ASE 2020",Conference Paper,Scopus
579,,CP-Detector: Using Configuration-related Performance Properties to Expose Performance Bugs,"He H., Jia Z., Li S., Xu E., Yu T., Yu Y., Wang J., Liao X.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099189983&doi=10.1145%2f3324884.3416531&partnerID=40&md5=8737592c18a8e065e6d32d337d59e127,10.1145/3324884.3416531,"Performance bugs are often hard to detect due to their non fail-stop symptoms. Existing debugging techniques can only detect performance bugs with known patterns (e.g., inefficient loops). The key reason behind this incapability is the lack of a general test oracle. Here, we argue that the performance (e.g., throughput, latency, execution time) expectation of configuration can serve as a strong oracle candidate for performance bug detection. First, prior work shows that most performance bugs are related to configurations. Second, the configuration change reflects common expectation on performance changes. If the actual performance is contrary to the expectation, the related code snippet is likely to be problematic. In this paper, we first conducted a comprehensive study on 173 real-world configuration-related performance bugs (CPBugs) from 12 representative software systems. We then derived seven configuration-related performance properties, which can serve as the test oracle in performance testing. Guided by the study, we designed and evaluated an automated performance testing framework, CP-DETECTOR, for detecting real-world configuration-related performance bugs. CP-DETECTOR was evaluated on 12 open-source projects. The results showed that it detected 43 out of 61 existing bugs and reported 13 new bugs. © 2020 ACM.",Performance bug detection; Performance property; Software configuration,"623, 634",,"Proceedings - 2020 35th IEEE/ACM International Conference on Automated Software Engineering, ASE 2020",Conference Paper,Scopus
580,,Attend and Represent: A Novel View on Algorithm Selection for Software Verification,"Richter C., Wehrheim H.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099187174&doi=10.1145%2f3324884.3416633&partnerID=40&md5=42d8e32da16a61136f2347ca76ebc2f4,10.1145/3324884.3416633,"Today, a plethora of different software verification tools exist. When having a concrete verification task at hand, software developers thus face the problem of algorithm selection. Existing algorithm selectors for software verification typically use handpicked program features together with (1) either manually designed selection heuristics or (2) machine learned strategies. While the first approach suffers from not being transferable to other selection problems, the second approach lacks interpretability, i.e., insights into reasons for choosing particular tools. In this paper, we propose a novel approach to algorithm selection for software verification. Our approach employs representation learning together with an attention mechanism. Representation learning circumvents feature engineering, i.e., avoids the handpicking of program features. Attention permits a form of interpretability of the learned selectors. We have implemented our approach and have experimentally evaluated and compared it with existing approaches. The evaluation shows that representation learning does not only outperform manual feature engineering, but also enables transferability of the learning model to other selection tasks. © 2020 ACM.",algorithm selection; attention; representation learning; Software verification,"1016, 1028",,"Proceedings - 2020 35th IEEE/ACM International Conference on Automated Software Engineering, ASE 2020",Conference Paper,Scopus
581,,BiLO-CPDP: Bi-Level Programming for Automated Model Discovery in Cross-Project Defect Prediction,"Li K., Xiang Z., Chen T., Tan K.C.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099185879&doi=10.1145%2f3324884.3416617&partnerID=40&md5=a4e1543b5f5c97b675072e4bc8361d9d,10.1145/3324884.3416617,"Cross-Project Defect Prediction (CPDP), which borrows data from similar projects by combining a transfer learner with a classifier, have emerged as a promising way to predict software defects when the available data about the target project is insufficient. However, developing such a model is challenge because it is difficult to determine the right combination of transfer learner and classifier along with their optimal hyper-parameter settings. In this paper, we propose a tool, dubbed BiLO-CPDP, which is the first of its kind to formulate the automated CPDP model discovery from the perspective of bi-level programming. In particular, the bi-level programming proceeds the optimization with two nested levels in a hierarchical manner. Specifically, the upper-level optimization routine is designed to search for the right combination of transfer learner and classifier while the nested lower-level optimization routine aims to optimize the corresponding hyper-parameter settings. To evaluate BiLO-CPDP, we conduct experiments on 20 projects to compare it with a total of 21 existing CPDP techniques, along with its single-level optimization variant and Auto-Sklearn, a state-of-the-art automated machine learning tool. Empirical results show that BiLO-CPDP champions better prediction performance than all other 21 existing CPDP techniques on 70% of the projects, while being overwhelmingly superior to Auto-Sklearn and its single-level optimization variant on all cases. Furthermore, the unique bi-level formalization in BiLO-CPDP also permits to allocate more budget to the upper-level, which significantly boosts the performance. © 2020 ACM.",automated parameter optimization; classification techniques; configurable software and tool; Cross-project defect prediction; Software defect analysis; transfer learning,"573, 584",,"Proceedings - 2020 35th IEEE/ACM International Conference on Automated Software Engineering, ASE 2020",Conference Paper,Scopus
582,,Learning based and Context Aware Non-Informative Comment Detection,"Liu M., Yang Y., Peng X., Wang C., Zhao C., Wang X., Xing S.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096717498&doi=10.1109%2fICSME46990.2020.00115&partnerID=40&md5=740c94e83ccf4df8a790a206a3b6d7b5,10.1109/ICSME46990.2020.00115,"This report introduces the approach that we have designed and implemented for the DeClutter challenge of Doc-Gen2, which detects non-informative code comments. The approach combines both comment based text classification and code context based prediction. Based on the approach, our ""fduse""team achieved the best F1 score (0.847) in the competition. © 2020 IEEE.",API Documentation; Deep Learning; Text Classification,"866, 867",,"Proceedings - 2020 IEEE International Conference on Software Maintenance and Evolution, ICSME 2020",Conference Paper,Scopus
583,,Sentiment Analysis for Software Engineering: How Far Can Pre-trained Transformer Models Go?,"Zhang T., Xu B., Thung F., Haryono S.A., Lo D., Jiang L.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096691094&doi=10.1109%2fICSME46990.2020.00017&partnerID=40&md5=3aa374a8ed8b6a17465544cf45aac7c2,10.1109/ICSME46990.2020.00017,"Extensive research has been conducted on sentiment analysis for software engineering (SA4SE). Researchers have invested much effort in developing customized tools (e.g., SentiStrength-SE, SentiCR) to classify the sentiment polarity for Software Engineering (SE) specific contents (e.g., discussions in Stack Overflow and code review comments). Even so, there is still much room for improvement. Recently, pre-trained Transformer-based models (e.g., BERT, XLNet) have brought considerable breakthroughs in the field of natural language processing (NLP). In this work, we conducted a systematic evaluation of five existing SA4SE tools and variants of four state-of-the-art pre-trained Transformer-based models on six SE datasets. Our work is the first to fine-tune pre-trained Transformer-based models for the SA4SE task. Empirically, across all six datasets, our fine-tuned pre-trained Transformer-based models outperform the existing SA4SE tools by 6.5-35.6% in terms of macro/micro-averaged F1 scores. © 2020 IEEE.",Natural Language Processing; Pre-trained Models; Sentiment Analysis; Software Mining,"70, 80",,"Proceedings - 2020 IEEE International Conference on Software Maintenance and Evolution, ICSME 2020",Conference Paper,Scopus
584,,CrossASR: Efficient Differential Testing of Automatic Speech Recognition via Text-To-Speech,"Asyrofi M.H., Thung F., Lo D., Jiang L.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096679680&doi=10.1109%2fICSME46990.2020.00066&partnerID=40&md5=b6eaf84de7552dc678aea9d79788f8b5,10.1109/ICSME46990.2020.00066,"Automatic speech recognition (ASR) systems are ubiquitous parts of modern life. It can be found in our smartphones, desktops, and smart home systems. To ensure its correctness in recognizing speeches, ASR needs to be tested. Testing ASR requires test cases in the form of audio files and their transcribed texts. Building these test cases manually, however, is tedious and time-consuming.To deal with the aforementioned challenge, in this work, we propose CrossASR, an approach that capitalizes the existing Text-To-Speech (TTS) systems to automatically generate test cases for ASR systems. CrossASR is a differential testing solution that compares outputs of multiple ASR systems to uncover erroneous behaviors among ASRs. CrossASR efficiently generates test cases to uncover failures with as few generated tests as possible; it does so by employing a failure probability predictor to pick the texts with the highest likelihood of leading to failed test cases. As a black-box approach, CrossASR can generate test cases for any ASR, including when the ASR model is not available (e.g., when evaluating the reliability of various third-party ASR services).We evaluated CrossASR using 4 TTSes and 4 ASRs on the Europarl corpus. The experimented ASRs are Deepspeech, Deepspeech2, wav2letter, and wit. Our experiments on a randomly sampled 20,000 English texts showed that within an hour, CrossASR can produce, on average from 3 experiments, 130.34, 123.33, 47.33, and 8.66 failed test cases using Google, Respon-siveVoice, Festival, and Espeak TTSes, respectively. Moreover, when we run CrossASR on the entire 20,000 texts, it can generate 13,572, 13,071, 5,911, and 1,064 failed test cases using Google, ResponsiveVoice, Festival, and Espeak TTSes, respectively. Based on a manual verification carried out on statistically representative sample size, we found that most samples are actual failed test cases (audio understandable to humans but cannot be transcribed properly by an ASR), demonstrating that CrossASR is highly reliable in determining failed test cases. We also make the source code for CrossASR and evaluation data available at https://github.com/soarsμCrossASR. © 2020 IEEE.",Automatic Speech Recognition; Differential Testing; Failure Probability Predictor; Test Case Generation; Text-to-Speech,"640, 650",,"Proceedings - 2020 IEEE International Conference on Software Maintenance and Evolution, ICSME 2020",Conference Paper,Scopus
585,,Learning Code-Query Interaction for Enhancing Code Searches,"Li W., Qin H., Yan S., Shen B., Chen Y.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096677993&doi=10.1109%2fICSME46990.2020.00021&partnerID=40&md5=757d128ef9bc5b7bd29d66ccb4fb0179,10.1109/ICSME46990.2020.00021,"Code search plays an important role in software development and maintenance. In recent years, deep learning (DL) has achieved a great success in this domain-several DL-based code search methods, such as DeepCS and UNIF, have been proposed for exploring deep, semantic correlations between code and queries; each method usually embeds source code and natural language queries into real vectors followed by computing their vector distances representing their semantic correlations. Meanwhile, deep learning-based code search still suffers from three main problems, i.e., the OOV (Out of Vocabulary) problem, the independent similarity matching problem, and the small training dataset problem.To tackle the above problems, we propose CQIL, a novel, deep learning-based code search method. CQIL learns code-query interactions and uses a CNN (Convolutional Neural Network) to compute semantic correlations between queries and code snippets. In particular, CQIL employs a hybrid representation to model code-query correlations, which solves the OOV problem. CQIL also deeply learns the code-query interaction for enhancing code searches, which solves the independent similarity matching and the small training dataset problems. We evaluate CQIL on two datasets (CODEnn and CosBench). The evaluation results show the strengths of CQIL-it achieves the MAP@1 values, 0.694 and 0.574, on CODEnn and CosBench, respectively. In particular, it outperforms DeepCS and UNIF, two state-of-the-art code search methods, by 13.6% and 18.1% in MRR, respectively, when the training dataset is insufficient. © 2020 IEEE.",code search; code-query interaction; deep learning; hybrid representation,"115, 126",,"Proceedings - 2020 IEEE International Conference on Software Maintenance and Evolution, ICSME 2020",Conference Paper,Scopus
586,,Automated Extraction of Requirement Entities by Leveraging LSTM-CRF and Transfer Learning,"Li M., Yang Y., Shi L., Wang Q., Hu J., Peng X., Liao W., Pi G.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096658547&doi=10.1109%2fICSME46990.2020.00029&partnerID=40&md5=c2fe599fdabbff48b7de07c2a16cb800,10.1109/ICSME46990.2020.00029,"Requirement entities, ""explicit specification of concepts that define the primary function objects"", play an important role in requirement analysis for software development and maintenance. It is a labor-intensive activity to extract requirement entities from textual requirements, which is typically done manually. A few existing studies propose automated methods to support key requirement concept extraction. However, they face two main challenges: lack of domain-specific natural language processing techniques and expensive labeling effort. To address the challenges, this study presents a novel approach named RENE, which employs LSTM-CRF model for requirement entity extraction and introduces the general knowledge to reduce the demands for labeled data. It consists of four phases: 1) Model construction, where RENE builds LSTM-CRF model and an isomorphic LSTM language model for transfer learning; 2) LSTM language model training, where RENE captures general knowledge and adapt to requirement context; 3) LSTM-CRF training, where RENE trains the LSTM-CRF model with the transferred layers; 4) Requirement entity extraction, where RENE applies the trained LSTM-CRF model to a new-coming requirement, and automatically extracts its requirement entities. RENE is evaluated using two methods: evaluation on historical dataset and user study. The evaluation on the historical dataset shows that RENE could achieve 79% precision, 81% recall, and 80% F1. The evaluation results from the user study also suggest that RENE could produce more accurate and comprehensive requirement entities, compared with those produced by engineers. © 2020 IEEE.",LSTM-CRF; Requirement Entity; Sequence Tagging; Transfer Learning,"208, 219",,"Proceedings - 2020 IEEE International Conference on Software Maintenance and Evolution, ICSME 2020",Conference Paper,Scopus
587,,Achieving Reliable Sentiment Analysis in the Software Engineering Domain using BERT,"Biswas E., Karabulut M.E., Pollock L., Vijay-Shanker K.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096644035&doi=10.1109%2fICSME46990.2020.00025&partnerID=40&md5=38c50de32cae606df0991b9b56f47c37,10.1109/ICSME46990.2020.00025,"Researchers have shown that sentiment analysis of software artifacts can potentially improve various software engineering tools, including API and library recommendation systems, code suggestion tools, and tools for improving communication among software developers. However, sentiment analysis techniques applied to software artifacts still have not yet yielded very high accuracy. Recent adaptations of sentiment analysis tools to the software domain have reported some improvements, but the f-measures for the positive and negative sentences still remain in the 0.4-0.64 range, which deters their practical usefulness for software engineering tools.In this paper, we explore the potential effectiveness of customizing BERT, a language representation model, which has recently achieved very good results on various Natural Language Processing tasks on English texts, for the task of sentiment analysis of software artifacts. We describe our application of BERT to analyzing sentiments of sentences in Stack Overflow posts and compare the impact of a BERT sentiment classifier to state-of-the-art sentiment analysis techniques when used on a domain-specific data set created from Stack Overflow posts. We also investigate how the performance of sentiment analysis changes when using a much (3 times) larger data set than previous studies. Our results show that the BERT classifier achieves reliable performance for sentiment analysis of software engineering texts. BERT combined with the larger data set achieves an overall f-measure of 0.87, with the f-measures for the negative and positive sentences reaching 0.91 and 0.78 respectively, a significant improvement over the state-of-the-art. © 2020 IEEE.",BERT; Sentiment Analysis; Software Engineering,"162, 173",,"Proceedings - 2020 IEEE International Conference on Software Maintenance and Evolution, ICSME 2020",Conference Paper,Scopus
588,,Owl Eyes: Spotting UI Display Issues via Visual Understanding,"Liu Z., Chen C., Wang J., Huang Y., Hu J., Wang Q.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094208436&doi=10.1145%2f3324884.3416547&partnerID=40&md5=8d1ce5ee6552b48f5f2bd097f6e55522,10.1145/3324884.3416547,"Graphical User Interface (GUI) provides a visual bridge between a software application and end users, through which they can interact with each other. With the development of technology and aesthetics, the visual effects of the GUI are more and more attracting. However, such GUI complexity posts a great challenge to the GUI implementation. According to our pilot study of crowdtesting bug reports, display issues such as text overlap, blurred screen, missing image always occur during GUI rendering on different devices due to the software or hardware compatibility. They negatively influence the app usability, resulting in poor user experience. To detect these issues, we propose a novel approach, Owl Eye, based on deep learning for modelling visual information of the GUI screenshot. Therefore, Owl Eye can detect GUIs with display issues and also locate the detailed region of the issue in the given GUI for guiding developers to fix the bug. We manually construct a large-scale labelled dataset with 4, 470 GUI screenshots with UI display issues and develop a heuristics-based data augmentation method for boosting the performance of our Owl Eye. The evaluation demonstrates that our Owl Eye can achieve 85% precision and 84% recall in detecting UI display issues, and 90% accuracy in localizing these issues. We also evaluate Owl Eye with popular Android apps on Google Play and F-droid, and successfully uncover 57 previously-undetected UI display issues with 26 of them being confirmed or fixed so far. © 2020 ACM.",Deep Learning; Mobile App; UI display; UI testing,"398, 409",,"Proceedings - 2020 35th IEEE/ACM International Conference on Automated Software Engineering, ASE 2020",Conference Paper,Scopus
589,,Survey of Lightweight Neural Network [轻量级神经网络架构综述],"Ge D.-H., Li H.-S., Zhang L., Liu R.-Y., Shen P.-Y., Miao Q.-G.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092418778&doi=10.13328%2fj.cnki.jos.005942&partnerID=40&md5=63d3e964ee7ed3a6dea8a47cdd3f2452,10.13328/j.cnki.jos.005942,"Deep neural network has been proved to be effective in solving problems in different fields such as image, natural language, and so on. At the same time, with the continuous development of mobile Internet technology, portable devices have been rapidly popularized, and users have put forward more and more demands. Therefore, how to design an efficient and high performance lightweight neural network is the key to solve the problem. In this paper, three methods of constructing lightweight neural network are described in detail, which are artificial design of lightweight neural network, compression algorithm of neural network model, and automatic neural network architecture design based on searching of neural network architecture. The characteristics of each method are summarized and analyzed briefly, and the typical algorithms of constructing lightweight neural network are introduced emphatically. Finally, the existing methods are summarized and the prospects for future development are given. © Copyright 2020, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Auto machine learning; Compression of neural network; Lightweight neural network; Mobile device; Neural network architecture searching,"2627, 2653",,Ruan Jian Xue Bao/Journal of Software,Review,Scopus
590,,Research on Weak-supervised Person Re-identification [弱监督场景下的行人重识别研究综述],"Qi L., Yu P.-Z., Gao Y.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092360136&doi=10.13328%2fj.cnki.jos.006083&partnerID=40&md5=45e9d8e5bb0246a583b28c51790edabe,10.13328/j.cnki.jos.006083,"Recently, with the development of the intelligent surveillance, person re-identification (Re-ID) has attracted lots of attention in the academic and industrial communities, which aims to associate person images of the same identity under different non-overlapping cameras. Most of the current research works focus on the supervised case where all given training samples have label information. Considering the high cost of data labeling, these methods designed for the supervised setting have poor generalization in practical applications. This study focuses on person re-identification algorithms under the weakly supervised case including the unsupervised case and the semi-supervised case and classify and describe several state-of-the-art methods. In the unsupervised setting, these methods are divided into five categories from different technology perspectives, which include the methods based on pseudo-label, image generation, instance classification, domain adaptation, and others. In the semi-supervised setting, these methods are divided into four categories according to the case discrepancy, which are the case where a small number of persons are labeled, the case where there are few labeled images for each person, the case based on tracklet learning, and the case where there are the intra-camera labels but no inter-camera label information. Finally, several benchmark person re-identification datasets are summarized and some experimental results of these weak-supervised person re-Identification algorithms are analyzed. © Copyright 2020, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Artificial intelligence; Deep learning; Person re-identification; Semi-supervised learning; Unsupervised learning,"2883, 2902",,Ruan Jian Xue Bao/Journal of Software,Review,Scopus
591,,Evaluating Representation Learning of Code Changes for Predicting Patch Correctness in Program Repair,"Tian H., Liu K., Kabore A.K., Koyuncu A., Li L., Klein J., Bissyande T.F.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090838760&doi=10.1145%2f3324884.3416532&partnerID=40&md5=807aa80dc39926ee3a76b026cd76a328,10.1145/3324884.3416532,"A large body of the literature of automated program repair develops approaches where patches are generated to be validated against an oracle (e.g., a test suite). Because such an oracle can be imperfect, the generated patches, although validated by the oracle, may actually be incorrect. While the state of the art explore research directions that require dynamic information or that rely on manually-crafted heuristics, we study the benefit of learning code representations in order to learn deep features that may encode the properties of patch correctness. Our empirical work mainly investigates different representation learning approaches for code changes to derive embeddings that are amenable to similarity computations. We report on findings based on embeddings produced by pre-trained and re-trained neural networks. Experimental results demonstrate the potential of embeddings to empower learning algorithms in reasoning about patch correctness: a machine learning predictor with BERT transformer-based embeddings associated with logistic regression yielded an AUC value of about 0.8 in the prediction of patch correctness on a deduplicated dataset of 1000 labeled patches. Our investigations show that learned representations can lead to reasonable performance when comparing against the state-of-the-art, PATCH-SIM, which relies on dynamic information. These representations may further be complementary to features that were carefully (manually) engineered in the literature. © 2020 ACM.",Distributed Representation Learning; Embeddings; Machine learning; Patch Correctness; Program Repair,"981, 992",,"Proceedings - 2020 35th IEEE/ACM International Conference on Automated Software Engineering, ASE 2020",Conference Paper,Scopus
592,,Learning actionable analytics from multiple software projects,"Krishna R., Menzies T.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088519438&doi=10.1007%2fs10664-020-09843-6&partnerID=40&md5=5585c124ac732d9667a13b9da22029f9,10.1007/s10664-020-09843-6,"The current generation of software analytics tools are mostly prediction algorithms (e.g. support vector machines, naive bayes, logistic regression, etc). While prediction is useful, after prediction comes planning about what actions to take in order to improve quality. This research seeks methods that generate demonstrably useful guidance on “what to do” within the context of a specific software project. Specifically, we propose XTREE (for within-project planning) and BELLTREE (for cross-project planning) to generating plans that can improve software quality. Each such plan has the property that, if followed, it reduces the expected number of future defect reports. To find this expected number, planning was first applied to data from release x. Next, we looked for change in release x + 1 that conformed to our plans. This procedure was applied using a range of planners from the literature, as well as XTREE. In 10 open-source JAVA systems, several hundreds of defects were reduced in sections of the code that conformed to XTREE’s plans. Further, when compared to other planners, XTREE’s plans were found to be easier to implement (since they were shorter) and more effective at reducing the expected number of defects. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.",Actionable analytics; Bellwethers; Data mining; Defect prediction; Planning,"3468, 3500",,Empirical Software Engineering,Article,Scopus
593,,Stain Color Adaptive Normalization (SCAN) algorithm: Separation and standardization of histological stains in digital pathology,"Salvi M., Michielli N., Molinari F.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083654294&doi=10.1016%2fj.cmpb.2020.105506&partnerID=40&md5=02cb6d50b0bac815c87cefc100792dd1,10.1016/j.cmpb.2020.105506,"Background and objective: The diagnosis of histopathological images is based on the visual analysis of tissue slices under a light microscope. However, the histological tissue appearance may assume different color intensities depending on the staining process, operator ability and scanner specifications. This stain variability affects the diagnosis of the pathologist and decreases the accuracy of computer-aided diagnosis systems. In this context, the stain normalization process has proved to be a powerful tool to cope with this issue, allowing to standardize the stain color appearance of a source image respect to a reference image. Methods: In this paper, novel fully automated stain separation and normalization approaches for hematoxylin and eosin stained histological slides are presented. The proposed algorithm, named SCAN (Stain Color Adaptive Normalization), is based on segmentation and clustering strategies for cellular structures detection. The SCAN algorithm is able to improve the contrast between histological tissue and background and preserve local structures without changing the color of the lumen and the background. Results: Both stain separation and normalization techniques were qualitatively and quantitively validated on a multi-tissue and multiscale dataset, with highly satisfactory results, outperforming the state-of-the-art approaches. SCAN was also tested on whole-slide images with high performances and low computational times. Conclusions: The potential contribution of the proposed standardization approach is twofold: the improvement of visual diagnosis in digital histopathology and the development of powerful pre-processing strategies to automated classification techniques for cancer detection. © 2020 Elsevier B.V.",Color deconvolution; Digital histopathology; H&E staining; Stain normalization; Whole-slide imaging,,,Computer Methods and Programs in Biomedicine,Article,Scopus
594,,Transfer learning using a multi-scale and multi-network ensemble for skin lesion classification,"Mahbod A., Schaefer G., Wang C., Dorffner G., Ecker R., Ellinger I.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082848696&doi=10.1016%2fj.cmpb.2020.105475&partnerID=40&md5=99428194d1bb64de5ead87f20f85a31d,10.1016/j.cmpb.2020.105475,"Background and objective: Skin cancer is among the most common cancer types in the white population and consequently computer aided methods for skin lesion classification based on dermoscopic images are of great interest. A promising approach for this uses transfer learning to adapt pre-trained convolutional neural networks (CNNs) for skin lesion diagnosis. Since pre-training commonly occurs with natural images of a fixed image resolution and these training images are usually significantly smaller than dermoscopic images, downsampling or cropping of skin lesion images is required. This however may result in a loss of useful medical information, while the ideal resizing or cropping factor of dermoscopic images for the fine-tuning process remains unknown. Methods: We investigate the effect of image size for skin lesion classification based on pre-trained CNNs and transfer learning. Dermoscopic images from the International Skin Imaging Collaboration (ISIC) skin lesion classification challenge datasets are either resized to or cropped at six different sizes ranging from 224 × 224 to 450 × 450. The resulting classification performance of three well established CNNs, namely EfficientNetB0, EfficientNetB1 and SeReNeXt-50 is explored. We also propose and evaluate a multi-scale multi-CNN (MSM-CNN) fusion approach based on a three-level ensemble strategy that utilises the three network architectures trained on cropped dermoscopic images of various scales. Results: Our results show that image cropping is a better strategy compared to image resizing delivering superior classification performance at all explored image scales. Moreover, fusing the results of all three fine-tuned networks using cropped images at all six scales in the proposed MSM-CNN approach boosts the classification performance compared to a single network or a single image scale. On the ISIC 2018 skin lesion classification challenge test set, our MSM-CNN algorithm yields a balanced multi-class accuracy of 86.2% making it the currently second ranked algorithm on the live leaderboard. Conclusions: We confirm that the image size has an effect on skin lesion classification performance when employing transfer learning of CNNs. We also show that image cropping results in better performance compared to image resizing. Finally, a straightforward ensembling approach that fuses the results from images cropped at six scales and three fine-tuned CNNs is shown to lead to the best classification performance. © 2020 Elsevier B.V.",Deep learning; Dermoscopy; Image cropping; Image resolution; Medical image analysis; Skin cancer; Transfer learning,,,Computer Methods and Programs in Biomedicine,Article,Scopus
595,,Data-driven benchmarking in software development effort estimation: The few define the bulk,"Mittas N., Angelis L.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081587836&doi=10.1002%2fsmr.2258&partnerID=40&md5=6c927557fa958193ca6df6ddf23917d9,10.1002/smr.2258,"Context: The rapid evolvement of software development effort estimation models created the need for empirical evaluation of their quality. The empirical evaluation is based either on hypothesis tests with respect to a single criterion or on aggregating methods for multiple criteria. However, a model can be considered as a multidimensional entity performing differently on alternative datasets and its performance can be divergent when expressed by alternative criteria. Objective: In this study, we explore this multidimensional nature of models by considering them as points in two different spaces (domain and criteria spaces). Method: Introducing an alternative approach for data-driven benchmarking, a new framework based on archetypal analysis is proposed for evaluation purposes of multiple models. Results: The benefits of the framework are illustrated through a large-scale experimental setup on a set of 93 effort estimation models, trained and tested on 10 datasets under 8 criteria providing answers to critical research questions. Conclusion: The results indicate that a small minority of reference models is enough to define the performance of the bulk of all models. The framework focuses on models that have behavior close to archetypes and especially those that are close to a “best” archetype. © 2020 John Wiley & Sons, Ltd.",archetypal analysis; benchmarks; performance measures; software development effort estimation,,,Journal of Software: Evolution and Process,Article,Scopus
596,,SeqPoint: Identifying Representative Iterations of Sequence-Based Neural Networks,"Pati S., Aga S., Sinclair M.D., Jayasena N.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097189347&doi=10.1109%2fISPASS48437.2020.00017&partnerID=40&md5=4160cf89450e2e38b4da5d91e81ce7d0,10.1109/ISPASS48437.2020.00017,"The ubiquity of deep neural networks (DNNs) continues to rise, making them a crucial application class for hardware optimizations. However, detailed profiling and characterization of DNN training remains difficult as these applications often run for hours to days on real hardware. Prior works have exploited the iterative nature of DNNs to profile a few training iterations to represent the entire training run. While such a strategy is sound for networks like convolutional neural networks (CNNs), where the nature of the computation is largely input independent, we observe in this work that this approach is sub-optimal for sequence-based neural networks (SQNNs) such as recurrent neural networks (RNNs). The amount and nature of computations in SQNNs can vary for each input, resulting in heterogeneity across iterations. Thus, arbitrarily selecting a few iterations is insufficient to accurately summarize the behavior of the entire training run. To tackle this challenge, we carefully study the factors that impact SQNN training iterations and identify input sequence length as the key determining factor for variations across iterations. We then use this observation to characterize all iterations of an SQNN training run (requiring no profiling or simulation of the application) and select representative iterations, which we term SeqPoints. We analyze two state-of-the-art SQNNs, DeepSpeech2 and Google's Neural Machine Translation (GNMT), and show that SeqPoints can represent their entire training runs accurately, resulting in geomean errors of only 0.11% and 0.53%, respectively, when projecting overall runtime and 0.13% and 1.50% when projecting speedups due to architectural changes. This high accuracy is achieved while reducing the time needed for profiling by 345x and 214x for the two networks compared to full training runs. As a result, SeqPoint can enable analysis of SQNN training runs in mere minutes instead of hours or days. © 2020 IEEE.",Deep Learning; Profiling; Recurrent Neural Networks; SimPoint,"69, 80",,"Proceedings - 2020 IEEE International Symposium on Performance Analysis of Systems and Software, ISPASS 2020",Conference Paper,Scopus
597,,AI on the Edge: Architectural Alternatives,"John M.M., Holmstrom Olsson H., Bosch J.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096567097&doi=10.1109%2fSEAA51224.2020.00015&partnerID=40&md5=9a09b62d6bdec5271618f2f074ea5a2e,10.1109/SEAA51224.2020.00015,"Since the advent of mobile computing and IoT, a large amount of data is distributed around the world. Companies are increasingly experimenting with innovative ways of implementing edge/cloud (re)training of AI systems to exploit large quantities of data to optimize their business value. Despite the obvious benefits, companies face challenges as the decision on how to implement edge/cloud (re)training depends on factors such as the task intent, the amount of data needed for (re)training, edge-to-cloud data transfer, the available computing and memory resources. Based on action research in a software-intensive embedded systems company where we study multiple use cases as well as insights from our previous collaborations with industry, we develop a generic framework consisting of five architectural alternatives to deploy AI on the edge utilizing transfer learning. We validate the framework in four additional case companies and present the challenges they face in selecting the optimal architecture. The contribution of the paper is threefold. First, we develop a generic framework consisting of five architectural alternatives ranging from a centralized architecture where cloud (re)training is given priority to a decentralized architecture where edge (re)training is instead given priority. Second, we validate the framework in a qualitative interview study with four additional case companies. As an outcome of validation study, we present two variants to the architectural alternatives identified as part of the framework. Finally, we identify the key challenges that experts face in selecting an ideal architectural alternative. © 2020 IEEE.",Action Research; Architectural alternatives; Artificial Intelligence; Cloud; Deep Learning; Edge; Machine Learning; Transfer Learning,"21, 28",,"Proceedings - 46th Euromicro Conference on Software Engineering and Advanced Applications, SEAA 2020",Conference Paper,Scopus
598,,A Systematic Methodology for Characterizing Scalability of DNN Accelerators using SCALE-Sim,"Samajdar A., Joseph J.M., Zhu Y., Whatmough P., Mattina M., Krishna T.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094759937&doi=10.1109%2fISPASS48437.2020.00016&partnerID=40&md5=584562794773e10b98c8e1f99a2db682,10.1109/ISPASS48437.2020.00016,"The compute demand for deep learning workloads is well known and is a prime motivator for powerful parallel computing platforms such as GPUs or dedicated hardware accelerators. The massive inherent parallelism of these workloads enables us to extract more performance by simply provisioning more compute hardware for a given task. This strategy can be directly exploited to build higher-performing hardware for DNN workloads, by incorporating as many parallel compute units as possible in a single system. This strategy is referred to as scaling up. Alternatively, it's feasible to arrange multiple hardware systems to work on a single problem, and in some cases, a cheaper alternative to exploit the given parallelism, or in other words, scaling out. As DNN based solutions become increasingly prevalent, so does the demand for computation, making the scaling choice (scale-up vs scale-out) critical. To study this design-space, this work makes two major contributions. (i) We describe a cycle-accurate simulator called SCALE-SIM for DNN inference on systolic arrays, which we use to model both scale-up and scale-out systems, modeling on-chip memory access, runtime, and DRAM bandwidth requirements for a given workload. (ii) We also present an analytical model to estimate the optimal scale-up vs scale-out ratio given hardware constraints (e.g, TOPS and DRAM bandwidth) for a given workload. We observe that a judicious choice of scaling can lead to performance improvements as high as 50 per layer, within the available DRAM bandwidth. This work demonstrates and analyzes the trade-off space for performance, DRAM bandwidth, and energy, and identifies sweet spots for various workloads and hardware configurations. © 2020 IEEE.",Accelerator Simulator; Cycle accurate simulation; DNN acclerator; scaling analysis,"58, 68",,"Proceedings - 2020 IEEE International Symposium on Performance Analysis of Systems and Software, ISPASS 2020",Conference Paper,Scopus
599,,Medical Image Segmentation Using Semi-supervised Conditional Generative Adversarial Nets [面向医学图像分割的半监督条件生成对抗网络],"Liu S.-P., Hong J.-M., Liang J.-P., Jia X.-P., Ouyang J., Yin J.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089901293&doi=10.13328%2fj.cnki.jos.005860&partnerID=40&md5=190e95ee587026a994924229362b69d3,10.13328/j.cnki.jos.005860,"Medical image segmentation is a key technology in computer aided diagnosis. As a widespread eye disease, glaucoma may cause permanent loss in vision and its screening and diagnosis requires accurate segmentation of optic cup and disc from fundus images. Most traditional computer vision methods segment optic cup and disc with artificial features lead to limited generalization ability. While the end-to-end learning models based on convolutional neural networks focus on optic disc and cup segmentation using automatically detected features, but fail to tackle the lack of labeled samples, thus the segmentation performance is still barely satisfactory. This study proposes an effective two-stage optic disc and cup segmentation method based on semi-supervised conditional generative adversarial nets, namely CDR- GANs. Each stage builds upon three players-A segmentation net, a generator, and a discriminator, where the segmentation net and generator concentrate on learning the conditional distributions between fundus images and their corresponding segmentation maps, and the discriminator distinguishes whether the image-label pairs come from the empirical joint distribution. The extensive experiments show that the proposed method achieves state-of-the-art optic cup and disc segmentation results on ORIGA dataset. © Copyright 2020, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Deep learning; Generative adversarial nets; Glaucoma screening; Medical image; Semi-supervised learning,"2588, 2602",,Ruan Jian Xue Bao/Journal of Software,Article,Scopus
600,,Survey of Machine Learning Enabled Software Self-adaptation [机器学习赋能的软件自适应性综述],"Zhang M.-Y., Jin Z., Zhao H.-Y., Luo Y.-X.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089897087&doi=10.13328%2fj.cnki.jos.006076&partnerID=40&md5=7d6c7ebc2ac2f5c5ef17b3ea9ba8a4df,10.13328/j.cnki.jos.006076,"Software self-adaptation (SSA) provides a way of dealing with dynamic environment and uncertain requirement. There are existing works that transform the dynamic and uncertainty concerned by SSA into regression, classification, cluster, or decision problems; and apply machine learning algorithms, including reinforcement learning, neural network/deep learning, Bayesian decision theory and probabilistic graphical model, rule learning, to problem formulation and solving. These kinds of work are called as ""machine learning enabled SSA"" in this study. The survey is conducted on the state-of-the-art research about machine learning enabled SSA by firstly explaining the related concepts of SSA and machine learning; and then proposing a taxonomy based on current work from SSA perspective and machine learning perspective respectively; analyzing the machine learning algorithms, software external interaction, software internal control, adaptation process, the relationship between SSA task and learning ability under this taxonomy; as well as identifying finally deficiency of current work and highlighting future research trends. © Copyright 2020, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Environment dynamic; Machine learning; Requirement uncertainty; Self-adaptive software system; Software self-adaptation,"2404, 2431",,Ruan Jian Xue Bao/Journal of Software,Review,Scopus
601,,Automated defect identification via path analysis-based features with transfer learning,"Zhang Y., Jin D., Xing Y., Gong Y.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083420285&doi=10.1016%2fj.jss.2020.110585&partnerID=40&md5=62cdd1a1ae83db9589e284fbae3c80e6,10.1016/j.jss.2020.110585,"Recently, artificial intelligence techniques have been widely applied to address various specialized tasks in software engineering, such as code generation, defect identification, and bug repair. Despite the diffuse usage of static analysis tools in automatically detecting potential software defects, developers consider the large number of reported alarms and the expensive cost of manual inspection to be a key barrier to using them in practice. To automate the process of defect identification, researchers utilize machine learning algorithms with a set of hand-engineered features to build classification models for identifying alarms as actionable or unactionable. However, traditional features often fail to represent the deep syntactic structure of alarms. To bridge the gap between programs’ syntactic structure and defect identification features, this paper first extracts a set of novel fine-grained features at variable-level, called path-variable characteristic, by applying path analysis techniques in the feature extraction process. We then raise a two-stage transfer learning approach based on our proposed features, called feature ranking-matching based transfer learning, to increase the performance of cross-project defect identification. Our experimental results for eight open-source projects show that the proposed features at variable-level are promising and can yield significant improvement on both within-project and cross-project defect identification. © 2020 The Author(s)",Automated defect identification; Machine learning; Model evaluation; Path analysis; Transfer learning,,,Journal of Systems and Software,Article,Scopus
602,,Catheter segmentation in X-ray fluoroscopy using synthetic data and transfer learning with light U-nets,"Gherardini M., Mazomenos E., Menciassi A., Stoyanov D.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081133582&doi=10.1016%2fj.cmpb.2020.105420&partnerID=40&md5=53c79a81695a0e18534cb5777622310e,10.1016/j.cmpb.2020.105420,"Background and objectivesAutomated segmentation and tracking of surgical instruments and catheters under X-ray fluoroscopy hold the potential for enhanced image guidance in catheter-based endovascular procedures. This article presents a novel method for real-time segmentation of catheters and guidewires in 2d X-ray images. We employ Convolutional Neural Networks (CNNs) and propose a transfer learning approach, using synthetic fluoroscopic images, to develop a lightweight version of the U-Net architecture. Our strategy, requiring a small amount of manually annotated data, streamlines the training process and results in a U-Net model, which achieves comparable performance to the state-of-the-art segmentation, with a decreased number of trainable parameters. MethodsThe proposed transfer learning approach exploits high-fidelity synthetic images generated from real fluroscopic backgrounds. We implement a two-stage process, initial end-to-end training and fine-tuning, to develop two versions of our model, using synthetic and phantom fluoroscopic images independently. A small number of manually annotated in-vivo images is employed to fine-tune the deepest 7 layers of the U-Net architecture, producing a network specialized for pixel-wise catheter/guidewire segmentation. The network takes as input a single grayscale image and outputs the segmentation result as a binary mask against the background. ResultsEvaluation is carried out with images from in-vivo fluoroscopic video sequences from six endovascular procedures, with different surgical setups. We validate the effectiveness of developing the U-Net models using synthetic data, in tests where fine-tuning and testing in-vivo takes place both by dividing data from all procedures into independent fine-tuning/testing subsets as well as by using different in-vivo sequences. Accurate catheter/guidewire segmentation (average Dice coefficient of ~ 0.55, ~ 0.26 and ~ 0.17) is obtained with both U-Net models. Compared to the state-of-the-art CNN models, the proposed U-Net achieves comparable performance ( ± 5% average Dice coefficients) in terms of segmentation accuracy, while yielding a 84% reduction of the testing time. This adds flexibility for real-time operation and makes our network adaptable to increased input resolution. ConclusionsThis work presents a new approach in the development of CNN models for pixel-wise segmentation of surgical catheters in X-ray fluoroscopy, exploiting synthetic images and transfer learning. Our methodology reduces the need for manually annotating large volumes of data for training. This represents an important advantage, given that manual pixel-wise annotations is a key bottleneck in developing CNN segmentation models. Combined with a simplified U-Net model, our work yields significant advantages compared to current state-of-the-art solutions. © 2020",Catheter segmentation; Deep learning; Fluoroscopy; Transfer learning,,,Computer Methods and Programs in Biomedicine,Article,Scopus
603,,Offline computer-aided diagnosis for Glaucoma detection using fundus images targeted at mobile devices,"Martins J., Cardoso J.S., Soares F.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081023830&doi=10.1016%2fj.cmpb.2020.105341&partnerID=40&md5=b651025ffb0e5467792161608a906f80,10.1016/j.cmpb.2020.105341,"Background and Objective: Glaucoma, an eye condition that leads to permanent blindness, is typically asymptomatic and therefore difficult to be diagnosed in time. However, if diagnosed in time, Glaucoma can effectively be slowed down by using adequate treatment; hence, an early diagnosis is of utmost importance. Nonetheless, the conventional approaches to diagnose Glaucoma adopt expensive and bulky equipment that requires qualified experts, making it difficult, costly and time-consuming to diagnose large amounts of people. Consequently, new alternatives to diagnose Glaucoma that suppress these issues should be explored. Methods: This work proposes an interpretable computer-aided diagnosis (CAD) pipeline that is capable of diagnosing Glaucoma using fundus images and run offline in mobile devices. Several public datasets of fundus images were merged and used to build Convolutional Neural Networks (CNNs) that perform segmentation and classification tasks. These networks are then used to build a pipeline for Glaucoma assessment that outputs a Glaucoma confidence level and also provides several morphological features and segmentations of relevant structures, resulting in an interpretable Glaucoma diagnosis. To assess the performance of this method in a restricted environment, this pipeline was integrated into a mobile application and time and space complexities were assessed. Results: Considering the test set, the developed pipeline achieved 0.91 and 0.75 of Intersection over Union (IoU) in the optic disc and optic cup segmentation, respectively. With regards to the classification, an accuracy of 0.87 with a sensitivity of 0.85 and an AUC of 0.93 were attained. Moreover, this pipeline runs on an average Android smartphone in under two seconds. Conclusions: The results demonstrate the potential that this method can have in the contribution to an early Glaucoma diagnosis. The proposed approach achieved similar or slightly better metrics than the current CAD systems for Glaucoma assessment while running on more restricted devices. This pipeline can, therefore, be used to construct accurate and affordable CAD systems that could enable large Glaucoma screenings, contributing to an earlier diagnose of this condition. © 2020",Computer-aided diagnosis (CAD); Deep learning; Fundus images; Glaucoma,,,Computer Methods and Programs in Biomedicine,Article,Scopus
604,,Automated classification of actions in bug reports of mobile apps,"Liu H., Shen M., Jin J., Jiang Y.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088925979&doi=10.1145%2f3395363.3397355&partnerID=40&md5=fd4761e0d157e14ee78e0f215f2450e3,10.1145/3395363.3397355,"When users encounter problems with mobile apps, they may commit such problems to developers as bug reports. To facilitate the processing of bug reports, researchers proposed approaches to validate the reported issues automatically according to the steps to reproduce specified in bug reports. Although such approaches have achieved high success rate in reproducing the reported issues, they often rely on a predefined vocabulary to identify and classify actions in bug reports. However, such manually constructed vocabulary and classification have significant limitations. It is challenging for the vocabulary to cover all potential action words because users may describe the same action with different words. Besides that, classification of actions solely based on the action words could be inaccurate because the same action word, appearing in different contexts, may have different meaning and thus belongs to different action categories. To this end, in this paper we propose an automated approach, called MaCa, to identify and classify action words in Mobile apps' bug reports. For a given bug report, it first identifies action words based on natural language processing. For each of the resulting action words, MaCa extracts its contexts, i.e., its enclosing segment, the associated UI target, and the type of its target element by both natural language processing and static analysis of the associated app. The action word and its contexts are then fed into a machine learning based classifier that predicts the category of the given action word in the given context. To train the classifier, we manually labelled 1,202 actions words from 525 bug reports that are associated with 207 apps. Our evaluation results on manually labelled data suggested that MaCa was accurate with high accuracy varying from 95% to 96.7%. We also investigated to what extent MaCa could further improve existing approaches (i.e., Yakusu and ReCDroid) in reproducing bug reports. Our evaluation results suggested that integrating MaCa into existing approaches significantly improved the success rates of ReCDroid and Yakusu by 22.7% = (69.2%-56.4%)/56.4% and 22.9%= (62.7%-51%)/51%, respectively. © 2020 ACM.",Bug report; Classification; Mobile Testing; Test Case Generation,"128, 140",,ISSTA 2020 - Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis,Conference Paper,Scopus
605,,Doodle2App: Native app code by freehand UI sketching,"Mohian S., Csallner C.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094821134&doi=10.1145%2f3387905.3388607&partnerID=40&md5=8d7c3caab469766e151ee20cd8cf2c9b,10.1145/3387905.3388607,"User interface development typically starts with freehand sketching, with pen on paper, which creates a big gap in the software development process. Recent advances in deep neural networks that have been trained on large sketch stroke sequence collections have enabled online sketch detection that supports many sketch element classes at high classification accuracy. This paper leverages the recent Google Quick, Draw! dataset of 50M sketch stroke sequences to pre-train a recurrent neural network and retrains it with sketch stroke sequences we collected via Amazon Mechanical Turk. The resulting Doodle2App website offers a paper substitute, i.e., a drawing interface with interactive UI preview and can convert sketches to a compilable single-page Android application. On 712 sketch samples Doodle2App achieved higher accuracy than the state-of-the-art tool Teleport. A video demo is at https://youtu.be/P4sb0pKTNEY © 2020 ACM.",deep learning; GUI; prototyping; sketching; user interface design,"81, 84",,"Proceedings - 2020 IEEE/ACM 7th International Conference on Mobile Software Engineering and Systems, MOBILESoft 2020",Conference Paper,Scopus
606,,A self-attentional neural architecture for code completion with multi-task learning,"Liu F., Li G., Wei B., Xia X., Fu Z., Jin Z.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091950217&doi=10.1145%2f3387904.3389261&partnerID=40&md5=275f8ec197d6e8b87c2de3cd4d74e4ac,10.1145/3387904.3389261,"Code completion, one of the most useful features in the IntegratedDevelopment Environments (IDEs), can accelerate software development by suggesting the libraries, APIs, and method namesin real-time. Recent studies have shown that statistical languagemodels can improve the performance of code completion toolsthrough learning from large-scale software repositories. However,these models suffer from three major drawbacks: a) The hierarchical structural information of the programs is not fully utilizedin the program's representation; b) In programs, the semantic relationships can be very long. Existing recurrent neural networksbased language models are not sufficient to model the long-termdependency. c) Existing approaches perform a specific task in onemodel, which leads to the underuse of the information from relatedtasks. To address these challenges, in this paper, we propose a selfattentional neural architecture for code completion with multi-tasklearning. To utilize the hierarchical structural information of theprograms, we present a novel method that considers the path fromthe predicting node to the root node. To capture the long-termdependency in the input programs, we adopt a self-attentional architecture based network as the base language model. To enable theknowledge sharing between related tasks, we creatively propose aMulti-Task Learning (MTL) framework to learn two related tasks incode completion jointly. Experiments on three real-world datasetsdemonstrate the effectiveness of our model when compared withstate-of-the-art methods. © 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Code completion; Hierarchical structure; Multi-task learning; Selfattention,"37, 47",,IEEE International Conference on Program Comprehension,Conference Paper,Scopus
607,,Adaptive deep code search,"Ling C., Lin Z., Zou Y., Xie B.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091941152&doi=10.1145%2f3387904.3389278&partnerID=40&md5=79a188fdb7432130be6aa41b489a9d78,10.1145/3387904.3389278,"Searching code in a large-scale codebase using natural languagequeries is a common practice during software development. Deeplearning-based code search methods demonstrate superior performance if models are trained with large amount of text-code pairs.However, few deep code search models can be easily transferredfrom one codebase to another. It can be very costly to preparetraining data for a new codebase and re-train an appropriate deeplearning model. In this paper, we propose AdaCS, an adaptive deepcode search method that can be trained once and transferred to newcodebases. AdaCS decomposes the learning process into embeddingdomain-specific words and matching general syntactic patterns.Firstly, an unsupervised word embedding technique is used to construct a matching matrix to represent the lexical similarities. Then,a recurrent neural network is used to capture latent syntactic patterns from these matching matrices in a supervised way. As thesupervised task learns general syntactic patterns that exist acrossdomains, AdaCS is transferable to new codebases. Experimentalresults show that: when extended to new software projects neverseen in the training data, AdaCS is more robust and significantlyoutperforms state-of-the-art deep code search methods. © 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Code search; Deep learning; Domain adaption,"48, 59",,IEEE International Conference on Program Comprehension,Conference Paper,Scopus
608,,How are deep learning models similar? an empirical study on clone analysis of deep learning software,"Wu X., Qin L., Yu B., Xie X., Ma L., Xue Y., Liu Y., Zhao J.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091912118&doi=10.1145%2f3387904.3389254&partnerID=40&md5=58b7a4c78dbf1bc43f5ba63ce405005b,10.1145/3387904.3389254,"Deep learning (DL) has been successfully applied to many cutting-edge applications, e.g., image processing, speech recognition, and natural language processing. As more and more DL software is made open-sourced, publicly available, and organized in model repositories and stores (Model Zoo, ModelDepot), there comes a need to understand the relationships of these DL models regarding their maintenance and evolution tasks. Although clone analysis has been extensively studied for traditional software, up to the present, clone analysis has not been investigated for DL software. Since DL software adopts the data-driven development paradigm, it is still not clear whether and to what extent the clone analysis techniques of traditional software could be adapted to DL software.In this paper, we initiate the first step on the clone analysis of DL software at three different levels, i.e., source code level, model structural level, and input/output (I/0)-semantic level, which would be a key in DL software management, maintenance and evolution. We intend to investigate the similarity between these DL models from clone analysis perspective. Several tools and metrics are selected to conduct clone analysis of DL software at three different levels. Our study on two popular datasets (i.e., MNIST and CIFAR-10) and eight DL models of five architectural families (i.e., LeNet, ResNet, DenseNet, AlexNet, and VGG) shows that: 1). the three levels of similarity analysis are generally adequate to find clones between DL models ranging from structural to semantic; 2). different measures for clone analysis used at each level yield similar results; 3) clone analysis of one single level may not render a complete picture of the similarity of DL models. Our findings open up several research opportunities worth further exploration towards better understanding and more effective clone analysis of DL software. © 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Code clone detection; Deep learning; Model similarity,"172, 183",,IEEE International Conference on Program Comprehension,Conference Paper,Scopus
609,,Accelerated dual-averaging primal–dual method for composite convex minimization,"Tan C., Qian Y., Ma S., Zhang T.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078478666&doi=10.1080%2f10556788.2020.1713779&partnerID=40&md5=d5b551136d8caf5fa66c1f7ff935cdf6,10.1080/10556788.2020.1713779,"Dual averaging-type methods are widely used in industrial machine learning applications due to their ability to promoting solution structure (e.g. sparsity) efficiently. In this paper, we propose a novel accelerated dual-averaging primal–dual algorithm for minimizing a composite convex function. We also derive a stochastic version of the proposed method that solves empirical risk minimization, and its advantages on handling sparse data are demonstrated both theoretically and empirically. © 2020, © 2020 Informa UK Limited, trading as Taylor & Francis Group.",acceleration; Dual averaging algorithm; empirical risk minimization; primal–dual; sparse data,"741, 766",,Optimization Methods and Software,Article,Scopus
610,,Collaborative Filtering Recommendation Based on Multi-Domain Semantic Fusion,"Li X., He J., Zhu N., Hou Z.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094170950&doi=10.1109%2fCOMPSAC48688.2020.00041&partnerID=40&md5=5269f45763b21d70bc2d8f184b5c3dba,10.1109/COMPSAC48688.2020.00041,"Collaborative filtering based on single domains has become widely used in today's recommendation system. Nevertheless, it has two problems that need to be solved, i.e., the cold start problem and the data sparseness problem. As the result, cross-domain recommendation technology has emerged, which aims at integrating user preference characteristics from different domains. This paper proposes a collaborative filtering recommendation method based on multi-domain semantic fusion (CF-MDS). CF-MDS achieves cross-domain item similarity calculation through semantic analysis and ontology and integrates data from different domains iteratively based on domain relevance to rate users on target domain items and to produce a cross-domain user-item rating matrix. Collaborative filtering technology is then combined with multi-domain fusion recommendation algorithm. Experimental results show that the proposed method can deal effectively with the cold start problem and data sparsity problem that exist in traditional recommendation systems as well as can improve the diversity of recommendation. Compared to other cross-domain recommendation methods, the proposed method can better meet personal needs of users and also improve the accuracy of recommendation. © 2020 IEEE.",collaborative filtering; cross-domain; item similarity; recommendation system; semantic analysis,"255, 261",,"Proceedings - 2020 IEEE 44th Annual Computers, Software, and Applications Conference, COMPSAC 2020",Conference Paper,Scopus
611,,SLA+: Narrowing the Difference between Data Sets in Heterogenous Cross-Project Defection Prediction,"Wu J., Wu Y., Zhou M., Jiang X.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094169201&doi=10.1109%2fCOMPSAC48688.2020.00-88&partnerID=40&md5=bc1c4c488b347a01f0a157304219f817,10.1109/COMPSAC48688.2020.00-88,"Different from existing cross-project defection prediction(CPDP) problems which assume that there is a close relation between the source data sets and the target data sets, in the heterogenous cross-project defection prediction(HCPDP) problem, the target data sets can be totally different from the source data sets. In order to narrow the difference between source data sets and target data sets, we implemented our own algorithm SLA + based on the selective learning algorithm. We select one of the multiple sources that have the highest similarity to the target data set as the source data set, and select one or more of the other source data sets that are similar to both the target data set and the source data set as an intermediate domain. We set up a bridge between the target domain and the source domain through the intermediate domain , breaking the large distribution gap for transferring knowledge between the source domain and the target domain. Besides, we achieve the purpose of dimensionality reduction by mining the potential relationship between features. We have done experiments on open source data sets, and the data sets used are all heterogeneous. The experiments prove that our method achieves comparable results compared with state-of-the-art HCPDP in most cases. © 2020 IEEE.",heterogenous cross-project defection prediction; intermediate domain; selective learning algorithm,"1229, 1234",,"Proceedings - 2020 IEEE 44th Annual Computers, Software, and Applications Conference, COMPSAC 2020",Conference Paper,Scopus
612,,Moving Vehicle Candidate Recognition and Classification Using Inception-ResNet-v2,"Thomas A., Harikrishnan P.M., Palanisamy P., Gopi V.P.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094159954&doi=10.1109%2fCOMPSAC48688.2020.0-207&partnerID=40&md5=f5cf9ede5df6b2a11d879d46f31b4d25,10.1109/COMPSAC48688.2020.0-207,Vehicle detection and classification are important tasks in the automatic traffic monitoring system. The proposed work focuses on vehicle detection and classification. Vehicle detection is carried out using the combination of dense optical flow method and integrated binary projection profile. Inception-ResNet-v2 is used as a feature extraction technique and extracted features are fed to two different classifiers such as Support Vector Machine and Random Forest to classify the vehicle type. The recognition performance of Inception-ResNet-v2 with these classifiers is significantly high and the proposed approach obtained an output accuracy as 99.89% and 98.615% in Support Vector Machine and Random forest respectively. © 2020 IEEE.,Inception-ResNet-v2; object detection; random forest; support vector machines,"467, 472",,"Proceedings - 2020 IEEE 44th Annual Computers, Software, and Applications Conference, COMPSAC 2020",Conference Paper,Scopus
613,,Interaction with Smartwatches Using Gesture Recognition: A Systematic Literature Review,"Horbylon Nascimento T., Ferreira C.B.R., Rodrigues W.G., Soares F.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094156585&doi=10.1109%2fCOMPSAC48688.2020.00-17&partnerID=40&md5=deac425c3983c0ac0c8f13ef11d9de5b,10.1109/COMPSAC48688.2020.00-17,"Smartwatches are wearable devices of emerging mobile technology and are increasingly in people's daily lives. They allow you to perform various tasks; however, the interaction with these devices still proves to be a significant challenge. Several studies carried out interaction studies with smartwatches using gesture recognition. Thus, this work presents a comprehensive Systematic Review of Literature (SRL) on interaction with smartwatches using gesture recognition, showing what has already been developed and what is state of the art. We searched in four databases with relevant scientific scope: ACM Digital Library, IEEE Xplore Digital Library, Science Direct, and Scopus. Work on this theme is diverse; gesture recognition is used to perform operations on smartwatches itself or even control other devices or virtual environments. We hope that this work will provide a rich base on the methods developed and that it will become a source of research for future researchers and that can support the development of other methods and applications. © 2020 IEEE.",Gesture Recognition; Smartwatch; Systematic Literature Review; Wrist-Worn; Wristband,"1661, 1666",,"Proceedings - 2020 IEEE 44th Annual Computers, Software, and Applications Conference, COMPSAC 2020",Conference Paper,Scopus
614,,Data Analytics for the COVID-19 Epidemic,"Wang R., Hu G., Jiang C., Lu H., Zhang Y.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094149840&doi=10.1109%2fCOMPSAC48688.2020.00-83&partnerID=40&md5=8b6bb555557193976ebd55c8b011f867,10.1109/COMPSAC48688.2020.00-83,"With the spread of COVID-19 worldwide, people¡¯s production and life have been significantly affected. Artificial intelligence and big data technologies have been vigorously developed in recent years. It is very significant to use data science and technology to help humans in a timely and accurate manner to prevent and control the development of the epidemic, maintain social stability and assess the impact of the epidemic. This paper explores how data science can play a role from the perspectives of epidemiology, social networking, and economics. In particular, for the existing epidemic model SIR, we present a parameter learning method using particle swarm optimization (PSO) and the least squares method, and use it to predict the trend of the epidemic. Aiming at the social network data, we provide a specific method to realize sentiment analysis during the epidemic and propose an explainable fake news detection technique based on a variety of data mining methods. © 2020 IEEE.",data mining; Data science; epidemic; fake news detection; sentiment analysis,"1261, 1266",,"Proceedings - 2020 IEEE 44th Annual Computers, Software, and Applications Conference, COMPSAC 2020",Conference Paper,Scopus
615,,Transfer Learning from Planar to Spherical Images,"Takeda T., Yoshida K.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094148306&doi=10.1109%2fCOMPSAC48688.2020.00037&partnerID=40&md5=7fcff42faec8101233282e79dfe62c3e,10.1109/COMPSAC48688.2020.00037,"In recent years, 360-degree cameras have been released, and making it easy to enjoy 360-degree (i.e., spherical) images. This new image format is accepted as a new personal recording medium. New businesses using it are also emerging. In such a background, 360-degree image search function increases its importance. In this paper, we propose a method for processing 360-degree images using a transfer learning framework. The reuse of network weights by transfer learning framework reduces the computing resources required for training period. The use of small image size reduces the computing resources required for test period. It also improves accuracy of image search function. This paper also discuss the relationship between this unexpected improvement and transfer learning method we used. © 2020 IEEE.",Deep learning; Spherical image; Transfer learning,"217, 224",,"Proceedings - 2020 IEEE 44th Annual Computers, Software, and Applications Conference, COMPSAC 2020",Conference Paper,Scopus
616,,An Early Warning System for Hemodialysis Complications Utilizing Transfer Learning from HD IoT Dataset,"Shih C., Youchen L., Chen C.-H., Chu W.C.-C.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094141852&doi=10.1109%2fCOMPSAC48688.2020.0-168&partnerID=40&md5=bb7f9089ac0a13f2e98d07dbfea181e0,10.1109/COMPSAC48688.2020.0-168,"According to the 2018 annual report of US Department of Kidney Data System (USRDS), Taiwan's dialysis rate and prevalence rate are the highest in the world due to population aging, diabetes and progresses in cardiovascular care. With the rise of artificial intelligence deep learning in recent years, various analytical software resources have gradually become easier to obtain. At the same time, wearable cyber physical sensors are becoming more and more popular. Measurements on vital signs such as heartbeat, electrocardiogram, and blood oxygenation blood pressure values are ubiquitous. We propose an integrated system that combines dialysis big data deep learning analysis with cross platform physiological sensing. We specifically tackle the early warning of dialysis discomfort such as hypotension, hypertension, cramps, etc., this requires a large amount of data collection, related training, data sources including dialysis treatment process and home physiological data. Although the Dialysis machine is able to produce huge amount of IoT data, the usable data for early warning system training is not as huge due to the limited physician labors devoted for labeling questionable samples. This generally leads to low accuracy for regular CNN training methods. We enhance the AI training performance via a transfer learning technique. The AI training accuracy reaches the value of 99% with the help of transfer learning, while that of an original CNN process on the HD data bears a low 60% accuracy. Given the high prediction accuracy of our AI engine, we are able to integrate the real time measurements from Dialysis machine with wearable devices such as ECG sensors and wrist health watches, and make precision prediction of incoming discomfort during the HD treatments. The ECG signal of the same group patients are also analyzed with the same technique. The same accuracy enhancement are also observed. © 2020 IEEE.",Deep learning; early warning system; Hemodialysis complications; medical IOT; mel-spectrogram; transfer learning,"759, 767",,"Proceedings - 2020 IEEE 44th Annual Computers, Software, and Applications Conference, COMPSAC 2020",Conference Paper,Scopus
617,,UNIFORM: Automatic Alignment of Open Learning Datasets,"Cagliero L., Canale L., Farinetti L.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094141130&doi=10.1109%2fCOMPSAC48688.2020.00022&partnerID=40&md5=3718549b7d34b7faefc40a3bedfb1610,10.1109/COMPSAC48688.2020.00022,"Learning Analytics aims at supporting the understanding of learning mechanisms and their effects by means of data-driven strategies. LA approaches commonly face two big challenges: first, due to privacy reasons, most of the analyzed data are not in the public domain. Secondly, the open data collections, which come from diverse learning contexts, are quite heterogeneous. Therefore, the research findings are not easily reproducible and the publicly available datasets are often too small to enable further data analytics. To overcome these issues, there is an increasing need for integrating open learning data into unified models. This paper proposes UNIFORM, an open relational database integrating various learning data sources. It presents also a machine learning supported approach to automatically extending the integrated dataset as soon as new data sources become available. The proposed approach exploits a classifier to predict attribute alignments based on the correlations among the corresponding textual attribute descriptions. The integration phase has reached a promising quality level on most of the analyzed bechmark datasets. Furthermore, the usability of the UNIFORM data model has been demonstrated in a real case study, where the integrated data have been exploited to support learners' outcome prediction. The F1-score achieved on the integrated data is approximately 30% higher that those obtained on the original data. © 2020 IEEE.",Classification; Data Integration; Learning Analytics,"95, 102",,"Proceedings - 2020 IEEE 44th Annual Computers, Software, and Applications Conference, COMPSAC 2020",Conference Paper,Scopus
618,,Generating Region of Interests for Invasive Breast Cancer in Histopathological Whole-Slide-Image,"Patil S.M., Tong L., Wang M.D.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094139642&doi=10.1109%2fCOMPSAC48688.2020.0-174&partnerID=40&md5=680f89be1de937b58c333ce7e56e9fdb,10.1109/COMPSAC48688.2020.0-174,"The detection of the region of interests (ROIs) on Whole Slide Images (WSIs) is one of the primary steps in computer-aided cancer diagnosis and grading. Early and accurate identification of invasive cancer regions in WSI is critical in the improvement of breast cancer diagnosis and further improvements in patient survival rates. However, invasive cancer ROI segmentation is a challenging task on WSI because of the low contrast of invasive cancer cells and their high similarity in terms of appearance, to non-invasive region. In this paper, we propose a CNN based architecture for generating ROIs through segmentation. The network tackles the constraints of data-driven learning and working with very low-resolution WSI data in the detection of invasive breast cancer. Our proposed approach is based on transfer learning and the use of dilated convolutions. We propose a highly modified version of U-Net based auto-encoder, which takes as input an entire WSI with a resolution of 320x320. The network was trained on low-resolution WSI from four different data cohorts and has been tested for inter as well as intra-dataset variance. The proposed architecture shows significant improvements in terms of accuracy for the detection of invasive breast cancer regions. © 2020 IEEE.",CNN; Deep Learning; Invasive Breast Cancer; ROI Segmentation; Whole-Slide-Image,"723, 728",,"Proceedings - 2020 IEEE 44th Annual Computers, Software, and Applications Conference, COMPSAC 2020",Conference Paper,Scopus
619,,Cinematographic Shot Classification through Deep Learning,"Vacchetti B., Cerquitelli T., Antonino R.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094129077&doi=10.1109%2fCOMPSAC48688.2020.0-222&partnerID=40&md5=bcced02e42c4a65f019bb77ac1cb27b1,10.1109/COMPSAC48688.2020.0-222,"Cinematographic shot classification assigns a category to each shot on the basis of the field size, which is determined by the portion of the subject and of the environment shown in the field of view of the camera. This task is very important in the context of the creative field and can help freelancers in their daily activities when it is performed automatically. Novel and effective approaches capable of processing large volumes of images/videos and analyzing them effectively are becoming increasingly important. This paper presents a data-driven methodology to automatically classify cinematographic shots through deep learning techniques. In our study, we consider four classes of film shots: full figure, half figure, half torso and close up and we discuss three different scenarios in which the proposed work can be helpful. A new dataset of images was created to evaluate performances of the proposed methodology and to compare them with state-of-the-art techniques. Experimental results demonstrate the effectiveness of the proposed approach in performing the classification task with good accuracy. © 2020 IEEE.",creative field; image classification; Machine learning,"345, 350",,"Proceedings - 2020 IEEE 44th Annual Computers, Software, and Applications Conference, COMPSAC 2020",Conference Paper,Scopus
620,,Dual Adversarial Networks for Land-Cover Classification,"An J., Wei R., Zha Z., Dong B.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094128642&doi=10.1109%2fCOMPSAC48688.2020.0-216&partnerID=40&md5=c24d9df981b15c9f08f762257e8fa6f0,10.1109/COMPSAC48688.2020.0-216,"River basin scene classification as an important application in the field of land-cover recognition has been arousing extensive concern. Traditional land-cover classification methods with multi-feature extractions on specific scene perform well on a single river basin, however, poorly address inter-basin classification owing to the varied texture shown in satellite images cross river basins (e.g., topography and climates). Current transfer learning approaches with domain adaptation, which can shorten the discrepancies between two river basins, pay less attention to diversity of multi-feature extractions given by remote sensing images, which may lead to negative transfer. To better address the above challenges, this paper proposes a model known as Dual Adversarial Networks for Land-cover Classification (DANLC). Our DANLC architecture consists of two domain adversarial networks in a paralleled structure, namely RGB and texture networks, for multi-feature extractions, which are able to capture the underlying representation of satellite images from different perspectives and get invariable transfer component. Results demonstrate the outstanding performance of our model in both the classification effect and robustness compared with traditional methods and state-of-the-art transfer learning approaches. © 2020 IEEE.",,"394, 399",,"Proceedings - 2020 IEEE 44th Annual Computers, Software, and Applications Conference, COMPSAC 2020",Conference Paper,Scopus
621,,A Novel Tax Evasion Detection Framework via Fused Transaction Network Representation,"Wu Y., Dong B., Zheng Q., Wei R., Wang Z., Li X.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094122766&doi=10.1109%2fCOMPSAC48688.2020.00039&partnerID=40&md5=9e6a15846885241854ddab055ca2e7da,10.1109/COMPSAC48688.2020.00039,"Tax evasion usually refers to the false declaration of taxpayers to reduce their tax obligations; this type of behavior leads to the loss of taxes and damage to the fair principle of taxation. Tax evasion detection plays a crucial role in reducing tax revenue loss. Currently, efficient auditing methods mainly include traditional data-mining-oriented methods, which cannot be well adapted to the increasingly complicated transaction relationships between taxpayers. Driven by this requirement, recent studies have been conducted by establishing a transaction network and applying the graphical pattern matching algorithm for tax evasion identification. However, such methods rely on expert experience to extract the tax evasion chart pattern, which is time-consuming and labor-intensive. More importantly, taxpayers' basic attributes are not considered and the dual identity of the taxpayer in the transaction network is not well retained. To address this issue, we have proposed a novel tax evasion detection framework via fused transaction network representation (TED-TNR), to detecting tax evasion based on fused transaction network representation, which jointly embeds transaction network topological information and basic taxpayer attributes into low-dimensional vector space, and considers the dual identity of the taxpayer in the transaction network. Finally, we conducted experimental tests on real-world tax data, revealing the superiority of our method, compared with state-of-the-art models. © 2020 IEEE.",Network representation learning; Tax evasion detection; Transaction network,"235, 244",,"Proceedings - 2020 IEEE 44th Annual Computers, Software, and Applications Conference, COMPSAC 2020",Conference Paper,Scopus
622,,TTED-PU:A Transferable Tax Evasion Detection Method Based on Positive and Unlabeled Learning,"Zhang F., Shi B., Dong B., Zheng Q., Ji X.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094106817&doi=10.1109%2fCOMPSAC48688.2020.00036&partnerID=40&md5=fc067b0c602a3ff571f085aa9178c87c,10.1109/COMPSAC48688.2020.00036,"Tax evasion usually refers to taxpayers making false declarations in order to reduce their tax obligations. One of the most common types of tax evasion is to lower the declared taxable amount. This kind of behavior will lead to the loss of tax revenues and damage the fairness of taxation. One of the main roles of the tax authorities is to conduct tax evasion testing through efficient auditing methods. At present, by using machine learning technology along with large amounts of labeled data, tax evasion detection models have achieved good results in specific areas. However, it is a long and costly process for tax experts to label large amounts of data. Since, the data distribution characteristics vary from region to region, models cannot be used across regions. In this paper, we propose a new method called a transferable tax evasion detection method based on positive and unlabeled learning (TTED-PU), which uses only semi-supervised techniques to detect tax evasion in the source domain. In addition, we use the idea of transfer to adapt to the domain to predict tax evasion behavior on the target domain where labeled tax data are unavailable. We evaluate our method on real-world tax data set. The experimental results show that our model can detect tax evasion in both the source and target domains. © 2020 IEEE.",positive and unlabeled learning; tax evasion; transfer learning,"207, 216",,"Proceedings - 2020 IEEE 44th Annual Computers, Software, and Applications Conference, COMPSAC 2020",Conference Paper,Scopus
623,,Unified Target-based Sentiment Analysis by Dual-pointer Tagging Scheme,"Zhou X., Liang T., Wang B.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094104530&doi=10.1109%2fCOMPSAC48688.2020.0-233&partnerID=40&md5=c859c1eb05bbcf7c80b597c5b8a4ded8,10.1109/COMPSAC48688.2020.0-233,"Target-based sentiment analysis aims at extracting opinion targets and predicting their sentiment polarities from a given sentence. Previous researches have been trying to joint the two sub-task in a single architecture. However, the target extraction schemes of these approaches are facing various issues, such as ignoring the sentiment consistency of composite targets, or being too sensitive or time-consuming in heuristic search. We propose a model on top of the BERT encoder by designing a dual-pointer binary classifier for target extraction and a joint multi-class classifier for sentiment classification. Experiments on two datasets show the effectiveness of our model. © 2020 IEEE.",Opinion-Target-Extraction; Sentiment-Classification; Target-based-Sentiment-Analysis,"272, 276",,"Proceedings - 2020 IEEE 44th Annual Computers, Software, and Applications Conference, COMPSAC 2020",Conference Paper,Scopus
624,,NEUD-TRI: Network Embedding Based on Upstream and Downstream for Transaction Risk Identification,"An J., Zheng Q., Wei R., Dong B., Li X.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094104493&doi=10.1109%2fCOMPSAC48688.2020.0-232&partnerID=40&md5=a3ed06834ee17831c70abc2ae2f70af8,10.1109/COMPSAC48688.2020.0-232,"Invoices serve as records of financial transactions of taxpayers and significant basis to controlling tax source and collection of tax, via analyzing which, we can discern diversified tasks of tax risk, such as industry identification, hidden transaction detection, and illegal behavior mining. Among all the existing studies related to the identification of tax risk, there are some weaknesses through the machine learning model and network analysis because of the dependence on tax knowledge. Different from the manual selection of indicators and the manual definitions mode with the guidance of tax knowledge in the past, in this paper, we propose a novel method, namely, network embedding based on upstream and downstream for tax risk identification (NEUD-TRI), which considers the taxpayers serving as both seller and purchaser. The method designs optimization functions respectively to capture local and global static network structures and dynamic network structure. In view of the significant discrepancy of weights in the transaction network, this paper normalizes the weight within the range of the upstream and downstream of the vertex. Negative sampling and edge sampling are adopted to deal with the large-scale trait of the transaction network. Empirical results on tax data-sets of Shanxi province substantiate the effectiveness of our models. © 2020 IEEE.",Network Representation Learning; Tax Risk Identification; Taxpayer Representation; Transaction Network,"277, 286",,"Proceedings - 2020 IEEE 44th Annual Computers, Software, and Applications Conference, COMPSAC 2020",Conference Paper,Scopus
625,,Survey of Machine Reading Comprehension Based on Neural Network [基于神经网络的机器阅读理解综述],"Gu Y.-J., Gui X.-L., Li D.-F., Shen Y., Liao D.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088219642&doi=10.13328%2fj.cnki.jos.006048&partnerID=40&md5=8c86f8ba13410f61e5fcbe30e525a7c6,10.13328/j.cnki.jos.006048,"The task of machine reading comprehension is to make the machine understand natural language text and correctly answer text-related questions. Due to the limitation of the dataset scale, most of the early machine reading comprehension methods were modeled based on manual features and traditional machine learning methods. In recent years, with the development of knowledge bases and crowdsourcing, high quality large-scale datasets have been proposed by researchers, which has brought a new opportunity for the advance of neural network models and machine reading comprehension. In this survey, an exhaustive review on the state-of-the-art research efforts on machine reading comprehension based on neural network is made. First, an overview of machine reading comprehension, including development process, problem formulation, and evaluation metric, is given. Then, a comprehensive review is conducted of related technologies in the most fashionable neural reading comprehension framework including the embedding layer, encoder layer, interaction layer, and output layer as well as the latest BERT pre-training model and its advantages are discussed. After that, this paper concludes the recent research progress of machine reading comprehension datasets and neural reading comprehension model, and gives a comparison and analysis of the most representative datasets and neural network models in detail. Finally, the research challenges and future direction of machine reading comprehension are presented. © Copyright 2020, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Attention mechanism; Machine reading comprehension; Natural language processing; Neural reading comprehension model,"2095, 2126",,Ruan Jian Xue Bao/Journal of Software,Review,Scopus
626,,Survey of Software Vulnerability Mining Methods Based on Machine Learning [基于机器学习的软件漏洞挖掘方法综述],"Li Y., Huang C.-L., Wang Z.-F., Yuan L., Wang X.-C.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088218353&doi=10.13328%2fj.cnki.jos.006055&partnerID=40&md5=520b54692d2d051f469a59aa8b1b97ed,10.13328/j.cnki.jos.006055,"The increasing complexity of software application brings great challenges to software security. Due to the increase of software scale and diversity of vulnerability forms, the high false positives and false negatives of traditional vulnerability mining methods cannot meet the requirements of software security analysis. In recent years, with the rise of artificial intelligence industry, a large number of machine learning methods have been tried to solve the problem of software vulnerability mining. Firstly, the latest research results of applying machine learning method to the research of vulnerability mining are summarized in recent years, and the technical characteristics and workflow are proposed. Then, starting from the core original data features extraction, the existing research is classified according to the code representation form, and the existing research is systematically compared. Finally, based on the summary of the existing research, the challenges in the field of software vulnerability mining based on machine learning are discussed, and the development trends of this field are proposed. © Copyright 2020, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Code representation; Deep learning; Machine learning; Software quality; Vulnerability mining,"2040, 2061",,Ruan Jian Xue Bao/Journal of Software,Review,Scopus
627,,Easy Way for Multilayer Gradient Supplies [一种简单的共享式多层梯度补给方法],"Du F., Yang Y., Hu Y.-Y., Cao L.-J.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088215796&doi=10.13328%2fj.cnki.jos.005822&partnerID=40&md5=bab10d9022608cef4333af2971b43d27,10.13328/j.cnki.jos.005822,"Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These have dramatically improved the state-of-the-art methods in speech recognition, visual object recognition, natural language processing, and many other domains. However, due to the large number of layers and large parameter scales, deep learning often results in gradient vanishing, falling into local optimal solution, overfitting, and so on. By using ensemble learning methods, this study proposes a novel deep sharing ensemble network. Through joint training many independent output layers in each hidden layer and injecting gradients, this network can reduce the gradient vanishing phenomenon, and through ensemble multi-output, it can get a better generalization performance. © Copyright 2020, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Deep learning; Ensemble learning; Gradients injection; Stacked generalization; Vanishing gradients,"2157, 2168",,Ruan Jian Xue Bao/Journal of Software,Article,Scopus
628,,Adversarial Training Triplet Network for Fine-grained Sketch Based Image Retrieval [面向细粒度草图检索的对抗训练三元组网络],"Chen J., Bai C., Ma Q., Hao P.-Y., Chen S.-Y.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088215492&doi=10.13328%2fj.cnki.jos.005934&partnerID=40&md5=fe32d516aa482920397e12fc35060849,10.13328/j.cnki.jos.005934,"Sketch based image retrieval means that the sketch is used as the query in the retrieval. Fine-grained image retrieval or intra-categoryretrieval was proposed in 2014 and attracted more attentions quickly. Triplet network is often used to do fine-grained retrieval and get promising performance. However, training triplet network is quite difficult, it is hard to converge and easy to over-fit in some situations. Inspired by the adversarial training, this study proposes SketchCycleGAN to improve the efficiency of the triplet network training process. In this proposal, pre-training the networks with other database is replaced by mining the information inside the database with the help of adversarial training. That could simplify the training procedure with better performance. This proposal could get better performance than other state-of-the-art methods in a series of experiments executed on widely used databases for fine-grained sketchbased retrieval. © Copyright 2020, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Adversarial training; Fine-grained retrieval; Sketch based image retrieval; Triplet network,"1933, 1942",,Ruan Jian Xue Bao/Journal of Software,Article,Scopus
629,,"Survey of Automatic Ultrasonographic Analysis for Thyroid and Breast [甲状腺, 乳腺超声影像自动分析技术综述]","Gong X., Yang F., Du Z.-J., Shi E., Zhao X., Yang Z.-Q., Zou H.-P., Luo J.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088213645&doi=10.13328%2fj.cnki.jos.006037&partnerID=40&md5=3932b5f4b4efd84187e80d0b276881f6,10.13328/j.cnki.jos.006037,"Ultrasonography is the first choice of imaging examination and preoperative evaluation for thyroid and breast cancer. However, ultrasonic characteristics of benign and malignant nodules are commonly overlapped. The diagnosis heavily relies on operator's experience other than quantitative and stable methods. In recent years, medical imaging analysis based on computer technology has developed rapidly, and a series of landmark breakthroughs have been made, which provides effective decision supports for medical imaging diagnosis. In this work, the research progress of computer vision and image recognition technologies in thyroid and breast ultrasound images is studied. A series of key technologies involved in automatic diagnosis of ultrasound images is the main lines of the work. The major algorithms in recent years are summarized and analyzed, such as ultrasound image preprocessing, lesion localization and segmentation, feature extraction and classification. Moreover, multi-dimensional analysis is made on the algorithms, data sets, and evaluation methods. Finally, existing problems related to automatic analysis of those two kinds of ultrasound imaging are discussed, research trend and development direction in the field of ultrasound images analysis are discussed. © Copyright 2020, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Automated diagnosis; Breast cancer; Computer aided diagnosis; Thyroid cancer; Ultrasound image,"2245, 2282",,Ruan Jian Xue Bao/Journal of Software,Review,Scopus
630,,Segmenting skin ulcers and measuring the wound area using deep convolutional networks,"Chino D.Y.T., Scabora L.C., Cazzolato M.T., Jorge A.E.S., Traina C., Jr., Traina A.J.M.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079356363&doi=10.1016%2fj.cmpb.2020.105376&partnerID=40&md5=b7669ffe9192a0371debb5e011730b4e,10.1016/j.cmpb.2020.105376,"Background and objectives:Bedridden patients presenting chronic skin ulcers often need to be examined at home. Healthcare professionals follow the evolution of the patients’ condition by regularly taking pictures of the wounds, as different aspects of the wound can indicate the healing stages of the ulcer, including depth, location, and size. The manual measurement of the wounds’ size is often inaccurate, time-consuming, and can also cause discomfort to the patient. In this work, we propose the Automatic Skin Ulcer Region Assessment ASURA framework to accurately segment the wound and automatically measure its size. Methods:ASURA uses an encoder/decoder deep neural network to perform the segmentation, which detects the measurement ruler/tape present in the image and estimates its pixel density. Results:Experimental results show that ASURA outperforms the state-of-the-art methods by up to 16% regarding the Dice score, being able to correctly segment the wound with a Dice score higher than 90%. ASURA automatically estimates the pixel density of the images with a relative error of 5%. When using a semi-automatic approach, ASURA was able to estimate the area of the wound in square centimeters with a relative error of 14%. Conclusions:The results show that ASURA is well-suited for the problem of segmenting and automatically measuring skin ulcers. © 2020",Deep convolutional neural networks; Image segmentation; Skin ulcer; Wound measurement,,,Computer Methods and Programs in Biomedicine,Article,Scopus
631,,Multiple skin lesions diagnostics via integrated deep convolutional networks for segmentation and classification,"Al-masni M.A., Kim D.-H., Kim T.-S.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078780551&doi=10.1016%2fj.cmpb.2020.105351&partnerID=40&md5=b232d473289572e7864d37e6af4b1eb3,10.1016/j.cmpb.2020.105351,"Background and objective: Computer automated diagnosis of various skin lesions through medical dermoscopy images remains a challenging task. Methods: In this work, we propose an integrated diagnostic framework that combines a skin lesion boundary segmentation stage and a multiple skin lesions classification stage. Firstly, we segment the skin lesion boundaries from the entire dermoscopy images using deep learning full resolution convolutional network (FrCN). Then, a convolutional neural network classifier (i.e., Inception-v3, ResNet-50, Inception-ResNet-v2, and DenseNet-201) is applied on the segmented skin lesions for classification. The former stage is a critical prerequisite step for skin lesion diagnosis since it extracts prominent features of various types of skin lesions. A promising classifier is selected by testing well-established classification convolutional neural networks. The proposed integrated deep learning model has been evaluated using three independent datasets (i.e., International Skin Imaging Collaboration (ISIC) 2016, 2017, and 2018, which contain two, three, and seven types of skin lesions, respectively) with proper balancing, segmentation, and augmentation. Results: In the integrated diagnostic system, segmented lesions improve the classification performance of Inception-ResNet-v2 by 2.72% and 4.71% in terms of the F1-score for benign and malignant cases of the ISIC 2016 test dataset, respectively. The classifiers of Inception-v3, ResNet-50, Inception-ResNet-v2, and DenseNet-201 exhibit their capability with overall weighted prediction accuracies of 77.04%, 79.95%, 81.79%, and 81.27% for two classes of ISIC 2016, 81.29%, 81.57%, 81.34%, and 73.44% for three classes of ISIC 2017, and 88.05%, 89.28%, 87.74%, and 88.70% for seven classes of ISIC 2018, respectively, demonstrating the superior performance of ResNet-50. Conclusions: The proposed integrated diagnostic networks could be used to support and aid dermatologists for further improvement in skin cancer diagnosis. © 2020",CAD; Classification; CNN; Deep learning; ISIC; Melanoma; Segmentation; Skin lesion,,,Computer Methods and Programs in Biomedicine,Article,Scopus
632,,Computer‐aided diagnosis of breast ultrasound images using ensemble learning from convolutional neural networks,"Moon W.K., Lee Y.-W., Ke H.-H., Lee S.H., Huang C.-S., Chang R.-F.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078543653&doi=10.1016%2fj.cmpb.2020.105361&partnerID=40&md5=6bc399514d7900c7d7139f2e5501bfe0,10.1016/j.cmpb.2020.105361,"Breast ultrasound and computer aided diagnosis (CAD) has been used to classify tumors into benignancy or malignancy. However, conventional CAD software has some problems (such as handcrafted features are hard to design; conventional CAD systems are difficult to confirm overfitting problems, etc.). In our study, we propose a CAD system for tumor diagnosis using an image fusion method combined with different image content representations and ensemble different CNN architectures on US images. The CNN-based method proposed in this study includes VGGNet, ResNet, and DenseNet. In our private dataset, there was a total of 1687 tumors that including 953 benign and 734 malignant tumors. The accuracy, sensitivity, specificity, precision, F1 score and the AUC of the proposed method were 91.10%, 85.14%, 95.77%, 94.03%, 89.36%, and 0.9697 respectively. In the open dataset (BUSI), there was a total of 697 tumors that including 437 benign lesions, 210 malignant tumors, and 133 normal images. The accuracy, sensitivity, specificity, precision, F1 score, and the AUC of the proposed method were 94.62%, 92.31%, 95.60%, 90%, 91.14%, and 0.9711. In conclusion, the results indicated different image content representations that affect the prediction performance of the CAD system, more image information improves the prediction performance, and the tumor shape feature can improve the diagnostic effect. © 2020",Breast cancer; Breast ultrasound; Computer-aided diagnosis; Convolutional neural network; Deep learning; Ensemble learning,,,Computer Methods and Programs in Biomedicine,Article,Scopus
633,,Cheating Death: A Statistical Survival Analysis of Publicly Available Python Projects,"Ali R.H., Parlett-Pelleriti C., Linstead E.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093696908&doi=10.1145%2f3379597.3387511&partnerID=40&md5=6393b68e2ef1fde04be6e0959c111a05,10.1145/3379597.3387511,"We apply survival analysis methods to a dataset of publicly-available software projects in order to examine the attributes that might lead to their inactivity over time. We ran a Kaplan-Meier analysis and fit a Cox Proportional-Hazards model to a subset of Software Heritage Graph Dataset, consisting of 3052 popular Python projects hosted on GitLab/GitHub, Debian, and PyPI, over a period of 165 months. We show that projects with repositories on multiple hosting services, a timeline of publishing major releases, and a good network of developers, remain healthy over time and should be worthy of the effort put in by developers and contributors. © 2020 ACM.",hazard ratios; open source software projects; software repository health; survival analysis,"6, 10",,"Proceedings - 2020 IEEE/ACM 17th International Conference on Mining Software Repositories, MSR 2020",Conference Paper,Scopus
634,,A Machine Learning Approach for Vulnerability Curation,"Chen Y., Santosa A.E., Yi A.M., Sharma A., Sharma A., Lo D.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093688307&doi=10.1145%2f3379597.3387461&partnerID=40&md5=69ea6acf785954d248b95ec09c1add64,10.1145/3379597.3387461,"Software composition analysis depends on database of open-source library vulerabilities, curated by security researchers using various sources, such as bug tracking systems, commits, and mailing lists. We report the design and implementation of a machine learning system to help the curation by by automatically predicting the vulnerability-relatedness of each data item. It supports a complete pipeline from data collection, model training and prediction, to the validation of new models before deployment. It is executed iteratively to generate better models as new input data become available. We use self-training to significantly and automatically increase the size of the training dataset, opportunistically maximizing the improvement in the models' quality at each iteration. We devised new deployment stability metric to evaluate the quality of the new models before deployment into production, which helped to discover an error. We experimentally evaluate the improvement in the performance of the models in one iteration, with 27.59% maximum PR AUC improvements. Ours is the first of such study across a variety of data sources. We discover that the addition of the features of the corresponding commits to the features of issues/pull requests improve the precision for the recall values that matter. We demonstrate the effectiveness of self-training alone, with 10.50% PR AUC improvement, and we discover that there is no uniform ordering of word2vec parameters sensitivity across data sources. © 2020 ACM.",application security; classifiers ensemble; machine learning; open-source software; self-training,"32, 42",,"Proceedings - 2020 IEEE/ACM 17th International Conference on Mining Software Repositories, MSR 2020",Conference Paper,Scopus
635,,Traceability Support for Multi-Lingual Software Projects,"Liu Y., Lin J., Cleland-Huang J.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093673754&doi=10.1145%2f3379597.3387440&partnerID=40&md5=5650523c11ffd3e68443cfefcaa28f78,10.1145/3379597.3387440,"Software traceability establishes associations between diverse software artifacts such as requirements, design, code, and test cases. Due to the non-trivial costs of manually creating and maintaining links, many researchers have proposed automated approaches based on information retrieval techniques. However, many globally distributed software projects produce software artifacts written in two or more languages. The use of intermingled languages reduces the efficacy of automated tracing solutions. In this paper, we first analyze and discuss patterns of intermingled language use across multiple projects, and then evaluate several different tracing algorithms including the Vector Space Model (VSM), Latent Semantic Indexing (LSI), Latent Dirichlet Allocation (LDA), and various models that combine mono-and cross-lingual word embeddings with the Generative Vector Space Model (GVSM). Based on an analysis of 14 Chinese-English projects, our results show that best performance is achieved using mono-lingual word embeddings integrated into GVSM with machine translation as a preprocessing step. © 2020 ACM.",Cross-lingual information retrieval; Generalized Vector Space Model; Traceability,"443, 454",,"Proceedings - 2020 IEEE/ACM 17th International Conference on Mining Software Repositories, MSR 2020",Conference Paper,Scopus
636,,AIMMX: Artificial Intelligence Model Metadata Extractor,"Tsay J., Braz A., Hirzel M., Shinnar A., Mummert T.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093646767&doi=10.1145%2f3379597.3387448&partnerID=40&md5=6d778f6e5b3f863f5617638a54f2f921,10.1145/3379597.3387448,"Despite all of the power that machine learning and artificial intelligence (AI) models bring to applications, much of AI development is currently a fairly ad hoc process. Software engineering and AI development share many of the same languages and tools, but AI development as an engineering practice is still in early stages. Mining software repositories of AI models enables insight into the current state of AI development. However, much of the relevant metadata around models are not easily extractable directly from repositories and require deduction or domain knowledge. This paper presents a library called AIMMX that enables simplified AI Model Metadata eXtraction from software repositories. The extractors have five modules for extracting AI model-specific metadata: model name, associated datasets, references, AI frameworks used, and model domain. We evaluated AIMMX against 7,998 open-source models from three sources: model zoos, arXiv AI papers, and state-of-the-art AI papers. Our platform extracted metadata with 87% precision and 83% recall. As preliminary examples of how AI model metadata extraction enables studies and tools to advance engineering support for AI development, this paper presents an exploratory analysis for data and method reproducibility over the models in the evaluation dataset and a catalog tool for discovering and managing models. Our analysis suggests that while data reproducibility may be relatively poor with 42% of models in our sample citing their datasets, method reproducibility is more common at 72% of models in our sample, particularly state-of-the-art models. Our collected models are searchable in a catalog that uses existing metadata to enable advanced discovery features for efficiently finding models. © 2020 ACM.",Artificial Intelligence; Machine Learning; Metadata Extraction; Model Catalog; Model Metadata; Model Mining,"81, 92",,"Proceedings - 2020 IEEE/ACM 17th International Conference on Mining Software Repositories, MSR 2020",Conference Paper,Scopus
637,,A hybrid approach combining control theory and AI for engineering self-adaptive systems,"Caldas R.D., Rodrigues A., Gil E.B., Rodrigues G.N., Vogel T., Pelliccione P.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093094141&doi=10.1145%2f3387939.3391595&partnerID=40&md5=cf522424b4c566813c87f4cac93438a2,10.1145/3387939.3391595,"Control theoretical techniques have been successfully adopted as methods for self-adaptive systems design to provide formal guarantees about the effectiveness and robustness of adaptation mechanisms. However, the computational effort to obtain guarantees poses severe constraints when it comes to dynamic adaptation. In order to solve these limitations, in this paper, we propose a hybrid approach combining software engineering, control theory, and AI to design for software self-adaptation. Our solution proposes a hierarchical and dynamic system manager with performance tuning. Due to the gap between high-level requirements specification and the internal knob behavior of the managed system, a hierarchically composed components architecture seek the separation of concerns towards a dynamic solution. Therefore, a two-layered adaptive manager was designed to satisfy the software requirements with parameters optimization through regression analysis and evolutionary meta-heuristic. The optimization relies on the collection and processing of performance, effectiveness, and robustness metrics w.r.t control theoretical metrics at the offline and online stages. We evaluate our work with a prototype of the Body Sensor Network (BSN) in the healthcare domain, which is largely used as a demonstrator by the community. The BSN was implemented under the Robot Operating System (ROS) architecture, and concerns about the system dependability are taken as adaptation goals. Our results reinforce the necessity of performing well on such a safety-critical domain and contribute with substantial evidence on how hybrid approaches that combine control and AI-based techniques for engineering self-adaptive systems can provide effective adaptation. © 2020 ACM.",control theory; hybrid; optimization; self-adaptive software,"9, 19",,"Proceedings - 2020 IEEE/ACM 15th International Symposium on Software Engineering for Adaptive and Self-Managing Systems, SEAMS 2020",Conference Paper,Scopus
638,,DATESSO: Self-adapting service composition with debt-aware two levels constraint reasoning,"Kumar S., Chen T., Bahsoon R., Buyya R.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087529805&doi=10.1145%2f3387939.3391604&partnerID=40&md5=56aec211cbf7287b6c716c28bfd7d048,10.1145/3387939.3391604,"The rapidly changing workload of service-based systems can easily cause under-/over-utilization on the component services, which can consequently affect the overall Quality of Service (QoS), such as latency. Self-adaptive services composition rectifies this problem, but poses several challenges: (i) the effectiveness of adaptation can deteriorate due to over-optimistic assumptions on the latency and utilization constraints, at both local and global levels; and (ii) the benefits brought by each composition plan is often short term and is not often designed for long-term benefits - -a natural prerequisite for sustaining the system. To tackle these issues, we propose a two levels constraint reasoning framework for sustainable self-adaptive services composition, called DATESSO. In particular, DATESSO consists of a refined formulation that differentiates the 'strictness' for latency/utilization constraints in two levels. To strive for long-term benefits, DATESSO leverages the concept of technical debt and time-series prediction to model the utility contribution of the component services in the composition. The approach embeds a debt-aware two level constraint reasoning algorithm in DATESSO to improve the efficiency, effectiveness and sustainability of self-adaptive service composition. We evaluate DATESSO on a service-based system with real-world WS-DREAM dataset and comparing it with other state-of-the-art approaches. The results demonstrate the superiority of DATESSO over the others on the utilization, latency and running time whilst likely to be more sustainable. © 2020 ACM.",constraint reasoning; search-based software engineering; self-adaptive systems; service composition; technical debt,"96, 107",,"Proceedings - 2020 IEEE/ACM 15th International Symposium on Software Engineering for Adaptive and Self-Managing Systems, SEAMS 2020",Conference Paper,Scopus
639,,Software visualization and deep transfer learning for effective software defect prediction,"Chen J., Hu K., Yu Y., Chen Z., Xuan Q., Liu Y., Filkov V.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094325423&doi=10.1145%2f3377811.3380389&partnerID=40&md5=2a1457f596ae5021a94c6cdb60b9e91b,10.1145/3377811.3380389,"Software defect prediction aims to automatically locate defective code modules to better focus testing resources and human effort. Typically, software defect prediction pipelines are comprised of two parts: the first extracts program features, like abstract syntax trees, by using external tools, and the second applies machine learningbased classification models to those features in order to predict defective modules. Since such approaches depend on specific feature extraction tools, machine learning classifiers have to be customtailored to effectively build most accurate models. To bridge the gap between deep learning and defect prediction, we propose an end-to-end framework which can directly get prediction results for programs without utilizing feature-extraction tools. To that end, we first visualize programs as images, apply the self-attention mechanism to extract image features, use transfer learning to reduce the difference in sample distributions between projects, and finally feed the image files into a pre-trained, deep learning model for defect prediction. Experiments with 10 open source projects from the PROMISE dataset show that our method can improve cross-project and within-project defect prediction. Our code and data pointers are available at https://zenodo.org/record/3373409#.XV0Oy5Mza35. © 2020 Association for Computing Machinery.",Cross-project defect prediction; Deep transfer learning; Self-attention; Software visualization; Within-project defect prediction,"578, 589",,Proceedings - International Conference on Software Engineering,Conference Paper,Scopus
640,,An investigation of cross-project learning in online just-in-time so ware defect prediction,"Tabassum S., Minku L.L., Feng D., Cabral G.G., Song L.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094324825&doi=10.1145%2f3377811.3380403&partnerID=40&md5=85d70bb95741ee5dd28f6636bace4c54,10.1145/3377811.3380403,"Just-In-Time Software Defect Prediction (JIT-SDP) is concerned with predicting whether software changes are defect-inducing or clean based on machine learning classifiers. Building such classifiers requires a sufficient amount of training data that is not available at the beginning of a software project. Cross-Project (CP) JIT-SDP can overcome this issue by using data from other projects to build the classifier, achieving similar (not better) predictive performance to classifiers trained on Within-Project (WP) data. However, such approaches have never been investigated in realistic online learning scenarios, where WP software changes arrive continuously over time and can be used to update the classifiers. It is unknown to what extent CP data can be helpful in such situation. In particular, it is unknown whether CP data are only useful during the very initial phase of the project when there is little WP data, or whether they could be helpful for extended periods of time. This work thus provides the first investigation of when and to what extent CP data are useful for JIT-SDP in a realistic online learning scenario. For that, we develop three different CP JIT-SDP approaches that can operate in online mode and be updated with both incoming CP and WP training examples over time.We also collect 2048 commits from three software repositories being developed by a software company over the course of 9 to 10 months, and use 19,8468 commits from 10 active open source GitHub projects being developed over the course of 6 to 14 years. The study shows that training classifiers with incoming CP+WP data can lead to improvements in G-mean of up to 53.90% compared to classifiers using only WP data at the initial stage of the projects. For the open source projects, which have been running for longer periods of time, using CP data to supplement WP data also helped the classifiers to reduce or prevent large drops in predictive performance that may occur over time, leading to up to around 40% better G-Mean during such periods. Such use of CP data was shown to be beneficial even after a large number of WP data were received, leading to overall G-means up to 18.5% better than those of WP classifiers. © 2020 Association for Computing Machinery.",Class imbalance; Concept drift; Cross-project learning; Online learning; Software defect prediction; Transfer learning; Verification latency,"554, 565",,Proceedings - International Conference on Software Engineering,Conference Paper,Scopus
641,,Caspar: Extracting and synthesizing user stories of problems from app reviews,"Guo H., Singh M.P.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094324613&doi=10.1145%2f3377811.3380924&partnerID=40&md5=047a556dd716b8881dfdef06963b0d21,10.1145/3377811.3380924,"A user's review of an app often describes the user's interactions with the app. These interactions, which we interpret as mini stories, are prominent in reviews with negative ratings. In general, a story in an app review would contain at least two types of events: user actions and associated app behaviors. Being able to identify such stories would enable an app's developer in better maintaining and improving the app's functionality and enhancing user experience. We present Caspar, a method for extracting and synthesizing user-reported mini stories regarding app problems from reviews. By extending and applying natural language processing techniques, Caspar extracts ordered events from app reviews, classifies them as user actions or app problems, and synthesizes action-problem pairs. Our evaluation shows that Caspar is effective in finding actionproblem pairs from reviews. First, Caspar classifies the events with an accuracy of 82.0% on manually labeled data. Second, relative to human evaluators, Caspar extracts event pairs with 92.9% precision and 34.2% recall. In addition, we train an inference model on the extracted action-problem pairs that automatically predicts possible app problems for different use cases. Preliminary evaluation shows that our method yields promising results. Caspar illustrates the potential for a deeper understanding of app reviews and possibly other natural language artifacts arising in software engineering. © 2020 Association for Computing Machinery.",,"628, 640",,Proceedings - International Conference on Software Engineering,Conference Paper,Scopus
642,,Trader: Trace divergence analysis and embedding regulation for debugging recurrent neural networks,"Tao G., Ma S., Liu Y., Xu Q., Zhang X.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094321849&doi=10.1145%2f3377811.3380423&partnerID=40&md5=9d83127358f004a3e4f11ebdef411ad9,10.1145/3377811.3380423,"Recurrent Neural Networks (RNN) can deal with (textual) input with various length and hence have a lot of applications in software systems and software engineering applications. RNNs depend on word embeddings that are usually pre-trained by third parties to encode textual inputs to numerical values. It is well known that problematic word embeddings can lead to low model accuracy. In this paper, we propose a new technique to automatically diagnose how problematic embeddings impact model performance, by comparing model execution traces from correctly and incorrectly executed samples. We then leverage the diagnosis results as guidance to harden/repair the embeddings. Our experiments show that TRADER can consistently and effectively improve accuracy for real world models and datasets by 5.37% on average, which represents substantial improvement in the literature of RNN models. © 2020 Association for Computing Machinery.",,"986, 998",,Proceedings - International Conference on Software Engineering,Conference Paper,Scopus
643,,An empirical study on program failures of deep learning jobs,"Zhang R., Xiao W., Zhang H., Liu Y., Lin H., Yang M.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094321455&doi=10.1145%2f3377811.3380362&partnerID=40&md5=b3dc9c027902dc850d064e47f9897957,10.1145/3377811.3380362,"Deep learning has made significant achievements in many application areas. To train and test models more efficiently, enterprise developers submit and run their deep learning programs on a shared, multi-tenant platform. However, some of the programs fail after a long execution time due to code/script defects, which reduces the development productivity and wastes expensive resources such as GPU, storage, and network I/O. This paper presents the first comprehensive empirical study on program failures of deep learning jobs. 4960 real failures are collected from a deep learning platform in Microsoft. We manually examine their failure messages and classify them into 20 categories. In addition, we identify the common root causes and bug-fix solutions on a sample of 400 failures. To better understand the current testing and debugging practices for deep learning, we also conduct developer interviews. Our major findings include: (1) 48.0% of the failures occur in the interaction with the platform rather than in the execution of code logic, mostly due to the discrepancies between local and platform execution environments; (2) Deep learning specific failures (13.5%) are mainly caused by inappropriate model parameters/structures and framework API misunderstanding; (3) Current debugging practices are not efficient for fault localization in many cases, and developers need more deep learning specific tools. Based on our findings, we further suggest possible research topics and tooling support that could facilitate future deep learning development. © 2020 Association for Computing Machinery.",Deep learning jobs; Empirical study; Program failures,"1159, 1170",,Proceedings - International Conference on Software Engineering,Conference Paper,Scopus
644,,Planning for untangling: Predicting the difficulty of merge conflicts,"Brindescu C., Ahmed I., Leano R., Sarma A.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094319338&doi=10.1145%2f3377811.3380344&partnerID=40&md5=469d46a59d1721455fc3fccce596924e,10.1145/3377811.3380344,"Merge conflicts are inevitable in collaborative software development and are disruptive. When they occur, developers have to stop their current work, understand the conflict and the surrounding code, and plan an appropriate resolution. However, not all conflicts are equally problematic-some can be easily fixed, while others might be complicated enough to need multiple people. Currently, there is not much support to help developers plan their conflict resolution. In this work, we aim to predict the difficulty of a merge conflict so as to help developers plan their conflict resolution. The ability to predict the difficulty of a merge conflict and to identify the underlying factors for its difficulty can help tool builders improve their conflict detection tools to prioritize and warn developers of difficult conflicts. In this work, we investigate the characteristics of difficult merge conflicts, and automatically classify them. We analyzed 6,380 conflicts across 128 java projects and found that merge conflict difficulty can be accurately predicted (AUC of 0.76) through machine learning algorithms, such as bagging. © 2020 Association for Computing Machinery.",Empirical analysis; Merge conflict difficulty prediction; Merge conflict resolution,"801, 811",,Proceedings - International Conference on Software Engineering,Conference Paper,Scopus
645,,Detection of hidden feature requests from massive chat messages via deep siamese network,"Shi L., Xing M., Li M., Wang Y., Li S., Wang Q.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094317439&doi=10.1145%2f3377811.3380356&partnerID=40&md5=a0b16e72f2e9642357cae872ca164aa6,10.1145/3377811.3380356,"Online chatting is gaining popularity and plays an increasingly significant role in software development. When discussing functionalities, developers might reveal their desired features to other developers. Automated mining techniques towards retrieving feature requests from massive chat messages can benefit the requirements gathering process. But it is quite challenging to perform such techniques because detecting feature requests from dialogues requires a thorough understanding of the contextual information, and it is also extremely expensive on annotating feature-request dialogues for learning. To bridge that gap, we recast the traditional text classification task of mapping single dialog to its class into the task of determining whether two dialogues are similar or not by incorporating few-shot learning. We propose a novel approach, named FRMiner, which can detect feature-request dialogues from chat messages via deep Siamese network. We design a BiLSTMbased dialog model that can learn the contextual information of a dialog in both forward and reverse directions. Evaluation on the realworld projects shows that our approach achieves average precision, recall and F1-score of 88.52%, 88.50% and 88.51%, which confirms that our approach could effectively detect hidden feature requests from chat messages, thus can facilitate gathering comprehensive requirements from the crowd in an automated way. © 2020 Association for Computing Machinery.",Deep learning; Feature requests; Requirements engineering; Siamese network,"641, 653",,Proceedings - International Conference on Software Engineering,Conference Paper,Scopus
646,,FuRong: Fusing report of automated android testing on multi-devices,"Tian Y., Yu S., Fang C., Li P.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094165411&doi=10.1145%2f3377812.3382138&partnerID=40&md5=adf7c690a88e00695f789e3b833a2a46,10.1145/3377812.3382138,"Automated testing has been widely used to ensure the quality ofAndroid applications. However, incomprehensible testing resultsmake it difficult for developers to understand and fix potential bugs.This paper proposes FuRong, a novel tool, to fuse bug reports ofhigh-readability and strong-guiding-ability via analyzing the automated testing results on multi-devices. FuRong builds a bug modelwith complete context information, such as screenshots, operationsequences, and logs from multi-devices, and then leverages pretrained Decision Tree classifier (with 18 bug category labels) toclassify bugs. FuRong deduplicates the classified bugs via Levenshtein distance and finally generates the easy-to-understand report,not only context information of bugs, where possible causes andfix suggestions for each bug category are also provided. An empirical study of 8 open-source Android applications with automatedtesting on 20 devices has been conducted, the results show theeffectiveness of FuRong, which has a bug classification precisionof 93.4% and a bug classification accuracy of 87.9%. Video URL:https://youtu.be/LUkFTc32B6k. © 2020 IEEE Computer Society. All rights reserved.",Android Testing; Automated Testing; Bug Classification; Bug Report,"49, 52",,Proceedings - International Conference on Software Engineering,Conference Paper,Scopus
647,,Machine translation testing via pathological invariance,Gupta S.,2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094155792&doi=10.1145%2f3377812.3382162&partnerID=40&md5=e2096ec00553108c6c0388995dbda090,10.1145/3377812.3382162,"Due to the rapid development of deep neural networks, in recentyears, machine translation software has been widely adopted in people's daily lives, such as communicating with foreigners or understanding political news from the neighbouring countries. However,machine translation software could return incorrect translationsbecause of the complexity of the underlying network. To addressthis problem, we introduce a novel methodology called PaInv forvalidating machine translation software. Our key insight is thatsentences of different meanings should not have the same translation (i.e., pathological invariance). Specifically, PaInv generatessyntactically similar but semantically different sentences by replacing one word in the sentence and filter out unsuitable sentencesbased on both syntactic and semantic information. We have appliedPaInv to Google Translate using 200 English sentences as inputwith three language settings: English→Hindi, English→Chinese,and English→German. PaInv can accurately find 331 pathologicalinvariants in total, revealing more than 100 translation errors. © 2020 Copyright held by the owner/author(s).",,"107, 109",,Proceedings - International Conference on Software Engineering,Conference Paper,Scopus
648,,Big code != big vocabulary: Open-vocabulary models for source code,"Karampatsis R.M., Babii H., Robbes R., Sutton C., Janes A.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094148542&doi=10.1145%2f3377811.3380342&partnerID=40&md5=9796c59edc482fb9d1ad57190b43c652,10.1145/3377811.3380342,"Statistical language modeling techniques have successfully been applied to large source code corpora, yielding a variety of new software development tools, such as tools for code suggestion, improving readability, and API migration. A major issue with these techniques is that code introduces new vocabulary at a far higher rate than natural language, as new identifier names proliferate. Both large vocabularies and out-of-vocabulary issues severely affect Neural Language Models (NLMs) of source code, degrading their performance and rendering them unable to scale. In this paper, we address this issue by: 1) studying how various modelling choices impact the resulting vocabulary on a large-scale corpus of 13,362 projects; 2) presenting an open vocabulary source code NLM that can scale to such a corpus, 100 times larger than in previous work; and 3) showing that such models outperform the state of the art on three distinct code corpora (Java, C, Python). To our knowledge, these are the largest NLMs for code that have been reported. All datasets, code, and trained models used in this work are publicly available. © 2020 Association for Computing Machinery.",Byte-pair encoding; Naturalness of code; Neural language models,"1073, 1085",,Proceedings - International Conference on Software Engineering,Conference Paper,Scopus
649,,SimilarAPI: Mining analogical APIs for library migration,Chen C.,2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094140794&doi=10.1145%2f3377812.3382140&partnerID=40&md5=eaa94e0bfb833a26d69f44e6c2550bda,10.1145/3377812.3382140,"Establishing API mappings between libraries is a prerequisite stepfor library migration tasks. Manually establishing API mappings istedious due to the large number of APIs to be examined, and existingmethods based on supervised learning requires unavailable alreadyported or functionality similar applications. Therefore, we proposean unsupervised deep learning based approach to embed both APIusage semantics and API description (name and document) semantics into vector space for inferring likely analogical API mappingsbetween libraries. We implement a proof-of-concept website SimilarAPI (https://similarapi.appspot.com) which can recommend analogical APIs for 583,501 APIs of 111 pairs of analogical Java librarieswith diverse functionalities. Video: https://youtu.be/EAwD6l24vLQ. © 2020 Copyright held by the owner/author(s).",Analogical API; Skip thoughts; Word embedding,"37, 40",,Proceedings - International Conference on Software Engineering,Conference Paper,Scopus
650,,Structure-invariant testing for machine translation,"He P., Meister C., Su Z.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094138593&doi=10.1145%2f3377811.3380339&partnerID=40&md5=47779157e2a039c6ce6f5836ccf31bd9,10.1145/3377811.3380339,"In recent years, machine translation software has increasingly been integrated into our daily lives. People routinely use machine translation for various applications, such as describing symptoms to a foreign doctor and reading political news in a foreign language. However, the complexity and intractability of neural machine translation (NMT) models that power modern machine translation make the robustness of these systems difficult to even assess, much less guarantee. Machine translation systems can return inferior results that lead to misunderstanding, medical misdiagnoses, threats to personal safety, or political conflicts. Despite its apparent importance, validating the robustness of machine translation systems is very difficult and has, therefore, been much under-explored. To tackle this challenge, we introduce structure-invariant testing (SIT), a novel metamorphic testing approach for validating machine translation software. Our key insight is that the translation results of similar source sentences should typically exhibit similar sentence structures. Specifically, SIT (1) generates similar source sentences by substituting one word in a given sentence with semantically similar, syntactically equivalent words; (2) represents sentence structure by syntax parse trees (obtained via constituency or dependency parsing); (3) reports sentence pairs whose structures differ quantitatively by more than some threshold. To evaluate SIT, we use it to test Google Translate and Bing Microsoft Translator with 200 source sentences as input, which led to 64 and 70 buggy issues with 69.5% and 70% top-1 accuracy, respectively. The translation errors are diverse, including under-translation, over-translation, incorrect modification, word/phrase mistranslation, and unclear logic. © 2020 Association for Computing Machinery.",Machine translation; Metamorphic testing; Structural invariance,"961, 973",,Proceedings - International Conference on Software Engineering,Conference Paper,Scopus
651,,Variability aware requirements reuse analysis,Abbas M.,2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094128412&doi=10.1145%2f3377812.3381399&partnerID=40&md5=1c7bb2962ad0781338993970e8940abe,10.1145/3377812.3381399,"Problem: The goal of a software product line is to aid quick andquality delivery of software products, sharing common features.Effectively achieving the above-mentioned goals requires reuseanalysis of the product line features. Existing requirements reuseanalysis approaches are not focused on recommending product linefeatures, that can be reused to realize new customer requirements.Hypothesis: Given that the customer requirements are linked toproduct line features' description satisfying them: then the customer requirements can be clustered based on patterns and similarities, preserving the historic reuse information. New customerrequirements can be evaluated against existing customer requirements and reuse of product line features can be recommended.Contributions: We treated the problem of feature reuse analysisas a text classification problem at the requirements-level. We useNatural Language Processing and clustering to recommend reuseof features based on similarities and historic reuse information.The recommendations can be used to realize new customer requirements. © 2020 Copyright held by the owner/author(s).",Product line; Requirements; Similarities; Software reuse; Variability,"190, 193",,Proceedings - International Conference on Software Engineering,Conference Paper,Scopus
652,,EvalDNN: A toolbox for evaluating deep neural network models,"Tian Y., Zeng Z., Wen M., Liu Y., Kuo T.-Y., Cheung S.-C.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094112889&doi=10.1145%2f3377812.3382133&partnerID=40&md5=c1615f91a1e51a742f1058cdbb415549,10.1145/3377812.3382133,"Recent studies have shown that the performance of deep learningmodels should be evaluated using various important metrics suchas robustness and neuron coverage, besides the widely-used prediction accuracy metric. However, major deep learning frameworkscurrently only provide APIs to evaluate a model's accuracy. In order to comprehensively assess a deep learning model, frameworkusers and researchers often need to implement new metrics bythemselves, which is a tedious job. What is worse, due to the largenumber of hyper-parameters and inadequate documentation, evaluation results of some deep learning models are hard to reproduce,especially when the models and metrics are both new.To ease the model evaluation in deep learning systems, we havedeveloped EvalDNN, a user-friendly and extensible toolbox supporting multiple frameworks and metrics with a set of carefullydesigned APIs. Using EvalDNN, evaluation of a pre-trained modelwith respect to different metrics can be done with a few lines ofcode. We have evaluated EvalDNN on 79 models from TensorFlow,Keras, GluonCV, and PyTorch. As a result of our effort made toreproduce the evaluation results of existing work, we release aperformance benchmark of popular models, which can be a useful reference to facilitate future research. The tool and benchmarkare available at https://github.com/yqtianust/EvalDNN and https://yqtianust.github.io/EvalDNN-benchmark/, respectively. A demovideo of EvalDNN is available at: https://youtu.be/v69bNJN2bJc. © 2020 Copyright held by the owner/author(s).",Deep Learning Model; Evaluation,"45, 48",,Proceedings - International Conference on Software Engineering,Conference Paper,Scopus
653,,Dialogue Act Classification for Virtual Agents for Software Engineers during Debugging,"Wood A., Eberhart Z., McMillan C.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093116715&doi=10.1145%2f3387940.3391487&partnerID=40&md5=fc0fee532b9d671381de2f544c9d32ac,10.1145/3387940.3391487,"A ""dialogue act""is a written or spoken action during a conversation. Dialogue acts are usually only a few words long, and are often categorized by researchers into a relatively small set of dialogue act types, such as eliciting information, expressing an opinion, or making a greeting. Research interest into automatic classification of dialogue acts has grown recently due to the proliferation of Virtual Agents (VA) e.g. Siri, Cortana, Alexa. But unfortunately, the gains made into VA development in one domain are generally not applicable to other domains, since the composition of dialogue acts differs in different conversations. In this paper, we target the problem of dialogue act classification for a VA for software engineers repairing bugs. A problem in the SE domain is that very little sample data exists - the only public dataset is a recently-released Wizard of Oz study with 30 conversations. Therefore, we present a transfer-learning technique to learn on a much larger dataset for general business conversations, and apply the knowledge to the SE dataset. In an experiment, we observe between 8% and 20% improvement over two key baselines. © 2020 ACM.",dialogue act classification; intelligent agents; software engineering; transfer learning,"462, 469",,"Proceedings - 2020 IEEE/ACM 42nd International Conference on Software Engineering Workshops, ICSEW 2020",Conference Paper,Scopus
654,,Sensemaking Practices in the Everyday Work of AI/ML Software Engineering,"Wolf C.T., Paine D.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093114735&doi=10.1145%2f3387940.3391496&partnerID=40&md5=23dfcbf30ba29fd1d4541ce0ca21366e,10.1145/3387940.3391496,"This paper considers sensemaking as it relates to everyday software engineering (SE) work practices and draws on a multi-year ethnographic study of SE projects at a large, global technology company building digital services infused with artificial intelligence (AI) and machine learning (ML) capabilities. Our findings highlight the breadth of sensemaking practices in AI/ML projects, noting developers' efforts to make sense of AI/ML environments (e.g., algorithms/methods and libraries), of AI/ML model ecosystems (e.g., pre-trained models and ""upstream""models), and of business-AI relations (e.g., how the AI/ML service relates to the domain context and business problem at hand). This paper builds on recent scholarship drawing attention to the integral role of sensemaking in everyday SE practices by empirically investigating how and in what ways AI/ML projects present software teams with emergent sensemaking requirements and opportunities. © 2020 ACM.",AI/ML; Enterprise computing; Ethnography; Process Teories; Sensemaking; Software Engineering,"86, 92",,"Proceedings - 2020 IEEE/ACM 42nd International Conference on Software Engineering Workshops, ICSEW 2020",Conference Paper,Scopus
655,,Recommendation of Move Method Refactoring Using Path-Based Representation of Code,"Kurbatova Z., Veselov I., Golubev Y., Bryksin T.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093099001&doi=10.1145%2f3387940.3392191&partnerID=40&md5=96c8e0baceacad9ad232dca088615aa6,10.1145/3387940.3392191,"Software refactoring plays an important role in increasing code quality. One of the most popular refactoring types is the Move Method refactoring. It is usually applied when a method depends more on members of other classes than on its own original class. Several approaches have been proposed to recommend Move Method refactoring automatically. Most of them are based on heuristics and have certain limitations (e.g., they depend on the selection of metrics and manually-defined thresholds). In this paper, we propose an approach to recommend Move Method refactoring based on a path-based representation of code called code2vec that is able to capture the syntactic structure and semantic information of a code fragment. We use this code representation to train a machine learning classifier suggesting to move methods to more appropriate classes. We evaluate the approach on two publicly available datasets: a manually compiled dataset of well-known open-source projects and a synthetic dataset with automatically injected code smell instances. The results show that our approach is capable of recommending accurate refactoring opportunities and outperforms JDeodorant and JMove, which are state of the art tools in this field. © 2020 ACM.",Automatic Refactoring Recommendation; Code Smells; Feature Envy; Move Method Refactoring; Path-based Representation,"315, 322",,"Proceedings - 2020 IEEE/ACM 42nd International Conference on Software Engineering Workshops, ICSEW 2020",Conference Paper,Scopus
656,,Evaluating Surprise Adequacy for Question Answering,"Kim S., Yoo S.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093095102&doi=10.1145%2f3387940.3391465&partnerID=40&md5=7b799f6bc62ef872e9a9580702347c8b,10.1145/3387940.3391465,"With the wide and rapid adoption of Deep Neural Networks (DNNs) in various domains, an urgent need to validate their behaviour has risen, resulting in various test adequacy metrics for DNNs. One of the metrics, Surprise Adequacy (SA), aims to measure how surprising a new input is based on the similarity to the data used for training. While SA has been evaluated to be effective for image classifiers based on Convolutional Neural Networks (CNNs), it has not been studied for the Natural Language Processing (NLP) domain. This paper applies SA to NLP, in particular to the question answering task: the aim is to investigate whether SA correlates well with the correctness of answers. An empirical evaluation using the widely used Stanford Question Answering Dataset (SQuAD) shows that SA can work well as a test adequacy metric for the question answering task. © 2020 ACM.",Deep Learning; Natural Language Processing; Software Testing,"197, 202",,"Proceedings - 2020 IEEE/ACM 42nd International Conference on Software Engineering Workshops, ICSEW 2020",Conference Paper,Scopus
657,,Assessing practitioner beliefs about software defect prediction,"Shrikanth N.C., Menzies T.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092591242&doi=10.1145%2f3377813.3381367&partnerID=40&md5=b3637566b3798dd7bd38b6741f6f3b60,10.1145/3377813.3381367,"Just because software developers say they believe in ""X"", that does not necessarily mean that ""X"" is true. As shown here, there exist numerous beliefs listed in the recent Software Engineering literature which are only supported by small portions of the available data. Hence we ask what is the source of this disconnect between beliefs and evidence?. To answer this question we look for evidence for ten beliefs within 300,000+ changes seen in dozens of open-source projects. Some of those beliefs had strong support across all the projects; specifically, ""A commit that involves more added and removed lines is more bug-prone"" and ""Files with fewer lines contributed by their owners (who contribute most changes) are bug-prone"". Most of the widely-held beliefs studied are only sporadically supported in the data; i.e. large effects can appear in project data and then disappear in subsequent releases. Such sporadic support explains why developers believe things that were relevant to their prior work, but not necessarily their current work. Our conclusion will be that we need to change the nature of the debate with Software Engineering. Specifically, while it is important to report the effects that hold right now, it is also important to report on what effects change over time. © 2020 IEEE Computer Society. All rights reserved.",Beliefs; Defects; Empirical software engineering; Practitioner,"182, 190",,Proceedings - International Conference on Software Engineering,Conference Paper,Scopus
658,,Understanding the automated parameter optimization on transfer learning for cross-project defect prediction: An empirical study,"Li K., Xiang Z., Chen T., Wang S., Tan K.C.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087529683&doi=10.1145%2f3377811.3380360&partnerID=40&md5=a353148dfae990e0103b62c73d77bb3c,10.1145/3377811.3380360,"Data-driven defect prediction has become increasingly important in software engineering process. Since it is not uncommon that data from a software project is insufficient for training a reliable defect prediction model, transfer learning that borrows data/konwledge from other projects to facilitate the model building at the current project, namely cross-project defect prediction (CPDP), is naturally plausible. Most CPDP techniques involve two major steps, i.e., transfer learning and classification, each of which has at least one parameter to be tuned to achieve their optimal performance. This practice fits well with the purpose of automated parameter optimization. However, there is a lack of thorough understanding about what are the impacts of automated parameter optimization on various CPDP techniques. In this paper, we present the first empirical study that looks into such impacts on 62 CPDP techniques, 13 of which are chosen from the existing CPDP literature while the other 49 ones have not been explored before. We build defect prediction models over 20 real-world software projects that are of different scales and characteristics. Our findings demonstrate that: (1) Automated parameter optimization substantially improves the defect prediction performance of 77% CPDP techniques with a manageable computational cost. Thus more efforts on this aspect are required in future CPDP studies. (2) Transfer learning is of ultimate importance in CPDP. Given a tight computational budget, it is more cost-effective to focus on optimizing the parameter configuration of transfer learning algorithms (3) The research on CPDP is far from mature where it is not difficult' to find a better alternative by making a combination of existing transfer learning and classification techniques. This finding provides important insights about the future design of CPDP techniques. © 2020 Association for Computing Machinery.",Automated parameter optimization; Classification techniques; Cross-project defect prediction; Transfer learning,"566, 577",,Proceedings - International Conference on Software Engineering,Conference Paper,Scopus
659,,Onboarding bot for newcomers to software engineering,"Dominic J., Ritter C., Rodeghero P.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092462077&doi=10.1145%2f3379177.3388901&partnerID=40&md5=fb01f796df697aff2812072de812a6cc,10.1145/3379177.3388901,"Software development teams dedicate considerable resources to training newcomers. Newcomers are new developers to a software project. The software onboarding process is more complicated than onboarding into other organizations. It is much more challenging and time-consuming. The role of a mentor in onboarding newcomers in software engineering is well understood. However, the disruptions to the work of an experienced developer can reduce the quality of their work and job satisfaction. We propose a conversational bot that can help onboard newcomers to a software project instead of an experienced programmer. The bot will act as a mentor for the newcomer, thus putting less stress on experienced programmers. The bot will also be able to scan outside sources, such as stack overflow, for solutions to issues a newcomer may face. The newcomer will be able to interact with the bot using natural language. We will use this bot to assess improvements to code quality in future studies. © 2020 ACM.",bot; newcomer; onboarding; open source software,"91, 94",,"Proceedings - 2020 IEEE/ACM International Conference on Software and System Processes, ICSSP 2020",Conference Paper,Scopus
660,,A speech quality classifier based on tree-cnn algorithm that considers network degradations,"Vieira S.T., Rosa R.L., Rodrguez D.Z.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090678655&doi=10.24138%2fjcomss.v16i2.1032&partnerID=40&md5=d5512d4f40e3914816fdf708bdbe6dff,10.24138/jcomss.v16i2.1032,"Many factors can affect the users' quality of experience (QoE) in speech communication services. The impairment factors appear due to physical phenomena that occur in the transmission channel of wireless and wired networks. The monitoring of users' QoE is important for service providers. In this context, a non-intrusive speech quality classifier based on the Tree Convolutional Neural Network (Tree-CNN) is proposed. The Tree-CNN is an adaptive network structure composed of hierarchical CNNs models, and its main advantage is to decrease the training time that is very relevant on speech quality assessment methods. In the training phase of the proposed classifier model, impaired speech signals caused by wired and wireless network degradation are used as input. Also, in the network scenario, different modulation schemes and channel degradation intensities, such as packet loss rate, signal-to-noise ratio, and maximum Doppler shift frequencies are implemented. Experimental results demonstrated that the proposed model achieves significant reduction of training time, reaching 25% of reduction in relation to another implementation based on DRBM. The accuracy reached by the Tree-CNN model is almost 95% for each quality class. Performance assessment results show that the proposed classifier based on the Tree-CNN overcomes both the current standardized algorithm described in ITU-T Rec. P.563 and the speech quality assessment method called ViSQOL. © 2020 CCIS.",deep learning; objective metrics; Speech quality; Tree-CNN; wired network; wireless network,"180, 187",,Journal of Communications Software and Systems,Article,Scopus
661,,Attention-Aware Convolutional Neural Network for Age-Related Macular Degeneration Classification,"Li S., Quan Z.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088891672&doi=10.1109%2fICCSN49894.2020.9139104&partnerID=40&md5=76d01f13477452ef11215e60b9f630fb,10.1109/ICCSN49894.2020.9139104,"Though age-related macular degeneration (AMD) poses an important personal and public health burden, studies on AMD is hampered by different approaches to classify AMD. In this paper, we propose convolutional neural networks (CNN) based models for fundus retinal images that classify four types of AMD automatically. We use deep residual network (ResNet50) to extract high-dimensional features and be trained end-to-end to classify AMD. Furthermore, we apply attention mechanism to deep residual network (Atten-ResNet) which enables to further select features adaptively. Experimental results show that comparing to HOG-SVM and Visual Geometry Group (VGG), the ResNet50 based method could achieve 17.2% and 12.1% overall classification accuracy improvement. The Atten-ResNet based method has more 0.4% accuracy improvement than ResNet50 based method. © 2020 IEEE.",AMD; Attention Mechanism; Classification; OCT; ResNet,"264, 269",,"2020 12th International Conference on Communication Software and Networks, ICCSN 2020",Conference Paper,Scopus
662,,Discovering software developer's coding expertise through deep learning,"Javeed F., Siddique A., Munir A., Shehzad B., Lali M.I.U.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087729926&doi=10.1049%2fiet-sen.2019.0290&partnerID=40&md5=f86547dd6667eb05f3ab1c71ecf88160,10.1049/iet-sen.2019.0290,"The field of software development is growing rapidly and prevailing in every walk of life. The role of software developers in such a challenging and complex activity is very much important. The allocation of right software developers (i.e. who possesses appropriate coding skills) to projects is one of the crucial factors for successful software development. The problem is that it is very difficult for a client, project manager, as well as for software development organisations to find out an appropriate developer and assign him/her to a particular project. To achieve this, there is a need for such a sound mechanism that could detect the level of software developer coding expertise. This study has formulated criteria for novice and expert developers and carried out such criteria to discover the level of coding expertise of software developers using three different models of deep learning. These models include long short-term memory (LSTM), convolution 1D and hybrid (a combination of LSTM and convolution 1D). The deep learning models have analysed software developers' previously written source code collected from the GitHub repository. An experiment was conducted to evaluate the performance of models. The results showed that the LSTM model performed better in comparison to other models by achieving 96.25% accuracy. © The Institution of Engineering and Technology 2020",,"213, 220",,IET Software,Article,Scopus
663,,Research Progress on Cross-domain Text Sentiment Classification [跨领域文本情感分类研究进展],"Zhao C.-J., Wang S.-G., Li D.-Y.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086475944&doi=10.13328%2fj.cnki.jos.006029&partnerID=40&md5=c9820ec14e60ea093660e91432ead8e3,10.13328/j.cnki.jos.006029,"As an important research topic in social media text sentiment analysis, cross-domain text sentiment classification aims to use the source domain resources or model transfer to serve the target domain text sentiment classification task, which can effectively solve the problem of insufficient data marking in specific domains. In order to solve the problem of cross-domain sentiment adaptation, this article summarizes the existing studies of cross-domain sentiment classification from three perspectives, i.e., (1) it can be divided into transductive and inductive cross-domain sentiment classification methods according to whether there is labeled data in the target domain; (2) it can be divided into instance transferring based, feature transferring based, model or parameters transferring based, sentiment dictionary based, joint sentiment topic based, and graph model based methods according to different sentiment adaption strategies; (3) it can also be divided into single-source domain and multi-source domains of cross-domain sentiment classification according to the number of available source domains. In addition, it is also introduced that a new approach of deep transfer learning to solve cross-domain sentiment classification problems, and summarize its latest research results in cross-domain sentiment classification. Finally, the challenges are combined with key issues of current cross-domain sentiment classification technology and further study directions are pointed out. © Copyright 2020, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Cross-domain sentiment classification; Domain adaptation; Research progress; Transfer leaning,"1723, 1746",,Ruan Jian Xue Bao/Journal of Software,Review,Scopus
664,,Chinese Sentence-Level Lip Reading Based on End-to-End Model [基于端到端句子级别的中文唇语识别研究],"Zhang X.-B., Gong H.-G., Yang F., Dai X.-L.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086461148&doi=10.13328%2fj.cnki.jos.005709&partnerID=40&md5=26cee8fa383bbb59797e9d08941a2e24,10.13328/j.cnki.jos.005709,"In recent years, with the widely application of deep learning, lip reading recognition technology has achieved rapid development. Different from traditional methods, lip reading recognition methods based on the deep learning usually use the neural network model both for the feature extraction and comprehension. According to the characteristics of Chinese language, a two-step end-to-end architecture is implemented, in which two deep neural network modules are applied to perform the recognition of picture-to-pinyin (P2P) and pinyin-to-hanzi (P2CC) respectively. After the two modules are trained with convergence, they are then jointly optimized to improve the overall performance. Due to the lack of Chinese lip reading dataset, the 6-month daily news broadcasts are collected from China Central Television (CCTV), and they are semi-automatically labelled into a 20.95 GB dataset CCTVDS with 14 975 samples. In addition, the supplementary dataset with 269 558 samples are collected during the pre-training of P2CC. According to experimental results trained on the CCTVDS, the proposed ChLipNet can achieve 45.7% sentence-level and 58.5% Pinyin-level accuracies. In addition, ChLipNet can not only accelerate training, reduce overfitting, but also overcome syntactic ambiguity in the recognition of Chinese language. © Copyright 2020, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Characteristics of Chinese language; Chinese lip reading recognition; Data collecting and preprocessing; Deep learning; End-to-end model,"1747, 1760",,Ruan Jian Xue Bao/Journal of Software,Article,Scopus
665,,Detecting Java software similarities by using different clustering techniques,"Capiluppi A., Di Ruscio D., Di Rocco J., Nguyen P.T., Ajienka N.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080069243&doi=10.1016%2fj.infsof.2020.106279&partnerID=40&md5=12a3e764608dcb83b3c07304bad93f96,10.1016/j.infsof.2020.106279,"Background: Research on empirical software engineering has increasingly been conducted by analysing and measuring vast amounts of software systems. Hundreds, thousands and even millions of systems have been (and are) considered by researchers, and often within the same study, in order to test theories, demonstrate approaches or run prediction models. A much less investigated aspect is whether the collected metrics might be context-specific, or whether systems should be better analysed in clusters. Objective: The objectives of this study are (i) to define a set of clustering techniques that might be used to group similar software systems, and (ii) to evaluate whether a suite of well-known object-oriented metrics is context-specific, and its values differ along the defined clusters. Method: We group software systems based on three different clustering techniques, and we collect the values of the metrics suite in each cluster. We then test whether clusters are statistically different between each other, using the Kolgomorov-Smirnov (KS) hypothesis testing. Results: Our results show that, for two of the used techniques, the KS null hypothesis (e.g., the clusters come from the same population) is rejected for most of the metrics chosen: the clusters that we extracted, based on application domains, show statistically different structural properties. Conclusions: The implications for researchers can be profound: metrics and their interpretation might be more sensitive to context than acknowledged so far, and application domains represent a promising filter to cluster similar systems. © 2020",Application Domains; Expert Opinions; FOSS (Free and open-source software); Latent Dirichlet Allocation; Machine Learning; OO (object-oriented),,,Information and Software Technology,Article,Scopus
666,,A survey on machine and statistical learning for longitudinal analysis of neuroimaging data in Alzheimer's disease,"Martí-Juan G., Sanroma-Guell G., Piella G.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078197156&doi=10.1016%2fj.cmpb.2020.105348&partnerID=40&md5=9dd7277c44ec3677e5f90f48515d4472,10.1016/j.cmpb.2020.105348,"Background and Objectives: Recently, longitudinal studies of Alzheimer's disease have gathered a substantial amount of neuroimaging data. New methods are needed to successfully leverage and distill meaningful information on the progression of the disease from the deluge of available data. Machine learning has been used successfully for many different tasks, including neuroimaging related problems. In this paper, we review recent statistical and machine learning applications in Alzheimer's disease using longitudinal neuroimaging. Methods: We search for papers using longitudinal imaging data, focused on Alzheimer's Disease and published between 2007 and 2019 on four different search engines. Results: After the search, we obtain 104 relevant papers. We analyze their approach to typical challenges in longitudinal data analysis, such as missing data and variability in the number and extent of acquisitions. Conclusions: Reviewed works show that machine learning methods using longitudinal data have potential for disease progression modelling and computer-aided diagnosis. We compare results and models, and propose future research directions in the field. © 2020 Elsevier B.V.",Alzheimer's disease; Disease progression; Longitudinal; Machine learning,,,Computer Methods and Programs in Biomedicine,Review,Scopus
667,,Artificial intelligence in multiparametric prostate cancer imaging with focus on deep-learning methods,"Wildeboer R.R., van Sloun R.J.G., Wijkstra H., Mischi M.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077760014&doi=10.1016%2fj.cmpb.2020.105316&partnerID=40&md5=985fefa41897b82dd47affabee5b61d2,10.1016/j.cmpb.2020.105316,"Prostate cancer represents today the most typical example of a pathology whose diagnosis requires multiparametric imaging, a strategy where multiple imaging techniques are combined to reach an acceptable diagnostic performance. However, the reviewing, weighing and coupling of multiple images not only places additional burden on the radiologist, it also complicates the reviewing process. Prostate cancer imaging has therefore been an important target for the development of computer-aided diagnostic (CAD) tools. In this survey, we discuss the advances in CAD for prostate cancer over the last decades with special attention to the deep-learning techniques that have been designed in the last few years. Moreover, we elaborate and compare the methods employed to deliver the CAD output to the operator for further medical decision making. © 2020 Elsevier B.V.",Artificial intelligence; Computer-aided detection; Computer-aided diagnosis; Machine learning; Magnetic resonance imaging; Multiparametric imaging; Prostate cancer; Ultrasound,,,Computer Methods and Programs in Biomedicine,Review,Scopus
668,,An improved sex-specific and age-dependent classification model for Parkinson's diagnosis using handwriting measurement,"Gupta U., Bansal H., Joshi D.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077709928&doi=10.1016%2fj.cmpb.2019.105305&partnerID=40&md5=535e067879f8c8f70e044c371e884551,10.1016/j.cmpb.2019.105305,"Background and Objectives: Diagnosis of Parkinson's with higher accuracy is always desirable to slow down the progression of the disease and improved quality of life. There are evidences of inherent neurological differences between male and females as well as between elderly and adults. However, the potential of such gender and age infomration have not been exploited yet for Parkinson's identification. Methods: In this paper, we develop a sex-specific and age-dependent classification method to diagnose the Parkinson's disease using the online handwriting recorded from individuals with Parkinson's (n = 37; m/f-19/18;age-69.3 ± 10.9yrs) and healthy controls (n = 38; m/f-20/18;age-62.4 ± 11.3yrs). A support vector machine ranking method is used to present the features specific to their dominance in sex and age group for Parkinson's diagnosis. Results: The sex-specific and age-dependent classifier was observed significantly outperforming the generalized classifier. An improved accuracy of 83.75% (SD = 1.63) with the female-specific classifier, and 79.55% (SD = 1.58) with the old-age dependent classifier was observed in comparison to 75.76% (SD = 1.17) accuracy with the generalized classifier. Conclusions: Combining the age and sex information proved to be encouraging in classification. A distinct set of features were observed to be dominating for higher classification accuracy in a different category of classification. © 2019 Elsevier B.V.",Age-dependent; Handwriting Features; Parkinson's Disease'; Sex-specific; Support Vector Machine,,,Computer Methods and Programs in Biomedicine,Article,Scopus
669,,Deep Transfer Learning for Source Code Modeling,"Hussain Y., Huang Z., Zhou Y., Wang S.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087335223&doi=10.1142%2fS0218194020500230&partnerID=40&md5=a6c7ef47e1fb71323243f0270945f00b,10.1142/S0218194020500230,"In recent years, deep learning models have shown great potential in source code modeling and analysis. Generally, deep learning-based approaches are problem-specific and data-hungry. A challenging issue of these approaches is that they require training from scratch for a different related problem. In this work, we propose a transfer learning-based approach that significantly improves the performance of deep learning-based source code models. In contrast to traditional learning paradigms, transfer learning can transfer the knowledge learned in solving one problem into another related problem. First, we present two recurrent neural network-based models RNN and GRU for the purpose of transfer learning in the domain of source code modeling. Next, via transfer learning, these pre-trained (RNN and GRU) models are used as feature extractors. Then, these extracted features are combined into attention learner for different downstream tasks. The attention learner leverages from the learned knowledge of pre-trained models and fine-tunes them for a specific downstream task. We evaluate the performance of the proposed approach with extensive experiments with the source code suggestion task. The results indicate that the proposed approach outperforms the state-of-the-art models in terms of accuracy, precision, recall and F-measure without training the models from scratch. © 2020 World Scientific Publishing Company.",attention learning; deep neural language models; source code modeling; Transfer learning,"649, 668",,International Journal of Software Engineering and Knowledge Engineering,Article,Scopus
670,,Empirical Studies on Deep-learning-based Security Bug Report Prediction Methods [基于深度学习的安全缺陷报告预测方法实证研究],"Zheng W., Chen J.-Z., Wu X.-X., Chen X., Xia X.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086640468&doi=10.13328%2fj.cnki.jos.005954&partnerID=40&md5=5eea732cfa79be807ab40e9c41475d03,10.13328/j.cnki.jos.005954,"The occurrence of software security issues can cause serious consequences in most cases. Early detection of security issues is one of the key measures to prevent security incidents. Security bug report prediction (SBR) can help developers identify hidden security issues in the bug tracking system and fix them as early as possible. However, since the number of security bug reports in real software projects is small, and the features are complex (i.e., there are many types of security vulnerabilities with different types of features), this makes the manual extraction of security features relatively difficult and lead to low accuracy of security bug report prediction with traditional machine learning classification algorithms. To solve this problem, a deep-learning-based security bug report prediction method is proposed. The text mining models TextCNN and TextRNN via deep learning are used to construct security bug report prediction models. For extracting textual features of security bug reports, the Skip-Gram method is used to construct a word embedding matrix. The constructed model has been empirically evaluated on five classical security bug report datasets with different scales. The results show that the deep learning model is superior to the traditional machine learning classification algorithm in 80% of the experimental cases, and the performance of the constructed models can improve 0.258 on average and 0.535 at most in terms of F1-score performance measure. Furthermore, different re-sampling strategies are applied to deal with class imbalance problem in gathered SBR prediction datasets, and the experiment results are discussed. © Copyright 2020, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Deep learning; Security bug; Security bug report prediction; Text mining,"1294, 1313",,Ruan Jian Xue Bao/Journal of Software,Article,Scopus
671,,Better software analytics via “DUO”: Data mining algorithms using/used-by optimizers,"Agrawal A., Menzies T., Minku L.L., Wagner M., Yu Z.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084054663&doi=10.1007%2fs10664-020-09808-9&partnerID=40&md5=b4e52076b1029dd6ea9a1433f4ae2de1,10.1007/s10664-020-09808-9,"This paper claims that a new field of empirical software engineering research and practice is emerging: data mining using/used-by optimizers for empirical studies, or DUO. For example, data miners can generate models that are explored by optimizers. Also, optimizers can advise how to best adjust the control parameters of a data miner. This combined approach acts like an agent leaning over the shoulder of an analyst that advises “ask this question next” or “ignore that problem, it is not relevant to your goals”. Further, those agents can help us build “better” predictive models, where “better” can be either greater predictive accuracy or faster modeling time (which, in turn, enables the exploration of a wider range of options). We also caution that the era of papers that just use data miners is coming to an end. Results obtained from an unoptimized data miner can be quickly refuted, just by applying an optimizer to produce a different (and better performing) model. Our conclusion, hence, is that for software analytics it is possible, useful and necessary to combine data mining and optimization using DUO. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.",Data mining; Evolutionary algorithms; Optimization; Software analytics,"2099, 2136",,Empirical Software Engineering,Article,Scopus
672,,Do different cross-project defect prediction methods identify the same defective modules?,"Chen X., Mu Y., Qu Y., Ni C., Liu M., He T., Liu S.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074642642&doi=10.1002%2fsmr.2234&partnerID=40&md5=23099fe9d55d641370684a253c9471c3,10.1002/smr.2234,"Cross-project defect prediction (CPDP) is needed when the target projects are new projects or the projects have less training data, since these projects do not have sufficient historical data to build high-quality prediction models. The researchers have proposed many CPDP methods, and previous studies have conducted extensive comparisons on the performance of different CPDP methods. However, to the best of our knowledge, it remains unclear whether different CPDP methods can identify the same defective modules, and this issue has not been thoroughly explored. In this article, we select 12 state-of-the-art CPDP methods, including eight supervised methods and four unsupervised methods. We first compare the performance of these methods in the same experiment settings on five widely used datasets (ie, NASA, SOFTLAB, PROMISE, AEEEM, and ReLink) and rank these methods via the Scott-Knott test. Final results confirm the competitiveness of unsupervised methods. Then we perform diversity analysis on defective modules for these methods by using the McNemar test. Empirical results verify that different CPDP methods may lead to difference in the modules predicted as defective, especially when the comparison is performed between the supervised methods and unsupervised methods. Finally, we also find there exist a certain number of defective modules, which cannot be correctly identified by any of the CPDP methods or can be correctly identified by only one CPDP method. These findings can be utilized to design more effective methods to further improve the performance of CPDP. © 2019 John Wiley & Sons, Ltd.",cross-project defect prediction; diversity analysis; empirical study; software defect prediction,,,Journal of Software: Evolution and Process,Article,Scopus
673,,Intelligent sentiment analysis approach using edge computing-based deep learning technique,"Sankar H., Subramaniyaswamy V., Vijayakumar V., Arun Kumar S., Logesh R., Umamakeswari A.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062320970&doi=10.1002%2fspe.2687&partnerID=40&md5=017cf7cd9a2ed196933bc96408afa249,10.1002/spe.2687,"Sentiment analysis and opinion mining has become a major tool for collecting information from customer reviews on user sentiments and emotions, especially for online video streaming services and social networks. The increasing use of smartphones has popularized subscription to various streaming services that provide streaming media and video-on-demand. These applications offer a gateway to analyze user reviews by introducing sentiment analysis in the mobile environment. Online user reviews can hold a lot of useful information and help predict user interests. Analysis of user reviews can provide substantive information for business processing. Sentiment classification of these reviews is a commonly used analysis technique. Usually, these reviews are given in a text format, with every word in each considered a feature, so selection should focus on optimal features from all available features present in the reviews. This study employs machine learning algorithms to extract the best features from the training review data set. Then, the selected features are fed into the convolutional neural network and other fully connected layers for further processing. The proposed approach is evaluated with the standard evaluation metrics, such as precision, accuracy, recall, and f-measure, using three distinct benchmark data sets: polarity, Rotten Tomatoes, and IMDb. This work has also employed a pretrained sentiment analysis model over an Android application framework to classify reviews on a Smartphone without the need for any cloud or server-side API. © 2019 John Wiley & Sons, Ltd.",convolutional neural network; deep learning; edge computing; mobile; sentiment analysis; word embeddings,"645, 657",,Software - Practice and Experience,Conference Paper,Scopus
674,,A protein identification algorithm optimization for mass spectrometry data using deep learning,"Xu R., Bai M., Shu K., Liang Y., Zhu Y., Chang C.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088667103&doi=10.1109%2fAEMCSE50948.2020.00110&partnerID=40&md5=32a9030865883fcfc3ee7f3b48c0290e,10.1109/AEMCSE50948.2020.00110,"Protein sequence database search is one of the most commonly used methods for protein identification in shotgun proteomics. In tradition, searching a protein sequence database is usually required to construct the theoretical spectrum for each peptide at first, which only considers the information of mass-to-charge ratio at present. However, the information related to isotope peak intensity is neglected. Thanks to the rapid development of artificial intelligence technique in recent years, deep learning-based MS/MS spectrum prediction tools have showed a high accuracy and great potentials to improve the sensitivity and accuracy of protein sequence database searching. In this study, we used a deep learning model (pDeep2) to predict the theoretical mass spectrum of all peptides and applied it to a database searching tool (DeepNovo), thus improving the sensitivity and accuracy of peptide identification. © 2020 IEEE.",Deep learning; Peptide identification; Protein identification; Spectrum prediction,"482, 486",,"Proceedings - 2020 3rd International Conference on Advanced Electronic Materials, Computers and Software Engineering, AEMCSE 2020",Conference Paper,Scopus
675,,An efficient faiss-based search method for mass spectral library searching,"Qin C., Deng C., Huang J., Shu K., Bai M.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088660335&doi=10.1109%2fAEMCSE50948.2020.00116&partnerID=40&md5=b106fad9bca78d3a811084337d3be914,10.1109/AEMCSE50948.2020.00116,"With billions of spectra acquired by MS/MS experiments, spectral library search tool, which is one of the most important tools for spectra identification, needs to be optimized to deal with big data. In this study, we propose an efficient spectral library search method based on faiss, which is a library for efficient similarity search proposed by Facebook-AI research team. Our faiss-based spectral library searching method contains two parts, spectral library's index construction and spectral library searching. In previous one, we embedded spectral library's spectra into vectors and then used faiss index add API to add these vectors to index file; In latter one, we created spectral library searching modular based on faiss index search API and then used this searching modular to search acquired spectra against spectral library's index file. The results show that our proposed method can search out more translation-supported identifications than SpectraST, which verify the effective identification performance of our method, but our method also searches out more unreliable identifications. And our method has a huge advantage in run-time, SpectraST took more than 4 days while our method only took 23 seconds on the test dataset. © 2020 IEEE.",Faiss; MS/MS spectra; Spectral library searching,"513, 518",,"Proceedings - 2020 3rd International Conference on Advanced Electronic Materials, Computers and Software Engineering, AEMCSE 2020",Conference Paper,Scopus
676,,Chinese explanatory opinion relationship recognition based on improved target attention mechanism,"Cao X., Zhu C., Lv C.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088643209&doi=10.1109%2fAEMCSE50948.2020.00126&partnerID=40&md5=dca955fb27be5470cb13d66ae12fb57c,10.1109/AEMCSE50948.2020.00126,"Opinion relationship recognition is an important part of the opinion mining task. Its main purpose is to extract the opinion element tuple from the user comment data and identify the relationship between them, such as evaluation object, evaluation content, opinion explanation, opinion object. Because the comments of the network having are characterized by randomness, diversity of opinions and different formats, it will become more difficult for the opinion mining task. If we can extract the interrelationships between the various explanatory opinion elements, it not only makes subsequent tasks easier but also applies its extracted results to other related tasks. For example, applying the opinion seven-tuple from the opinion extraction task to the text summary generation task can greatly improve the effectiveness of the text summary generation task. In this paper, we have improved on the traditional LSTM-Attention model and proposed an opinion relationship recognition framework based on improved Target Attention Mechanism. Also, we conducted experiments in two different domains, and the experimental results show that the performance has been effectively improved in two domains. We also explored two different pre-training strategies, Word2vec and Elmo, to further analyze the impact of pre-training on this experiment. © 2020 IEEE.",Opinion mining; Opinion relationship recognition; Target attention mechanism,"578, 583",,"Proceedings - 2020 3rd International Conference on Advanced Electronic Materials, Computers and Software Engineering, AEMCSE 2020",Conference Paper,Scopus
677,,A multi-neural network fusion based method for financial event subject extraction,"Wang Z., Liu Z., Luo L., Chen X.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088638279&doi=10.1109%2fAEMCSE50948.2020.00084&partnerID=40&md5=e0d645c705bc04e41d93227f26c12497,10.1109/AEMCSE50948.2020.00084,"Event extraction is a fundamental task in the domain of public opinion monitoring and financial risk control. Subject extraction of events with specific types is the kernel of event extraction. At present, there are some problems still existing in the mainstream event subject extraction methods, such as the inadequate use of semantic relationship between Chinese characters and the weak ability of feature learning. In order to solve these problems, this paper introduces the BERT (Bidirectional Encoder Representations from Transformers) pre-training model to enhance the semantic representation of characters, then proposes a novel event subject extraction method combing convolutional neural network (CNN) and long short-term memory (LSTM) to improve the ability of feature learning in the model. Experimental results show that the F1 score of the method proposed in this paper can reach 86.99%, which greatly improves the identification accuracy of the event subject in the financial domain. © 2020 IEEE.",BERT; Event subject extraction; Multi-neural network fusion,"360, 365",,"Proceedings - 2020 3rd International Conference on Advanced Electronic Materials, Computers and Software Engineering, AEMCSE 2020",Conference Paper,Scopus
678,,Facial ethnicity recognition based on transfer learning from deep convolutional networks,"Gao S., Zeng C., Bai M., Shu K.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088631158&doi=10.1109%2fAEMCSE50948.2020.00073&partnerID=40&md5=8a9bb2c175d5f90b10ef630b6e0ddcde,10.1109/AEMCSE50948.2020.00073,"With the development of deep learning, computer face recognition has made significant progress. However, face ethnic characteristics information is rarely used in face recognition technology. The research of facial ethnicity recognition had not only been directly applied in daily life, but also avoided racial effects and improved model generalization performance. In the paper, we proposed a Chinese facial ethnicity recognition (CFER) model based on transfer learning from deep convolution networks. First, we collected 5 Chinese ethnic groups to build a face dataset containing ethnicity information; then we have applied CFER to recognize Chinese ethnicity characteristics and 10-fold cross validation method to estimate mainly the accuracy rate of the model. The average recognition rate of the model is 80.5%, meanwhile, the model also has good generalization performance. It's proved that deep learning method is feasible for facial ethnicity recognition. © 2020 IEEE.",Convolutional networks; Facial ethnicity recognition; Transfer learning,"310, 314",,"Proceedings - 2020 3rd International Conference on Advanced Electronic Materials, Computers and Software Engineering, AEMCSE 2020",Conference Paper,Scopus
679,,Concept Drift Detection Method Based on Online Performance Test [基于在线性能测试的概念漂移检测方法],"Guo H.-S., Zhang A.-J., Wang W.-J.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084857594&doi=10.13328%2fj.cnki.jos.005917&partnerID=40&md5=30ee6a93adb60d6ada702f8bd2c43dfb,10.13328/j.cnki.jos.005917,"Concept drift is a common problem in dynamic streaming data mining, but the false concept drift generated by the mixed noise data or too small scale size training data will cause similar results to the concept drift, that is, the instability fluctuation of model online testing performance, which leads to confusion between them, and the false alarm of concept drift. To address the problem which is easy to confuse the authenticity of concept drift, concept drift detection method based on online performance test, namely CDPT, is presented. With CDPT, the latest acquired data are evenly divided into groups, and online learning is performed on each group sub sets. At the same time, the classification accuracy vectors obtained by training and testing of each group sub sets are recorded, and the accuracy difference between adjacent learning time units is calculated. The effective fluctuation points are obtained according to the testing accuracy decline threshold. Then, the effective fluctuation points in different groups are integrated by cross checking to eliminate the detection interference caused by the instability of the model due to the small training samples in the online learning process of streaming data, and the consistent fluctuation points are obtained according to the consistency of accuracy fluctuation. Finally, by tracking the classification accuracy of online learning, the change of testing accuracy can be achieved of neighborhood reference points of consistent fluctuation points, and the decline and convergence of model testing accuracy can be compared of neighborhood reference points of consistent fluctuation points, so as to effectively detect the true concept drift points of the consistent fluctuation points. The experimental results demonstrate that the proposed CDPT method can effectively identify the true concept drift occurring in the online learning process of streaming data, effectively avoid the negative impact of too small training samples or noise on the detection results, and improve the generalization performance of the model. © Copyright 2020, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Concept drift; Concept drift point; Consistent fluctuation point; Cross checking; Effective fluctuation point; Streaming data,"932, 947",,Ruan Jian Xue Bao/Journal of Software,Article,Scopus
680,,Cooperative Prediction Method Based on Dynamic and Static Representation for Crowdfunding [基于动静态表征的众筹协同预测方法],"Zhang K., Zhao H.-K., Liu Q., Pan Z., Chen E.-H.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084829738&doi=10.13328%2fj.cnki.jos.005921&partnerID=40&md5=b47cb9600b8dbad0a2d44cc98b601f0f,10.13328/j.cnki.jos.005921,"Crowdfunding is an emerging finance platform for creators to fund their efforts by soliciting relatively small contributions from a large number of individuals using the Internet. Due to the unique rules, a campaign succeeds in trading only when it collects adequate funds in a given time. To prevent creators and backers from wasting time and efforts on failing campaigns, dynamically estimating the success probability of a campaign is very important. However, existing crowdfunding systems neither have the mechanism of dynamic predictive tracking, nor consider the dynamic interaction between project sponsors and investors on the platform. To address these issues, a novel dynamic and static collaborative prediction model is designed based on long and short-term memory network. This model focuses on user behavior, including the emotional tendency of reviews and the dynamic incremental information in the financing process, so as to deeply mine and analyze the interaction between financing projects and investors. Firstly, for the static features and dynamic user behavior data on the platform, their deep characterization is obtained by different embedding methods. On this basis, a collaborative prediction model based on attention mechanism is further designed to understand the impact of timing information of project financing on the final results. Finally, experiments on real crowdfunding datasets show that the proposed dynamic and static representation prediction method is more effective than other prediction methods. © Copyright 2020, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Attention mechanism; Deep semantic representation; Dynamic tracking; LSTM network; User behavior analysis,"967, 980",,Ruan Jian Xue Bao/Journal of Software,Article,Scopus
681,,Image recognition in UAV videos using convolutional neural networks,"Quiñonez Y., Lizarraga C., Peraza J., Zatarain O.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083312579&doi=10.1049%2fiet-sen.2019.0045&partnerID=40&md5=89fd9ef7d3b224d3512aebbdc448770f,10.1049/iet-sen.2019.0045,"In recent years, unmanned aerial vehicles (UAVs) have been used in different areas of applications such as rescue operations, surveillance, agriculture, aerial mapping, engineering applications and research, among others, in order to perform tasks with greater efficiency. This work focuses on the use of UAVs in the fishing sector in order to optimise the detection process of a shoal of fish. In this sense, the main idea is to perform images recognition using the images acquired through videos captured by UAV in the open sea; to achieve the objective the convolutional neural networks were used, a new dataset with different images captured through UAV videos in the open sea were taken into account, these classes correspond to dolphin, dolphin-pod, open-sea, and seabirds. The training tests were by transfer of learning using the following models: Inception V3, MobileNet V2, and NASNet-A (large) trained on TensorFlow platform. The experimental results show the detection performance with high-precision values in reasonable processing time This study ends with a critical discussion of the experimental results. © The Institution of Engineering and Technology 2019.",,"176, 181",,IET Software,Article,Scopus
682,,Region-of-Interest based sparse feature learning method for Alzheimer's disease identification,"Wang L., Liu Y., Zeng X., Cheng H., Wang Z., Wang Q.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077509286&doi=10.1016%2fj.cmpb.2019.105290&partnerID=40&md5=f1c02a43a0d3690cadbb9dec34abc1be,10.1016/j.cmpb.2019.105290,"Background and Objective: In recent years, some clinical parameters, such as the volume of gray matter (GM) and cortical thickness, have been used as anatomical features to identify Alzheimer's disease (AD) from Healthy Controls (HC) in some feature-based machine learning methods. However, fewer image-based feature parameters have been proposed, which are equivalent to these clinical parameters, to describe the atrophy of regions-of-interest (ROIs) of the brain. In this study, we aim to extract effective image-based feature parameters to improve the diagnostic performance of AD with magnetic resonance imaging (MRI) data. Methods: A new subspace-based sparse feature learning method is proposed, which builds a union-of-subspace representation model to realize feature extraction and disease identification. Specifically, the proposed method estimates feature dimensions reasonably, at the same time, it protects local features for the specified ROIs of the brain, and realizes image-based feature extraction and classification automatically instead of computing the volume of GM or cortical thickness preliminarily. Results: Experimental results illustrate the effectiveness and robustness of the proposed method on feature extraction and classification, which are based on the sampled clinical dataset from Peking University Third Hospital of China and the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset. The extracted image-based feature parameters describe the atrophy of ROIs of the brain well as clinical parameters but show better performance in AD identification than clinical parameters. Based on them, the important ROIs for AD identification can be identified even for correlated variables. Conclusion: The extracted features and the proposed identification parameters show high correlation with the volume of GM and the clinical mini-mental state examination (MMSE) score respectively. The proposed method will be useful in denoting the changes of cerebral pathology and cognitive function in AD patients. © 2019 Elsevier B.V.",Alzheimer's disease; Computer-aided disease diagnosis; Elastic net; Machine learning; Sparse feature learning,,,Computer Methods and Programs in Biomedicine,Article,Scopus
683,,Skin lesion segmentation using high-resolution convolutional neural network,"Xie F., Yang J., Liu J., Jiang Z., Zheng Y., Wang Y.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076259205&doi=10.1016%2fj.cmpb.2019.105241&partnerID=40&md5=65a9479d32c91238451dce65a04894ef,10.1016/j.cmpb.2019.105241,"Background and Objective: Skin lesion segmentation is an important but challenging task in computer-aided diagnosis of dermoscopy images. Many segmentation methods based on convolutional neural networks often fail to extract accurate lesion boundaries because the spatial size of feature maps decreases as the maps are processed throughout the network layers. We propose skin lesion segmentation in dermoscopy images based on a convolutional neural network with an attention mechanism, which can preserve edge details. Methods: We devised a high-resolution feature block containing three branches, namely, main, spatial attention, and channel-wise attention branches. The main branch takes high-resolution feature maps as input to extract spatial details around boundaries. The other two attention branches boost the discriminative features in the main branch regarding the spatial and channel-wise dimensions. By fusing the branch outputs, robust features with detailed spatial information can be extracted, and accurate skin lesion boundaries can be obtained. Results: Experiments on datasets from the International Symposium on Biomedical Imaging in 2016 and 2017 and the PH2 dataset retrieved Jaccard indices of 0.783, 0.858, and 0.857, respectively, for the proposed method. Hence, our method can accurately extract skin lesion boundaries and is robust to hair fibers and artifacts in the images. Overall, our method outperforms two typical segmentation networks (FCN-8 s and U-Net) and other state-of-the-art skin lesion segmentation methods. Conclusions: The proposed network endowed with high-resolution feature blocks preserves spatial details during feature extraction, and its attention mechanism enhances representative features while suppressing noise. Hence, the proposed approach provides high-performance skin lesion segmentation. © 2019",Attention mechanism; Convolutional neural network; High-resolution feature; Skin lesion segmentation,,,Computer Methods and Programs in Biomedicine,Article,Scopus
684,,Deep learning to detect Alzheimer's disease from neuroimaging: A systematic literature review,"Ebrahimighahnavieh M.A., Luo S., Chiong R.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076257091&doi=10.1016%2fj.cmpb.2019.105242&partnerID=40&md5=45bf979591473a568b683be1398d8c87,10.1016/j.cmpb.2019.105242,"Alzheimer's Disease (AD) is one of the leading causes of death in developed countries. From a research point of view, impressive results have been reported using computer-aided algorithms, but clinically no practical diagnostic method is available. In recent years, deep models have become popular, especially in dealing with images. Since 2013, deep learning has begun to gain considerable attention in AD detection research, with the number of published papers in this area increasing drastically since 2017. Deep models have been reported to be more accurate for AD detection compared to general machine learning techniques. Nevertheless, AD detection is still challenging, and for classification, it requires a highly discriminative feature representation to separate similar brain patterns. This paper reviews the current state of AD detection using deep learning. Through a systematic literature review of over 100 articles, we set out the most recent findings and trends. Specifically, we review useful biomarkers and features (personal information, genetic data, and brain scans), the necessary pre-processing steps, and different ways of dealing with neuroimaging data originating from single-modality and multi-modality studies. Deep models and their performance are described in detail. Although deep learning has achieved notable performance in detecting AD, there are several limitations, especially regarding the availability of datasets and training procedures. © 2019 Elsevier B.V.",Alzheimer's disease; Auto-encoders; Convolutional neural networks; Deep learning; Recurrent neural networks; Transfer learning,,,Computer Methods and Programs in Biomedicine,Article,Scopus
685,,An incremental learning system for atrial fibrillation detection based on transfer learning and active learning,"Shi H., Wang H., Qin C., Zhao L., Liu C.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075581200&doi=10.1016%2fj.cmpb.2019.105219&partnerID=40&md5=57b32d499b7dd9c1e6b360badfe0f330,10.1016/j.cmpb.2019.105219,"Background and objective: Atrial fibrillation (AF) is a type of arrhythmia with high incidence. Automatic AF detection methods have been studied in previous works. However, a model cannot be used all the time without any improvement. And updating model requires adequate data and cost. Therefore, this study aims at finding a low-cost way to choose learning samples and developing an incremental learning system for AF detection. Methods: Based on transfer learning and active learning, this paper proposed a loop-locked framework integrating AF diagnose, label query, and model fine-tuning. In the pre-training stage, a novel multiple-input deep neural network (MIDNN) is pre-trained using labeled samples from an original training set. In practical application, the model can be used for AF detection. Meanwhile, continuous data is collected to form the candidate set. In the incremental learning stage, the model was fine-tuned continuously by the most informative samples in the candidate set. These samples are selected from the candidate set based on the pre-trained model and a new active learning strategy. The strategy combines the features and the uncertainty of the predicted results. Results: In order to evaluate the method, the MIT-BIH atrial fibrillation database was used for pre-training and samples of the MIT-BIH arrhythmia database were taken as candidate set. The initial values of Acc, Sen, and PPV were 87.40%, 97.46%, and 81.11%. These indexes reached to the top values of 97.53%, 100.00%, and 95.29% after 14 iterations. Hence, the number of queries was saved by 90.67%. Conclusions: The proposed system is able to update the model continuously and reduce the labeling cost over 90%. The comparisons demonstrated the effectiveness of MIDNN model and the suitability of novel learning strategy for AF. Moreover, this framework can be extended to other biomedical applications. © 2019",Active learning; Atrial fibrillation; Deep neural network; Electrocardiogram (ECG); Transfer learning,,,Computer Methods and Programs in Biomedicine,Article,Scopus
686,,Brain pathology identification using computer aided diagnostic tool: A systematic review,"Gudigar A., Raghavendra U., Hegde A., Kalyani M., Ciaccio E.J., Rajendra Acharya U.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075552852&doi=10.1016%2fj.cmpb.2019.105205&partnerID=40&md5=63140623646a988acea678fb9f346f11,10.1016/j.cmpb.2019.105205,"Computer aided diagnostic (CAD) has become a significant tool in expanding patient quality-of-life by reducing human errors in diagnosis. CAD can expedite decision-making on complex clinical data automatically. Since brain diseases can be fatal, rapid identification of brain pathology to prolong patient life is an important research topic. Many algorithms have been proposed for efficient brain pathology identification (BPI) over the past decade. Constant refinement of the various image processing algorithms must take place to expand performance of the automatic BPI task. In this paper, a systematic survey of contemporary BPI algorithms using brain magnetic resonance imaging (MRI) is presented. A summarization of recent literature provides investigators with a helpful synopsis of the domain. Furthermore, to enhance the performance of BPI, future research directions are indicated. © 2019 Elsevier B.V.",Brain pathology; Classification; Computer aided diagnostic; Deep learning; Feature extraction; Magnetic resonance imaging,,,Computer Methods and Programs in Biomedicine,Review,Scopus
687,,Classification of DNA damages on segmented comet assay images using convolutional neural network,"Atila Ü., Baydilli Y.Y., Sehirli E., Turan M.K.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074768788&doi=10.1016%2fj.cmpb.2019.105192&partnerID=40&md5=20f12c6a4e7c61ad1c08a0b78ccfdd33,10.1016/j.cmpb.2019.105192,"Background and Objective: Identification and quantification of DNA damage is a very significant subject in biomedical research area which still needs more robust and effective methods. One of the cheapest, easy to use and most successful method for DNA damage analyses is comet assay. In this study, performance of Convolutional Neural Network was examined on quantification of DNA damage using comet assay images and was compared to other methods in the literature. Methods: 796 single comet grayscale images with 170 x 170 resolution labeled by an expert and classified into 4 classes each having approximately 200 samples as G0 (healthy), G1 (poorly defective), G2 (defective) and G3 (very defective) were utilized. 120 samples were used as test dataset and the rest were used in data augmentation process to achieve better performance with training of Convolutional Neural Network. The augmented data having a total of 9995 images belonging to four classes were used as network training data set. Results: The proposed model, which was not dependent to pre-processing parameters of image processing for DNA damage classification, was able to classify comet images into 4 classes with an overall accuracy rate of 96.1%. Conclusions: This paper primarily focuses on features and usage of Convolutional Neural Network as a novel method to classify comet objects on segmented comet assay images. © 2019",Comet assay; Convolutional neural Network; Deep learning; DNA damage,,,Computer Methods and Programs in Biomedicine,Article,Scopus
688,,Super-resolution reconstruction of knee magnetic resonance imaging based on deep learning,"Qiu D., Zhang S., Liu Y., Zhu J., Zheng L.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072690971&doi=10.1016%2fj.cmpb.2019.105059&partnerID=40&md5=9c86b2aea93f3dd8e10b461b49759f6d,10.1016/j.cmpb.2019.105059,"Background and objective: With the rapid development of medical imaging and intelligent diagnosis, artificial intelligence methods have become a research hotspot of radiography processing technology in recent years. The low definition of knee magnetic resonance image texture seriously affects the diagnosis of knee osteoarthritis. This paper presents a super-resolution reconstruction method to address this problem. Methods: In this paper, we propose an efficient medical image super-resolution (EMISR) method, in which we mainly adopted three hidden layers of super-resolution convolution neural network (SRCNN) and a sub-pixel convolution layer of efficient sub-pixel convolution neural network (ESPCN). The addition of the efficient sub-pixel convolutional layer in the hidden layer and the small network replacement consisting of concatenated convolutions to address low-resolution images but not high-resolution images are important. The EMISR method also uses cascaded small convolution kernels to improve reconstruction speed and deepen the convolution neural network to improve reconstruction quality. Results: The proposed method is tested in the public dataset IDI, and the reconstruction quality of the algorithm is higher than that of the sparse coding-based network (SCN) method, the SRCNN method, and the ESPCN method (+ 2.306 dB, + 2.540 dB, + 1.089 dB improved); moreover, the reconstruction speed is faster than its counterparts (+ 4.272 s, + 1.967 s, and + 0.073 s improved). Conclusion: The experimental results show that our EMISR framework has improved performance and greatly reduces the number of parameters and training time. Furthermore, the reconstructed image presents more details, and the edges are more complete. Therefore, the EMISR technique provides a more powerful medical analysis in knee osteoarthritis examinations. © 2019 Elsevier B.V.",Convolutional neural network; Deep learning; Low-resolution image; Medical imaging; Super resolution,,,Computer Methods and Programs in Biomedicine,Article,Scopus
689,,A transfer learning method with deep residual network for pediatric pneumonia diagnosis,"Liang G., Zheng L.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068048842&doi=10.1016%2fj.cmpb.2019.06.023&partnerID=40&md5=af1e1494b18c1e425d58e61a4e98d60c,10.1016/j.cmpb.2019.06.023,"Background and Objective: Computer aided diagnosis systems based on deep learning and medical imaging is increasingly becoming research hotspots. At the moment, the classical convolutional neural network generates classification results by hierarchically abstracting the original image. These abstract features are less sensitive to the position and orientation of the object, and this lack of spatial information limits the further improvement of image classification accuracy. Therefore, how to develop a suitable neural network framework and training strategy in practical clinical applications to avoid this problem is a topic that researchers need to continue to explore. Methods: We propose a deep learning framework that combines residual thought and dilated convolution to diagnose and detect childhood pneumonia. Specifically, based on an understanding of the nature of the child pneumonia image classification task, the proposed method uses the residual structure to overcome the over-fitting and the degradation problems of the depth model, and utilizes dilated convolution to overcome the problem of loss of feature space information caused by the increment in depth of the model. Furthermore, in order to overcome the problem of difficulty in training model due to insufficient data and the negative impact of the introduction of structured noise on the performance of the model, we use the model parameters learned on large-scale datasets in the same field to initialize our model through transfer learning. Results: Our proposed method has been evaluated for extracting texture features associated with pneumonia and for accurately identifying the performance of areas of the image that best indicate pneumonia. The experimental results of the test dataset show that the recall rate of the method on children pneumonia classification task is 96.7%, and the f1-score is 92.7%. Compared with the prior art methods, this approach can effectively solve the problem of low image resolution and partial occlusion of the inflammatory area in children chest X-ray images. Conclusions: The novel framework focuses on the application of advanced classification that directly performs lesion characterization, and has high reliability in the classification task of children pneumonia. © 2019 Elsevier B.V.",Deep learning; Image classification; Pneumonia; Residual network; Transfer Learning,,,Computer Methods and Programs in Biomedicine,Article,Scopus
690,,Improved white blood cells classification based on pre-trained deep learning models,"Mohamed E.H., El-Behaidy W.H., Khoriba G., Li J.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084797561&doi=10.24138%2fjcomss.v16i1.818&partnerID=40&md5=486f191fbda230fbaf12d32139f7bb40,10.24138/jcomss.v16i1.818,"Leukocytes, or white blood cells (WBCs), are microscopic organisms that fight against infectious disease, bacteria, viruses and others. The manual method to classify and count WBCs is tedious, time-consuming and may have inaccurate results, whereas the automated methods are costly. This research aims to automatically identify and classify WBCs in a microscopic image into four types with higher accuracy. BCCD is the used dataset in this study, which is a scaled-down blood cell detection dataset. BCCD is firstly preprocessed by passing through various processes such as segmentation and augmentation; then, it is passed to the proposed model. Our model combines the advantage of deep models in automatically extracting features with the higher classification accuracy of traditional machine learning classifiers. The proposed model consists of two main stages: a shallow tuning pre-trained model and a traditional machine learning classifier on top of it. In this study, ten different pre-trained models with six types of machine learning are used. Moreover, the fully connected network (FCN) of pre-trained models is used as a baseline classifier for comparison. The evaluation process shows that the hybrid of MobileNet-224 as a feature extractor and logistic regression as classifier has a higher rank-1 accuracy of 97.03%. Besides, the proposed hybrid model outperformed the baseline FCN by 25.78% on average. © 2020 CCIS",Classification; Deep learning; Feature extraction; White blood cells (WBCs),"37, 45",,Journal of Communications Software and Systems,Article,Scopus
691,,Survey on Privacy Attacks and Defenses in Machine Learning [机器学习中的隐私攻击与防御],"Liu R.-X., Chen H., Guo R.-Y., Zhao D., Liang W.-J., Li C.-P.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083031258&doi=10.13328%2fj.cnki.jos.005904&partnerID=40&md5=1b70915bb2a78e69d0d025ab834239db,10.13328/j.cnki.jos.005904,"In the era of big data, a rich source of data prompts the development of machine learning technology. However, risks of privacy leakage of models' training data in data collecting and training stages pose essential challenges to data management in the artificial intelligence age. Traditional privacy preserving methods of data management and analysis could not satisfy the complex privacy problems in various stages and scenarios of machine learning. This study surveys the state-of-the-art works of privacy attacks and defenses in machine learning. On the one hand, scenarios of privacy leakage and adversarial models of privacy attacks are illustrated. Also, specific works of privacy attacks are classified with respect to adversarial strategies. On the other hand, 3 main technologies which are commonly applied in privacy preserving of machine learning are introduced and key problems of their applications are pointed out. In addition, 5 defense strategies and corresponding specific mechanisms are elaborated. Finally, future works and challenges of privacy preserving in machine learning are concluded. © Copyright 2020, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Data management; Machine learning; Privacy attack; Privacy preserving,"866, 892",,Ruan Jian Xue Bao/Journal of Software,Article,Scopus
692,,"Cross-version defect prediction: use historical data, cross-project data, or both?",Amasaki S.,2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078471864&doi=10.1007%2fs10664-019-09777-8&partnerID=40&md5=398e8d5e5e041fcb92a20e3a4b7773a1,10.1007/s10664-019-09777-8,"Context: Although a long-running project has experienced many releases, removing defects from a product is still a challenge. Cross-version defect prediction (CVDP) regards project data of prior releases as a useful source for predicting fault-prone modules based on defect prediction techniques. Recent studies have explored cross-project defect prediction (CPDP) that uses the project data from outside a project for defect prediction. While CPDP techniques and CPDP data can be diverted to CVDP, its effectiveness has not been investigated. Objective: To investigate whether CPDP approaches and CPDP data are useful for CVDP. The investigation also compared the usage of prior release data. Method: We chose a style of replication of a previous comparative study on CPDP approaches. Results: Some CPDP approaches could improve the performance of CVDP. The use of the latest prior release was the best choice. If one has no CVDP data, the use of CPDP data for CVDP was found to be effective. Conclusions: 1) Some CPDP approaches could improve CVDP, 2), if one can access project data from the latest release, project data from older releases would not bring clear benefit, and 3) even if one has no CVDP data, appropriate CPDP approaches would be able to deliver quality prediction with CPDP data. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.",Comparative study; Cross-project defect prediction; Cross-version defect prediction,"1573, 1595",,Empirical Software Engineering,Article,Scopus
693,,"Deep learning, reusable and problem-based architectures for detection of consolidation on chest X-ray images","Behzadi-khormouji H., Rostami H., Salehi S., Derakhshande-Rishehri T., Masoumi M., Salemi S., Keshavarz A., Gholamrezanezhad A., Assadi M., Batouli A.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074729601&doi=10.1016%2fj.cmpb.2019.105162&partnerID=40&md5=f4c92eaa5ab7106402c36bf265791646,10.1016/j.cmpb.2019.105162,"Background and objective: In most patients presenting with respiratory symptoms, the findings of chest radiography play a key role in the diagnosis, management, and follow-up of the disease. Consolidation is a common term in radiology, which indicates focally increased lung density. When the alveolar structures become filled with pus, fluid, blood cells or protein subsequent to a pulmonary pathological process, it may result in different types of lung opacity in chest radiograph. This study aims at detecting consolidations in chest x-ray radiographs, with a certain precision, using artificial intelligence and especially Deep Convolutional Neural Networks to assist radiologist for better diagnosis. Methods: Medical image datasets usually are relatively small to be used for training a Deep Convolutional Neural Network (DCNN), so transfer learning technique with well-known DCNNs pre-trained with ImageNet dataset are used to improve the accuracy of the models. ImageNet feature space is different from medical images and in the other side, the well-known DCNNs are designed to achieve the best performance on ImageNet. Therefore, they cannot show their best performance on medical images. To overcome this problem, we designed a problem-based architecture which preserves the information of images for detecting consolidation in Pediatric Chest X-ray dataset. We proposed a three-step pre-processing approach to enhance generalization of the models. To demonstrate the correctness of numerical results, an occlusion test is applied to visualize outputs of the model and localize the detected appropriate area. A different dataset as an extra validation is used in order to investigate the generalization of the proposed model. Results: The best accuracy to detect consolidation is 94.67% obtained by our problem based architecture for the understudy dataset which outperforms the previous works and the other architectures. Conclusions: The designed models can be employed as computer aided diagnosis tools in real practice. We critically discussed the datasets and the previous works based on them and show that without some considerations the results of them may be misleading. We believe, the output of AI should be only interpreted as focal consolidation. The clinical significance of the finding can not be interpreted without integration of clinical data. © 2019",Chest X-ray; Consolidation; Consolidation; Deep Convolutional Neural Network; Histogram equalization; Histogram matching; Medical imaging; Pediatric pneumonia; Pneumonia; Transfer learning,,,Computer Methods and Programs in Biomedicine,Article,Scopus
694,,A neural network approach to segment brain blood vessels in digital subtraction angiography,"Zhang M., Zhang C., Wu X., Cao X., Young G.S., Chen H., Xu X.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074652212&doi=10.1016%2fj.cmpb.2019.105159&partnerID=40&md5=2c7cf19c37fc51e3755fe9dd75dac17b,10.1016/j.cmpb.2019.105159,"Background and objective: Cerebrovascular diseases (CVDs) affect a large number of patients and often have devastating outcomes. The hallmarks of CVDs are the abnormalities formed on brain blood vessels, including protrusions, narrows, widening, and bifurcation of the blood vessels. CVDs are often diagnosed by digital subtraction angiography (DSA) yet the interpretation of DSA is challenging as one must carefully examine each brain blood vessel. The objective of this work is to develop a computerized analysis approach for automated segmentation of brain blood vessels. Methods: In this work, we present a U-net based deep learning approach, combined with pre-processing, to track and segment brain blood vessels in DSA images. We compared the results given by the deep learning approach with manually marked ground truth using accuracy, sensitivity, specificity, and Dice coefficient. Results: Our results showed that the proposed approach achieved an accuracy of 0.978, with a standard deviation of 0.00796, a sensitivity of 0.76 with a standard deviation of 0.096, a specificity of 0.994 with a standard deviation of 0.0036, and an average Dice coefficient was 0.8268 with a standard deviation of 0.052. Conclusions: Our findings show that the deep learning approach can achieve satisfactory performance as a computer-aided analysis tool to assist clinicians in diagnosing CVDs. © 2019",Brain blood vessels; Deep learning; Digital subtraction angiography (DSA); Neural network; Segmentation,,,Computer Methods and Programs in Biomedicine,Article,Scopus
695,,Integration of convolutional neural networks for pulmonary nodule malignancy assessment in a lung cancer classification pipeline,"Bonavita I., Rafael-Palou X., Ceresa M., Piella G., Ribas V., González Ballester M.A.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074637539&doi=10.1016%2fj.cmpb.2019.105172&partnerID=40&md5=a786bdbf9252a9d0a7e55699582c6ef6,10.1016/j.cmpb.2019.105172,"Background and Objective: The early identification of malignant pulmonary nodules is critical for a better lung cancer prognosis and less invasive chemo or radio therapies. Nodule malignancy assessment done by radiologists is extremely useful for planning a preventive intervention but is, unfortunately, a complex, time-consuming and error-prone task. This explains the lack of large datasets containing radiologists malignancy characterization of nodules; Methods: In this article, we propose to assess nodule malignancy through 3D convolutional neural networks and to integrate it in an automated end-to-end existing pipeline of lung cancer detection. For training and testing purposes we used independent subsets of the LIDC dataset; Results: Adding the probabilities of nodules malignity in a baseline lung cancer pipeline improved its F1-weighted score by 14.7%, whereas integrating the malignancy model itself using transfer learning outperformed the baseline prediction by 11.8% of F1-weighted score; Conclusions: Despite the limited size of the lung cancer datasets, integrating predictive models of nodule malignancy improves prediction of lung cancer. © 2019",Deep learning; Lung cancer; Machine learning; Nodule malignancy,,,Computer Methods and Programs in Biomedicine,Article,Scopus
696,,An empirical study of factors affecting cross-project aging-related bug prediction with TLAP,"Qin F., Wan X., Yin B.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074566426&doi=10.1007%2fs11219-019-09460-7&partnerID=40&md5=d2de6b5e32a293ce3fd1c694aaa2373c,10.1007/s11219-019-09460-7,"Software aging is a phenomenon in which long-running software systems show an increasing failure rate and/or progressive performance degradation. Due to their nature, Aging-Related Bugs (ARBs) are hard to discover during software testing and are also challenging to reproduce. Therefore, automatically predicting ARBs before software release can help developers reduce ARB impact or avoid ARBs. Many bug prediction approaches have been proposed, and most of them show effectiveness in within-project prediction settings. However, due to the low presence and reproducing difficulty of ARBs, it is usually hard to collect sufficient training data to build an accurate prediction model. A recent work proposed a method named Transfer Learning based Aging-related bug Prediction (TLAP) for performing cross-project ARB prediction. Although this method considerably improves cross-project ARB prediction performance, it has been observed that its prediction result is affected by several key factors, such as the normalization methods, kernel functions, and machine learning classifiers. Therefore, this paper presents the first empirical study to examine the impact of these factors on the effectiveness of cross-project ARB prediction in terms of single-factor pattern, bigram pattern, and triplet pattern and validates the results with the Scott-Knott test technique. We find that kernel functions and classifiers are key factors affecting the effectiveness of cross-project ARB prediction, while normalization methods do not show statistical influence. In addition, the order of values in three single-factor patterns is maintained in three bigram patterns and one triplet pattern to a large extent. Similarly, the order of values in the three bigram patterns is also maintained in the triplet pattern. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.",Aging-related bugs; Cross-project; Empirical study; Software aging,"107, 134",,Software Quality Journal,Article,Scopus
697,,An ensemble learning approach for brain cancer detection exploiting radiomic features,"Brunese L., Mercaldo F., Reginelli A., Santone A.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074154276&doi=10.1016%2fj.cmpb.2019.105134&partnerID=40&md5=b49995e5dfb88c8f9c39aff032446979,10.1016/j.cmpb.2019.105134,"Background and Objective: The brain cancer is one of the most aggressive tumour: the 70% of the patients diagnosed with this malignant cancer will not survive. Early detection of brain tumours can be fundamental to increase survival rates. The brain cancers are classified into four different grades (i.e., I, II, III and IV) according to how normal or abnormal the brain cells look. The following work aims to recognize the different brain cancer grades by analysing brain magnetic resonance images. Methods: A method to identify the components of an ensemble learner is proposed. The ensemble learner is focused on the discrimination between different brain cancer grades using non invasive radiomic features. The considered radiomic features are belonging to five different groups: First Order, Shape, Gray Level Co-occurrence Matrix, Gray Level Run Length Matrix and Gray Level Size Zone Matrix. We evaluate the features effectiveness through hypothesis testing and through decision boundaries, performance analysis and calibration plots thus we select the best candidate classifiers for the ensemble learner. Results: We evaluate the proposed method with 111,205 brain magnetic resonances belonging to two freely available data-sets for research purposes. The results are encouraging: we obtain an accuracy of 99% for the benign grade I and the II, III and IV malignant brain cancer detection. Conclusion: The experimental results confirm that the ensemble learner designed with the proposed method outperforms the current state-of-the-art approaches in brain cancer grade detection starting from magnetic resonance images. © 2019 Elsevier B.V.",Astrocytoma; Brain cancer; Ensemble learning; Glioblastoma; Machine learning; Radiomics,,,Computer Methods and Programs in Biomedicine,Article,Scopus
698,,Cross-project bug type prediction based on transfer learning,"Du X., Zhou Z., Yin B., Xiao G.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073827187&doi=10.1007%2fs11219-019-09467-0&partnerID=40&md5=badb04ec205641f3feb7aded53daf2c4,10.1007/s11219-019-09467-0,"The prediction of bug types provides useful insights into the software maintenance process. It can improve the efficiency of software testing and help developers adopt corresponding strategies to fix bugs before releasing software projects. Typically, the prediction tasks are performed through machine learning classifiers, which rely heavily on labeled data. However, for a software project that has insufficient labeled data, it is difficult to train the classification model for predicting bug types. Although labeled data of other projects can be used as training data, the results of the cross-project prediction are often poor. To solve this problem, this paper proposes a cross-project bug type prediction framework based on transfer learning. Transfer learning breaks the assumption of traditional machine learning methods that the training set and the test set should follow the same distribution. Our experiments show that the results of cross-project bug type prediction have significant improvement by adopting transfer learning. In addition, we have studied the factors that influence the prediction results, including different pairs of source and target projects, and the number of bug reports in the source project. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.",Bug prediction; Bug report; Cross-project; Transfer learning,"39, 57",,Software Quality Journal,Article,Scopus
699,,Leveraging Machine Learning for Software Redocumentation,"Geist V., Moser M., Pichler J., Beyer S., Pinzger M.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083579151&doi=10.1109%2fSANER48275.2020.9054838&partnerID=40&md5=8e1866f4f37d70c3aeb7dbfca56d5445,10.1109/SANER48275.2020.9054838,"Source code comments contain key information about the underlying software system. Many redocumentation approaches, however, cannot exploit this valuable source of information. This is mainly due to the fact that not all comments have the same goals and target audience and can therefore only be used selectively for redocumentation. Performing a required classification manually, e.g. in the form of heuristic rules, is usually time-consuming and error-prone and strongly dependent on programming languages and guidelines of concrete software systems. By leveraging machine learning, it should be possible to classify comments and thus transfer valuable information from the source code into documentation with less effort but the same quality. We applied different machine learning techniques to a COBOL legacy system and compared the results with industry-strength heuristic classification. As a result, we found that machine learning outperforms the heuristics in number of errors and less effort. © 2020 IEEE.",CNNs; comment classification pipeline; heuristic rules; legacy system; machine learning; NLP; software redocumentation,"622, 626",,"SANER 2020 - Proceedings of the 2020 IEEE 27th International Conference on Software Analysis, Evolution, and Reengineering",Conference Paper,Scopus
700,,Deep Learning Based Identification of Suspicious Return Statements,"Li G., Liu H., Jin J., Umer Q.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083556267&doi=10.1109%2fSANER48275.2020.9054826&partnerID=40&md5=c5a8f3f4b8a450c6d2aebf306b9ea223,10.1109/SANER48275.2020.9054826,"Identifiers in source code are composed of terms in natural languages. Such terms, as well as phrases composed of such terms, convey rich semantics that could be exploited for program analysis and comprehension. To this end, in this paper we propose a deep learning based approach, called MLDetector, to identifying suspicious return statements by leveraging semantics conveyed by the natural language phrases that are used as identifiers in the source code. We specially design a deep neural network to tell whether a given return statement matches its corresponding method signature. The rationale is that both method signature and return value should explicitly specify the output of the method, and thus a significant mismatch between method signature and return value may suggest a suspicious return statement. To address the challenge of lacking negative training data, i.e., incorrect return statements, we generate negative training data automatically by transforming real-world correct return statements. To feed code into neural network, we convert them into vectors by Word2Vec, an unsupervised neural network based learning algorithm. We evaluate the proposed approach in two parts. In the first part, we evaluate it on 500 open-source applications by automatically generating labeled training data. Results suggest that the precision of the proposed approach varies from 83% to 90%. In the second part, we conduct a case study on 100 real-world applications. Evaluation results suggest that 42 out of 65 real-world incorrect return statements are detected (with precision of 59%). © 2020 IEEE.",Bug Detection; Code Quality; Deep Learning; Identification; Program Analysis; Return Value,"480, 491",,"SANER 2020 - Proceedings of the 2020 IEEE 27th International Conference on Software Analysis, Evolution, and Reengineering",Conference Paper,Scopus
701,,Cross-Dataset Design Discussion Mining,"Mahadi A., Tongay K., Ernst N.A.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083555688&doi=10.1109%2fSANER48275.2020.9054792&partnerID=40&md5=9dbc463d70f0901b3072e404686ff20e,10.1109/SANER48275.2020.9054792,"Being able to identify software discussions that are primarily about design - which we call design mining - can improve documentation and maintenance of software systems. Existing design mining approaches have good classification performance using natural language processing (NLP) techniques, but the conclusion stability of these approaches is generally poor. A classifier trained on a given dataset of software projects has so far not worked well on different artifacts or different datasets. In this study, we replicate and synthesize these earlier results in a meta - analysis. We then apply recent work in transfer learning for NLP to the problem of design mining. However, for our datasets, these deep transfer learning classifiers perform no better than less complex classifiers. We conclude by discussing some reasons behind the transfer learning approach to design mining. © 2020 IEEE.",empirical software engineering; mining software design; replication; transfer learning,"149, 160",,"SANER 2020 - Proceedings of the 2020 IEEE 27th International Conference on Software Analysis, Evolution, and Reengineering",Conference Paper,Scopus
702,,Req2Lib: A Semantic Neural Model for Software Library Recommendation,"Sun Z., Liu Y., Cheng Z., Yang C., Che P.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083549113&doi=10.1109%2fSANER48275.2020.9054865&partnerID=40&md5=82c4852973eca893bfa43a9ae018a47d,10.1109/SANER48275.2020.9054865,"Third-party libraries are crucial to the development of software projects. To get suitable libraries, developers need to search through millions of libraries by filtering, evaluating, and comparing. The vast number of libraries places a barrier for programmers to locate appropriate ones. To help developers, researchers have proposed automated approaches to recommend libraries based on library usage pattern. However, these prior studies can not sufficiently match user requirements and suffer from cold-start problem. In this work, we would like to make recommendations based on requirement descriptions to avoid these problems. To this end, we propose a novel neural approach called Req2Lib which recommends libraries given descriptions of the project requirement. We use a Sequence-to-Sequence model to learn the library linked-usage information and semantic information of requirement descriptions in natural language. Besides, we apply a domain-specific pre-trained word2vec model for word embedding, which is trained over textual corpus from Stack Overflow posts. In the experiment, we train and evaluate the model with data from 5,625 Java projects. Our preliminary evaluation demonstrates that Req2Lib can recommend libraries accurately. © 2020 IEEE.",Deep Learning; GitHub; Library Recommendation,"542, 546",,"SANER 2020 - Proceedings of the 2020 IEEE 27th International Conference on Software Analysis, Evolution, and Reengineering",Conference Paper,Scopus
703,,Domain Adaptation Approach for Cross-project Software Defect Prediction [一种基于领域适配的跨项目软件缺陷预测方法],"Chen S., Ye J.-M., Liu T.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083002323&doi=10.13328%2fj.cnki.jos.005632&partnerID=40&md5=50d1bec5c37a52913c7047f11af48ba4,10.13328/j.cnki.jos.005632,"Software defect prediction aims at the very early step of software quality control, helps software engineers focus their attention on defect-prone parts during verification process. Cross-project defect predictions are proposed in which prediction models are trained by using sufficient training data from already existed software projects and predict defect in some other projects, however, their performances are always poor. The main reason is that, the divergence of the data distribution among different software projects causes a dramatic impact on the prediction accuracy. This study proposed an approach of cross-project defect prediction by applying a supervised domain adaptation based on instance weighting. The sufficient instances drawn from some source project are weighted by assigning target-dependent weights to the loss function of the prediction model when minimizing the expected loss over the distribution of source data, so that the distribution properties of the data from target project can be matched to the source project. Experiments including dataset selection, data preprocessing and results are described over different experiment strategies on ten open-source software projects. Over fitting problems are also studied through different levels including dataset, prediction model and domain adaptation process. The results show that the proposed approach is close to the performance of within-project defect prediction, better than similar approach and significantly better that of the baseline. © Copyright 2020, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Domain adaptation; Machine learning; Software defect metrics; Software defect prediction; Transfer learning,"266, 281",,Ruan Jian Xue Bao/Journal of Software,Article,Scopus
704,,Building a Credit Scoring Model Based on Data Mining Approaches,"Nalić J., Martinovic G.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082402028&doi=10.1142%2fS0218194020500072&partnerID=40&md5=d21c814800cd221ded0b82d11b2af43a,10.1142/S0218194020500072,"Nowadays, one of the biggest challenges in banking sector, certainly, is assessment of the client's creditworthiness. In order to improve the decision-making process and risk management, banks resort to using data mining techniques for hidden patterns recognition within a wide data. The main objective of this study is to build a high-performance customized credit scoring model. The model named Reliable client is based on Bank's real dataset and originally built by applying four different classification algorithms: decision tree (DT), naive Bayes (NB), generalized linear model (GLM) and support vector machine (SVM). Since it showed the greatest results, but also seemed as the most appropriate algorithm, the adopted model is based on GLM algorithm. The results of this model are presented based on many performance measures that showed great predictive confidence and accuracy, but we also demonstrated significant impact of data pre-processing on model performance. Statistical analysis of the model identified the most significant parameters on the model outcome. In the end, created credit scoring model was evaluated using another set of real data of the same Bank. © 2020 World Scientific Publishing Company.",Classification; credit scoring; data mining; generalized linear model (GLM); logistic regression (LR),"147, 169",,International Journal of Software Engineering and Knowledge Engineering,Article,Scopus
705,,Fully automatic estimation of pelvic sagittal inclination from anterior-posterior radiography image using deep learning framework,"Jodeiri A., Zoroofi R.A., Hiasa Y., Takao M., Sugano N., Sato Y., Otake Y.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077087377&doi=10.1016%2fj.cmpb.2019.105282&partnerID=40&md5=c774fd91ce8ceb59dc2b0924e7993d11,10.1016/j.cmpb.2019.105282,"Background and Objective: Malposition of the acetabular component causes dislocation and prosthetic impingement after Total Hip Arthroplasty (THA), which significantly affects the postoperative quality of life and implant longevity. The position of the acetabular component is determined by the Pelvic Sagittal Inclination (PSI), which not only varies among different people but also changes in different positions. It is important to recognize individual dynamic changes of the PSI for patient-specific planning of the THA. Previously PSI was estimated by registering the CT and radiography images. In this study, we introduce a new method for accurate estimation of functional PSI without requiring CT image in order to lower radiation exposure of the patient which opens up the possibility of increasing its application in a larger number of hospitals where CT is not acquired as a routine protocol. Methods: The proposed method consists of two main steps: First, the Mask R-CNN framework was employed to segment the pelvic shape from the background in the radiography images. Then, following the segmentation network, another convolutional network regressed the PSI angle. We employed a transfer learning paradigm where the network weights were initialized by non-medical images followed by fine-tuning using radiography images. Furthermore, in the training process, augmented data was generated to improve the performance of both networks. We analyzed the role of segmentation network in our system and investigated the Mask R-CNN performance in comparison with the U-Net, which is commonly used for the medical image segmentation. Results: In this study, the Mask R-CNN utilizing multi-task learning, transfer learning, and data augmentation techniques achieve 0.960 ± 0.008 DICE coefficient, which significantly outperforms the U-Net. The cascaded system is capable of estimating the PSI with 4.04° ± 3.39° error for the radiography images. Conclusions: The proposed framework suggests a fully automatic and robust estimation of the PSI using only an anterior-posterior radiography image. © 2019",Convolutional neural network; Deep learning; Mask R-CNN; Pelvic tilt; Segmentation; Total hip arthroplasty,,,Computer Methods and Programs in Biomedicine,Article,Scopus
706,,A cross-dataset deep learning-based classifier for people fall detection and identification,"Delgado-Escaño R., Castro F.M., Cózar J.R., Marín-Jiménez M.J., Guil N., Casilari E.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076865469&doi=10.1016%2fj.cmpb.2019.105265&partnerID=40&md5=d95274dffc957063f0f0919ebe7028f9,10.1016/j.cmpb.2019.105265,"Background and Objective. Fall detection is an important problem for vulnerable sectors of the population such as elderly people, who frequently live alone. Note that a fall can be very dangerous for them if they cannot ask for help. Hence, in those situations, an automatic system that detected and informed to emergency services about the fall and subject identity could help to save lives. This way, they would know not only when but also who to help. Thus, our objective is to develop a new approach, based on deep learning, for fall detection and people identification that can be used in different datasets without any fine-tuning of the model parameters. Methods.We present a dataset-independent deep learning-based model that, by employing a multi-task learning approach, uses raw inertial information as input to solve simultaneously two tasks: fall detection and subject identification. By this way, our approach is able to automatically learn the best representations without any constraint introduced by the pre-processed features. Results. Our cross-dataset classifier is able to detect falls with more than a 98% of accuracy in four datasets recorded under different conditions (i.e. accelerometer device, sampling rate, sequence length, age of the subjects, etc.). Moreover, the number of false positives is very low – on average less than 1.6% – establishing a new state-of-the-art. Finally, our classifier is also capable of correctly identifying people with an average accuracy of 79.6%. Conclusions. The presented approach performs both tasks (fall detection and people identification) by using a single model and achieving real-time execution. The obtained results allow us to assert that a single model can be used for both fall detection and people identification under different conditions, easing its real implementation, as it is not necessary to train the model for new subjects. © 2019",Activities of daily living; Convolutional neural network; Fall detection; Inertial sensors; Long short-term memory; Multi-task,,,Computer Methods and Programs in Biomedicine,Article,Scopus
707,,Constructing a PM2.5 concentration prediction model by combining auto-encoder with Bi-LSTM neural networks,"Zhang B., Zhang H., Zhao G., Lian J.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076181809&doi=10.1016%2fj.envsoft.2019.104600&partnerID=40&md5=4d9d5aa4ce81e03ba90bc8b4653e3646,10.1016/j.envsoft.2019.104600,"Air pollution problems have a severe effect on the natural environment and public health. The application of machine learning to air pollutant data can result in a better understanding of environmental quality. Of these methods, the deep learning method has proven to be a very efficient and accurate method to forecast complex air quality data. This paper proposes a deep learning model based on an auto-encoder and bidirectional long short-term memory (Bi-LSTM) to forecast PM2.5 concentrations to reveal the correlation between PM2.5 and multiple climate variables. The model comprises several aspects, including data preprocessing, auto-encoder layer, and Bi-LSTM layer. The performance of the proposed model was verified based on a real-world air pollution dataset, and the results indicated this model can improve the prediction accuracy in an experimental scenario. © 2019",Air pollution; Auto-encoder; Bi-LSTM; Data preprocessing; Deep learning; PM2.5 concentration prediction,,,Environmental Modelling and Software,Article,Scopus
708,,CVE-assisted large-scale security bug report dataset construction method,"Wu X., Zheng W., Chen X., Wang F., Mu D.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074980613&doi=10.1016%2fj.jss.2019.110456&partnerID=40&md5=23e143e2efb99dffae256ae090910f7c,10.1016/j.jss.2019.110456,"Identifying SBRs (security bug reports) is crucial for eliminating security issues during software development. Machine learning are promising ways for SBR prediction. However, the effectiveness of the state-of-the-art machine learning models depend on high-quality datasets, while gathering large-scale datasets are expensive and tedious. To solve this issue, we propose an automated data labeling approach based on iterative voting classification. It starts with a small group of ground-truth traing samples, which can be labeled with the help of authoritative vulnerability records hosted in CVE (Common Vulnerabilities and Exposures). The accuracy of the prediction model is improved with an iterative voting strategy. By using this approach, we label over 80k bug reports from OpenStack and 40k bug reports from Chromium. The correctness of these labels are then manually reviewed by three experienced security testing members. Finally, we construct a large-scale SBR dataset with 191 SBRs and 88,472 NSBRs (non-security bug reports) from OpenStack; and improve the quality of existing SBR dataset Chromium by identifying 64 new SBRs from previously labeled NSBRs and filtering out 173 noise bug reports from this dataset. These share datasets as well as the proposed dataset construction method help to promote research progress in SBR prediction research domain. © 2019 Elsevier Inc.",Common vulnerabilities and exposures; Dataset construction; Security bug report prediction; Voting classification,,,Journal of Systems and Software,Article,Scopus
709,,Deep contextualized embeddings for quantifying the informative content in biomedical text summarization,"Moradi M., Dorffner G., Samwald M.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073186155&doi=10.1016%2fj.cmpb.2019.105117&partnerID=40&md5=e83ca21c07065c195c8e9f537fc69da1,10.1016/j.cmpb.2019.105117,"Background and Objective: Capturing the context of text is a challenging task in biomedical text summarization. The objective of this research is to show how contextualized embeddings produced by a deep bidirectional language model can be utilized to quantify the informative content of sentences in biomedical text summarization. Methods: We propose a novel summarization method that utilizes contextualized embeddings generated by the Bidirectional Encoder Representations from Transformers (BERT) model, a deep learning model that recently demonstrated state-of-the-art results in several natural language processing tasks. We combine different versions of BERT with a clustering method to identify the most relevant and informative sentences of input documents. Using the ROUGE toolkit, we evaluate the summarizer against several methods previously described in literature. Results: The summarizer obtains state-of-the-art results and significantly improves the performance of biomedical text summarization in comparison to a set of domain-specific and domain-independent methods. The largest language model not specifically pretrained on biomedical text outperformed other models. However, among language models of the same size, the one further pretrained on biomedical text obtained best results. Conclusions: We demonstrate that a hybrid system combining a deep bidirectional language model and a clustering method yields state-of-the-art results without requiring labor-intensive creation of annotated features or knowledge bases or computationally demanding domain-specific pretraining. This study provides a starting point towards investigating deep contextualized language models for biomedical text summarization. © 2019","Biomedical text mining; Clustering; Contextualized embeddings; Deep learning, domain knowledge; Text summarization",,,Computer Methods and Programs in Biomedicine,Article,Scopus
710,,Exploration of neural machine translation in autoformalization of mathematics in Mizar,"Wang Q., Brown C., Kaliszyk C., Urban J.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079441882&doi=10.1145%2f3372885.3373827&partnerID=40&md5=866abbcc33dd79f8c8afd9bb28088a84,10.1145/3372885.3373827,"In this paper we share several experiments trying to automatically translate informal mathematics into formal mathematics. In our context informal mathematics refers to humanwritten mathematical sentences in the LaTeX format; and formal mathematics refers to statements in the Mizar language. We conducted our experiments against three established neural network-based machine translation models that are known to deliver competitive results on translating between natural languages. To train these models we also prepared four informal-to-formal datasets. We compare and analyze our results according to whether the model is supervised or unsupervised. In order to augment the data available for auto-formalization and improve the results, we develop a custom type-elaboration mechanism and integrate it in the supervised translation. © 2020 ACM.",Automating Formalization; Machine Learning; Mizar; Neural Machine Translation; ProofAssistants,"85, 98",,"CPP 2020 - Proceedings of the 9th ACM SIGPLAN International Conference on Certified Programs and Proofs, co-located with POPL 2020",Conference Paper,Scopus
711,,Data science and machine learning techniques for case-based learning in medical bioengineering education,"Buiu C., Dănăilă V.-R.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096602626&doi=10.12753%2f2066-026X-20-194&partnerID=40&md5=35a7218dcaf77e16ba15105c077b9348,10.12753/2066-026X-20-194,"Medical bioengineering professionals are expected to play a leading role in the development of advanced algorithms and methods with the greater goal to enhance health care in fields like diagnosis, monitoring, and therapy. In a recent study on the current research areas in big data analytics and artificial intelligence in health care, after a systematic review of literature, it was found that the primary interest area is medical image processing and analysis followed by decision-support systems and text mining. Case-based learning is an instructional design model that is learner-centered and intensively used across a variety of disciplines. In this paper, it is presented a set of deep learning tools and a case study that help medical bioengineering students to grasp both theoretical concepts (medical, such as gynecological disorders and technological, such as deep learning, neural network architectures, learning algorithms) and delve into practical applications of these techniques in medical image processing. The case study concerns the automated diagnosis of cervigrams (also called cervicographic images), that are colposcopy images used by the gynecologist for cervical cancer diagnosis. The tools described in this paper are based on using PyTorch and are implemented in a Jupyter notebook. The notebook introduces the students to the problem of cervigrams classification, provides access to the dataset, allows data augmentation, data visualization and performing transfer learning. The notebook ends with study questions. © 2020, National Defence University-Carol I Printing House. All rights reserved.",Artificial intelligence; Case-based learning; Cervigrams; Colposcopy; Data science; Deep learning,"186, 191",,eLearning and Software for Education Conference,Conference Paper,Scopus
712,,Sentiment analysis based on deep learning techniques applied to children in logical games from nonformal education,"Munteanu D., Munteanu N.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096474700&doi=10.12753%2f2066-026X-20-007&partnerID=40&md5=fc8d0503978d32915c3854dc8d8db2dd,10.12753/2066-026X-20-007,"This paper presents an analysis of the behavioral and temperamental changes of children in non-formal education lessons within the Logic Games discipline. The experimental setup was made using 2 video cameras that alternatively recorded and monitored groups of 2 children during the lessons from the Logic Games discipline for the entire scholar year. Emotion plays an important role in the lesson of non-formal education. Chess as part of the Logic Games involves the training of qualities such as will, ambition, perseverance, attention to detail, distributive attention, patience, evaluation and anticipation of many alternatives and possibilities to move both your own and your opponent's and to choose the moves. optimal. It is very important to be mentally trained to deal with the negative emotions generated by losing or losing the match. Therefore, before learning how to win you must learn how to lose. The sentiment analysis in this paper refers only to the automatic recognition and identification of facial expressions. The images extracted from the video recordings were processed, and then classified into the categories: happy, sad, angry, disappointed, pleasantly surprised, proud, panicked / worried or stressed Deep Learning techniques such as Convolutional Neuronal Network, RCNN, Faster RCNN and Mask RCNN and Transfer Learning technique were used to classify the images. The contribution of the paper is given by the application of these image classification algorithms in the non-formal education process. There was a correlation between the feelings detected, frequency of occurrence and the end result of the game in order to improve the educational process to optimize the automatic feedback needed by the teacher to adjust the instructional-educational process, benefiting from the support of an automatic assistant. The results were illustrated in graphs regarding the evolution of the behavioral states/the flow of feelings of the children during lessons throughout the experiment. © 2020, National Defence University - Carol I Printing House. All rights reserved.",Automatic feedback; Deep learning; Logical games; Nonformal education; Sentiment analysis,"572, 653",,eLearning and Software for Education Conference,Conference Paper,Scopus
713,,An empirical study on issue knowledge transfer from python to R for machine learning software,"Huang W., Ji Z., Li Y.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090510892&doi=10.18293%2fSEKE2020-102&partnerID=40&md5=bdb533b93344f9d6a04e093ad386e3b1,10.18293/SEKE2020-102,"Background: With the blowout of programming languages, developers employ different languages to solve similar problems (e.g., to implement machine learning algorithms) separately and frequently, which gives rise to knowledge transfer across different language development. Since GitHub provides an issue tracking system for developers and users to follow with issues, knowledge about how to deal with issues is a main part of available knowledge on GitHub. Such issue knowledge could be directly transferred to help developers handle new issues on current projects from similar projects in different languages. Aims: Inspired by a large amount of developed and developing machine learning software written in Python and R on GitHub, we aim to discover how much issue knowledge can be transferred from Python projects to R projects. Method: We investigate totally 1161 issues from 15 popular machine learning projects in R and 7496 issues from Scikit-Learn in Python on GitHub. After computing the text similarity between issues from R and Python projects, we match top 5 similar Scikit-Learn issues for each R issue and manually judge 1161×5 issue-pairs to label and group them. Results: We observe that a) 13% (149/1161) of R issues can refer to related Python issues; b) 47% (71/149) of related R issues can be linked to Python issues by the text mining technique BM25 at the very early stage; c) 83% (124/149) of related Python issues support code and description about the similar machines learning problems; d) reference knowledge is considered as the most useful knowledge from Python issues. Conclusion: We put forward the following suggestions: a) referring to the corresponding cross languages issues is an efficient way for developers, especially there is the lack of related information in current language; b) the text mining technique BM25 is helpful for developers to start earlier for searching similar issues cross languages. © 2020 Knowledge Systems Institute Graduate School. All rights reserved.",,"168, 173",,"Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE",Conference Paper,Scopus
714,,A semantic convolutional auto-encoder model for software defect prediction,"Wang Z., Lu L.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090510868&doi=10.18293%2fSEKE2020-036&partnerID=40&md5=62fc36c1399e94f0cdd3a9e619db695f,10.18293/SEKE2020-036,"In traditional software defect prediction, previous researches mainly focused on manually designing complex features and building classifiers based on features. However, such traditional features often fail in capturing rich syntactic and semantic information in programs. Thus, an effecient prediction model is unable to be constructed in some cases. In this study, a framework called semantic convolutional auto-encoder (SCAE) is proposed to effectivelly extract semantic features from source code. Token vectors are extracted from Abstract Syntax Trees (ASTs) of programs and then encoded as numerical vectors. Convolutional autoencoder (CAE) can learn semantic features from the numerical vectors by decreasing the reconstruction error between input and output. After that, the CAE-based features are utilized to train a classifier. To enhance the transferability of CAE-based features for different projects, we perform domain adaptation by matching kernel embedding of layer representations across domains in reproducing kernel Hilbert spaces. Extensive experimental results verify that the SCAE yields referential methods on ten open-source projects. © 2020 Knowledge Systems Institute Graduate School. All rights reserved.",Convolutional auto-encoder; Semantic feature learning; Software defect prediction; Transfer learning,"323, 328",,"Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE",Conference Paper,Scopus
715,,SLK-NER: Exploiting second-order lexicon knowledge for Chinese NER,"Hu D., Wei L.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090510319&doi=10.18293%2fSEKE2020-153&partnerID=40&md5=08a84b4157efcf3dc1a3f96072a6e752,10.18293/SEKE2020-153,"Although character-based models using lexicon have achieved promising results for Chinese named entity recognition (NER) task, some lexical words would introduce erroneous information due to wrongly matched words. Existing researches proposed many strategies to integrate lexicon knowledge. However, they performed with simple first-order lexicon knowledge, which provided insufficient word information and still faced the challenge of matched word boundary conflicts; or explored the lexicon knowledge with graph where higher-order information introducing negative words may disturb the identification. To alleviate the above limitations, we present new insight into second-order lexicon knowledge (SLK) of each character in the sentence to provide more lexical word information including semantic and word boundary features. Based on these, we propose a SLK-based model with a novel strategy to integrate the above lexicon knowledge. The proposed model can exploit more discernible lexical words information with the help of global context. Experimental results on three public datasets demonstrate the validity of SLK. The proposed model achieves more excellent performance than the state-of-the-art comparison methods. © 2020 Knowledge Systems Institute Graduate School. All rights reserved.",Attention mechanism; Chinese named entity recognition; Lexicon knowledge,"413, 417",,"Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE",Conference Paper,Scopus
716,,A hands-on tutorial on deep learning for object and pattern recognition,"Zulkernine F., Gasmallah M., Isah H., Lam J., Mahfuz S., Khan S.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087416922&partnerID=40&md5=47341fc06b6a9a94ca761a69054e719a,,"Deep learning is a machine learning technique that is inspired by human brain. It enables information processing in multiple hierarchical layers to understand representations and features from raw data. Deep learning architectures have been applied to various fields including Computer Vision, Object and Speech Recognition, Natural Language Processing (NLP), and the Internet of Things (IoT). Deep learning methods in recent years have shown excellent performance in terms of accuracy on many challenging and complex learning tasks. This workshop was designed to provide an introduction and a hands-on tutorial on deep learning to researchers and industry practitioners interested in integrating the power of deep learning in their research or business applications. © 2019 Copyright held by the owner/author(s).",Deep learning; Google Colaboratory; IoT; NLP; Object detection; Python; Tensorflow; Tutorial,"386, 387",,CASCON 2019 Proceedings - Conference of the Centre for Advanced Studies on Collaborative Research - Proceedings of the 29th Annual International Conference on Computer Science and Software Engineering,Conference Paper,Scopus
717,,Classification of histopathological biopsy images using ensemble of deep learning networks,"Kassani S.H., Kassani P.H., Wesolowski M.J., Schneider K.A., Deters R.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087411560&partnerID=40&md5=357c6e3bf9eea8e345e2e74a3392780d,,"Breast cancer is one of the leading causes of death across the world in women. Early diagnosis of this type of cancer is critical for treatment and patient care. Computer-aided detection (CAD) systems using convolutional neural networks (CNN) could assist in the classification of abnormalities. In this study, we proposed an ensemble deep learning-based approach for automatic binary classification of breast histology images. The proposed ensemble model adapts three pre-trained CNNs, namely VGG19, MobileNet, and DenseNet. The ensemble model is used for the feature representation and extraction steps. The extracted features are then fed into a multi-layer perceptron classifier to carry out the classification task. Various pre-processing and CNN tuning techniques such as stain-normalization, data augmentation, hyperparameter tuning, and fine-tuning are used to train the model. The proposed method is validated on four publicly available benchmark datasets, i.e., ICIAR, BreakHis, PatchCamelyon, and Bioimaging. The proposed multi-model ensemble method obtains better predictions than single classifiers and machine learning algorithms with accuracies of 98.13%, 95.00%, 94.64% and 83.10% for BreakHis, ICIAR, PatchCamelyon and Bioimaging datasets, respectively. © 2019 Copyright held by the owner/author(s).",Computer-aided diagnosis; Deep learning; Feature extraction; Multimodel ensemble; Transfer learning,"92, 99",,CASCON 2019 Proceedings - Conference of the Centre for Advanced Studies on Collaborative Research - Proceedings of the 29th Annual International Conference on Computer Science and Software Engineering,Conference Paper,Scopus
718,,IMNRFixer: A hybrid approach to alleviate class-imbalance problem for predicting the fixability of Non-Reproducible bugs,"Goyal A., Sardana N.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087151662&doi=10.1002%2fsmr.2290&partnerID=40&md5=c218702f45a4f3eaad8f6179e6d2fce1,10.1002/smr.2290,"Software maintenance is an important phase in the software development life cycle. Software projects maintain bug repositories to gather, organize, and keep track of bug reports. These bug reports are resolved by numerous software developers. Whenever the reported bug does not get resolved by the assigned developer, he marks the resolution of bug report as Non-Reproducible (NR). When NR bugs are reconsidered, few of them get resolved, and their resolution changes from NR to fix (NRF). The main aim of this paper is to predict these fixable NRF bug reports. A major challenge in predicting NRF bugs from NR bugs is that only a small portion of NR bugs get fixed, i.e., class-imbalance problem. For example, NRF bugs account for only 8.64%, 4.73 %, 4.56%, and 1.06% in NetBeans, Eclipse, Open Office, and Mozilla Firefox projects respectively. In this paper, we work on improving the classification performance on these imbalanced datasets. We propose IMNRFixer, a novel and hybrid NRF prediction tool. IMNRFixer uses three different techniques to combat class-imbalance problem: undersampling, oversampling, and ensemble models. We evaluate the performance of IMNRFixer models on four large and open-source projects of Bugzilla repository. Our results show that IMNRFixer outperforms conventional machine learning techniques. IMNRFixer achieves performance up to 71.7%, 93.1%, 91.7%, and 96.5% while predicting the minority class (NRF) for NetBeans, Eclipse, Open Office, and Mozilla Firefox projects, respectively. © 2020 John Wiley & Sons, Ltd.",bug fixing; bug report; class-imbalance; classification; ensemble techniques; machine learning; mining software repositories; Non-Reproducible bugs; prediction tool; sampling,,,Journal of Software: Evolution and Process,Conference Paper,Scopus
719,,Automated Expansion of Abbreviations Based on Semantic Relation and Transfer Expansion,"Jiang Y., Liu H., Jin J., Zhang L.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085757408&doi=10.1109%2fTSE.2020.2995736&partnerID=40&md5=7899d505ff4a2b58d08af2468a4e65b1,10.1109/TSE.2020.2995736,"Although the negative impact of abbreviations in source code is well-recognized, abbreviations are common for various reasons. To this end, a number of approaches have been proposed to expand abbreviations in identifiers. However, such approaches are either inaccurate or confined to specific identifiers. To this end, in this paper, we propose a generic and accurate approach to expand identifier abbreviations by leveraging both semantic relation and transfer expansion. One of the key insight of the approach is that abbreviations in the name of software entity e have great chance to find their full terms in names of software entities that are semantically related to e. Consequently, the proposed approach builds a knowledge graph to represent such entities and their relationships with e, and searches the graph for full terms. Another key insight is that literally identical abbreviations within the same application are likely (but not necessary) to have identical expansions, and thus the semantics-based expansion in one place may be transferred to other places. To investigate when abbreviation expansion could be transferred safely, we conduct a case study on three open-source applications. The results suggest that a significant part (75%) of expansions could be transferred among lexically identical abbreviations within the same application. However, the risk of transfer varies according to various factors, e.g., length of abbreviations, physical distance between abbreviations, and semantic relations between abbreviations. Based on these findings, we design nine heuristics for transfer expansion, and propose a learning based approach to prioritize both transfer heuristics and semantic-based expansion heuristics. Evaluation results on nine open-source applications suggest that the proposed approach significantly improves the state of the art, improving recall from 29% to 89% and precision from 39% to 92%. IEEE",Abbreviation; Context; Dictionaries; Encyclopedias; Expansion; Internet; Manuals; Open source software; Quality; Semantics; Transfer,,,IEEE Transactions on Software Engineering,Article,Scopus
720,,Survey on Generating Adversarial Examples [对抗样本生成技术综述],"Pan W.-W., Wang X.-Y., Song M.-L., Chen C.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083078871&doi=10.13328%2fj.cnki.jos.005884&partnerID=40&md5=08e40024f7bceb056915f9fdc57d1f53,10.13328/j.cnki.jos.005884,"Recently, deep learning has been widely used in image classification and image recognition, which has achieved satisfactory results and has become the important part of AI applications. During the continuous exploration of the accuracy of models, recent studies have proposed the concept of ""adversarial examples"". By adding small perturbations to the original samples, it can greatly reduce the accuracy of the original classifier and achieve the purpose of anti-deep learning, which provides new ideas for deep learning attackers, and also puts forward new requirements for defenders. On the basis of introducing the origin and principle of generating adversarial examples, this paper summarizes the research and papers on generating adversarial examples in recent years, and divides these algorithms into two categories: entire pixel perturbation and partial pixel perturbation. Then, the secondary classification criteria (targeted and not targeted, black-box test and white-box test, visible and invisible) were used for secondary classification. At the same time, the MNIST data set is used to validate the methods, which proves the advantages and disadvantages of the various methods. Finally, this paper summarizes the challenges of generating adversarial examples and the direction of their development, and also discusses the future of them. © Copyright 2020, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Adversarial examples; Black-box test; Deep learning; No targeted; Perturbation; Targeted,"67, 81",,Ruan Jian Xue Bao/Journal of Software,Review,Scopus
721,,SIEVE: Helping developers sift wheat from chaff via cross-platform analysis,"Sulistya A., Prana G.A.A., Sharma A., Lo D., Treude C.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074344807&doi=10.1007%2fs10664-019-09775-w&partnerID=40&md5=115b769527c3aa81135d467bf82d992a,10.1007/s10664-019-09775-w,"Software developers have benefited from various sources of knowledge such as forums, question-and-answer sites, and social media platforms to help them in various tasks. Extracting software-related knowledge from different platforms involves many challenges. In this paper, we propose an approach to improve the effectiveness of knowledge extraction tasks by performing cross-platform analysis. Our approach is based on transfer representation learning and word embedding, leveraging information extracted from a source platform which contains rich domain-related content. The information extracted is then used to solve tasks in another platform (considered as target platform) with less domain-related content. We first build a word embedding model as a representation learned from the source platform, and use the model to improve the performance of knowledge extraction tasks in the target platform. We experiment with Software Engineering Stack Exchange and Stack Overflow as source platforms, and two different target platforms, i.e., Twitter and YouTube. Our experiments show that our approach improves performance of existing work for the tasks of identifying software-related tweets and helpful YouTube comments. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.",Software engineering; Transfer representation learning; Word embedding,"996, 1030",,Empirical Software Engineering,Article,Scopus
722,,Two-step deep neural network for segmentation of deep white matter hyperintensities in migraineurs,"Hong J., Park B.-Y., Lee M.J., Chung C.-S., Cha J., Park H.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073763385&doi=10.1016%2fj.cmpb.2019.105065&partnerID=40&md5=79813ca8f8219c4c2c6f1c6275690943,10.1016/j.cmpb.2019.105065,"Background and Objective: Patients with migraine show an increased presence of white matter hyperintensities (WMHs), especially deep WMHs. Segmentation of small, deep WMHs is a critical issue in managing migraine care. Here, we aim to develop a novel approach to segmenting deep WMHs using deep neural networks based on the U-Net. Methods: 148 non-elderly subjects with migraine were recruited for this study. Our model consists of two networks: the first identifies potential deep WMH candidates, and the second reduces the false positives within the candidates. The first network for initial segmentation includes four down-sampling layers and four up-sampling layers to sort the candidates. The second network for false positive reduction uses a smaller field-of-view and depth than the first network to increase utilization of local information. Results: Our proposed model segments deep WMHs with a high true positive rate of 0.88, a low false discovery rate of 0.13, and F1 score of 0.88 tested with ten-fold cross-validation. Our model was automatic and performed better than existing models based on conventional machine learning. Conclusion: We developed a novel segmentation framework tailored for deep WMHs using U-Net. Our algorithm is open-access to promote future research in quantifying deep WMHs and might contribute to the effective management of WMHs in migraineurs. © 2019",Deep neural network; Deep white matter hyperintensity; Migraine; Segmentation,,,Computer Methods and Programs in Biomedicine,Article,Scopus
723,,Transfer learning in neural networks: An experience report,"Shtern M., Ejaz R., Tzerpos V.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073229496&partnerID=40&md5=01103451558eb0b680fdf3c129aae746,,"Perhaps the most important characteristic of deep neural networks is their ability to discover and extract the necessary features for a particular machine learning task from a raw input representation. This requires a significant time commitment, both in terms of assembling the training dataset, and training the neural network. Reusing the knowledge inherent in a trained neural network for a machine learning task in a related domain can provide significant improvements in terms of the time required to complete the task. In this paper, we present our experience with such a transfer learning situation. We reuse a neural network that was trained on a real world image dataset, for the task of classifying music in terms of genre, instrumentation, composer etc. (audio files are converted to spectrograms for this purpose). Even though the image and music domains are not directly related, our experiments show that features extracted to recognize images allow for high accuracy in many music classification tasks. © 2017 Copyright held by the owner/author(s).",Deep learning; Music classification; Transfer learning,"201, 210",,"Proceedings of the 27th Annual International Conference on Computer Science and Software Engineering, CASCON 2017",Conference Paper,Scopus
724,,Run-time evaluation of architectures: A case study of diversification in IoT,"Sobhy D., Minku L., Bahsoon R., Chen T., Kazman R.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073076930&doi=10.1016%2fj.jss.2019.110428&partnerID=40&md5=41b170dc2ef251d4fcf5459237c1af79,10.1016/j.jss.2019.110428,"Run-time properties of modern software system environments, such as Internet of Things (IoT), are a challenge for existing software architecture evaluation methods. Such systems are largely data-driven, characterized by their dynamism, unpredictability in operation, hyper-connectivity, and scale. Properties, such as performance, delayed delivery, and scalability, are acknowledged to pose great risk and are difficult to evaluate at design-time. Run-time evaluation could potentially be used to complement design-time evaluation, enabling significant deviations from the expected performance values to be captured. However, there are no systematic software architecture evaluation methods that intertwine and interleave design-time and run-time evaluation. This paper addresses this gap by proposing a novel run-time architecture evaluation method suited for systems that exhibit uncertainty and dynamism in their operation. Our method uses machine learning and cost-benefit analysis at run-time to continuously profile the architecture decisions made, to assess their added value. We demonstrate the applicability and effectiveness of this approach in the context of an IoT system architecture, where some architecture design decisions were diversified to meet Quality of Service (QoS) requirements. Our approach provides run-time assessment for these decisions which can inform deployment, refinement, and/or phasing-out decisions. © 2019",Design diversity; Internet of things; IoT; Run-time architecture evaluation; Runtime architecture evaluation; Software architectures for dynamic environments,,,Journal of Systems and Software,Article,Scopus
725,,A superpixel-driven deep learning approach for the analysis of dermatological wounds,"Blanco G., Traina A.J.M., Traina Jr. C., Azevedo-Marques P.M., Jorge A.E.S., de Oliveira D., Bedo M.V.N.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072544143&doi=10.1016%2fj.cmpb.2019.105079&partnerID=40&md5=b2971d686d60540a58ae9ff9afbd5c43,10.1016/j.cmpb.2019.105079,"Background: The image-based identification of distinct tissues within dermatological wounds enhances patients’ care since it requires no intrusive evaluations. This manuscript presents an approach, we named QTDU, that combines deep learning models with superpixel-driven segmentation methods for assessing the quality of tissues from dermatological ulcers. Method: QTDU consists of a three-stage pipeline for the obtaining of ulcer segmentation, tissues’ labeling, and wounded area quantification. We set up our approach by using a real and annotated set of dermatological ulcers for training several deep learning models to the identification of ulcered superpixels. Results: Empirical evaluations on 179,572 superpixels divided into four classes showed QTDU accurately spot wounded tissues (AUC = 0.986, sensitivity = 0.97, and specificity = 0.974) and outperformed machine-learning approaches in up to 8.2% regarding F1-Score through fine-tuning of a ResNet-based model. Last, but not least, experimental evaluations also showed QTDU correctly quantified wounded tissue areas within a 0.089 Mean Absolute Error ratio. Conclusions: Results indicate QTDU effectiveness for both tissue segmentation and wounded area quantification tasks. When compared to existing machine-learning approaches, the combination of superpixels and deep learning models outperformed the competitors within strong significant levels. © 2019 Elsevier B.V.",Deep learning; Dermatological wounds; Superpixel segmentation; Tissue recognition,,,Computer Methods and Programs in Biomedicine,Article,Scopus
726,,Application of deep canonically correlated sparse autoencoder for the classification of schizophrenia,"Li G., Han D., Wang C., Hu W., Calhoun V.D., Wang Y.-P.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072091523&doi=10.1016%2fj.cmpb.2019.105073&partnerID=40&md5=0db919d21bb916b9fb9d559b6d1e6440,10.1016/j.cmpb.2019.105073,"Background and objective: Imaging genetics has been widely used to help diagnose and treat mental illness, e.g., schizophrenia, by combining magnetic resonance imaging of the brain and genomic information for comprehensive and systematic analysis. As a result, utilizing the correlation between magnetic resonance imaging of the brain and genomic information is becoming an important challenge. Methods: In this paper, the joint analysis of single nucleotide polymorphisms and functional magnetic resonance imaging is conducted for comprehensive study of schizophrenia. We developed a deep canonically correlated sparse autoencoder to classify schizophrenia patients from healthy controls, which can address the limitation of many existing methods such as canonical correlation analysis, deep canonical correlation analysis and sparse autoencoder. Results: The proposed deep canonically correlated sparse autoencoder can not only use complex nonlinear transformation and dimension reduction, but also achieve more accurate classifications. Our experiments showed the proposed method achieved an accuracy of 95.65% for SNP data sets and an accuracy of 80.53% for fMRI data sets. Conclusions: Experiments demonstrated higher accuracy of using the proposed method over other conventional models when classifying schizophrenia patients and healthy controls. © 2019",Canonical correlation analysis; Deep canonically correlated sparse autoencoder; Imaging-genetic associations; Schizophrenia classification; Sparse autoencoder,,,Computer Methods and Programs in Biomedicine,Article,Scopus
727,,Improving change prediction models with code smell-related information,"Catolino G., Palomba F., Fontana F.A., De Lucia A., Zaidman A., Ferrucci F.",2020,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070076631&doi=10.1007%2fs10664-019-09739-0&partnerID=40&md5=7d5a5da57ef7db65063b149302c323d2,10.1007/s10664-019-09739-0,"Code smells are sub-optimal implementation choices applied by developers that have the effect of negatively impacting, among others, the change-proneness of the affected classes. Based on this consideration, in this paper we conjecture that code smell-related information can be effectively exploited to improve the performance of change prediction models, i.e., models having the goal of indicating which classes are more likely to change in the future. We exploit the so-called intensity index—a previously defined metric that captures the severity of a code smell—and evaluate its contribution when added as additional feature in the context of three state of the art change prediction models based on product, process, and developer-based features. We also compare the performance achieved by the proposed model with a model based on previously defined antipattern metrics, a set of indicators computed considering the history of code smells in files. Our results report that (i) the prediction performance of the intensity-including models is statistically better than the baselines and, (ii) the intensity is a better predictor than antipattern metrics. We observed some orthogonality between the set of change-prone and non-change-prone classes correctly classified by the models relying on intensity and antipattern metrics: for this reason, we also devise and evaluate a smell-aware combined change prediction model including product, process, developer-based, and smell-related features. We show that the F-Measure of this model is notably higher than other models. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.",Change prediction; Code smells; Empirical study,"49, 95",,Empirical Software Engineering,Article,Scopus
728,,FPGA-Accelerated Machine Learning Inference as a Service for Particle Physics Computing,"Duarte J., Harris P., Hauck S., Holzman B., Hsu S.-C., Jindariani S., Khan S., Kreis B., Lee B., Liu M., Lončar V., Ngadiuba J., Pedro K., Perez B., Pierini M., Rankin D., Tran N., Trahms M., Tsaris A., Versteeg C., Way T.W., Werran D., Wu Z.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084216418&doi=10.1007%2fs41781-019-0027-2&partnerID=40&md5=769d7fde8fcff042768d7bf5a34bbc4f,10.1007/s41781-019-0027-2,"Large-scale particle physics experiments face challenging demands for high-throughput computing resources both now and in the future. New heterogeneous computing paradigms on dedicated hardware with increased parallelization, such as Field Programmable Gate Arrays (FPGAs), offer exciting solutions with large potential gains. The growing applications of machine learning algorithms in particle physics for simulation, reconstruction, and analysis are naturally deployed on such platforms. We demonstrate that the acceleration of machine learning inference as a web service represents a heterogeneous computing solution for particle physics experiments that potentially requires minimal modification to the current computing model. As examples, we retrain the ResNet-50 convolutional neural network to demonstrate state-of-the-art performance for top quark jet tagging at the LHC and apply a ResNet-50 model with transfer learning for neutrino event classification. Using Project Brainwave by Microsoft to accelerate the ResNet-50 image classification model, we achieve average inference times of 60 (10) ms with our experimental physics software framework using Brainwave as a cloud (edge or on-premises) service, representing an improvement by a factor of approximately 30 (175) in model inference latency over traditional CPU inference in current experimental hardware. A single FPGA service accessed by many CPUs achieves a throughput of 600–700 inferences per second using an image batch of one, comparable to large batch-size GPU throughput and significantly better than small batch-size GPU throughput. Deployed as an edge or cloud service for the particle physics computing model, coprocessor accelerators can have a higher duty cycle and are potentially much more cost-effective. © 2019, Springer Nature Switzerland AG.",FPGA; Heterogeneous computing; Machine learning; Particle physics,,,Computing and Software for Big Science,Article,Scopus
729,,Fuzzy Classification Method for Small- and Medium-scale Datasets [一种面向中小规模数据集的模糊分类方法],"Zhou T., Deng Z.-H., Jiang Y.-Z., Wang S.-T.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082550207&doi=10.13328%2fj.cnki.jos.005590&partnerID=40&md5=6515c562d0f1644777160f88a3d80268,10.13328/j.cnki.jos.005590,"Although Takagi-Sugeno-Kang (TSK) is widely used in practically every profession, how to enhance its classification accuracy and interpretability is still a research focus. In this study, a deep TSK fuzzy classifier is proposed. This classifier (i.e., RCC- DTSK-C) can randomly select features and combine features and own triplely concise interpretability for fuzzy rules. There are several other varieties of RCC-DTSK-C such as reasonable structure for rule representation, namely, (1) the proposed RCC-DTSK-C consists of many base-training units and each base-training unit can be trained independently. According to the principle of stacked generalization, the input of the next base-training unit consists of the training set and random result obtained from random projections about prediction results of current base-training unit. (2) In RCC-DTSK-C, the hidden layer of each base-training unit is represented by triplely concise interpretable fuzzy rules which are in the sense of randomly selected features. These features are selected by dividing into the not-fixed several fuzzy partitions and randomly combining rules and keeping the same input space in every base-training unit. (3) The source data set is mapped into each of the independent base-training units as the same input space, which effectively ensures that all the features of the source data are preserved in each separate training unit. The extensive experimental results show RCC-DTSK-C can achieve the enhanced classification performance and triplely concise interpretability for fuzzy rules. © Copyright 2019, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Deep learning; Feature combination; Interpretability; Random partition; Stacked structure; Takagi-Sugeno-Kang (TSK),"3637, 3650",,Ruan Jian Xue Bao/Journal of Software,Article,Scopus
730,,Supporting High-Performance and High-Throughput Computing for Experimental Science,"Huerta E.A., Haas R., Jha S., Neubauer M., Katz D.S.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078229813&doi=10.1007%2fs41781-019-0022-7&partnerID=40&md5=4dc75d69804af966f80d66cc5b000cda,10.1007/s41781-019-0022-7,"The advent of experimental science facilities—instruments and observatories, such as the Large Hadron Collider, the Laser Interferometer Gravitational Wave Observatory, and the upcoming Large Synoptic Survey Telescope —has brought about challenging, large-scale computational and data processing requirements. Traditionally, the computing infrastructure to support these facility’s requirements were organized into separate infrastructure that supported their high-throughput needs and those that supported their high-performance computing needs. We argue that to enable and accelerate scientific discovery at the scale and sophistication that is now needed, this separation between high-performance computing and high-throughput computing must be bridged and an integrated, unified infrastructure provided. In this paper, we discuss several case studies where such infrastructure has been implemented. These case studies span different science domains, software systems, and application requirements as well as levels of sustainability. A further aim of this paper is to provide a basis to determine the common characteristics and requirements of such infrastructure, as well as to begin a discussion of how best to support the computing requirements of existing and future experimental science facilities. © 2019, Springer Nature Switzerland AG.",ATLAS; Blue Waters; CMS; Containers; HPC; HTC; LIGO; OSG; Titan,,,Computing and Software for Big Science,Review,Scopus
731,,Systematic literature review of preprocessing techniques for imbalanced data,"Felix E.A., Lee S.P.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075836982&doi=10.1049%2fiet-sen.2018.5193&partnerID=40&md5=d05b5aeff8bd1ba1e03c4233223ceab5,10.1049/iet-sen.2018.5193,"Data preprocessing remains an important step in machine learning studies. This is because proper preprocessing of imbalanced data can enable researchers to reduce defects as much as possible, which, in turn, may lead to the elimination of defects in existing data sets. Despite the remarkable achievements that have been accomplished in machine learning studies, systematic literature reviews of imbalanced data preprocessing techniques are lacking. Consequently, there are a limited number of systematic literature review studies on imbalanced data preprocessing. In this study, the authors assess the existing literature to identify the key issues related to data quality and handling and to provide a convenient collection of the techniques used to address these issues when performing data preprocessing. They applied a systematic literature review method involving a manual search to select articles published from January 2010 to September 2018 for review. The qualities of the existing studies were assessed using certain quality assessment criteria. Of the 118 relevant studies found, only 2% were identified as having been conducted following systematic literature review guidelines. This study, therefore, calls for more systematic literature review studies on data preprocessing to improve the quality of the data applied in machine learning studies. © The Institution of Engineering and Technology 2019",,"479, 496",,IET Software,Review,Scopus
732,,LDFR: Learning deep feature representation for software defect prediction,"Xu Z., Li S., Xu J., Liu J., Luo X., Zhang Y., Zhang T., Keung J., Tang Y.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072198290&doi=10.1016%2fj.jss.2019.110402&partnerID=40&md5=abda901529ce30099075791267ebd65a,10.1016/j.jss.2019.110402,"Software Defect Prediction (SDP) aims to detect defective modules to enable the reasonable allocation of testing resources, which is an economically critical activity in software quality assurance. Learning effective feature representation and addressing class imbalance are two main challenges in SDP. Ideally, the more discriminative the features learned from the modules and the better the rescue performed on the imbalance issue, the more effective it should be in detecting defective modules. In this study, to solve these two challenges, we propose a novel framework named LDFR by Learning Deep Feature Representation from the defect data for SDP. Specifically, we use a deep neural network with a new hybrid loss function that consists of a triplet loss to learn a more discriminative feature representation of the defect data and a weighted cross-entropy loss to remedy the imbalance issue. To evaluate the effectiveness of the proposed LDFR framework, we conduct extensive experiments on a benchmark dataset with 27 defect data (each with three types of features), using three traditional and three effort-aware indicators. Overall, the experimental results demonstrate the superiority of our LDFR framework in detecting defective modules when compared with 27 baseline methods, except in terms of the indicator of Precision. © 2019",Deep feature representation; Deep neural network; Software defect prediction; Triplet loss; Weighted cross-entropy loss,,,Journal of Systems and Software,Article,Scopus
733,,Multitask defect prediction,"Ni C., Chen X., Xia X., Gu Q., Zhao Y.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071777875&doi=10.1002%2fsmr.2203&partnerID=40&md5=d35f88df732c6388924b13ceafe7b8d0,10.1002/smr.2203,"Within-project defect prediction assumes that we have sufficient labeled data from the same project, while cross-project defect prediction assumes that we have plenty of labeled data from source projects. However, in practice, we might only have limited labeled data from both the source and target projects in some scenarios. In this paper, we want to apply multitask learning to investigate such a new scenario. To our best knowledge, this problem (ie, both the source project and the target project have limited labeled data) has not been thoroughly investigated, and we are the first to propose a novel multitask defect prediction approach mask. mask consists of a differential evolution optimization phase and a multitask learning phase. The former phase aims to find optimal weights for shared and nonshared information in related projects (ie, the target project and its related source projects), while the latter phase builds prediction models for each project simultaneously. To verify the effectiveness of mask, we perform experimental studies on 18 real-world software projects and compare our approach with four state-of-the-art baseline approaches: single-task learning (STL), simple combined learning (SCL), Peters filter, and Burak filter. Experimental results show that mask can achieve F1 of 0.397 and AUC of 0.608 on average with a few labeled data (ie, 10% of data). Across the 18 projects, mask can outperform baseline methods significantly in terms of F1 and AUC. Therefore, by utilizing the relatedness among multiple projects, mask can perform significantly better than the state-of-the-art methods. The results confirm that mask is promising for software defect prediction when the source and target projects both have limited training data. © 2019 John Wiley & Sons, Ltd.",differential evolution; empirical studies; multitask learning; software defect prediction,,,Journal of Software: Evolution and Process,Article,Scopus
734,,DEEPLINK: Recovering issue-commit links based on deep learning,"Ruan H., Chen B., Peng X., Zhao W.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071724162&doi=10.1016%2fj.jss.2019.110406&partnerID=40&md5=bfeed1062b2e4c3731bc17ea07aa9a2d,10.1016/j.jss.2019.110406,"The links between issues in an issue-tracking system and commits resolving the issues in a version control system are important for a variety of software engineering tasks (e.g., bug prediction, bug localization and feature location). However, only a small portion of such links are established by manually including issue identifiers in commit logs, leaving a large portion of them lost in the evolution history. To recover issue-commit links, heuristic-based and learning-based techniques leverage the metadata and text/code similarity in issues and commits; however, they fail to capture the embedded semantics in issues and commits and the hidden semantic correlations between issues and commits. As a result, this semantic gap inhibits the accuracy of link recovery. To bridge this gap, we propose a semantically-enhanced link recovery approach, named DEEPLINK, which is built on top of deep learning techniques. Specifically, we develop a neural network architecture, using word embedding and recurrent neural network, to learn the semantic representation of natural language descriptions and code in issues and commits as well as the semantic correlation between issues and commits. In experiments, to quantify the prevalence of missing issue-commit links, we analyzed 1078 highly-starred GitHub Java projects (i.e., 583,795 closed issues) and found that only 42.2% of issues were linked to corresponding commits. To evaluate the effectiveness of DEEPLINK, we compared DEEPLINK with a state-of-the-art link recovery approach FRLink using ten GitHub Java projects and demonstrated that DEEPLINK can outperform FRLink in terms of F-measure. © 2019 Elsevier Inc.",Deep learning; Issue-commit links; Semantic understanding,,,Journal of Systems and Software,Article,Scopus
735,,Improving the Performance of CBIR Using XGBoost Classifier with Deep CNN-Based Feature Extraction,"Pardede J., Sitohang B., Akbar S., Khodra M.L.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085582264&doi=10.1109%2fICoDSE48700.2019.9092754&partnerID=40&md5=42902a65ea9560546e95baa300993466,10.1109/ICoDSE48700.2019.9092754,"The main challenge faced by CBIR systems is the semantic gap, namely the existence of semantic differences between low-level pixel images captured by machines and high-level semantics perceived by humans. This is caused by the CBIR system which is very dependent on the feature extraction used. The success of deep learning techniques, especially Convolutional Neural Networks (CNN) in solving the problem of computer vision applications has inspired researchers to overcome the problem of the semantic gap. Until now, what is known by researchers, Deep CNN research for CBIR uses softmax and SVM classifiers. However, the XGBoost classifier achieves extraordinary performance even though compared to SVM. By applying the advantages of Deep CNN techniques and XGBoost classifier, the Deep CNN model is proposed as a feature extractor and XGBoost to form a classification model replacing the softmax and SVM classifier. From the proposed Deep CNN model, two Fully Connected (FC) layers (FC1 and FC2) are taken as feature representations. In this study, Features Vector (FV) taken from the Deep CNN model that have been produced are used as extractor features (FV.FC1 and FV.FC2). The performance of the Deep CNN for CBIR tasks generated using softmax, SVM, and XGBoost classifier observed. The performance evaluated is accuracy, precision, recall, and f1-score. Based on experimental results on the Wang, GHIM-10k, and Fruit-360 dataset, XGBoost classifier can increase accuracy, precision, recall, and f1-score in all datasets. The best performance of the CBIR system is using XGBoost classifier with the best feature extractor taken from FV.FC2. Whereas the FV.FC1 feature extractor does not produce significant performance when compared to the resources needed. © 2019 IEEE.",CBIR; Deep CNN; feature extractor; performance; XGBoost,,,"Proceedings of 2019 International Conference on Data and Software Engineering, ICoDSE 2019",Conference Paper,Scopus
736,,A Novel Application based on Spectrogram and Convolutional Neural Network for ECG Classification,"Diker A., Comert Z., Avci E., Togacar M., Ergen B.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079245453&doi=10.1109%2fUBMYK48245.2019.8965506&partnerID=40&md5=53edca9afd805c901e34665445ab23d5,10.1109/UBMYK48245.2019.8965506,"Electrocardiogram (ECG) is a biomedical signal which represents the electrical activity of the human heart. Various cardiac diseases have been detected using the outputs of ECG devices. Recently, advances in signal processing techniques bring out a new horizon for processing the ECG signals. In this scope, a novel application based on the spectrogram, which is a graphical representation of time-frequency information of the signal, and the convolutional neural network (CNN) is proposed so as to distinguish ECG signals. To this aim, a publicly available data set in Physionet was utilized. Firstly, the spectrograms of each signal were obtained. Then, these colorful spectrogram images were applied as the input to CNNs that are AlexNet, VGG-16, and ResNet-18. The transfer learning and fine-tuning approach were used for training and validation of the models. As a result, the most efficient results were provided by AlexNet with an accuracy of 83.82%. The experimental results of this study show that the proposed model ensures promising results for the ECG signal classification. © 2019 IEEE.",Biomedical signal processing; classification; convolutional neural network; decision-making support system,,,"1st International Informatics and Software Engineering Conference: Innovative Technologies for Digital Transformation, IISEC 2019 - Proceedings",Conference Paper,Scopus
737,,Vibration Signal Processing Based Bearing Defect Diagnosis with Transfer Learning,"Tastimur C., Karakose M., Akin E.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079240121&doi=10.1109%2fUBMYK48245.2019.8965451&partnerID=40&md5=ffeb0dd0cc998106571959ed203dede6,10.1109/UBMYK48245.2019.8965451,"It is very important to diagnose the fault condition of the bearing machines in order to make the machine run healthier. There are many successful studies in the detection of failures in the bearing machine made by conventional machine learning methods. However, these studies produce successful results in cases where the machines operate under the same condition and feature space is the same. Therefore, a deep learning-based diagnostic has been proposed for changing machine operating conditions. In the scope of our study, Keras and Tensorflow libraries, CNN network from scratch, VGG16 model and VGG19 deep learning models have been used for the classification of vibration images. Freeze the weights for transfer learning and remove the last fully connected layer to update the network in a problem-specific manner. The number of iterations and batch size has been determined by experimental studies. In this study, four faulty conditions have been successfully classified. While the accuracy rate of CNN network from scratch is 25%, the accuracy rate obtained by the VGG16 transfer learning method is 93% and the loss rate is 0.17% and the accuracy rate obtained by the VGG19 transfer learning method is 95% and the loss rate is 0.13%. © 2019 IEEE.",Bearing; Deep learning; Fault diagnosis; Transfer Learning; Vibration signal,,,"1st International Informatics and Software Engineering Conference: Innovative Technologies for Digital Transformation, IISEC 2019 - Proceedings",Conference Paper,Scopus
738,,"Brain Hemorrhage Detection based on Heat Maps, Autoencoder and CNN Architecture","Togacar M., Comert Z., Ergen B., Budak U.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079237728&doi=10.1109%2fUBMYK48245.2019.8965576&partnerID=40&md5=a6b87e9bd1b80b9fabe92f2e7e91819d,10.1109/UBMYK48245.2019.8965576,"Brain hemorrhage refers to hemorrhage within the brain tissue or between the surrounding bone. Therefore, head hemorrhage can lead to many dangerous consequences, especially brain hemorrhage. Early and correct intervention by experts in such cases is important for the patient's life. In this study, computed tomography images of brain hemorrhage are classified by AlexNet which is one of the convolutional neural network models used recently in the biomedical field. In this scope, the data set is restructured with the autoencoder network model and heat maps of each image in the data set are extracted to improve the classification success. The number of images in the data set is then increased by approximately 10 times using the data augmentation technique. The classification process is performed using support vector machines. As a result, the best success rate in the classification was 98.57%. In conclusion, the proposed approach contributed to the classification of cerebral hemorrhage images. © 2019 IEEE.",autoencoder network; Biomedical image processing; brain hemorrhage; deep learning; heat map,,,"1st International Informatics and Software Engineering Conference: Innovative Technologies for Digital Transformation, IISEC 2019 - Proceedings",Conference Paper,Scopus
739,,Analysis of Images Obtained by Unmanned Aerial Vehicle by Deep Learning Methods [Insansiz Hava Araci Ile Elde Edilen Görüntülerin Derin Ögrenme Yöntemleri Ile Analizi],"Kutlu O., Demir O., Dogan B.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079224049&doi=10.1109%2fUBMYK48245.2019.8965587&partnerID=40&md5=98cb862bc969f3ff1f7c93af82aa61a1,10.1109/UBMYK48245.2019.8965587,"In artificial intelligence applications, many sub-methods such as machine learning, artificial neural networks, classification, clustering algorithms are used. One of these methods is deep learning. Deep learning is an advanced machine learning class. Using the Deep Learning method, video analysis, image classification, speech recognition and natural language processing are very successful. The data and experiences to be provided by the projects covering Deep Learning and Unmanned Aerial Vehicles will increase the number of qualified studies on these issues and contribute to the development of high value-added products in these technologies. In this study, a control software that evaluates image data from unmanned aerial vehicles and makes various inferences (classification, positioning, marking) is created. By using the method of retraining the last layers of the pre-trained artificial neural network models with our data set, it has been tried to reduce the training time and increase the success. In these studies, 2 pre-educated models were used and as a result of training of these models, as a result of 190 thousand steps of training, 25.39 and 27.87 mAP values were reached. © 2019 IEEE.",artificial neural network; classification; deep learning; unmanned aerial vehicle),,,"1st International Informatics and Software Engineering Conference: Innovative Technologies for Digital Transformation, IISEC 2019 - Proceedings",Conference Paper,Scopus
740,,Classification of Apricot Leaves with Extreme Learning Machines Using Deep Features [Derin Öznitelikler Kullanilarak Asiri Ögrenme Makineleri ile Kayisi Yapraklarinin Siniflandirilmasi],"Ari B., Ari A., Sengur A., Tuncer S.A.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079223298&doi=10.1109%2fUBMYK48245.2019.8965491&partnerID=40&md5=0a073a6cc8f0fa7410f0a4c6f3b68c85,10.1109/UBMYK48245.2019.8965491,"Machine learning and image processing-based classification of automated plant species is significant for plant experts/herbalists. Many studies on the subject have been gained to the literature. Today, researchers have applied deep learning to various image-based object recognition tasks. In this study, the classification of automatic apricot species based on Deep Convolutional Neural Networks (DCNN) has been made. The proposed method used the VGG19 model, a pre-trained DCNN model. Seven different feature vectors were obtained by combining the features obtained from three different fully connected layers in different combinations. These feature vectors were given to the input of Excessive Learning Machines and seven different apricot types were classified. The highest performance rate was obtained from the fc8 layer as 98.8%, and the lowest performance rate was obtained from the feature vector obtained from the combination of fc6 and fc7 layers as 95.2%. © 2019 IEEE.",deep convolutional neural network; deep learning classification; extreme learning machines,,,"1st International Informatics and Software Engineering Conference: Innovative Technologies for Digital Transformation, IISEC 2019 - Proceedings",Conference Paper,Scopus
741,,Mechanism of Bitcoin and Investigation of the Studies in the Literature Related to Bitcoin [Bitcoin Mekanizmasi ve Bitcoin'le Alakali Literatürde Yapilan Çalismalarin Incelenmesi],"Kadiroglu Z., Akilotu B.N., Sengur A.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079216552&doi=10.1109%2fUBMYK48245.2019.8965449&partnerID=40&md5=9dc180a0a7f044a03c202c112da4903b,10.1109/UBMYK48245.2019.8965449,"Bitcoin is a digital currency that uses cryptographic rules to regulation and generation units of currencies. In 2009, it was introduced by a person (or a group of people) using the name Satoshi Nakamoto before putting on the market as open source software. Bitcoin is a new payment system and investment tool for the purchase, storage and use of money as digitally. However, when people want to invest in bitcoin, the biggest factor they have to consider is how the price will change in the future. Recent developments in the field of machine learning have shown that some algorithms can provide appropriate solutions to predict future prices of crypto currencies. In this study, a detailed literature review is made about the studies using machine learning methods to estimate the future price of bitcoin. Basic and current information about bitcoin was included in this study. In addition, the theory of machine learning methods used is explained. The findings of the literature review show that machine learning methods can be successful in predicting bitcoin price. © 2019 IEEE.",bitcoin; crypto currency; machine learning; price prediction,,,"1st International Informatics and Software Engineering Conference: Innovative Technologies for Digital Transformation, IISEC 2019 - Proceedings",Conference Paper,Scopus
742,,ACTGAN: Automatic configuration tuning for software systems with generative adversarial networks,"Bao L., Liu X., Wang F., Fang B.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078948875&doi=10.1109%2fASE.2019.00051&partnerID=40&md5=44a84cfa38a6042e1e1a8b278388d825,10.1109/ASE.2019.00051,"Complex software systems often provide a large number of parameters so that users can configure them for their specific application scenarios. However, configuration tuning requires a deep understanding of the software system, far beyond the abilities of typical system users. To address this issue, many existing approaches focus on exploring and learning good performance estimation models. The accuracy of such models often suffers when the number of available samples is small, a thorny challenge under a given tuning-time constraint. By contrast, we hypothesize that good configurations often share certain hidden structures. Therefore, instead of trying to improve the performance estimation of a given configuration, we focus on capturing the hidden structures of good configurations and utilizing such learned structure to generate potentially better configurations. We propose ACTGAN to achieve this goal. We have implemented and evaluated ACTGAN using 17 workloads with eight different software systems. Experimental results show that ACTGAN outperforms default configurations by 76.22% on average, and six state-of-the-art configuration tuning algorithms by 6.58%-64.56%. Furthermore, the ACTGAN-generated configurations are often better than those used in training and show certain features consisting with domain knowledge, both of which supports our hypothesis. © 2019 IEEE.",Automatic configuration tuning; Generative adversarial networks; Software system,"465, 476",,"Proceedings - 2019 34th IEEE/ACM International Conference on Automated Software Engineering, ASE 2019",Conference Paper,Scopus
743,,Empirical evaluation of the impact of class overlap on software defect prediction,"Gong L., Jiang S., Wang R., Jiang L.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078943587&doi=10.1109%2fASE.2019.00071&partnerID=40&md5=6b42d429df513b888a47b9daea797f86,10.1109/ASE.2019.00071,"Software defect prediction (SDP) utilizes the learning models to detect the defective modules in project, and their performance depends on the quality of training data. The previous researches mainly focus on the quality problems of class imbalance and feature redundancy. However, training data often contains some instances that belong to different class but have similar values on features, and this leads to class overlap to affect the quality of training data. Our goal is to investigate the impact of class overlap on software defect prediction. At the same time, we propose an improved K-Means clustering cleaning approach (IKMCCA) to solve both the class overlap and class imbalance problems. Specifically, we check whether K-Means clustering cleaning approach (KMCCA) or neighborhood cleaning learning (NCL) or IKMCCA is feasible to improve defect detection performance for two cases (i) within-project defect prediction (WPDP) (ii) cross-project defect prediction (CPDP). To have an objective estimate of class overlap, we carry out our investigations on 28 open source projects, and compare the performance of state-of-the-art learning models for the above-mentioned cases by using IKMCCA or KMCCA or NCL VS. without cleaning data. The experimental results make clear that learning models obtain significantly better performance in terms of balance, Recall and AUC for both WPDP and CPDP when the overlapping instances are removed. Moreover, it is better to consider both class overlap and class imbalance. © 2019 IEEE.",Class overlap; K Means clustering; Machine learning; Software defect prediction,"698, 709",,"Proceedings - 2019 34th IEEE/ACM International Conference on Automated Software Engineering, ASE 2019",Conference Paper,Scopus
744,,Lancer: Your code tell me what you need,"Zhou S., Shen B., Zhong H.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078934371&doi=10.1109%2fASE.2019.00137&partnerID=40&md5=5b78d590fc37cfd0520ad6d34e602210,10.1109/ASE.2019.00137,"Programming is typically a difficult and repetitive task. Programmers encounter endless problems during programming, and they often need to write similar code over and over again. To prevent programmers from reinventing wheels thus increase their productivity, we propose a context-aware code-to-code recommendation tool named Lancer. With the support of a Library-Sensitive Language Model (LSLM) and the BERT model, Lancer is able to automatically analyze the intention of the incomplete code and recommend relevant and reusable code samples in real-time. A video demonstration of Lancer can be found at https://youtu.be/tO9nhqZY35g. Lancer is open source and the code is available at https://github.com/sfzhou5678/Lancer. © 2019 IEEE.",Code recommendation; Code reuse; Language model,"1202, 1205",,"Proceedings - 2019 34th IEEE/ACM International Conference on Automated Software Engineering, ASE 2019",Conference Paper,Scopus
745,,Emotions extracted from text vs. true emotions-an empirical evaluation in SE context,Wang Y.,2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078877215&doi=10.1109%2fASE.2019.00031&partnerID=40&md5=4e7abd692fd6e741e06b89457551772c,10.1109/ASE.2019.00031,"Emotion awareness research in SE context has been growing in recent years. Currently, researchers often rely on textual communication records to extract emotion states using natural language processing techniques. However, how well these extracted emotion states reflect people's real emotions has not been thoroughly investigated. In this paper, we report a multi-level, longitudinal empirical study with 82 individual members in 27 project teams. We collected their self-reported retrospective emotion states on a weekly basis during their year-long projects and also extracted corresponding emotions from the textual communication records. We then model and compare the dynamics of these two types of emotions using multiple statistical and time series analysis methods. Our analyses yield a rich set of findings. The most important one is that the dynamics of emotions extracted using text-based algorithms often do not well reflect the dynamics of self-reported retrospective emotions. Besides, the extracted emotions match self-reported retrospective emotions better at the team-level. Our results also suggest that individual personalities and the team's emotion display norms significantly impact the match/mismatch. Our results should warn the research community about the limitations and challenges of applying text-based emotion recognition tools in SE research. © 2019 IEEE.",Emotion dynamics; Emotion recognition; Organizational norms; Personality; Text based NLP techniques; Time series analysis,"230, 242",,"Proceedings - 2019 34th IEEE/ACM International Conference on Automated Software Engineering, ASE 2019",Conference Paper,Scopus
746,,Deep Belief Network Based on Noisy Data and Clean Data [基于噪声数据与干净数据的深度置信网络],"Zhang N., Ding S.-F., Zhang J., Zhao X.-Y.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077382602&doi=10.13328%2fj.cnki.jos.005574&partnerID=40&md5=df3704bcd9cd8cf3a05675e34c2908e8,10.13328/j.cnki.jos.005574,"Stacking restricted Boltzmann machines (RBM) to create deep networks, such as deep belief networks (DBN), has become one of the most important research fields in deep learning. Point-wise gated restricted Boltzmann machines (pgRBM), an RBM variant, can effectively find the task-relevant patterns from data containing irrelevant patterns and thus achieves satisfied classification results. Given that train data is composed of noisy data and clean data, how the clean data is applied to promote the performance of the pgRBM is a problem. To address the problem, this study first proposes a method, named as pgRBM based on random noisy data and clean data (pgrncRBM). The pgrncRBM makes use of RBM and the clean data to obtain the initial values of the task-relevant weights, so it can learn the ""clean"" data from the data containing random noisy. In the pgrncRBM, the general RBM is used to pre-train the weights of task-relevant patterns from data and irrelevant patterns. If the noise is an image, the pgrncRBM cannot learn the task-relevant patterns from the noisy data. Spike-and-Slab RBM, an RBM variant, uses two types of hidden layers to determine the mean and covariance of each visible unit. Threrfore, this study combines ssRBM with pgRBM and proposes a method, named as pgRBM based on image noisy data and clean data (pgincRBM). The pgincRBM uses the ssRBM to model the noise, so it can learn the ""clean"" data from the data containing image noisy. And then, this study stacks pgrncRBM, pgincRBM, and RBMs to create deep networks, and discusses the feasibility that the weight uncertainty method is developed to prevent overfitting in the proposed networks. Experimental results on MNIST variation datasets show that pgrncRBM and pgincRBM are effective neural networks learning methods. © Copyright 2019, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Deep belief network; Feature selection; Restricted Boltzmann machine (RBM); Weight uncertainty,"3326, 3339",,Ruan Jian Xue Bao/Journal of Software,Article,Scopus
747,,Hybrid Neural Network Models for Human-machine Dialogue Intention Classification [面向人机对话意图分类的混合神经网络模型],"Zhou J.-Z., Zhu Z.-K., He Z.-Q., Chen W.-L., Zhang M.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077358151&doi=10.13328%2fj.cnki.jos.005862&partnerID=40&md5=991704d3dbdf762ef88921ce2a7488a8,10.13328/j.cnki.jos.005862,"With the development of human-machine dialogue, it is of great significance for the computer to accurately understand the user's query intention in human-machine dialogue systems. Intention classification aims at judging the user's intention in human machine dialogue and improves the accuracy and naturalness of the human machine dialogue system. This study first analyzes the advantages and disadvantages of multiple classification models in the intention classification task. On this basis, this study proposes a hybrid neural network model to comprehensively utilize the diversity outputs of multiple deep network models. To further improve the perfoance, the language model embedding is used in the input feature preprocessing and the semantic mining ability possessed for the hybrid network which can effectively improve the expression ability of the model. The proposed model achieves 2.95% and 3.85% performance improvement on the two data sets respectively compared to the best benchmark model. The proposed model also achieves the top performance in a shared task. © Copyright 2019, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Attention mechanism; Capsule network; Hybrid model; Intention classification; Language model,"3313, 3325",,Ruan Jian Xue Bao/Journal of Software,Article,Scopus
748,,An empirical study towards characterizing deep learning development and deployment across different frameworks and platforms,"Guo Q., Chen S., Xie X., Ma L., Hu Q., Liu H., Liu Y., Zhao J., Li X.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076167331&doi=10.1109%2fASE.2019.00080&partnerID=40&md5=f35b59f851ca1f5871e109ac10a340fa,10.1109/ASE.2019.00080,"Deep Learning (DL) has recently achieved tremendous success. A variety of DL frameworks and platforms play a key role to catalyze such progress. However, the differences in architecture designs and implementations of existing frameworks and platforms bring new challenges for DL software development and deployment. Till now, there is no study on how various mainstream frameworks and platforms influence both DL software development and deployment in practice. To fill this gap, we take the first step towards understanding how the most widely-used DL frameworks and platforms support the DL software development and deployment. We conduct a systematic study on these frameworks and platforms by using two types of DNN architectures and three popular datasets. (1) For development process, we investigate the prediction accuracy under the same runtime training configuration or same model weights/biases. We also study the adversarial robustness of trained models by leveraging the existing adversarial attack techniques. The experimental results show that the computing differences across frameworks could result in an obvious prediction accuracy decline, which should draw the attention of DL developers. (2) For deployment process, we investigate the prediction accuracy and performance (refers to time cost and memory consumption) when the trained models are migrated/quantized from PC to real mobile devices and web browsers. The DL platform study unveils that the migration and quantization still suffer from compatibility and reliability issues. Meanwhile, we find several DL software bugs by using the results as a benchmark. We further validate the results through bug confirmation from stakeholders and industrial positive feedback to highlight the implications of our study. Through our study, we summarize practical guidelines, identify challenges and pinpoint new research directions, such as understanding the characteristics of DL frameworks and platforms, avoiding compatibility and reliability issues, detecting DL software bugs, and reducing time cost and memory consumption towards developing and deploying high quality DL systems effectively. © 2019 IEEE.",Deep learning deployment; Deep learning frameworks; Deep learning platforms; Empirical study,"810, 822",,"Proceedings - 2019 34th IEEE/ACM International Conference on Automated Software Engineering, ASE 2019",Conference Paper,Scopus
749,,The usefulness of software metric thresholds for detection of bad smells and fault prediction,"Bigonha M.A.S., Ferreira K., Souza P., Sousa B., Januário M., Lima D.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070497621&doi=10.1016%2fj.infsof.2019.08.005&partnerID=40&md5=0d9990bff49a694cc29d0c6b3efda160,10.1016/j.infsof.2019.08.005,"Context: Software metrics may be an effective tool to assess the quality of software, but to guide their use it is important to define their thresholds. Bad smells and fault also impact the quality of software. Extracting metrics from software systems is relatively low cost since there are tools widely used for this purpose, which makes feasible applying software metrics to identify bad smells and to predict faults. Objective: To inspect whether thresholds of object-oriented metrics may be used to aid bad smells detection and fault predictions. Method: To direct this research, we have defined three research questions (RQ), two related to identification of bad smells, and one for identifying fault in software systems. To answer these RQs, we have proposed detection strategies for the bad smells: Large Class, Long Method, Data Class, Feature Envy, and Refused Bequest, based on metrics and their thresholds. To assess the quality of the derived thresholds, we have made two studies. The first one was conducted to evaluate their efficacy on detecting these bad smells on 12 systems. A second study was conducted to investigate for each of the class level software metrics: DIT, LCOM, NOF, NOM, NORM, NSC, NSF, NSM, SIX, and WMC, if the ranges of values determined by thresholds are useful to identify fault in software systems. Results: Both studies confirm that metric thresholds may support the prediction of faults in software and are significantly and effective in the detection of bad smells. Conclusion: The results of this work suggest practical applications of metric thresholds to identify bad smells and predict faults and hence, support software quality assurance activities.Their use may help developers to focus their efforts on classes that tend to fail, thereby minimizing the occurrence of future problems. © 2019 Elsevier B.V.",Bad smell; Detection strategies; Fault prediction; Software metrics; Software quality; Thresholds,"79, 92",,Information and Software Technology,Article,Scopus
750,,Semi-supervised encoding for outlier detection in clinical observation data,"Estiri H., Murphy S.N.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059964511&doi=10.1016%2fj.cmpb.2019.01.002&partnerID=40&md5=77e8f3f30ddb5207a885d665e3d5fa33,10.1016/j.cmpb.2019.01.002,"Background and Objective: Electronic Health Record (EHR) data often include observation records that are unlikely to represent the “truth” about a patient at a given clinical encounter. Due to their high throughput, examples of such implausible observations are frequent in records of laboratory test results and vital signs. Outlier detection methods can offer low-cost solutions to flagging implausible EHR observations. This article evaluates the utility of a semi-supervised encoding approach (super-encoding) for constructing non-linear exemplar data distributions from EHR observation data and detecting non-conforming observations as outliers. Methods: Two hypotheses are tested using experimental design and non-parametric hypothesis testing procedures: (1) adding demographic features (e.g., age, gender, race/ethnicity) can increase precision in outlier detection, (2) sampling small subsets of the large EHR data can increase outlier detection by reducing noise-to-signal ratio. The experiments involved applying 492 encoder configurations (involving different input features, architectures, sampling ratios, and error margins) to a set of 30 datasets EHR observations including laboratory tests and vital sign records extracted from the Research Patient Data Registry (RPDR) from Partners HealthCare. Results: Results are obtained from (30 × 492) 14,760 encoders. The semi-supervised encoding approach (super-encoding) outperformed conventional autoencoders in outlier detection. Adding age of the patient at the observation (encounter) to the baseline encoder that only included observation value as the input feature slightly improved outlier detection. Top-nine performing encoders are introduced. The best outlier detection performance was from a semi-supervised encoder, with observation value as the single feature and a single hidden layer, built on one percent of the data and one percent reconstruction error. At least one encoder configurations had a Youden's J index higher than 0.9999 for all 30 observation types. Conclusion: Given the multiplicity of distributions for a single observation in EHR data (i.e., same observation represented with different names or units), as well as non-linearity of human observations, encoding offers huge promises for outlier detection in large-scale data repositories. https://github.com/hestiri/superencoder © 2019 Elsevier B.V.",Data quality; Electronic Health Records; Encoding; Neural Networks; Outlier detection; Semi-supervised encoding,,,Computer Methods and Programs in Biomedicine,Article,Scopus
751,,Approximate Oracles and Synergy in Software Energy Search Spaces,"Bruce B.R., Petke J., Harman M., Barr E.T.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045647379&doi=10.1109%2fTSE.2018.2827066&partnerID=40&md5=08f2124e78a862ccf549e1f23a219827,10.1109/TSE.2018.2827066,"Reducing the energy consumption of software systems through optimisation techniques such as genetic improvement is gaining interest. However, efficient and effective improvement of software systems requires a better understanding of the code-change search space. One important choice practitioners have is whether to preserve the system's original output or permit approximation, with each scenario having its own search space characteristics. When output preservation is a hard constraint, we report that the maximum energy reduction achievable by the modification operators is 2.69 percent (0.76 percent on average). By contrast, this figure increases dramatically to 95.60 percent (33.90 percent on average) when approximation is permitted, indicating the critical importance of approximate output quality assessment for code optimisation. We investigate synergy, a phenomenon that occurs when simultaneously applied source code modifications produce an effect greater than their individual sum. Our results reveal that 12.0 percent of all joint code modifications produced such a synergistic effect, though 38.5 percent produce an antagonistic interaction in which simultaneously applied modifications are less effective than when applied individually. This highlights the need for more advanced search-based techniques. © 1976-2012 IEEE.",antagonism; approximation; energy consumption; genetic improvement; oracle; search space; Search-based software engineering; synergy,"1150, 1169",,IEEE Transactions on Software Engineering,Article,Scopus
752,,Bellwethers: A Baseline Method for Transfer Learning,"Krishna R., Menzies T.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044791951&doi=10.1109%2fTSE.2018.2821670&partnerID=40&md5=b3d6059205c6928084f5b65b295a4c3a,10.1109/TSE.2018.2821670,"Software analytics builds quality prediction models for software projects. Experience shows that (a) the more projects studied, the more varied are the conclusions; and (b) project managers lose faith in the results of software analytics if those results keep changing. To reduce this conclusion instability, we propose the use of 'bellwethers': given N projects from a community the bellwether is the project whose data yields the best predictions on all others. The bellwethers offer a way to mitigate conclusion instability because conclusions about a community are stable as long as this bellwether continues as the best oracle. Bellwethers are also simple to discover (just wrap a for-loop around standard data miners). When compared to other transfer learning methods (TCA+, transfer Naive Bayes, value cognitive boosting), using just the bellwether data to construct a simple transfer learner yields comparable predictions. Further, bellwethers appear in many SE tasks such as defect prediction, effort estimation, and bad smell detection. We hence recommend using bellwethers as a baseline method for transfer learning against which future work should be compared. © 1976-2012 IEEE.",bad smells; defect prediction; effort estimation; issue close time; prediction; Transfer learning,"1081, 1105",,IEEE Transactions on Software Engineering,Article,Scopus
753,,Say and find it: A multimodal wearable interface for people with visual impairment,"Kim T., Kim S., Choi J., Lee Y., Lee B.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074853145&doi=10.1145%2f3332167.3357104&partnerID=40&md5=d738998df2f5988a380b504ae1b1c16d,10.1145/3332167.3357104,"Recent advances in computer vision and natural language processing using deep neural networks (DNNs) have enabled rich and intuitive multimodal interfaces. However, research on intelligent assistance systems for persons with visual impairment has not been well explored. In this work, we present an interactive object recognition and guidance interface based on multimodal interaction for blind and partially sighted people using an embedded mobile device. We demonstrate that the proposed solution using DNNs can effectively assist visually impaired people. We believe that this work will provide new and helpful insights for designing intelligent assistance systems in the future. © 2019 Copyright is held by the owner/author(s).",Assistive system; Mobile interface; Multimodal wearable interface; Visual impairment,"27, 29",,UIST 2019 Adjunct - Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology,Conference Paper,Scopus
754,,CalliScan: On-device privacy-preserving image-based handwritten text recognition with visual hints,"Viatchaninov O., Dziubliuk V., Radyvonenko O., Yakishyn Y., Zlotnyk M.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074822745&doi=10.1145%2f3332167.3357119&partnerID=40&md5=6bfc036d5aa2517e45bd77ae68c832cd,10.1145/3332167.3357119,"UPDATED-September 5, 2019. In this work, a solution for handwriting text extraction from images with visual user assistance is proposed. Use of end-to-end systems that pipe together text detection and recognition is often awkward because the user cannot influence the detection stage. On the other hand, glossing over the word's regions to help system with text localization requires a manual job and can be unacceptable. This paper proposes a solution that gives visual cues to the user during a detection stage. These hints differ from traditional bounding boxes in two ways. Firstly, the found text is surrounded with polygonal bounding reflecting a possible complex nature of text blocks. Secondly, TextRadar scanning effect provides a non-overloaded camera view, helping the user to capture the most relevant part of the text on image on-the-fly. CalliScan works on-device and keeps the user's privacy. The evaluation study has shown that users need such a solution, but it is necessary to carefully handle the text layout complexity. © 2019 Copyright is held by the owner/author(s).",Handwriting recognition; HTR; On-device processing; Recurrent networks,"72, 74",,UIST 2019 Adjunct - Adjunct Publication of the 32nd Annual ACM Symposium on User Interface Software and Technology,Conference Paper,Scopus
755,,Taming extreme heterogeneity via machine learning based design of autonomous manycore systems,"Bogdan P., Chen F., Deshwal A., Doppa J.R., Joardar B.K., Li H., Nazarian S., Song L., Xiao Y.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077312656&doi=10.1145%2f3349567.3357376&partnerID=40&md5=6ddbd60ebcbbeeeaf1215baf7237afcc,10.1145/3349567.3357376,"To avoid rewriting software code for new computer architectures and to take advantage of the extreme heterogeneous processing, communication and storage technologies, there is an urgent need for determining the right amount and type of specialization while making a heterogeneous system as programmable and flexible as possible. To enable both programmability and flexibility in the heterogeneous computing era, we propose a novel complex network inspired model of computation and efficient optimization algorithms for determining the optimal degree of parallelization from old software code. This mathematical framework allows us to determine the required number and type of processing elements, the amount and type of deep memory hierarchy, and the degree of reconfiguration for the communication infrastructure, thus opening new avenues to performance and energy efficiency. Our framework enables heterogeneous manycore systems to autonomously adapt from traditional switching techniques to network coding strategies in order to sustain on-chip communication in the order of terabytes. While this new programming model enables the design of self-programmable autonomous heterogeneous manycore systems, a number of open challenges will be discussed. © 2019 Association for Computing Machinery.",Autonomous design optimization; Machine learning; Manycore systems; Model of computation; Processing-in-memory; ReRAM; Self-programming computing architectures,,,"Proceedings of the International Conference on Hardware/Software Codesign and System Synthesis Companion, CODES/ISSS 2019",Conference Paper,Scopus
756,,Microstructural analysis in foods of vegetal origin: An approach with convolutional neural networks [Análisis microestructural en alimentos de origen vegetal: Una aproximación con redes neuronales convolucionales],"Castro W., Yoshida H., Gil L.S., Lopez L.M., Oblitas J., De-La-Torre M., Avila-George H.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084991135&doi=10.1109%2fCIMPS49236.2019.9082421&partnerID=40&md5=e566a0280eb703f6804ea7d0b766938c,10.1109/CIMPS49236.2019.9082421,"The microstructure is a factor in the knowledge and prediction of properties in food and the associated changes during processing. The objective of this work was to evaluate the feasibility of using a convolution neural network (CNN) for the discrimination of structures in foods of vegetable origin. Micrographs of pumpkin were processed digitally to improve the detection of structures (cells and intercellular spaces). Later the found elements were classified in two sets, using a trained operator. The implementation made use of a pre-trained network AlexNet, performing cross-validation, and one hundred repetitions randomizing the information delivered to the training and validation processes. The statistics obtained were accuracy and F-measure. Therefore, the use of convolutional neural networks shows potential for the discrimination of structures in foods of vegetal origin. © 2019 IEEE.",CNN; Digital image; micrograph; vegetal tissue,,,"2019 8th International Conference on Software Process Improvement, CIMPS 2019 - Applications in Software Engineering",Conference Paper,Scopus
757,,Images' partially blurred part location and restoration based on the calculation model and GAN algorithm,"Wu T., Mao H., Xie K., Xie Y.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082880217&doi=10.1109%2fICSESS47205.2019.9040737&partnerID=40&md5=ea25a78f1f34fbe73086568ceb0d9ef1,10.1109/ICSESS47205.2019.9040737,"Digital images would be blurred due to defocus and motion of objects. A lot of researches have been done on the restoration of motion blurred images. Unlike the global motion blurred image, the local blurred image needs different methods to recover as there is both clear and blurred part in an image. In this paper, we proposed a method combining calculation model and Generative Adversarial Network (GAN), which can automatically identify the local blurred region and deblurring. Also, the blurred part can be filled back correctly. Firstly, the gradient image of the complete image is calculated based on the Sobel operator. The boundary information of the fuzzy region and the Gauss function variance of the blurred and clear region are obtained. And the power spectral gradient of the whole image and each local pixel block are further compared with the variance of the Gauss function. After the analysis is accomplished, a more accurate fixed position is achieved. After finishing extracting with clipping pixels, the extracted fuzzy region is input into the pre-trained Generative Adversarial Network to restore the local image. Finally, the alpha channel algorithm is used to calculate the RGB component without the alpha channel of the two images. The simulation results show the feasibility of this method. © 2019 IEEE.",Generative adversarial network; Partial blurred images; Power spectral gradient; Sobel operator,"171, 175",,"Proceedings of the IEEE International Conference on Software Engineering and Service Sciences, ICSESS",Conference Paper,Scopus
758,,Research on Chinese naming recognition model based on BERT embedding,Cai Q.,2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082878726&doi=10.1109%2fICSESS47205.2019.9040736&partnerID=40&md5=76ce8d3b4d17219b9967366def861d66,10.1109/ICSESS47205.2019.9040736,"Named entity recognition (NER) is one of the foundations of natural language processing(NLP). In the method of Chinese named entity recognition based on neural network, the vector representation of words is an important step. Traditional word embedding method map words or chars into a single vector, which can not represent the polysemy of words. To solve this problem, a named entity recognition method based on BERT Embedding model is proposed. The method enhances the semantic representation of words by BERT(Bidirectional Encoder Representations from Transformers) pre-trained language model. BERT can generates the semantic vectors dynamically according to the context of the words, and then inputs the word vectors into BiGRU-CRF for training. The whole model can be trained during training. It is also possible to fix the BERT and train only the BiGRU-CRF part. Experiments show that the two training methods of the model reach 95.43% F1 and 94.18% F1 in MSRA corpus, respectively, which are better than the current optimal Lattice-LSTM model. © 2019 IEEE.",BERT; BiGRU; Chinese nER; Component; CRF; Pre-trained language model,"471, 474",,"Proceedings of the IEEE International Conference on Software Engineering and Service Sciences, ICSESS",Conference Paper,Scopus
759,,"Proceedings of 2019 IEEE 10th International Conference on Software Engineering and Service Science, ICSESS 2019",[No author name available],2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082872452&partnerID=40&md5=a1c27ed6b6d79853c7545d73008188e8,,The proceedings contain 152 papers. The topics discussed include: trends of Intel MIC application in bioinformatics; using format migration and preservation metadata to support digital preservation of scientific data; research and implementation of online judgment system based on micro service; two-stream attention-aware network for spontaneous micro-expression movement spotting; programming error repair guidance based on historical learning behavior; vehicle type recognition based on radon-CDT hybrid transfer learning; joint makespan-aware and load balance-aware optimization of task scheduling in cloud; combine discussion mechanism and chaos strategy on particle swarm optimization algorithm; an identification method for critical nodes of fault propagation based on software dynamic execution network; and a new distributed power system for stability prediction and analysis.,,,734.0,"Proceedings of the IEEE International Conference on Software Engineering and Service Sciences, ICSESS",Conference Review,Scopus
760,,Vehicle type recognition based on radon-CDT hybrid transfer learning,"Guan S., Liao B., Du Y., Yin X.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082865696&doi=10.1109%2fICSESS47205.2019.9040687&partnerID=40&md5=49a4bbeefe8f61a7fcfde56068cbf429,10.1109/ICSESS47205.2019.9040687,"Recognition of vehicle type is an important and challenging tasks. To improve the accuracy and reduce the computational complexity of model for vehicle type recognition, this paper proposes the vehicle type recognition method based on Radon-CDT hybrid transfer learning. Deep features of images are obtained by the pre-trained CNNs, then Radon-CDT are used to capture the non-linearly property of the features. This method makes full use of the excellent feature extraction ability of convolution layer and the property to make data linearly separable of Radon-CDT. Experimental results show that the proposed method is a very promising alternative to deal with the recognition of vehicle type, which obtains a better recognition accuracy than other popular models (AlexNet, VGG and ResNet). © 2019 IEEE.",Convolution neural network; Radon-CDT; Transfer learning; Vehicle type recognition,"491, 494",,"Proceedings of the IEEE International Conference on Software Engineering and Service Sciences, ICSESS",Conference Paper,Scopus
761,,Language to code with open source software,"Tang L., Mao X., Zhang Z.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082855232&doi=10.1109%2fICSESS47205.2019.9040748&partnerID=40&md5=ddfd41436eec005f838e0a72688f0c54,10.1109/ICSESS47205.2019.9040748,"With the development of deep learning, it has been applied in various field of computer science. Generating computer executable code from natural language descriptions is an urgent problem in the artificial intelligence. This paper proposed a solution based on deep learning for code generation. Encoder-Decoder model is used in our method to convert natural language description into target code. Because of the rapid development of information technology, all aspects of software resources have been greatly enriched. The deep learning model we designed takes the natural language description as input and generates the corresponding object code by extracting the code from the open source software library. We collected natural language descriptions of 20 problems that undergraduate students often encounter in their daily programming. Experimental results show that our method is practicable. Our approach also provides a good idea to extract useful code from open resource for code generation. © 2019 IEEE.",Code generation; Encoder-Decoder; LSTM; Open resource software,"561, 564",,"Proceedings of the IEEE International Conference on Software Engineering and Service Sciences, ICSESS",Conference Paper,Scopus
762,,A Chinese question answering system based on GPT,"Liu S., Huang X.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082848538&doi=10.1109%2fICSESS47205.2019.9040807&partnerID=40&md5=c2e121d69a05b0c4f727126733cc4981,10.1109/ICSESS47205.2019.9040807,"The Chinese question-answering system needs to select the most appropriate answer from the answer library for user according to the given question on the natural language form. Previous question-answering systems required modeling for specific task characteristics and designing multiple modules. This paper first proposes to use the Generative Pre-trained Transformer (GPT) to implement the Chinese question-answering system. To optimize and improve the model, this Chinese model pays more attention to the contextual content and semantic characteristics, and we designed a new method to train this model. This model reduces the number of modules in the question-answering system. This paper evaluates the model on the Document-Based Chinese Question and Answer (DBQA) dataset and achieves a 2.5% improvement in MRR/MAP over the latest lattice convolutional neural networks (Lattice CNNs). (Abstract) © 2019 IEEE.",Chinese question-Answering system; GPT; Self-Attention (key words); Transformer,"533, 537",,"Proceedings of the IEEE International Conference on Software Engineering and Service Sciences, ICSESS",Conference Paper,Scopus
763,,Semantic segmentation of intracranial hemorrhages in head CT scans,"Qiu Y., Chang C.S., Yan J.L., Ko L., Chang T.S.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082847186&doi=10.1109%2fICSESS47205.2019.9040733&partnerID=40&md5=4280d4d045e01fd3f1d29c436b005424,10.1109/ICSESS47205.2019.9040733,"This paper presents a semantic segmentation method that can distinguish six different types of intracranial hemorrhage and calculate the amount of blood loss. The major challenge of medical image segmentation are the lack of enough data due to the difficulty of data collection and labeling. In this paper, we propose to adopt a pretrained U-Net model with fine tuning to solve this problem. The best final test accuracy can reach 94.1%, which is 10.5% higher than the model training from scratch, proving its advantages in dealing with relatively complex datasets with a small amount of data, and the success of the proposed segmentation method. © 2019 IEEE.",Blood loss; Intracranial hemorrhage; Pretrained; Segmentation; U-Net,"112, 115",,"Proceedings of the IEEE International Conference on Software Engineering and Service Sciences, ICSESS",Conference Paper,Scopus
764,,Sequential multi-kernel convolutional recurrent network for sentiment classification,"Oluwasanmi A., Akeem S., Jehoaida J., Aftab M.U., Hundera N., Kumeda B., Baagere E., Qin Z.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082846429&doi=10.1109%2fICSESS47205.2019.9040746&partnerID=40&md5=b3e0c30341046f21272162c462db9c20,10.1109/ICSESS47205.2019.9040746,"The emergence of deep learning as a commanding technique for learning heterogeneous layers of feature representations have consequently substituted traditional machine learning algorithms which are generally poor in analyzing compound sentences. Additionally, convolutional and recurrent neural networks have auspiciously yielded state-of-the-art results in sentiment classification and Natural Language Processing (NLP). In this paper, a deep sentiment representation model through the combination of multiple Convolutional Neural Networks (CNN) kernels with Long Short-Term Memory (LSTM) is proposed for sentiment classification. Our model gains word vector representation using pre-trained Global Vectors for Word Representation (GloVe) embeddings, thereafter used as input to the CNN layer which extracts higher local text representations. Finally, Bidirectional LSTM (biLSTM) generates sentiment classification of sentence representation based on context dependent features. Our combined approach of CNN and biLSTM was experimented using the Stanford Large Movie Review Dataset (IMDB) and Stanford Sentiment Treebank Dataset (SSTB) for binary classification. The evaluation achieves outstanding results in outperforming several existing approaches with 90.4% accuracy on the Stanford Sentiment Treebank dataset and 94.8% accuracy on the Stanford Large Movie Review dataset. These results are achieved with a drastic reduction of model parameters and without a pooling layer in the CNN architecture, helping to retain local and structural information in comparison to other existing deep neural network frameworks. (Abstract) © 2019 IEEE.",Component; Convolutional neural network; Natral language processing; Recurrent neural network: word embeddings.; Sentiment analysis,"129, 133",,"Proceedings of the IEEE International Conference on Software Engineering and Service Sciences, ICSESS",Conference Paper,Scopus
765,,Identifying Crashing Fault Residence Based on Cross Project Model,"Xu Z., Zhang T., Zhang Y., Tang Y., Liu J., Luo X., Keung J., Cui X.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081107799&doi=10.1109%2fISSRE.2019.00027&partnerID=40&md5=c39948e585fbac7cf86d34ad755aa837,10.1109/ISSRE.2019.00027,"Analyzing the crash reports recorded upon software crashes is a critical activity for software quality assurance. Predicting whether or not the fault causing the crash (crashing fault for short) resides in the stack traces of crash reports can speed-up the program debugging process and determine the priority of the debugging efforts. Previous work mostly collected label information from bug-fixing logs, and extracted crash features from stack traces and source code to train classification models for the Identification of Crashing Fault Residence (ICFR) of newly-submitted crashes. However, labeled data are not always fully available in real applications. Hence the classifier training is not always feasible. In this work, we make the first attempt to develop a cross project ICFR model to address the data scarcity problem. This is achieved by transferring the knowledge from external projects to the current project via utilizing a state-of-the-art Balanced Distribution Adaptation (BDA) based transfer learning method. BDA not only combines both marginal distribution and conditional distribution across projects but also assigns adaptive weights to the two distributions for better adjusting specific cross project pair. The experiments on 7 software projects show that BDA is superior to 9 baseline methods in terms of 6 indicators overall. © 2019 IEEE.",Crashing fault; Cross project model; Stack trace; Transfer learning,"183, 194",,"Proceedings - International Symposium on Software Reliability Engineering, ISSRE",Conference Paper,Scopus
766,,Supervised Representation Learning Approach for Cross-Project Aging-Related Bug Prediction,"Wan X., Zheng Z., Qin F., Qiao Y., Trivedi K.S.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081106928&doi=10.1109%2fISSRE.2019.00025&partnerID=40&md5=a6810bd0132ab7cb17d8fd084488af58,10.1109/ISSRE.2019.00025,"Software aging, which is caused by Aging-Related Bugs (ARBs), tends to occur in long-running systems and may lead to performance degradation and increasing failure rate during software execution. ARB prediction can help developers discover and remove ARBs, thus alleviating the impact of software aging. However, ARB-prone files occupy a small percentage of all the analyzed files. It is usually difficult to gather sufficient ARB data within a project. To overcome the limited availability of training data, several researchers have recently developed cross-project models for ARB prediction. A key point for cross-project models is to learn a good representation for instances in different projects. Nevertheless, most of the previous approaches neither consider the reconstruction property of new representation nor encode source samples' label information in learning representation. To address these shortcomings, we propose a Supervised Representation Learning Approach (SRLA), which is based on double encoding-layer autoencoder, to perform cross-project ARB prediction. Moreover, we present a transfer cross-validation framework to select the hyper-parameters of cross-project models. Experiments on three large open-source projects demonstrate the effectiveness and superiority of our approach compared with the state-of-the-art approach TLAP. © 2019 IEEE.",Aging-related bug; Cross-project bug prediction; Software aging; Supervised representation learning approach,"163, 172",,"Proceedings - International Symposium on Software Reliability Engineering, ISSRE",Conference Paper,Scopus
767,,Research Progress of Software Defect Prediction [软件缺陷预测技术研究进展],"Gong L.-N., Jiang S.-J., Jiang L.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075574877&doi=10.13328%2fj.cnki.jos.005790&partnerID=40&md5=77318eaad8a1fecd1a31516ad606d374,10.13328/j.cnki.jos.005790,"With the improvement of the scale and complexity of software, software quality problems become the focus of attention. Software defect is the opposite of software quality, threatening the software quality. How to dig up defect modules in the early stages of software development has become a urgent problem that needs to be solved. Software defect prediction (SDP) designs the internal metrics related defects by mining software history repositories, and then in advance finds and locks the defect modules with the aid of machine learning methods, so as to allocate the limited resources reasonably. Therefore, SDP is one of the important ways of software quality assurance (SQA), which has become a very important research subject in software engineering in recent years. Based on the form of defect perfection, this research offers a systematic analysis of the existing research achievements of the domestic and foreign researchers in recent eight years (2010~2017). First, the research framework of SDP is given.Then the existing research achievements are classified and compared from three aspects, including datasets of SDP, the methods models and the evaluation indicators. Finally, the possible research directions are pointed out. © Copyright 2019, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Data preprocessing; Machine learning; Performance evaluation criteria; Software defect prediction (SDP); Software metrics,"3090, 3114",,Ruan Jian Xue Bao/Journal of Software,Review,Scopus
768,,Non Contact Multi-channel Natural Interactive Surgical Environment under Sterile Conditions [无菌条件非接触式多通道自然交互手术环境],"Tao J.-H., Yang M.-H., Wang Z.-L., Ban X.-J., Xie L., Wang Y.-H., Zeng Q., Wang F., Wang H.-Q., Liu B., Han Z.-S., Pan H., Chen W.-Z.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075559359&doi=10.13328%2fj.cnki.jos.005785&partnerID=40&md5=fdcff0571a5efc9bb3e0577fbf9d2a38,10.13328/j.cnki.jos.005785,"Sterile and non-contact environment is the basic requirement of medical operating room, which makes the computer room and operating room need to be physically isolated. At the same time, if the attending doctor needs to look at the image of the lesion during the operation, he usually instructs the nurse or the assistant to operate the image of the lesion in the computer operating room, because of the isolation between the operating room and the computer room, and because the intention between the attending doctor and the assistant may not be understood accurately, it is easy to lead the nurse or surgical assistant in the operating room and computer room to and fro many times, which increases the risk of prolonged operation time, increased blood loss, and organ exposure time, minimizing the time to locate the lesion image in the operation is important for doctors and patients. To meet the above requirements, a non-contact multi-channel natural interactive surgical environment under aseptic conditions is constructed by means of human skeleton extraction, gesture tracking and understanding, far-field speech recognition in operating room environment, multi-modal information processing, and fusion technology. This environment allows the attending physician to quickly locate the lesion to be observed during surgery by combining voice commands, gestures, and the above interaction. In the experimental environment close to the real environment, the non-contact multi-channel natural interactive surgical environment established in this study can significantly reduce the localization time of the lesion image under the condition of ensuring the accuracy. Intelligent interactive operating room in aseptic environment provides technical and methodological validation for the next generation of efficient surgery. © Copyright 2019, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Intention understanding; Multimodal information fusion; Operation room,"2986, 3004",,Ruan Jian Xue Bao/Journal of Software,Article,Scopus
769,,WAAC: An End-to-End Web API Automatic Calls Approach for Goal-Oriented Intelligent Services,"Li Y., Liu S., Jin T., Gao H.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074455415&doi=10.1142%2fS0218194019500487&partnerID=40&md5=535e454b7629ae240d1229bbd5b9aa2a,10.1142/S0218194019500487,"Web API recommendations have recently been studied extensively. However, recommending an API for a service is different than service intelligence. Web API automatic calls are widely used in question-answer dialog applications and service-composed workflow systems to achieve intelligent services. To finish an automatic Web API call not only requires the Web API ID, but also its input parameters. In this paper, we propose an end-to-end Web API automatic calls approach, named WAAC, that translates a goal's natural language sentences directly to the Web API invoking sequences including its ID and parameters. This end-to-end approach based on the seq2seq encoder-decoder framework, adopts character-level RNN for the Chinese sentences and introduces a copying mechanism to retrieve API parameters. To train the network, a Chinese version dataset of over 1 million natural sentences and API invoking sequence pairs are generated with some manually labeled data and 72 real Web API invoking logs. Experiments obtain a 96% precision on predicting API invoking sequences and show that the character-level RNN and copying mechanism both contribute considerably to achieving a high precision Web API automatic call system for goal-oriented services. © 2019 World Scientific Publishing Company.",goal-oriented dialog systems; Seq2Seq; service intelligence; Web API automation,"1539, 1555",,International Journal of Software Engineering and Knowledge Engineering,Article,Scopus
770,,Investigating the use of duration-based windows and estimation by analogy for COCOMO,"Nguyen V., Huynh T., Boehm B., Huang L., Truong T.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074158143&doi=10.1002%2fsmr.2176&partnerID=40&md5=98ae68367a91906baed5d9c965376811,10.1002/smr.2176,"In model-based software estimation, using the right training data is a key contributor for making accurate predictions, which is crucial for the success of software projects. This study investigates the use of duration-based windows and estimation by analogy to calibrate COCOMO and assess their estimation performance. We compare these approaches as well as the use of all available historical data using the COCOMO data set of 341 projects and NASA data set of 93 projects. The results show that timing information exists in the data sets affecting estimation accuracy. Given sufficient data for calibration, using recently completed projects within short durations generates more accurate estimates than retaining all historical data or using k-nearest neighbors based on estimation by analogy. More training data spanning a long period of time may not lead to improved estimation accuracy. This study offers evidence to support the use of projects completed within recent years for training estimation models. © 2019 John Wiley & Sons, Ltd.",COCOMO; duration-based windows; estimation by analogy; k-nearest neighbors; moving windows; software estimation,,,Journal of Software: Evolution and Process,Article,Scopus
771,,An improved transfer adaptive boosting approach for mixed-project defect prediction,"Gong L., Jiang S., Jiang L.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074148533&doi=10.1002%2fsmr.2172&partnerID=40&md5=a607be62238de23c9be06677e32c3021,10.1002/smr.2172,"Software defect prediction (SDP) has been a very important research topic in software engineering, since it can provide high-quality results when given sufficient historical data of the project. Unfortunately, there are not abundant data to bulid the defect prediction model at the beginning of a project. For this scenario, one possible solution is to use data from other projects in the same company. However, using these data practically would get poor performance because of different distributional characteristics among projects. Also, software has more non-defective instances than defective instances that may cause a significant bias towards defective instances. Considering these two problems, we propose an improved transfer adaptive boosting (ITrAdaBoost) approach for being given a small number of labeled data in the testing project. In our approach, ITrAdaBoost can not only employ the Matthews correlation coefficient (MCC) as the measure instead of accuracy rate but also use the asymmetric misclassification costs for non-defective and defective instances. Extensive experiments on 18 public projects from four datasets indicate that: (a) our approach significantly outperforms state-of-the-art cross-project defect prediction (CPDP) approaches, and (b) our approach can obtain comparable prediction performances in contrast with within project prediction results. Consequently, the proposed approach can build an effective prediction model with a small number of labeled instances for mixed-project defect prediction (MPDP). © 2019 John Wiley & Sons, Ltd.",class imbalance; cross-project; mixed-project; software defect prediction; transfer learning,,,Journal of Software: Evolution and Process,Article,Scopus
772,,Joint distribution matching model for distribution-adaptation-based cross-project defect prediction,"Qiu S., Lu L., Jiang S.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073164052&doi=10.1049%2fiet-sen.2018.5131&partnerID=40&md5=11a95e90d459d388b05dabeba1e94e63,10.1049/iet-sen.2018.5131,"Using classification methods to predict software defect is receiving a great deal of attention and most of the existing studies primarily conduct prediction under the within-project setting. However, there usually had no or very limited labelled data to train an effective prediction model at an early phase of the software lifecycle. Thus, cross-project defect prediction (CPDP) is proposed as an alternative solution, which is learning a defect predictor for a target project by using labelled data from a source project. Differing from previous CPDP methods that mainly apply instances selection and classifiers adjustment to improve the performance, in this study, the authors put forward a novel distribution-adaptation-based CPDP approach, joint distribution matching (JDM). Specifically, JDM aims to minimise the joint distribution divergence between the source and target project to improve the CPDP performance. By constructing an adaptive weight vector for the instances of the source project, JDM can be effective and robust at reducing marginal distribution discrepancy and conditional distribution discrepancy simultaneously. Extensive experiments verify that JDM can outperform related distribution-adaptation-based methods on 15 open-source projects that are derived from two types of repositories. © The Institution of Engineering and Technology 2019.",,"393, 402",,IET Software,Article,Scopus
773,,Fruit Image Classification Using Convolutional Neural Networks,"Ashraf S., Kadery I., Chowdhury A.A., Mahbub T.Z., Rahman R.M.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072181206&doi=10.4018%2fIJSI.2019100103&partnerID=40&md5=fddc4bff1ebcbb76d03937ad42385550,10.4018/IJSI.2019100103,"Convolutional neural networks (CNN) are the most popular class of models for image recognition and classification task nowadays. Most of the superstores and fruit vendors resort to human inspection to check the quality of the fruits stored in their inventory. However, this process can be automated. We propose a system that can be trained with a fruit image dataset and then detect whether a fruit is rotten or fresh from an input image. We built the initial model using the Inception V3 model and trained with our dataset applying transfer learning. © 2019, IGI Global.",Classification Problem; Convolutional Neural Networks; Fruit Image Classification; Image Analysis; Inception V3 Model; Machine Learning,"51, 70",,International Journal of Software Innovation,Article,Scopus
774,,Automated integrated system for stained neuron detection: An end-to-end framework with a high negative predictive rate,"Yoon J.-S., Choi E.Y., Saad M., Choi T.-S.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070784674&doi=10.1016%2fj.cmpb.2019.105028&partnerID=40&md5=067a18e6aff74b5d85fcbb54123a1fa2,10.1016/j.cmpb.2019.105028,"Background and objective: Mapping the architecture of the brain is essential for identifying the neural computations that affect behavior. Traditionally in histology, stained objects in tissue slices are hand-marked under a microscope in a manually intensive, time-consuming process. An integrated hardware and software system is needed to automate image acquisition, image processing, and object detection. Such a system would enable high throughput tissue analysis to rapidly map an entire brain. Methods: We demonstrate an automated system to detect neurons using a monkey brain slice immunohistochemically stained for retrogradely labeled neurons. The proposed system obtains a reconstructed image of the sample, and stained neurons are detected in three steps. First, the reconstructed image is pre-processed using adaptive histogram equalization. Second, candidates for stained neurons are segmented from each region via marker-controlled watershed transformation (MCWT) using maximally stable extremal regions (MSERs). Third, the candidates are categorized as neurons or non-neurons using deep transfer learning via pre-trained convolutional neural networks (CNN). Results: The proposed MCWT algorithm was compared qualitatively against MorphLibJ and an IHC analysis tool, while our unified classification algorithm was evaluated quantitatively using ROC analyses. The proposed classification system was first compared with five previously developed layers (AlexNet, VGG-16, VGG-19, GoogleNet, and ResNet). A comparison with conventional multi-stage frameworks followed using six off-the-shelf classifiers [Bayesian network (BN), support vector machines (SVM), decision tree (DT), bagging (BAG), AdaBoost (ADA), and logistic regression (LR)] and two descriptors (LBP and HOG). The system achieved a 0.918 F1-score with an 86.6% negative prediction value. Remarkably, other metrics such as precision, recall, and F-scores surpassed the 90% threshold compared to traditional methods. Conclusions: We demonstrate a fully automated, integrated hardware and software system for rapidly acquiring focused images and identifying neurons from a stained brain slice. This system could be adapted for the identification of stained features of any biological tissue. © 2019",Convolutional neural networks (CNN); Histological image analysis; Machine learning; Marker-controlled-watershed transformation (MCWT); Maximally stable extremal regions (MSERs); Monkey brain tissue; Stained neurons,,,Computer Methods and Programs in Biomedicine,Article,Scopus
775,,Recognition of peripheral blood cell images using convolutional neural networks,"Acevedo A., Alférez S., Merino A., Puigví L., Rodellar J.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070674943&doi=10.1016%2fj.cmpb.2019.105020&partnerID=40&md5=db448520de680217fbc3ece2b5885aee,10.1016/j.cmpb.2019.105020,"Background and objectives: Morphological analysis is the starting point for the diagnostic approach of more than 80% of hematological diseases. However, the morphological differentiation among different types of normal and abnormal peripheral blood cells is a difficult task that requires experience and skills. Therefore, the paper proposes a system for the automatic classification of eight groups of peripheral blood cells with high accuracy by means of a transfer learning approach using convolutional neural networks. With this new approach, it is not necessary to implement image segmentation, the feature extraction becomes automatic and existing models can be fine-tuned to obtain specific classifiers. Methods: A dataset of 17,092 images of eight classes of normal peripheral blood cells was acquired using the CellaVision DM96 analyzer. All images were identified by pathologists as the ground truth to train a model to classify different cell types: neutrophils, eosinophils, basophils, lymphocytes, monocytes, immature granulocytes (myelocytes, metamyelocytes and promyelocytes), erythroblasts and platelets. Two designs were performed based on two architectures of convolutional neural networks, Vgg-16 and Inceptionv3. In the first case, the networks were used as feature extractors and these features were used to train a support vector machine classifier. In the second case, the same networks were fine-tuned with our dataset to obtain two end-to-end models for classification of the eight classes of blood cells. Results: In the first case, the experimental test accuracies obtained were 86% and 90% when extracting features with Vgg-16 and Inceptionv3, respectively. On the other hand, in the fine-tuning experiment, global accuracy values of 96% and 95% were obtained using Vgg-16 and Inceptionv3, respectively. All the models were trained and tested using Keras and Tensorflow with a Nvidia Titan XP Graphics Processing Unit. Conclusions: The main contribution of this paper is a classification scheme involving a convolutional neural network trained to discriminate among eight classes of cells circulating in peripheral blood. Starting from a state-of-the-art general architecture, we have established a fine-tuning procedure to develop an end-to-end classifier trained using a dataset with over 17,000 cell images obtained from clinical practice. The performance obtained when testing the system has been truly satisfactory, the values of precision, sensitivity, and specificity being excellent. To summarize, the best overall classification accuracy has been 96.2%. © 2019",Blood cell automatic recognition; Blood cell morphology; Convolutional neural networks; Deep learning; Fine-tuning,,,Computer Methods and Programs in Biomedicine,Article,Scopus
776,,Multiscale spatial gradient features for 18F-FDG PET image-guided diagnosis of Alzheimer's disease,"Pan X., Adel M., Fossati C., Gaidon T., Wojak J., Guedj E.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070674229&doi=10.1016%2fj.cmpb.2019.105027&partnerID=40&md5=af05566170fc10ef3967a9ac495d10e3,10.1016/j.cmpb.2019.105027,"Background and Objective: 18F-FluoroDeoxyGlucose Positron Emission Tomography (18F-FDG PET) is one of the imaging biomarkers to diagnose Alzheimer's Disease (AD). In 18F-FDG PET images, the changes of voxels’ intensities reflect the differences of glucose rates, therefore voxel intensity is usually used as a feature to distinguish AD from Normal Control (NC), or at earlier stage to distinguish between progressive and stable Mild Cognitive Impairment (pMCI and sMCI). In this paper, 18F-FDG PET images are characterized in an alternative way—the spatial gradient, which is motivated by the observation that the changes of 18F-FDG rates also cause gradient changes. Methods: We improve Histogram of Oriented Gradient (HOG) descriptor to quantify spatial gradients, thereby achieving the goal of diagnosing AD. First, the spatial gradient of 18F-FDG PET image is computed, and then each subject is segmented into different regions by using an anatomical atlas. Second, two types of improved HOG features are extracted from each region, namely Small Scale HOG and Large Scale HOG, then some relevant regions are selected based on a classifier fed with spatial gradient features. Last, an ensemble classification framework is designed to make a decision, which considers the performance of both individual and concatenated selected regions. Results: the evaluation is done on ADNI dataset. The proposed method outperforms other state-of-the-art 18F-FDG PET-based algorithms for AD vs. NC with an accuracy, a sensitivity and a specificity values of 93.65%, 91.22% and 96.25%, respectively. For the case of pMCI vs. sMCI, the three metrics are 75.38%, 74.84% and 77.11%, which is significantly better than most existing methods. Besides, promising results are also achieved for multiple classifications under 18F-FDG PET modality. Conclusions: 18F-FDG PET images can be characterized by spatial gradient features for diagnosing AD and its early stage, and the proposed ensemble framework can enhance the classification performance. © 2019 Elsevier B.V.",18F-FDG PET; Alzheimer's disease; Ensemble classification; Multiscale spatial gradients,,,Computer Methods and Programs in Biomedicine,Article,Scopus
777,,Learning from adversarial medical images for X-ray breast mass segmentation,"Shen T., Gou C., Wang F.-Y., He Z., Chen W.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070217245&doi=10.1016%2fj.cmpb.2019.105012&partnerID=40&md5=72e5b5e1548cf16aed9cb15749c8333a,10.1016/j.cmpb.2019.105012,"Background and Objective: Simulation of diverse lesions in images is proposed and applied to overcome the scarcity of labeled data, which has hindered the application of deep learning in medical imaging. However, most of current studies focus on generating samples with class labels for classification and detection rather than segmentation, because generating images with precise masks remains a challenge. Therefore, we aim to generate realistic medical images with precise masks for improving lesion segmentation in mammagrams. Methods: In this paper, we propose a new framework for improving X-ray breast mass segmentation performance aided by generated adversarial lesion images with precise masks. Firstly, we introduce a conditional generative adversarial network (cGAN) to learn the distribution of real mass images as well as a mapping between images and corresponding segmentation masks. Subsequently, a number of lesion images are generated from various binary input masks using the generator in the trained cGAN. Then the generated adversarial samples are concatenated with original samples to produce a dataset with increased diversity. Furthermore, we introduce an improved U-net and train it on the previous augmented dataset for breast mass segmentation. Results: To demonstrate the effectiveness of our proposed method, we conduct experiments on publicly available mammogram database of INbreast and a private database provided by Nanfang Hospital in China. Experimental results show that an improvement up to 7% in Jaccard index can be achieved over the same model trained on original real lesion images. Conclusions: Our proposed method can be viewed as one of the first steps toward generating realistic X-ray breast mass images with masks for precise segmentation. © 2019 Elsevier B.V.",Generative adversarial network; Lesion segmentation; Medical image synthesis; X-ray breast mass,,,Computer Methods and Programs in Biomedicine,Article,Scopus
778,,Improving defect prediction with deep forest,"Zhou T., Sun X., Xia X., Li B., Chen X.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068541005&doi=10.1016%2fj.infsof.2019.07.003&partnerID=40&md5=1bff476ff9d8dcf12771501623c1d27a,10.1016/j.infsof.2019.07.003,"Context: Software defect prediction is important to ensure the quality of software. Nowadays, many supervised learning techniques have been applied to identify defective instances (e.g., methods, classes, and modules). Objective: However, the performance of these supervised learning techniques are still far from satisfactory, and it will be important to design more advanced techniques to improve the performance of defect prediction models. Method: We propose a new deep forest model to build the defect prediction model (DPDF). This model can identify more important defect features by using a new cascade strategy, which transforms random forest classifiers into a layer-by-layer structure. This design takes full advantage of ensemble learning and deep learning. Results: We evaluate our approach on 25 open source projects from four public datasets (i.e., NASA, PROMISE, AEEEM and Relink). Experimental results show that our approach increases AUC value by 5% compared with the best traditional machine learning algorithms. Conclusion: The deep strategy in DPDF is effective for software defect prediction. © 2019 Elsevier B.V.",Cascade strategy; Deep forest; Empirical evaluation; Software defect prediction,"204, 216",,Information and Software Technology,Article,Scopus
779,,Heterogeneous defect prediction with two-stage ensemble learning,"Li Z., Jing X.-Y., Zhu X., Zhang H., Xu B., Ying S.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067246074&doi=10.1007%2fs10515-019-00259-1&partnerID=40&md5=ec0985396c401899741387574da7a0f7,10.1007/s10515-019-00259-1,"Heterogeneous defect prediction (HDP) refers to predicting defect-prone software modules in one project (target) using heterogeneous data collected from other projects (source). Recently, several HDP methods have been proposed. However, these methods do not sufficiently incorporate the two characteristics of the defect data: (1) data could be linear inseparable, and (2) data could be highly imbalanced. These two data characteristics make it challenging to build an effective HDP model. In this paper, we propose a novel Two-Stage Ensemble Learning (TSEL) approach to HDP, which contains two stages: ensemble multi-kernel domain adaptation (EMDA) stage and ensemble data sampling (EDS) stage. In the EMDA stage, we develop an Ensemble Multiple Kernel Correlation Alignment (EMKCA) predictor, which combines the advantage of multiple kernel learning and domain adaptation techniques. In the EDS stage, we employ RESample with replacement (RES) technique to learn multiple different EMKCA predictors and use average ensemble to combine them together. These two stages create an ensemble of defect predictors. Extensive experiments on 30 public projects show that the proposed TSEL approach outperforms a range of competing methods. The improvement is 20.14–33.92% in AUC, 36.05–54.78% in f-measure, and 5.48–19.93% in balance, respectively. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.",Class imbalance; Data sampling; Domain adaptation; Heterogeneous defect prediction; Linear inseparability; Multiple kernel learning; Two-stage ensemble learning,"599, 651",,Automated Software Engineering,Article,Scopus
780,,EmoD: An End-to-End Approach for Investigating Emotion Dynamics in Software Development,"Neupane K., Cheung K., Wang Y.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077214440&doi=10.1109%2fICSME.2019.00038&partnerID=40&md5=81163b5cf1f1154c7b14869d644fe86f,10.1109/ICSME.2019.00038,"Emotions are an integral part of human nature. Emotion awareness is critical to any form of interpersonal communication and collaboration, including these in the software development process. Recently, the SE community starts having growing interests in emotion awareness in software development. While researchers have accomplished many valuable results, most extant research ignores the dynamic nature of emotion. To investigate the emotion dynamics, SE community needs an effective approach to capture and model emotion dynamics rather than focuses on extracting isolated emotion states. In this paper, we proposed such an approach-EmoD. EmoD is able to automatically collect project teams' communication records, identify the emotions and their intensities in them, model the emotion dynamics into time series, and provide efficient data management. We developed a prototype tool that instantiates the EmoD approach by assembling state-of-the-art NLP, SE, and time series techniques. We demonstrate the utility of the tool using the IPython's project data on GitHub and a visualization solution built on EmoD. Thus, we demonstrate that EmoD can provide end-to-end support for various emotion awareness research and practices through automated data collection, modeling, storage, analysis, and presentation. © 2019 IEEE.",Emotion awareness; emotion dynamics; emotion intensity; software project team; time series database,"252, 256",,"Proceedings - 2019 IEEE International Conference on Software Maintenance and Evolution, ICSME 2019",Conference Paper,Scopus
781,,Losing Confidence in Quality: Unspoken Evolution of Computer Vision Services,"Cummaudo A., Vasa R., Grundy J., Abdelrazek M., Cain A.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077208480&doi=10.1109%2fICSME.2019.00051&partnerID=40&md5=b46867e9eb0706e508e3249307adc5a1,10.1109/ICSME.2019.00051,"Recent advances in artificial intelligence (AI) and machine learning (ML), such as computer vision, are now available as intelligent services and their accessibility and simplicity is compelling. Multiple vendors now offer this technology as cloud services and developers want to leverage these advances to provide value to end-users. However, there is no firm investigation into the maintenance and evolution risks arising from use of these intelligent services; in particular, their behavioural consistency and transparency of their functionality. We evaluated the responses of three different intelligent services (specifically computer vision) over 11 months using 3 different data sets, verifying responses against the respective documentation and assessing evolution risk. We found that there are: (1) inconsistencies in how these services behave; (2) evolution risk in the responses; and (3) a lack of clear communication that documents these risks and inconsistencies.We propose a set of recommendations to both developers and intelligent service providers to inform risk and assist maintainability. © 2019 IEEE.",computer vision; documentation; evolution risk; intelligent service; Machine learning; quality assurance,"333, 342",,"Proceedings - 2019 IEEE International Conference on Software Maintenance and Evolution, ICSME 2019",Conference Paper,Scopus
782,,Performance-Influence Model for Highly Configurable Software with Fourier Learning and Lasso Regression,"Ha H., Zhang H.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077205596&doi=10.1109%2fICSME.2019.00080&partnerID=40&md5=6f28790900e746691a835334da7890d3,10.1109/ICSME.2019.00080,"Many software systems are highly configurable, which provide a large number of configuration options for users to choose from. During the maintenance and operation of these configurable systems, it is important to estimate the system performance under any specific configurations and understand the performance-influencing configuration options. However, it is often not feasible to measure the system performance under all the possible configurations as the combination of configurations could be exponential. In this paper, we propose PerLasso, a performance modeling and prediction method based on Fourier Learning and Lasso (Least absolute shrinkage and selection operator) regression techniques. Using a small sample of measured performance values of a configurable system, PerLasso produces a performance-influence model, which can 1) predict system performance under a new configuration; 2) explain the influence of the individual features and their interactions on the software performance. Besides, to reduce the number of Fourier coefficients to be estimated for large-scale systems, we also design a novel dimension reduction algorithm. Our experimental results on four synthetic and six real-world datasets confirm the effectiveness of our approach. Compared to the existing performance-influence models, our models have higher or comparable prediction accuracy. © 2019 IEEE.",configurable systems; performance influence model; software performance prediction,"470, 480",,"Proceedings - 2019 IEEE International Conference on Software Maintenance and Evolution, ICSME 2019",Conference Paper,Scopus
783,,Know-How in Programming Tasks: From Textual Tutorials to Task-Oriented Knowledge Graph,"Sun J., Xing Z., Chu R., Bai H., Wang J., Peng X.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077200772&doi=10.1109%2fICSME.2019.00039&partnerID=40&md5=bc8feace6ae2a725fd69231f8ae76e07,10.1109/ICSME.2019.00039,"Accomplishing a program task usually involves performing multiple activities in a logical order. Task-solving activities may have different relationships, such as subactivityof, precede-follow, and different attributes, such as location, condition, API, code. We refer to task-solving activities and their relationships and attributes as know-how knowledge. Programming task know-how knowledge is commonly documented in semi-structured textual tutorials. A formative study of the 20 top-viewed Android-tagged how-to questions on Stack Overflow suggests that developers are faced with three information barriers (incoherent modeling of task intent, tutorial information overload and unstructured task activity description) for effectively discovering and understanding task-solving knowledge in textual tutorials. Knowledge graph has been shown to be effective in representing relational knowledge and supporting knowledge search in a structured way. Unfortunately, existing knowledge graphs extract only know-what information (e.g., APIs, API caveats and API dependencies) from software documentation. In this paper, we devise open information extraction (OpenIE) techniques to extract candidates for task activities, activity attributes and activity relationships from programming task tutorials. The resulting knowledge graph, TaskKG, includes a hierarchical taxonomy of activities, three types of activities relationships and five types of activity attributes, and enables activity-centric knowledge search. As a proof-of-concept, we apply our approach to Android Developer Guide. A comprehensive evaluation of TaskKG shows high accuracy of our OpenIE techniques. A user study shows that TaskKG is promising in helping developers finding correct answers to programming how-to questions. © 2019 IEEE.",Knowledge Graph; Programming Tutorials; Task Search,"257, 268",,"Proceedings - 2019 IEEE International Conference on Software Maintenance and Evolution, ICSME 2019",Conference Paper,Scopus
784,,Cross-project estimation of software development effort using in-house sources and data mining methods - An experiment,"Karna H., Masnov A., Jurko D., Peric T.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075863462&doi=10.23919%2fSOFTCOM.2019.8903752&partnerID=40&md5=277d86bb4c0c704dd110f238e807a5b4,10.23919/SOFTCOM.2019.8903752,"Application of data mining methods is well suited for problems of effort estimation in the field of software engineering. During development process, data mining can provide software engineers with valuable inputs that support decision makings. This way it is possible to overcome the problems caused by erroneous estimates made by the experts. Use of in-house data sources is encouraged because predictive models built on top of them typically provide better estimates compared to the models generated by using data from other sources. Model predictions can either confirm experts statements or propose an alternative solution. This way software development process can become more efficient. In this empirical investigation data from five different software projects originating from the same environment were used to conduct a formal experiment. The experiment uses cross-project estimation in which data sets from other projects are used to build predictive models for the project being estimated. Predictive models implement advanced learners that in general provided more accurate predictions, thus reducing the estimation error. The evaluation of results obtained during data mining process uses established criteria. The organization of the research carried out, distinctive model and structuring of the data together with obtained results encourage the application of similar models in practice. © 2019 University of Split, FESB.",Data mining; Effort estimation; In-house data; Prediction; Software engineering; Software project management,,,"2019 27th International Conference on Software, Telecommunications and Computer Networks, SoftCOM 2019",Conference Paper,Scopus
785,,Towards Standardizing and Improving Classification of Bug-Fix Commits,"Zafar S., Malik M.Z., Walia G.S.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074293829&doi=10.1109%2fESEM.2019.8870174&partnerID=40&md5=390e499b0fda4f631a0aa7130b09e526,10.1109/ESEM.2019.8870174,"Background: Open source software repositories like GitHub are mined to gain useful empirical software engineering insights and answer critical research questions. However, the present state of the art mining approaches suffers from high error rate in the labeling of data that is used for such analysis. This is particularly true when labels are automatically generated from the commit message, and seriously undermines the results of these studies. Aim: Our goal is to label commit comments with high accuracy automatically. In this work, we focus on classifying a commit as a 'Bug-Fix commit' or not. Method: Traditionally, researchers have utilized keyword-based approaches to identify bug fix commits that leads to a significant increase in the error rate. We present an alternative methodology leveraging a deep neural network model called Bidirectional Encoder Representations from Transformers (BERT) that can understand the context of the commit message. We provide the rules for semantic interpretation of commit comments. We construct a hand-labeled dataset from real GitHub commits according to these rules and fine-tune BERT for classification. Results: Our initial evaluation shows that our approach significantly reduces the error rate, with up to 10% relative improvement in classification over keyword-based approaches. Future Direction: We plan on extending our dataset to cover more corner cases and reduce programming language specific biases. We also plan on refining the semantic rules. In this work, we have only considered a simple binary classification problem (Bug-Fix or not), which we plan to extend to other classes and extend the approach to consider multiclass problems. Conclusion: The rules, data, and the model proposed in this paper have the potential to be used by people analyzing open source repositories to improve the labeling of data used in their analysis. © 2019 IEEE.",Human Factors; Mining Software Repositories; Predictive Models; Software Maintenance,,,International Symposium on Empirical Software Engineering and Measurement,Conference Paper,Scopus
786,,An extensive evaluation of ensemble techniques for software change prediction,"Catolino G., Ferrucci F.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070074608&doi=10.1002%2fsmr.2156&partnerID=40&md5=51e62ad6dbffedf34e6dadd380bf4901,10.1002/smr.2156,"Predicting the areas of the source code having a higher likelihood to change in the future represents an important activity to allow developers to plan preventive maintenance operations. For this reason, several change prediction models have been proposed. Moreover, research community demonstrated how different classifiers impact on the performance of devised models as well as classifiers tend to perform similarly even though they are able to correctly predict the change proneness of different code elements, possibly indicating the presence of some complementarity among them. In this paper, we deeper investigated whether the use of ensemble approaches, ie, machine learning techniques able to combine multiple classifiers, can improve the performances of change prediction models. Specifically, we built three change prediction models based on different predictors, ie, product-, process- metrics-, and developer-related factors, comparing the performances of four ensemble techniques (ie, Boosting, Random Forest, Bagging, and Voting) with those of standard machine learning classifiers (ie, Logistic Regression, Naive Bayes, Simple Logistic, and Multilayer Perceptron). The study was conducted on 33 releases of 10 open-source systems, and the results showed how ensemble methods and in particular Random Forest provide a significant improvement of more than 10% in terms of F measure. Indeed, the statistical analyses conducted confirm the superiority of this ensemble technique. Moreover, the model built using developer-related factors performed better than the other models that exploit product and process metrics and achieves an overall median of F measure around 77%. © 2019 John Wiley & Sons, Ltd.",change prediction; empirical study; ensemble techniques; machine learning,,,Journal of Software: Evolution and Process,Conference Paper,Scopus
787,,Projecting Australia's forest cover dynamics and exploring influential factors using deep learning,"Ye L., Gao L., Marcos-Martinez R., Mallants D., Bryan B.A.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069715429&doi=10.1016%2fj.envsoft.2019.07.013&partnerID=40&md5=2c6784b9c6315404c6366ac0ea9322df,10.1016/j.envsoft.2019.07.013,"This study presents the first application of deep learning techniques in capturing long-term, time-continuous forest cover dynamics at a continental scale. We developed a spatially-explicit ensemble model for projecting Australia's forest cover change using Long Short-Term Memory (LSTM) deep learning neural networks applied to a multi-dimensional, high-resolution spatiotemporal dataset and run on a high-performance computing cluster. We further quantified the influence of explanatory variables on the spatiotemporal dynamics of continental forest cover. Deep learning greatly outperformed a state-of-the-art spatial-econometric model at continental, state, and grid-cell scales. For example, at the continental scale, compared to the spatial-econometric model, the deep learning model improved projection performance by 44% (root-mean-square error) and 12% (pseudo R-squared). The results illustrate the robustness and effectiveness of the LSTM model. This work provides a reliable tool for projecting forest cover and agricultural production under given future scenarios, supporting decision-making in sustainable land development, management, and conservation. © 2019 Elsevier Ltd",Deep learning; Deforestation; Forest cover change; Long short-term memory; Projections; Spatiotemporal data,"407, 417",,Environmental Modelling and Software,Article,Scopus
788,,A new optical density granulometry-based descriptor for the classification of prostate histological images using shallow and deep Gaussian processes,"Esteban Á.E., López-Pérez M., Colomer A., Sales M.A., Molina R., Naranjo V.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068771220&doi=10.1016%2fj.cmpb.2019.07.003&partnerID=40&md5=00b62af519cc6a9e9bdc8f790aade82d,10.1016/j.cmpb.2019.07.003,"Background and objective: Prostate cancer is one of the most common male tumors. The increasing use of whole slide digital scanners has led to an enormous interest in the application of machine learning techniques to histopathological image classification. Here we introduce a novel family of morphological descriptors which, extracted in the appropriate image space and combined with shallow and deep Gaussian process based classifiers, improves early prostate cancer diagnosis. Method: We decompose the acquired RGB image in its RGB and optical density hematoxylin and eosin components. Then, we define two novel granulometry-based descriptors which work in both, RGB and optical density, spaces but perform better when used on the latter. In this space they clearly encapsulate knowledge used by pathologists to identify cancer lesions. The obtained features become the inputs to shallow and deep Gaussian process classifiers which achieve an accurate prediction of cancer. Results: We have used a real and unique dataset. The dataset is composed of 60 Whole Slide Images. For a five fold cross validation, shallow and deep Gaussian Processes obtain area under ROC curve values higher than 0.98. They outperform current state of the art patch based shallow classifiers and are very competitive to the best performing deep learning method. Models were also compared on 17 Whole Slide test Images using the FROC curve. With the cost of one false positive, the best performing method, the one layer Gaussian process, identifies 83.87% (sensitivity) of all annotated cancer in the Whole Slide Image. This result corroborates the quality of the extracted features, no more than a layer is needed to achieve excellent generalization results. Conclusion: Two new descriptors to extract morphological features from histological images have been proposed. They collect very relevant information for cancer detection. From these descriptors, shallow and deep Gaussian Processes are capable of extracting the complex structure of prostate histological images. The new space/descriptor/classifier paradigm outperforms state-of-art shallow classifiers. Furthermore, despite being much simpler, it is competitive to state-of-art CNN architectures both on the proposed SICAPv1 database and on an external database. © 2019 Elsevier B.V.",Deep Gaussian processes; Gaussian processes; Granulometries; Histopathological images; Prostate cancer; Variational inference,"303, 317",,Computer Methods and Programs in Biomedicine,Article,Scopus
789,,Application of data clustering for automated feedback generation about student well-being,"Kylväjä M., Kumpulainen P., Konu A.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077819394&doi=10.1145%2f3340435.3342720&partnerID=40&md5=3be2ee6424a3f231b6d4f5f1e2c1b87a,10.1145/3340435.3342720,"Investment in the well-being of today’s schoolchildren is an important investment in the future. We believe that learning does not happen in the absence of well-being. This data-oriented research studies how automation utilizing data analysis algorithms could help provide the students with feedback and guidance about their well-being related issues. We implemented a system that combines data processing methods and research-based knowledge to serve that purpose. Our target was to develop an automated feedback system utilizing information from a large data set collected from well-being surveys from students, as well as research-based well-being knowledge. The system can be used to provide automated feedback for students who answer a well-being survey. © 2019 Association for Computing Machinery",Application; Automation; Clustering; Design Science; Feedback; Hierarchical clustering; Student; Well-being,"21, 26",,"EASEAI 2019 - Proceedings of the 1st ACM SIGSOFT International Workshop on Education through Advanced Software Engineering and Artificial Intelligence, co-located with ESEC/FSE 2019",Conference Paper,Scopus
790,,Assessing and optimizing the performance impact of the just-in-time configuration parameters - a case study on PyPy,"Li Y., Jiang Z.M.J.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062714161&doi=10.1007%2fs10664-019-09691-z&partnerID=40&md5=19b993f22299bb9c84d5bff0cb2391e7,10.1007/s10664-019-09691-z,"Many modern programming languages (e.g., Python, Java, and JavaScript) support just-in-time (JIT) compilation to speed up the execution of a software system. During runtime, the JIT compiler translates the frequently executed part of the system into efficient machine code, which can be executed much faster compared to the default interpreted mode. There are many JIT configuration parameters, which vary based on the programming languages and types of the jitting strategies (method vs. tracing-based). Although there are many existing works trying to improve various aspects of the jitting process, there are very few works which study the performance impact of the JIT configuration settings. In this paper, we performed an empirical study on the performance impact of the JIT configuration settings of PyPy. PyPy is a popular implementation of the Python programming language. Due to PyPy’s efficient JIT compiler, running Python programs under PyPy is usually much faster than other alternative implementations of Python (e.g., cPython, Jython, and IronPython). To motivate the need for tuning PyPy’s JIT configuration settings, we first performed an exploratory study on two microbenchmark suites. Our findings show that systems executed under PyPy’s default JIT configuration setting may not yield the best performance. Optimal JIT configuration settings vary from systems to systems. Larger portions of the code being jitted do not necessarily lead to better performance. To cope with these findings, we developed an automated approach, ESM-MOGA, to tuning the JIT configuration settings. ESM-MOGA, which stands for effect-size measure-based multi-objective genetic algorithm, automatically explores the PyPy’s JIT configuration settings for optimal solutions. Case studies on three open source systems show that systems running under the resulting configuration settings significantly out-perform (5% - 60% improvement in average peak performance) the default configuration settings. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.",Just-in-time compilation; Performance analysis; Performance optimization; Performance testing; Software configuration,"2323, 2363",,Empirical Software Engineering,Article,Scopus
791,,SAR: Learning cross-language API mappings with little knowledge,"Bui N.D.Q., Yu Y., Jiang L.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071903401&doi=10.1145%2f3338906.3338924&partnerID=40&md5=ccf9647fcd933df30e3be0fc38a06f50,10.1145/3338906.3338924,"To save effort, developers often translate programs from one programming language to another, instead of implementing it from scratch. Translating application program interfaces (APIs) used in one language to functionally equivalent ones available in another language is an important aspect of program translation. Existing approaches facilitate the translation by automatically identifying the API mappings across programming languages. However, these approaches still require large amount of parallel corpora, ranging from pairs of APIs or code fragments that are functionally equivalent, to similar code comments. To minimize the need of parallel corpora, this paper aims at an automated approach that can map APIs across languages with much less a priori knowledge than other approaches. The approach is based on an realization of the notion of domain adaption, combined with code embedding, to better align two vector spaces. Taking as input large sets of programs, our approach first generates numeric vector representations of the programs (including the APIs used in each language), and it adapts generative adversarial networks (GAN) to align the vectors in different spaces of two languages. For a better alignment, we initialize the GAN with parameters derived from API mapping seeds that can be identified accurately with a simple automatic signature-based matching heuristic. Then the cross language API mappings can be identified via nearest-neighbors queries in the aligned vector spaces. We have implemented the approach (SAR, named after three main technical components in the approach) in a prototype for mapping APIs across Java and C# programs. Our evaluation on about 2 million Java files and 1 million C# files shows that the approach can achieve 54% and 82% mapping accuracy in its top-1 and top-10 API mapping results with only 174 automatically identified seeds, more accurate than other approaches using the same or much more mapping seeds. © 2019 ACM.",Cross-language API mapping; Domain adaptation; Generative Adversarial Network; Vector space alignment; Word embedding,"796, 806",,ESEC/FSE 2019 - Proceedings of the 2019 27th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,Conference Paper,Scopus
792,,The computer nose best,"Jilani S., Ugail H., Logan A.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081066214&doi=10.1109%2fSKIMA47702.2019.8982474&partnerID=40&md5=1c41430b74144e3c1cb367d61196de7e,10.1109/SKIMA47702.2019.8982474,"The nose is the most central feature on the face which is known to exhibit both gender and ethnic differences. It is a robust feature, invariant to expression and known to contain depth information. In this paper we address the topic of binary ethnicity classificiation from images of the nose, using a novel dataset of South Asian, Pakistani images. To the best of our knowledge, we are one of the first to attempt demographic (ethnicity) based identification based solely on information from the nose.A two-category (Pakistani vs Non-Pakistani) task was used in combination with Deep learning (ResNet) based and VGG-based pre-trained models. A series of experiments were conducted using ResNet-50, ResNet-101, ResNet-152, VGG-Face, VGG-16 and VGG-19, for feature extraction and a Linear Support Vector Machine for classification. The experimental results demonstrate ResNet-50 achieves the highest performance accuracy of 94.1%. In comparison, the highest score for the VGG-based models (VGG-16) was 90.8%. These results demonstrate that information from the nose is sufficient for deep learning models to achieve >90% accuracy on judgements of ethnicity. © 2019 IEEE.",Deep Residual Neural Networks; Ethnicity; Feature Extraction; Human Attributes; Nose; Pakistani Face Database,,,"2019 13th International Conference on Software, Knowledge, Information Management and Applications, SKIMA 2019",Conference Paper,Scopus
793,,Deep learning and cultural heritage: The CEPROQHA project case study,"Belhi A., Gasmi H., Al-Ali A.K., Bouras A., Foufou S., Yu X., Zhang H.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081066163&doi=10.1109%2fSKIMA47702.2019.8982520&partnerID=40&md5=eb73167190ce6b43f44980bcaa3d1bc7,10.1109/SKIMA47702.2019.8982520,"Cultural heritage takes an important part of the history of humankind as it is one of the most powerful tools for the transfer and preservation of moral identity. As a result, these cultural assets are considered highly valuable and sometimes priceless. Digital technologies provided multiple tools that address challenges related to the promotion and information access in the cultural context. However, the large data collections of cultural information have more potential to add value and address current challenges in this context with the recent progress in artificial intelligence (AI) with deep learning and data mining tools. Through the present paper, we investigate several approaches that are used or can potentially be used to promote, curate, preserve and value cultural heritage through new and evolutionary techniques based on deep learning tools. The deep learning approaches entirely developed by our team are intended to classify and annotate cultural data, complete missing data, or map existing data schemes and information to standardized schemes with language processing tools. © 2019 IEEE.",Artificial Intelligence; CEPROQHA Project; Cultural Heritage; Deep Learning; Digital Heritage,,,"2019 13th International Conference on Software, Knowledge, Information Management and Applications, SKIMA 2019",Conference Paper,Scopus
794,,A deep learning approach to tumour identification in fresh frozen tissues,"Ugail H., Alzorgani M., Bukar A., Hussain H., Burn C., Sein T.M., Betmouni S.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081058389&doi=10.1109%2fSKIMA47702.2019.8982508&partnerID=40&md5=d56ba6c65ee4d4a606a33bc603f80811,10.1109/SKIMA47702.2019.8982508,"The demand for pathology services are significantly increasing whilst the numbers of pathologists are significantly decreasing. In order to overcome these challenges, a growing interest in faster and efficient diagnostic methods such as computer-aided diagnosis (CAD) have been observed. An increase in the use of CAD systems in clinical settings has subsequently led to a growing interest in machine learning. In this paper, we show the use of machine learning algorithms in the prediction of tumour content in Fresh Frozen (FF) histological samples of head and neck. More specifically, we explore a pre-trained convolutional neural network (CNN), namely the AlexNet, to build two common machine learning classifiers. For the first classifier, the pre-trained AlexNet network is used to extract features from the activation layer and then Support Vector Machine (SVM) based classifier is trained by using these extracted features. In the second case, we replace the last three layers of the pre-trained AlexNet network and then fine tune these layers on the FF histological image samples. The results of our experiments are very promising. We have obtained percentage classification rates in the high 90s, and our results show there is little difference between SVM and transfer learning. Thus, the present study show that an AlexNet driven CNN with SVM and fine-tuned classifiers are a suitable choice for accurate discrimination between tumour and non-tumour histological samples from the head and neck. © 2019 IEEE.",AlexNet; Convolutional Neural Networks; Deep Learning; Fresh Frozen Tissue Analysis; Histopathological Diagnosis; Tumour Identification,,,"2019 13th International Conference on Software, Knowledge, Information Management and Applications, SKIMA 2019",Conference Paper,Scopus
795,,An automatic cluster-based approach for depth estimation of single 2D images,"Shoukat M.A., Sargano A.B., Habib Z., You L.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081050257&doi=10.1109%2fSKIMA47702.2019.8982472&partnerID=40&md5=d9a5d11b5b19720cd0abc2c3f9e12ea4,10.1109/SKIMA47702.2019.8982472,"In this paper, the problem of single 2D image depth estimation is considered. This is a very important problem due to its various applications in the industry. Previous learning-based methods are based on a key assumption that color images having photometric resemblance are likely to present similar depth structure. However, these methods search the whole dataset for finding corresponding images using handcrafted features, which is quite cumbersome and inefficient process. To overcome this, we have proposed a clustering-based algorithm for depth estimation of a single 2D image using transfer learning. To realize this, images are categorized into clusters using K-means clustering algorithm and features are extracted through a pre-trained deep learning model i.e., ResNet-50. After clustering, an efficient step of replacing feature vector is embedded to speedup the process without compromising on accuracy. After then, images with similar structure as an input image, are retrieved from the best matched cluster based on their correlation values. Then, retrieved candidate depth images are employed to initialize prior depth of a query image using weighted-correlation-average (WCA). Finally, the estimated depth is improved by removing variations using cross-bilateral-filter. In order to evaluate the performance of proposed algorithm, experiments are conducted on two benchmark datasets, NYU v2 and Make3D. © 2019 IEEE.",2D to 3D conversion; Depth estimation; K-means clustering; Transfer learning,,,"2019 13th International Conference on Software, Knowledge, Information Management and Applications, SKIMA 2019",Conference Paper,Scopus
796,,Assay type detection using advanced machine learning algorithms,"Tania M.H., Lwin K.T., Shabut A.M., Abu-Hassan K.J., Kaiser M.S., Hossain M.A.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081048532&doi=10.1109%2fSKIMA47702.2019.8982449&partnerID=40&md5=07c60942d44aee6bd3b89ce374a690c9,10.1109/SKIMA47702.2019.8982449,"The colourimetric analysis has been used in diversified fields for years. This paper provides a unique overview of colourimetric tests from the perspective of computer vision by describing different aspects of a colourimetric test in the context of image processing, followed by an investigation into the development of a colorimetric assay type detection system using advanced machine learning algorithms. To the best of our knowledge, this is the first attempt to define colourimetric assay types from the eyes of a machine and perform any colorimetric test using deep learning. This investigation utilizes the state-of-the-art pre-trained models of Convolutional Neural Network (CNN) to perform the assay type detection of an enzyme-linked immunosorbent assay (ELISA) and lateral flow assay (LFA). The ELISA dataset contains images of both positive and negative samples, prepared for the plasmonic ELISA based TB-antigen specific antibody detection. The LFA dataset contains images of the universal pH indicator paper of eight pH levels. It is noted that the pre-trained models offered 100% accurate visual recognition for the assay type detection. Such detection can assist novice users to initiate a colorimetric test using his/her personal digital devices. The assay type detection can also aid in calibrating an image-based colorimetric classification. © 2019 IEEE.",Colorimetric test; Computer vision; Deep learning; Diagnosis; Point-of-care system; Transfer learning,,,"2019 13th International Conference on Software, Knowledge, Information Management and Applications, SKIMA 2019",Conference Paper,Scopus
797,,Adversarial Examples Generation Approach for Tendency Classification on Chinese Texts [面向中文文本倾向性分类的对抗样本生成方法],"Wang W.-Q., Wang R., Wang L.-N., Tang B.-X.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073416119&doi=10.13328%2fj.cnki.jos.005765&partnerID=40&md5=98b400d91482da182af537d87aa947fd,10.13328/j.cnki.jos.005765,"Studies have shown that the adversarial example attack is that small perturbations are added on the input to make deep neural network (DNN) misbehave. Meanwhile, these attacks also exist in Chinese text sentiment orientation classification based on DNN and a method ""WordHandling"" is proposed to generate this kind of adversarial examples. This method designs a new algorithm aiming at calculating important words. Then the words are replaced with homonym to generate adversarial examples, which are used to conduct an adversarial example attack in black-box scenario. This study also verifies the effectiveness of the proposed method with real data set, i.e. Jingdong shopping and Ctrip hotel review, on long short-term memory network (LSTM) and convolutional neural network (CNN). The experimental results show that the adversarial examples in this study can mislead Chinese text orientation detection system well. © Copyright 2019, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Adversarial examples; Black box; Chinese text; Deep learning models; Score function,"2415, 2427",,Ruan Jian Xue Bao/Journal of Software,Article,Scopus
798,,Computer-aided diagnosis of endobronchial ultrasound images using convolutional neural network,"Chen C.-H., Lee Y.-W., Huang Y.-S., Lan W.-R., Chang R.-F., Tu C.-Y., Chen C.-Y., Liao W.-C.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066448694&doi=10.1016%2fj.cmpb.2019.05.020&partnerID=40&md5=4636dcc8a41deb736c29d44a6b5ffcb4,10.1016/j.cmpb.2019.05.020,"Background and objective: In the United States, lung cancer is the leading cause of cancer death. The survival rate could increase by early detection. In recent years, the endobronchial ultrasonography (EBUS)images have been utilized to differentiate between benign and malignant lesions and guide transbronchial needle aspiration because it is real-time, radiation-free and has better performance. However, the diagnosis depends on the subjective judgment from doctors. In some previous studies, which using the grayscale image textures of the EBUS images to classify the lung lesions but it belonged to semi-automated system which still need the experts to select a part of the lesion first. Therefore, the main purpose of this study was to achieve full automation assistance by using convolution neural network. Methods: First of all, the EBUS images resized to the input size of convolution neural network (CNN). And then, the training data were rotated and flipped. The parameters of the model trained with ImageNet previously were transferred to the CaffeNet used to classify the lung lesions. And then, the parameter of the CaffeNet was optimized by the EBUS training data. The features with 4096 dimension were extracted from the 7th fully connected layer and the support vector machine (SVM)was utilized to differentiate benign and malignant. This study was validated with 164 cases including 56 benign and 108 malignant. Results: According to the experiment results, applying the classification by the features from the CNN with transfer learning had better performance than the conventional method with gray level co-occurrence matrix (GLCM)features. The accuracy, sensitivity, specificity, and the area under ROC achieved 85.4% (140/164), 87.0% (94/108), 82.1% (46/56), and 0.8705, respectively. Conclusions: From the experiment results, it has potential ability to diagnose EBUS images with CNN. © 2019 Elsevier B.V.",Computer-aided diagnosis (CAD); Convolutional neural network (CNN); Endobronchial ultrasound images (EBUS); Lung cancer; Transfer learning,"175, 182",,Computer Methods and Programs in Biomedicine,Article,Scopus
799,,An empirical evaluation of deep learning for ICD-9 code assignment using MIMIC-III clinical notes,"Huang J., Osorio C., Sy L.W.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066271656&doi=10.1016%2fj.cmpb.2019.05.024&partnerID=40&md5=32c4d633e5779b5cce270aa82b2bc9ab,10.1016/j.cmpb.2019.05.024,"Background and Objective: Code assignment is of paramount importance in many levels in modern hospitals, from ensuring accurate billing process to creating a valid record of patient care history. However, the coding process is tedious and subjective, and it requires medical coders with extensive training. This study aims to evaluate the performance of deep-learning-based systems to automatically map clinical notes to ICD-9 medical codes. Methods: The evaluations of this research are focused on end-to-end learning methods without manually defined rules. Traditional machine learning algorithms, as well as state-of-the-art deep learning methods such as Recurrent Neural Networks and Convolution Neural Networks, were applied to the Medical Information Mart for Intensive Care (MIMIC-III) dataset. An extensive number of experiments was applied to different settings of the tested algorithm. Results: Findings showed that the deep learning-based methods outperformed other conventional machine learning methods. From our assessment, the best models could predict the top 10 ICD-9 codes with 0.6957 F1 and 0.8967 accuracy and could estimate the top 10 ICD-9 categories with 0.7233 F1 and 0.8588 accuracy. Our implementation also outperformed existing work under certain evaluation metrics. Conclusion: A set of standard metrics was utilized in assessing the performance of ICD-9 code assignment on MIMIC-III dataset. All the developed evaluation tools and resources are available online, which can be used as a baseline for further research. © 2019 Elsevier B.V.",Clinical notes; CNNs; Code assignment; Deep learning; ICD-9; Machine learning; Medical codes; MIMIC-III; RNNs,"141, 153",,Computer Methods and Programs in Biomedicine,Article,Scopus
800,,Brain tumor detection using statistical and machine learning method,"Amin J., Sharif M., Raza M., Saba T., Anjum M.A.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065866193&doi=10.1016%2fj.cmpb.2019.05.015&partnerID=40&md5=2b3b8f2972e9b141df3fffca37093546,10.1016/j.cmpb.2019.05.015,"Background and Objective: Brain tumor occurs because of anomalous development of cells. It is one of the major reasons of death in adults around the globe. Millions of deaths can be prevented through early detection of brain tumor. Earlier brain tumor detection using Magnetic Resonance Imaging (MRI) may increase patient's survival rate. In MRI, tumor is shown more clearly that helps in the process of further treatment. This work aims to detect tumor at an early phase. Methods: In this manuscript, Weiner filter with different wavelet bands is used to de-noise and enhance the input slices. Subsets of tumor pixels are found with Potential Field (PF) clustering. Furthermore, global threshold and different mathematical morphology operations are used to isolate the tumor region in Fluid Attenuated Inversion Recovery (Flair) and T2 MRI. For accurate classification, Local Binary Pattern (LBP) and Gabor Wavelet Transform (GWT) features are fused. Results: The proposed approach is evaluated in terms of peak signal to noise ratio (PSNR), mean squared error (MSE) and structured similarity index (SSIM) yielding results as 76.38, 0.037 and 0.98 on T2 and 76.2, 0.039 and 0.98 on Flair respectively. The segmentation results have been evaluated based on pixels, individual features and fused features. At pixels level, the comparison of proposed approach is done with ground truth slices and also validated in terms of foreground (FG) pixels, background (BG) pixels, error region (ER) and pixel quality (Q). The approach achieved 0.93 FG and 0.98 BG precision and 0.010 ER on a local dataset. On multimodal brain tumor segmentation challenge dataset BRATS 2013, 0.93 FG and 0.99 BG precision and 0.005 ER are acquired. Similarly on BRATS 2015, 0.97 FG and 0.98 BG precision and 0.015 ER are obtained. In terms of quality, the average Q value and deviation are 0.88 and 0.017. At the fused feature based level, specificity, sensitivity, accuracy, area under the curve (AUC) and dice similarity coefficient (DSC) are 1.00, 0.92, 0.93, 0.96 and 0.96 on BRATS 2013, 0.90, 1.00, 0.97, 0.98 and 0.98 on BRATS 2015 and 0.90, 0.91, 0.90, 0.77 and 0.95 on local dataset respectively. Conclusion: The presented approach outperformed as compared to existing approaches. © 2019",Fused features; LBP; PF clustering; Pixel based results; Weiner Filter,"69, 79",,Computer Methods and Programs in Biomedicine,Article,Scopus
801,,“Bad smells” in software analytics papers,"Menzies T., Shepperd M.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064913389&doi=10.1016%2fj.infsof.2019.04.005&partnerID=40&md5=e901827dae373dc5bf64cff32257d8c8,10.1016/j.infsof.2019.04.005,"Context: There has been a rapid growth in the use of data analytics to underpin evidence-based software engineering. However the combination of complex techniques, diverse reporting standards and poorly understood underlying phenomena are causing some concern as to the reliability of studies. Objective: Our goal is to provide guidance for producers and consumers of software analytics studies (computational experiments and correlation studies). Method: We propose using “bad smells”, i.e., surface indications of deeper problems and popular in the agile software community and consider how they may be manifest in software analytics studies. Results: We list 12 “bad smells” in software analytics papers (and show their impact by examples). Conclusions: We believe the metaphor of bad smell is a useful device. Therefore we encourage more debate on what contributes to the validity of software analytics studies (so we expect our list will mature over time). © 2019",,"35, 47",,Information and Software Technology,Article,Scopus
802,,TSTSS: A two-stage training subset selection framework for cross version defect prediction,"Xu Z., Li S., Luo X., Liu J., Zhang T., Tang Y., Xu J., Yuan P., Keung J.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064625005&doi=10.1016%2fj.jss.2019.03.027&partnerID=40&md5=97b8cc1da74232c5f5b4cb4008f434db,10.1016/j.jss.2019.03.027,"Cross Version Defect Prediction (CVDP) is a practical scenario by training the classification model on the historical data of the prior version and then predicting the defect labels of modules in the current version. Unfortunately, the differences of data distribution across versions may hinder the effectiveness of the trained CVDP model. Thus, it is not trivial to select a suitable training subset from the prior version to promote the CVDP performance. In this paper, we propose a novel method, called Two-Stage Training Subset Selection (TSTSS), to address this challenging issue. In the first stage, TSTSS utilizes a sparse modeling representative selection method to select an initial module subset from the prior version which can well reconstruct the data of the prior version. In the second stage, TSTSS leverages a dissimilarity-based sparse subset selection method to further refine the selected module subset, which enables the selected modules to well represent the modules of the current version. Finally, we use a novel weighted extreme learning machine classifier to construct the CVDP model. We evaluate the CVDP performance of TSTSS on 50 cross-version pairs using 6 indicators. The experiments show that TSTSS can efficiently improve the CVDP performance compared with 11 baseline methods. © 2019",Cross version defect prediction; Spare modeling; Training subset selection; Weighted extreme learning machine,"59, 78",,Journal of Systems and Software,Article,Scopus
803,,A cost-effective strategy for software vulnerability prediction based on bellwether analysis*,"Kudjo P.K., Chen J.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070571753&doi=10.1145%2f3293882.3338985&partnerID=40&md5=54ef4a04d51ab322dcb1137aa6e7bd3e,10.1145/3293882.3338985,"Vulnerability Prediction Models (VPMs) aims to identify vulnerable and non-vulnerable components in large software systems. Consequently, VPMs presents three major drawbacks (i) finding an effective method to identify a representative set of features from which to construct an effective model. (ii) the way the features are utilized in the machine learning setup (iii) making an implicit assumption that parameter optimization would not change the outcome of VPMs. To address these limitations, we investigate the significant effect of the Bellwether analysis on VPMs. Specifically, we first develop a Bellwether algorithm to identify and select an exemplary subset of data to be considered as the Bellwether to yield improved prediction accuracy against the growing portfolio benchmark. Next, we build a machine learning approach with different parameter settings to show the improvement of performance of VPMs. The prediction results of the suggested models were assessed in terms of precision, recall, F-measure, and other statistical measures. The preliminary result shows the Bellwether approach outperforms the benchmark technique across the applications studied with F-measure values ranging from 51.1% - 98.5%. © 2019 Copyright held by the owner/author(s). Publication rights licensed to the",,"424, 427",,ISSTA 2019 - Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis,Conference Paper,Scopus
804,,Motor Bearing Fault Diagnosis Based on Deep Learning,"Zhang W., Hu Y., Zeng D., Luo W., Li G., Liu M.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077964392&doi=10.1109%2fSNPD.2019.8935673&partnerID=40&md5=494cdea9c58d840fb182cc6394982992,10.1109/SNPD.2019.8935673,"The effective fault monitoring of the motor bearings not only can ensure the smooth and efficient operation of equipment, but also can detect and eliminate the running fault in time to prevent major accidents. Based on deep learning algorithm, this paper constructs a stacked auto-encoder (SAE) network. The input data are compressed by introduced sparsity constraint, so that the network can accurately extract the fault characteristics of the input data. And the fault recognition ability of the network can be improved by introducing random noise to reconstruct input data. The simulation result shows that the SAE network can not only overcome the shortcomings of traditional fault diagnosis methods that requires personnel to distinguish fault samples and needs a large number of prior knowledge; but also realize the self-learning of fault feature information. The accuracy rates of fault identification reach 98%, 94%, 96% and 95.5% in four different working conditions. What's more, the network can adapt to the actual multi-load cases, demonstrate strong robustness under different working conditions. © 2019 IEEE.",data reconstruction; deep learning; fault diagnosis; sparsity constraint; stacked auto-encoder,"8, 14",,"Proceedings - 20th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing, SNPD 2019",Conference Paper,Scopus
805,,Assets Predictive Maintenance Using Convolutional Neural Networks,"Silva W., Capretz M.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077955092&doi=10.1109%2fSNPD.2019.8935752&partnerID=40&md5=3b55f509d06a6832fd6f6a83f8cc74b7,10.1109/SNPD.2019.8935752,"Predictive Maintenance (PdM) performs maintenance based on the asset's health status indicators. Sensors can measure an unusual pattern of these indicators, such as an increased motor's vibration level or higher energy consumption, and, in most cases, failures are preceded by an unusual pattern of these measurements. Convolutional Neural Network (CNN) is a Machine Learning technique capable of extracting data representation. This paper presents a CNN framework to tackle assets predictive maintenance problem and a method to transform 1-dimensional (1-D) data into an image-like representation (2-D). A data transformation step is very important to make the use of CNN feasible. To evaluate the proposed framework two datasets were obtained from fans, with distinct electrical pattern, from a building at Western University. The data was preprocessed, transformed in a image-like representation and fed to a tuned classifier. The results presented by the CNN-PdM framework showed that the combination of CNN with the proposed data transformation method outperformed traditional machine learning techniques (Random Forest, Support Vector Machine, and Multi-Layer Perceptron). The model created by the CNN-PdM framework achieved accuracy rates as high as 98% for one of the datasets and 95% for the other. © 2019 IEEE.",Convolutional Neural Networks; Machine Learning; Predictive Maintenance,"59, 66",,"Proceedings - 20th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing, SNPD 2019",Conference Paper,Scopus
806,,Information Extraction based on Named Entity for Tourism Corpus,"Chantrapornchai C., Tunsakul A.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074236490&doi=10.1109%2fJCSSE.2019.8864166&partnerID=40&md5=729c3673e48ca32ceb9ed583725318e6,10.1109/JCSSE.2019.8864166,"Tourism information is scattered around nowadays. To search for the information, it is usually time consuming to browse through the results from search engine, select and view the details of each accommodation. In this paper, we present a methodology to extract particular information from full text returned from the search engine to facilitate the users. The approach is based on name entity recognition (NER). The main steps are 1) building training data and 2) building the model. The key task is the building training data: First, the tourism data are gathered and the vocabularies are built. Several minor steps include sentence extraction, relation and name entity extraction for tagging purpose. Then, the recognition model of a given entity type can be built. From the experiments, given hotel description, the model can extract the desired entity,i.e, name, location, facility as well as relation type. The extracted data can further be stored as a structured information, e.g., in the ontology format, for future querying and inference. The model for automatic named entity identification, based on machine learning, yields the error ranging 5%-25%. © 2019 IEEE.",,"187, 192",,JCSSE 2019 - 16th International Joint Conference on Computer Science and Software Engineering: Knowledge Evolution Towards Singularity of Man-Machine Intelligence,Conference Paper,Scopus
807,,Natural Language Contents Evaluation System for Detecting Fake News using Deep Learning,"Ahn Y.-C., Jeong C.-S.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074227967&doi=10.1109%2fJCSSE.2019.8864171&partnerID=40&md5=fd5c1115e4e30f0a7b521cef710fc062,10.1109/JCSSE.2019.8864171,"This Recently, a lot of information is spreading rapidly on SNS. Inaccurate communication of news media includes fears about unreliable sources and fake news that lacks confirmation of facts. Fake news is spread through SNS, causing social confusion and further economic loss. The purpose of the news is accurate information transmission. In this regard, it is very important to judge the discrepancies in the contents of the text and the distorted reports. We try to solve the problem of judging whether the sentence to be verified is correct after collecting the facts. This paper defines the problem of extracting the related sentences from the input sentence in Fact Data Corpus which is assumed to be fact and judging whether the extracted sentence and the input sentence are true or false. In the various NLP tasks, we create a Korean-specific pre-Training model using state-of-The-Art BERT. Using this model, fine-Tuning is performed to match the data set detected by Korean fake news. The AUROC score of 83.8% is derived from the test set generated using the fine-Tuned model. © 2019 IEEE.",BERT; Fake news Detection; NLP,"289, 292",,JCSSE 2019 - 16th International Joint Conference on Computer Science and Software Engineering: Knowledge Evolution Towards Singularity of Man-Machine Intelligence,Conference Paper,Scopus
808,,Convolutional Neural Networks Using MobileNet for Skin Lesion Classification,"Sae-Lim W., Wettayaprasit W., Aiyarak P.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074222224&doi=10.1109%2fJCSSE.2019.8864155&partnerID=40&md5=675792f5d7b7b70fcca88e5517042cb7,10.1109/JCSSE.2019.8864155,"Skin lesion classification is a particular interesting area of research in dermatoscopic lesion image processing. In this paper, we present a skin lesion classification approach based on the light weight deep Convolutional Neural Networks (CNNs), called MobileNet. We employed MobileNet and proposed the modified MobileNet for skin lesion classification. For the evaluation of our model, we had used the official dataset of Human Against Machine with 10,000 training images (HAM 10000) which was a collection of multisource dermatoscopic images. Data up-sampling and data augmentation method were used in our study for improving the efficiency of the classifier. The comparison results showed that our modified model had achieved higher accuracy, specificity, sensitivity, and F1-score than the traditional MobileNet. © 2019 IEEE.",convolutional neural networks; deep neural networks; image processing; MobileNet; skin lesion classification,"242, 247",,JCSSE 2019 - 16th International Joint Conference on Computer Science and Software Engineering: Knowledge Evolution Towards Singularity of Man-Machine Intelligence,Conference Paper,Scopus
809,,Classification of nutrient deficiency in black gram using deep convolutional neural networks,"Han K.A.M., Watchareeruetai U.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074221928&doi=10.1109%2fJCSSE.2019.8864224&partnerID=40&md5=2e46c122b77dd1831994ecafb964e16e,10.1109/JCSSE.2019.8864224,"This paper investigates the use of various deep convolutional neural networks (CNNs) with transfer learning to identify nutrient deficiencies from a leaf image. Experiments were conducted with a dataset containing 4,088 images of black gram (Vigna mungo) leaves grown under seven different treatments, i.e., complete nutrient treatment and six nutrient deficiency treatment, including calcium (Ca), iron (Fe), magnesium (Mg), nitrogen (N), potassium (K), and phosphorus (P) deficiencies. Experimental results indicate that a deep CNN model known as ResNet50 was the best among all experimented models with a test accuracy of 65.44% and a F-measure of 66.15%. In addition, We found that the ResNet50 model obviously outperformed a block-based method and the human performance reported in a literature. © 2019 IEEE.",black gram; convolutional neural network; deep learning; pretrained network; transfer learning,"277, 282",,JCSSE 2019 - 16th International Joint Conference on Computer Science and Software Engineering: Knowledge Evolution Towards Singularity of Man-Machine Intelligence,Conference Paper,Scopus
810,,Restricted Boltzmann Machines: A Review [受限玻尔兹曼机研究综述],"Zhang J., Ding S.-F., Zhang N., Du P., Du W., Yu W.-J.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073878063&doi=10.13328%2fj.cnki.jos.005840&partnerID=40&md5=172b646fb529ca5b7cb2b7fd4deb555c,10.13328/j.cnki.jos.005840,"The Probabilistic graph is a research hotspot in machine learning at present. Generative models based on probabilistic graphs model have been widely used in image generation and speech processing. The restricted Boltzmann machines (RBMs) is a probabilistic undirected graph, which has important research value in modeling data distribution. On the one hand, the RBMs model can be used to construct deep neural network, and on the other hand, it can provide statistical support of deep nets. This paper mainly summarizes the related research of RBMs based probability graph model and their applications in image recognition. Firstly, this paper introduces the basic concepts and training algorithms of RBMs. Secondly, this paper summarizes the applications of RBMs in deep learning; and then, this paper discusses existing problems in research of neural nets and RBMs. Finally, this paper gives a summary and prospect of the research on the RBMs. © Copyright 2019, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Deep learning; Neural net; Probabilistic undirected graph; Restricted Boltzmann machine,"2073, 2090",,Ruan Jian Xue Bao/Journal of Software,Review,Scopus
811,,MVSE: Effort-Aware Heterogeneous Defect Prediction via Multiple-View Spectral Embedding,"Xu Z., Ye S., Zhang T., Xia Z., Pang S., Wang Y., Tang Y.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073802117&doi=10.1109%2fQRS.2019.00015&partnerID=40&md5=b60f7ecc56ca65bfd3744dfff589299b,10.1109/QRS.2019.00015,"Cross-Project Defect Prediction (CPDP) predicts defects in a target project using the defect information of the external project. Existing CPDP methods assume that the data of two projects share identical features. When cross-project data contain heterogeneous features, traditional CPDP methods become ineffective. In this paper, we propose a novel approach called Multiple-View Spectral Embedding (MVSE) to address the heterogeneous CPDP issue. MVSE treats the cross-project data as two different views and exploits the spectral embedding method to map the heterogeneous feature sets into a consistent space where the two mapped feature sets have maximal similarity. To evaluate MVSE in the realistic setting, we employ an effort-Aware performance indicator that considers the cost of inspection in the context of heterogeneous CPDP scenario. We have conducted extensive experiments to compare MVSE with two state-of-The-Art heterogeneous CPDP methods and within-project setting. The experiments on 94 cross project pairs show that MVSE achieves promising results. © 2019 IEEE.",effort-Aware evaluation; heterogeneous cross-project defect prediction; multiple-view learning; spectral embedding,"10, 17",,"Proceedings - 19th IEEE International Conference on Software Quality, Reliability and Security, QRS 2019",Conference Paper,Scopus
812,,An Exploratory Study on Judicial Image Quality Assessment Based on Deep Learning,"Gu Q., Cai W., Yu S., Chen Z.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073801938&doi=10.1109%2fQRS.2019.00046&partnerID=40&md5=3fbde74283adc292031c7071474a3271,10.1109/QRS.2019.00046,"Images are important judicial materials. With the deepening of intelligent systems in the judicial area, image quality plays a vital role in the result of many judicial applications. This paper firstly introduces deep learning into judicial image quality assessment. Pre-Trained convolutional neural network (CNN) models are fine-Tuned and then used to extract image features. Based on the features extracted from CNN models, we convert them into specific numbers representing the quality. A preliminary experiment has been designed and conducted on three types of judicial images. The experimental results show that our approach can outperform the existing image processing technique. Images used as investigation materials are more distinctive than the other two types, and they need an independent model for analyzing. © 2019 IEEE.",Deep Learning; Image Quality Assessment; Judicial Image,"300, 305",,"Proceedings - 19th IEEE International Conference on Software Quality, Reliability and Security, QRS 2019",Conference Paper,Scopus
813,,A Transfer Learning Based Interpretable User Experience Model on Small Samples,"Yu Q., Che X., Yang Y., Wang L.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073784123&doi=10.1109%2fQRS.2019.00035&partnerID=40&md5=a24ca57155b72661f0fa41544155c3ab,10.1109/QRS.2019.00035,"User experience (UX) is a key factor that affects software survival time. A rich line of research has studied the relationships between UX and software factors to modify software and improve user satisfaction. However, the existing machine learning models for predicting UX on small data set is not accurate enough, and research with traditional statistical methods only obtained indistinct relations among UX, user characteristics and software factors. With the goal of improving the accuracy of UX model and obtaining sufficient UX relationships, we propose Transfer in Cart (TrCart) algorithm and Transfer Adaboost in Cart (TrAdaBoostCart) algorithm. To verify this approach, we present the UX study on a desktop game and an android game. According to the experimental results, we find that the TrAdaBoostCart has better accuracy and interpretable results. Hence, the proposed approach provides important guidelines for the design process of mobile applications. © 2019 IEEE.",decision tree; mobile application; transfer learning; user characteristics; user experience,"186, 196",,"Proceedings - 19th IEEE International Conference on Software Quality, Reliability and Security, QRS 2019",Conference Paper,Scopus
814,,Assessing the significant impact of concept drift in software defect prediction,"Kabir M.A., Keung J.W., Benniny K.E., Zhang M.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072713940&doi=10.1109%2fCOMPSAC.2019.00017&partnerID=40&md5=37059993a46c9cdb56cf75ab5037ec61,10.1109/COMPSAC.2019.00017,"Concept drift is a known phenomenon in software data analytics. It refers to the changes in the data distribution over time. The performance of analytic and prediction models degrades due to the changes in the data over time. To improve prediction performance, most studies propose that the prediction model be updated when concept drift occurs. In this work, we investigate the existence of concept drift and its associated effects on software defect prediction performance. We adopt the strategy of an empirically proven method DDM (Drift Detection Method) and evaluate its statistical significance using the chi-square test with Yates continuity correction. The objective is to empirically determine the concept drift and to calibrate the base model accordingly. The empirical study indicates that the concept drift occurs in software defect datasets, and its existence subsequently degrades the performance of prediction models. Two types of concept drifts (gradual and sudden drifts) were identified using the chi-square test with Yates continuity correction in the software defect datasets studied. We suggest concept drift should be considered by software quality assurance teams when building prediction models. © 2019 IEEE.",Concept drift detection; Defect prediction; Empirical software engineering; Software quality; Streaming data,"53, 58",,Proceedings - International Computer Software and Applications Conference,Conference Paper,Scopus
815,,"Proceedings - 2019 IEEE 43rd Annual Computer Software and Applications Conference, COMPSAC 2019",[No author name available],2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072710187&partnerID=40&md5=b5abdbe44bb5099b4169bf6a03fd6b62,,The proceedings contain 266 papers. The topics discussed include: precise and robust detection of advertising fraud; dockerfile TF smell detection based on dynamic and static analysis methods; using gamification to motivate occupants to energy efficiency in a social setting of a building automation system; view-adaptive weighted deep transfer learning for distributed time-series classification; context aware image annotation in active learning with batch mode; the SAMBA approach for self-adaptive model-based online testing of services orchestrations; bi-dimensional representation of patients for diagnosis prediction; formal verification of blockchain smart contract based on colored Petri net models; SpacePhaser: phase space embedding visual analytics; and fuzzy value-at-risk forecasts using a novel data-driven neuro volatility predictive model.,,"991, nan",,Proceedings - International Computer Software and Applications Conference,Conference Review,Scopus
816,,View-adaptive weighted deep transfer learning for distributed time-series classification,"Bhattacharjee S.D., Tolone W.J., Mahabal A., Elshambakey M., Cho I., Djorgovski G.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072704754&doi=10.1109%2fCOMPSAC.2019.00061&partnerID=40&md5=f11930b7f1211f7fe02114dd2e347f01,10.1109/COMPSAC.2019.00061,"In this paper, we propose an effective, multi-view, deep, transfer learning framework for multivariate time-series data. Though widely used for tasks such as computer vision, the application of transfer learning to time-series classification problems (e.g., classification of light curves) is underexplored. The proposed framework makes several important contributions to facilitate knowledge sharing, while simultaneously ensuring an effective solution for domain specific fine-level categorizations. First, in contrast to the traditional approaches, the proposed framework describes pairwise view similarity by identifying a smaller subset of source-view samples that closely resemble the target data patterns. Second, by means of two-phase learning, a generic baseline model is learned on a larger source data collection and later fine-tuned on a smaller target data collection, precisely approximating the target data patterns. Third, an effective view-adaptive timestamp weighting scheme evaluates the relative importance of each timestamp in a more data-driven manner, which enables a more flexible yet discriminative feature representation scheme in the presence of evolving data characteristics. As shown by experiments, compared to the existing approaches, our proposed deep transfer learning framework improves classification performance by around 2-3% in the UCI multi-view activity recognition dataset, while also showing a robust, generalized representation capacity in classifying several large-scale multi-view light curve collections. © 2019 IEEE.",Deep Learning; Distributed Time-Series Analysi; LSTM; Minimum Description Length; Mult-iview Classification; RNN; Transfer Learning,"373, 381",,Proceedings - International Computer Software and Applications Conference,Conference Paper,Scopus
817,,Chinese social media entity linking based on effective context with topic semantics,"Ma C., Sha Y., Tan J., Guo L., Peng H.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072694474&doi=10.1109%2fCOMPSAC.2019.00063&partnerID=40&md5=4277947ce133787e733fc2599762e4e6,10.1109/COMPSAC.2019.00063,"On social media, entity linking is very important for natural language processing tasks, such as Sentiment Analysis, Question Answering (QA) and Machine Translation. Compared to English-oriented entity linking, Chinese entity linking has its special difficulties. Just like the entity linking for short text, Chinese microblogs have lots of noise and the mention lacks effective context information. In order to solve these problems, we present a new model for Chinese microblogs entity linking. Entity linking usually includes two steps: candidate entities generation and candidate entities ranking. First, based on the characteristics of Chinese, we put forward multi-method fusion strategies for candidate generation to improve the recall rate of candidate entities. Second, we propose a new neural network model called TAS (Topic attention Siamese) for candidate entities ranking. In TAS model, we add effective topic semantics on Siamese network to learn representations of context, mention and entity, and rank the mention-entity similarity. The representation of mention incorporates information from multiple sentences on the same topic, which can effectively solve the problem of the lack of contextual information. We also use Character-enhanced Word Embedding model (CWE) to pre-train both word embedding and characters embedding to work out noise and word segmentation impact. Experimental results demonstrate that our method significantly outperforms the state-of-the-art results for entity linking on Chinese social media. © 2019 IEEE.",Chinese microblogs; Effective contexts; Entity linking; Siamese network; Topic semantics,"386, 395",,Proceedings - International Computer Software and Applications Conference,Conference Paper,Scopus
818,,A scalable framework for multilevel streaming data analytics using deep learning,"Ge S., Isah H., Zulkernine F., Khan S.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072677284&doi=10.1109%2fCOMPSAC.2019.10205&partnerID=40&md5=3c7b971f5300f84747d34cca43e9f1c3,10.1109/COMPSAC.2019.10205,"The rapid growth of data in velocity, volume, value, variety, and veracity has enabled exciting new opportunities and presented big challenges for businesses of all types. Recently, there has been considerable interest in developing systems for processing continuous data streams with the increasing need for real-time analytics for decision support in the business, healthcare, manufacturing, and security. The analytics of streaming data usually relies on the output of offline analytics on static or archived data. However, businesses and organizations like our industry partner Gnowit, strive to provide their customers with real time market information and continuously look for a unified analytics framework that can integrate both streaming and offline analytics in a seamless fashion to extract knowledge from large volumes of hybrid streaming data. We present our study on designing a multilevel streaming text data analytics framework by comparing leading edge scalable open-source, distributed, and in-memory technologies. We demonstrate the functionality of the framework for a use case of multilevel text analytics using deep learning for language understanding and sentiment analysis including data indexing and query processing. Our framework combines Spark streaming for real time text processing, the Long Short Term Memory (LSTM) deep learning model for higher level sentiment analysis, and other tools for SQL-based analytical processing to provide a scalable solution for multilevel streaming text analytics. © 2019 IEEE",Deep learning; Natural language processing; News media; Sentiment analysis; Unstructured data,"189, 194",,Proceedings - International Computer Software and Applications Conference,Conference Paper,Scopus
819,,Automatic multi-class non-functional software requirements classification using neural networks,"Baker C., Deng L., Chakraborty S., Dehlinger J.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072673024&doi=10.1109%2fCOMPSAC.2019.10275&partnerID=40&md5=919c36bdf795686b3ca74f52ac935366,10.1109/COMPSAC.2019.10275,"Advances in machine learning (ML) algorithms, graphics processing units, and readily available ML libraries have enabled the application of ML to open software engineering challenges. Yet, the use of ML to enable decision-making during the software engineering lifecycle is not well understood as there are various ML models requiring parameter tuning. In this paper, we leverage ML techniques to develop an effective approach to classify software requirements. Specifically, we investigate the design and application of two types of neural network models, an artificial neural network (ANN) and a convolutional neural network (CNN), to classify non-functional requirements (NFRs) into the following five categories: maintainability, operability, performance, security and usability. We illustrate and experimentally evaluate this work through two widely used datasets consisting of nearly 1,000 NFRs. Our results indicate that our CNN model can effectively classify NFRs by achieving precision ranging between 82% and 94%, recall ranging between 76% and 97% with an F-score ranging between 82% and 92%. © 2019 IEEE",Machine learning; Non-functional requirements; Requirements engineering,"610, 615",,Proceedings - International Computer Software and Applications Conference,Conference Paper,Scopus
820,,"Proceedings - 2019 IEEE 43rd Annual Computer Software and Applications Conference, COMPSAC 2019",[No author name available],2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072663199&partnerID=40&md5=954628725faa5e46394b3a99d8a6d817,,The proceedings contain 266 papers. The topics discussed include: precise and robust detection of advertising fraud; dockerfile TF smell detection based on dynamic and static analysis methods; using gamification to motivate occupants to energy efficiency in a social setting of a building automation system; view-adaptive weighted deep transfer learning for distributed time-series classification; context aware image annotation in active learning with batch mode; the SAMBA approach for self-adaptive model-based online testing of services orchestrations; bi-dimensional representation of patients for diagnosis prediction; formal verification of blockchain smart contract based on colored Petri net models; SpacePhaser: phase space embedding visual analytics; and fuzzy value-at-risk forecasts using a novel data-driven neuro volatility predictive model.,,,1709.0,Proceedings - International Computer Software and Applications Conference,Conference Review,Scopus
821,,Feature boosting in natural image classification,"Saxena P., Saxena D., Nie X., Helmers A., Ramachandran N., Sakib N., Ahamed S.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072656182&doi=10.1109%2fCOMPSAC.2019.10184&partnerID=40&md5=9307fe2cdc888a9a8731ecd90e18468d,10.1109/COMPSAC.2019.10184,"Computer Vision has become the poster child for Deep Learning. The image classification accuracy of convolutional neural nets on benchmark data sets has increased every year since their inception. This has been aided with advances in feature fusion. The increase in the availability of image-text occurrence has lead to text augmented feature spaces that have lead to higher accuracy in in image classification tasks. However, these works are limited to instances where text is readily available. This study presents an approach to featurize text within natural images with the goal of augmenting image features for image classification tasks. Text extraction and featurization in natural images is a challenging task due to challenges in reliable text localization and OCR results, both being impeded by the variability in image text and errors in OCR. We overcome these challenges by implementing a novel bounding box concatenation algorithm and a novel feature boosting algorithm. The result is a pipeline that encodes an image into a text feature space. Classifiers trained on the text based feature space have comparable accuracy to the state of the art Convolutional Neural Nets (CNN’s) while being significantly inexpensive computationally. Moreover, the augmentation of text features to image features generates a hybrid feature space with a higher information content for a classification problem when compared to a feature space comprised exclusively of image features. Thus, we see a rise in classification accuracy across all state of the art machine learning algorithms. © 2019 IEEE",Computer vision; Feature space augmentation; Index Terms—deep learning; Transfer learning,"61, 67",,Proceedings - International Computer Software and Applications Conference,Conference Paper,Scopus
822,,Segmenting brain tumors from FLAIR MRI using fully convolutional neural networks,"Ribalta Lorenzo P., Nalepa J., Bobek-Billewicz B., Wawrzyniak P., Mrukwa G., Kawulok M., Ulrych P., Hayball M.P.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065640821&doi=10.1016%2fj.cmpb.2019.05.006&partnerID=40&md5=e347f09822b7147624456d24759e0bc7,10.1016/j.cmpb.2019.05.006,"Background and Objective: Magnetic resonance imaging (MRI) is an indispensable tool in diagnosing brain-tumor patients. Automated tumor segmentation is being widely researched to accelerate the MRI analysis and allow clinicians to precisely plan treatment—accurate delineation of brain tumors is a critical step in assessing their volume, shape, boundaries, and other characteristics. However, it is still a very challenging task due to inherent MR data characteristics and high variability, e.g., in tumor sizes or shapes. We present a new deep learning approach for accurate brain tumor segmentation which can be trained from small and heterogeneous datasets annotated by a human reader (providing high-quality ground-truth segmentation is very costly in practice). Methods: In this paper, we present a new deep learning technique for segmenting brain tumors from fluid attenuation inversion recovery MRI. Our technique exploits fully convolutional neural networks, and it is equipped with a battery of augmentation techniques that make the algorithm robust against low data quality, and heterogeneity of small training sets. We train our models using only positive (tumorous) examples, due to the limited amount of available data. Results: Our algorithm was tested on a set of stage II-IV brain-tumor patients (image data collected using MAGNETOM Prisma 3T, Siemens). Rigorous experiments, backed up with statistical tests, revealed that our approach outperforms the state-of-the-art approach (utilizing hand-crafted features) in terms of segmentation accuracy, offers very fast training and instant segmentation (analysis of an image takes less than a second). Building our deep model is 1.3 times faster compared with extracting features for extremely randomized trees, and this training time can be controlled. Finally, we showed that too aggressive data augmentation may lead to deteriorated performance of the model, especially in the fixed-budget training (with maximum numbers of training epochs). Conclusions: Our method yields the better performance when compared with the state of the art method which utilizes hand-crafted features. In addition, our deep network can be effectively applied to difficult (small, imbalanced, and heterogeneous) datasets, offers controllable training time, and infers in real-time. © 2019 Elsevier B.V.",Brain tumor; Deep neural network; Image segmentation; MRI,"135, 148",,Computer Methods and Programs in Biomedicine,Article,Scopus
823,,A new approach for arrhythmia classification using deep coded features and LSTM networks,"Yildirim O., Baloglu U.B., Tan R.-S., Ciaccio E.J., Acharya U.R.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065567077&doi=10.1016%2fj.cmpb.2019.05.004&partnerID=40&md5=30642c31b1e341636c20ba00eb8cfbda,10.1016/j.cmpb.2019.05.004,"Background and objective: For diagnosis of arrhythmic heart problems, electrocardiogram (ECG)signals should be recorded and monitored. The long-term signal records obtained are analyzed by expert cardiologists. Devices such as the Holter monitor have limited hardware capabilities. For improved diagnostic capacity, it would be helpful to detect arrhythmic signals automatically. In this study, a novel approach is presented as a candidate solution for these issues. Methods: A convolutional auto-encoder (CAE)based nonlinear compression structure is implemented to reduce the signal size of arrhythmic beats. Long-short term memory (LSTM)classifiers are employed to automatically recognize arrhythmias using ECG features, which are deeply coded with the CAE network. Results: Based upon the coded ECG signals, both storage requirement and classification time were considerably reduced. In experimental studies conducted with the MIT-BIH arrhythmia database, ECG signals were compressed by an average 0.70% percentage root mean square difference (PRD)rate, and an accuracy of over 99.0% was observed. Conclusions: One of the significant contributions of this study is that the proposed approach can significantly reduce time duration when using LSTM networks for data analysis. Thus, a novel and effective approach was proposed for both ECG signal compression, and their high-performance automatic recognition, with very low computational cost. © 2019 Elsevier B.V.",Arrhythmia detection; Autoencoders; Deep learning; ECG compression; LSTM,"121, 133",,Computer Methods and Programs in Biomedicine,Article,Scopus
824,,Extracting chemical-protein interactions from biomedical literature via granular attention based recurrent neural networks,"Lu H., Li L., He X., Liu Y., Zhou A.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065403091&doi=10.1016%2fj.cmpb.2019.04.020&partnerID=40&md5=b62609c7e74c771ddb0a92835ecec725,10.1016/j.cmpb.2019.04.020,"Background and objective: The extraction of interactions between chemicals and proteins from biomedical literature is important for many biomedical tasks such as drug discovery and precision medicine. In the existing systems, the methods achieving competitive results are combined of several models or implemented in multi-stage, and they are challenged by high cost because numerous external features are employed. These problems can be avoided by deep learning algorithms, but the performance of the deep learning based models is limited by inadequate exploration of the information. Our goal is to devise a system to improve the performance of the automatic extraction between chemical entities and protein entities from biomedical literature. Methods: In this paper, we propose a model based on recurrent neural networks integrating granular attention mechanism. The granular attention can explore the inner information of the context vectors, which are represented in multiple dimensions that play different roles in the extraction of the interactions. Furthermore, we employ Swish activation function in the neural networks for the chemical-protein interactions extraction task for the first time. Results: The proposed method is evaluated on BioCreative VI chemical-protein track test corpus. The experimental results show that this method achieves an F-score of 65.14%, which is 1.04% higher than the state-of-the-art system. Conclusions: The model synthesizing recurrent neural networks and granular attention mechanism, exploring the inner information of the context vectors, can improve the extraction performance without extra hand-crafted features. The experimental results demonstrate that the proposed model is promising for further study on the interaction extraction between chemicals and proteins. © 2019 Elsevier B.V.",Chemical-protein interactions extraction; Granular attention mechanism; Natural language processing; Recurrent neural networks; Swish activation function,"61, 68",,Computer Methods and Programs in Biomedicine,Article,Scopus
825,,An adverse drug effect mentions extraction method based on weighted online recurrent extreme learning machine,"El-allaly E.-D., Sarrouti M., En-Nahnahi N., Ouatik El Alaoui S.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065194700&doi=10.1016%2fj.cmpb.2019.04.029&partnerID=40&md5=3ffe548fe3aa278cfda4439d601fad75,10.1016/j.cmpb.2019.04.029,"Background and objective: Automatic extraction of adverse drug effect (ADE) mentions from biomedical texts is a challenging research problem that has attracted significant attention from the pharmacovigilance and biomedical text mining communities. Indeed, deep learning based methods have recently been employed to solve this issue with great success. However, they fail to effectively identify the boundary of mentions. In this paper, we propose a weighted online recurrent extreme learning machine (WOR-ELM) based method to overcome this drawback. Methods: The proposed method for ADE mentions extraction from biomedical texts is divided into two stages: span detection and ADE mentions classification. At the first stage, we identify the boundary of the mentions irrespective of their types with a WOR-ELM in a given sentence. At the second stage, another WOR-ELM is used to classify the identified mentions to the appropriate type. Both stages use the concatenation of character-level and word-level embeddings as features. The character-level embedding is obtained using a modified online recurrent extreme learning machine, whereas the word-level embedding is obtained from a pre-trained model. Results: Several experiments were carried out on a well-known ADE corpus to evaluate the effectiveness and demonstrate the usefulness of the proposed method. The obtained results show that our method achieves an F-score of 87.5%, which outperforms the current state-of-the-art methods. Conclusions: Our research results indicate that the proposed method for adverse drug effect mentions extraction from text can significantly improve performance over existing methods. Our experiments show the effectiveness of incorporating word-level and character level embeddings as features for WOR-ELM. They also illustrate the benefits of using IOU segment to represent ADE mentions. © 2019 Elsevier B.V.",Adverse drug effect; Biomedical informatics; Biomedical named entity recognition; Natural language processing; Pharmacovigilance; Weighted online recurrent extreme learning machine,"33, 41",,Computer Methods and Programs in Biomedicine,Article,Scopus
826,,Deep feature descriptor based hierarchical dense matching for X-ray angiographic images,"Fan J., Yang J., Wang Y., Yang S., Ai D., Huang Y., Song H., Wang Y., Shen D.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064888961&doi=10.1016%2fj.cmpb.2019.04.006&partnerID=40&md5=94b896e8300a50beb763445ce2bd7a92,10.1016/j.cmpb.2019.04.006,"Backgroud and Objective: X-ray angiography, a powerful technique for blood vessel visualization, is widely used for interventional diagnosis of coronary artery disease because of its fast imaging speed and perspective inspection ability. Matching feature points in angiographic images is a considerably challenging task due to repetitive weak-textured regions. Methods: In this paper, we propose an angiographic image matching method based on the hierarchical dense matching framework, where a novel deep feature descriptor is designed to compute multilevel correlation maps. In particular, the deep feature descriptor is computed by a deep learning model specifically designed and trained for angiographic images, thereby making the correlation maps more distinctive for corresponding feature points in different angiographic images. Moreover, point correspondences are further hierarchically extracted from multilevel correlation maps with the highest similarity response(s), which is relatively robust and accurate. To overcome the problem regarding the lack of training samples, the convolutional neural network (designed for deep feature descriptor) is initially trained on samples from natural images and then fine-tuned on manually annotated angiographic images. Finally, a dense matching completion method, based on the distance between deep feature descriptors, is proposed to generate dense matches between images. Results: The proposed method has been evaluated on the number and accuracy of extracted matches and the performance of subtraction images. Experiments on a variety of angiographic images show promising matching accuracy, compared with state-of-the-art methods. Conclusions: The proposed angiographic image matching method is shown to be accurate and effective for feature matching in angiographic images, and further achieves good performance in image subtraction. © 2019 Elsevier B.V.",Convolutional neural network; Coronary artery; Hierarchical dense matching,"233, 242",,Computer Methods and Programs in Biomedicine,Article,Scopus
827,,A Deep Learning Model for Estimating Story Points,"Choetkiertikul M., Dam H.K., Tran T., Pham T., Ghose A., Menzies T.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041008989&doi=10.1109%2fTSE.2018.2792473&partnerID=40&md5=ff731031e2923bc2022ca59070f41d80,10.1109/TSE.2018.2792473,"Although there has been substantial research in software analytics for effort estimation in traditional software projects, little work has been done for estimation in agile projects, especially estimating the effort required for completing user stories or issues. Story points are the most common unit of measure used for estimating the effort involved in completing a user story or resolving an issue. In this paper, we propose a prediction model for estimating story points based on a novel combination of two powerful deep learning architectures: long short-term memory and recurrent highway network. Our prediction system is end-to-end trainable from raw input data to prediction outcomes without any manual feature engineering. We offer a comprehensive dataset for story points-based estimation that contains 23,313 issues from 16 open source projects. An empirical evaluation demonstrates that our approach consistently outperforms three common baselines (Random Guessing, Mean, and Median methods) and six alternatives (e.g., using Doc2Vec and Random Forests) in Mean Absolute Error, Median Absolute Error, and the Standardized Accuracy. © 1976-2012 IEEE.",deep learning; effort estimation; Software analytics; story point estimation,"637, 656",,IEEE Transactions on Software Engineering,Article,Scopus
828,,Design Rule Spaces: A New Model for Representing and Analyzing Software Architecture,"Cai Y., Xiao L., Kazman R., Mo R., Feng Q.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041003076&doi=10.1109%2fTSE.2018.2797899&partnerID=40&md5=c5f7749b1b18ab26318fd64962a96e67,10.1109/TSE.2018.2797899,"In this paper, we propose an architecture model called Design Rule Space (DRSpace). We model the architecture of a software system as multiple overlapping DRSpaces, reflecting the fact that any complex software system must contain multiple aspects, features, patterns, etc. We show that this model provides new ways to analyze software quality. In particular, we introduce an Architecture Root detection algorithm that captures DRSpaces containing large numbers of a project's bug-prone files, which are called Architecture Roots (ArchRoots). After investigating ArchRoots calculated from 15 open source projects, the following observations become clear: from 35 to 91 percent of a project's most bug-prone files can be captured by just 5 ArchRoots, meaning that bug-prone files are likely to be architecturally connected. Furthermore, these ArchRoots tend to live in the system for significant periods of time, serving as the major source of bug-proneness and high maintainability costs. Moreover, each ArchRoot reveals multiple architectural flaws that propagate bugs among files and this will incur high maintenance costs over time. The implication of our study is that the quality, in terms of bug-proneness, of a large, complex software project cannot be fundamentally improved without first fixing its architectural flaws. © 1976-2012 IEEE.",bug localization; code smells; defect prediction; reverse-engineering; Software architecture; technical debt,"657, 682",,IEEE Transactions on Software Engineering,Article,Scopus
829,,The Impact of Automated Parameter Optimization on Defect Prediction Models,"Tantithamthavorn C., McIntosh S., Hassan A.E., Matsumoto K.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040906635&doi=10.1109%2fTSE.2018.2794977&partnerID=40&md5=18ab81cbe534df4b8d4c9105bc133d67,10.1109/TSE.2018.2794977,"Defect prediction models-classifiers that identify defect-prone software modules-have configurable parameters that control their characteristics (e.g., the number of trees in a random forest). Recent studies show that these classifiers underperform when default settings are used. In this paper, we study the impact of automated parameter optimization on defect prediction models. Through a case study of 18 datasets, we find that automated parameter optimization: (1) improves AUC performance by up to 40 percentage points; (2) yields classifiers that are at least as stable as those trained using default settings; (3) substantially shifts the importance ranking of variables, with as few as 28 percent of the top-ranked variables in optimized classifiers also being top-ranked in non-optimized classifiers; (4) yields optimized settings for 17 of the 20 most sensitive parameters that transfer among datasets without a statistically significant drop in performance; and (5) adds less than 30 minutes of additional computation to 12 of the 26 studied classification techniques. While widely-used classification techniques like random forest and support vector machines are not optimization-sensitive, traditionally overlooked techniques like C5.0 and neural networks can actually outperform widely-used techniques after optimization is applied. This highlights the importance of exploring the parameter space when using parameter-sensitive classification techniques. © 1976-2012 IEEE.",classification techniques; differential evolution; experimental design; genetic algorithm; grid search; parameter optimization; random search; search-based software engineering; Software defect prediction,"683, 711",,IEEE Transactions on Software Engineering,Article,Scopus
830,,Evasion attacks against watermarking techniques found in MLaaS systems,"Hitaj D., Hitaj B., Mancini L.V.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073892912&doi=10.1109%2fSDS.2019.8768572&partnerID=40&md5=2e27fe0ff5d01272b87372f4fb9753bd,10.1109/SDS.2019.8768572,"Deep neural networks have had enormous impact on various domains of computer science applications, considerably outperforming previous state-of-the-art machine learning techniques. To achieve this performance, neural networks need large quantities of data and huge computational resources, which heavily increase their costs. The increased cost of building a good deep neural network model gives rise to a need for protecting this investment from potential copyright infringements. Legitimate owners of a machine learning model want to be able to reliably track and detect a malicious adversary that tries to steal the intellectual property related to the model. This threat is very relevant to Machine Learning as a Service (MLaaS) systems, where a provider supplies APIs to clients, allowing them to interact with their trained proprietary deep learning models. Recently, this problem was tackled by introducing in deep neural networks the concept of watermarking, which allows a legitimate owner to embed some secret information (watermark) in a given model. Through the use of this watermark, the legitimate owners, remotely interacting with a model through input queries, are able to detect a copyright infringement, and prove the ownership of their models that were stolen/copied illegally. In this paper, we focus on assessing the robustness and reliability of state-of-the-art deep neural network watermarking schemes. In particular we show that, a malicious adversary, even in scenarios where the watermark is difficult to remove, can still evade the verification of copyright infringements from the legitimate owners, thus avoiding the detection of the model theft. © 2019 IEEE.",Backdoors; Deep Neural Networks; Machine Learning as a Service; Security and Privacy; Watermarking,"55, 63",,"2019 6th International Conference on Software Defined Systems, SDS 2019",Conference Paper,Scopus
831,,FineLocator: A novel approach to method-level fine-grained bug localization by query expansion,"Zhang W., Li Z., Wang Q., Li J.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062847394&doi=10.1016%2fj.infsof.2019.03.001&partnerID=40&md5=4f26b8eb7ad161b0cfd8ecf8805a78d8,10.1016/j.infsof.2019.03.001,"Context: Bug localization, namely, to locate suspicious snippets from source code files for developers to fix the bug, is crucial for software quality assurance and software maintenance. Effective bug localization technique is desirable for software developers to reduce the effort involved in bug resolution. State-of-the-art bug localization techniques concentrate on file-level coarse-grained localization by lexical matching bug reports and source code files. However, this would bring about a heavy burden for developers to locate feasible code snippets to make change with the goal of fixing the bug. Objective: This paper proposes a novel approach called FineLocator to method-level fine-grained bug localization by using semantic similarity, temporal proximity and call dependency for method expansion. Method: Firstly, the bug reports and the methods of source code are represented by numeric vectors using word embedding (word2vec) and the TF-IDF method. Secondly, we propose three query expansion scores as semantic similarity score, temporal proximity score and call dependency score to address the representation sparseness problem caused by the short lengths of methods in the source code. Then, the representation of a method with short length is augmented by elements of its neighboring methods with query expansion. Thirdly, when a new bug report is incoming, FineLocator will retrieve the methods in source code by similarity ranking on the bug report and the augmented methods for bug localization. Results: We collect bug repositories of ArgoUML, Maven, Kylin, Ant and AspectJ projects to investigate the performance of the proposed FineLocator approach. Experimental results demonstrate that the proposed FineLocator approach can improve the performances of method-level bug localization at average by 20%, 21% and 17% measured by Top-N indicator, MAP and MRR respectively, in comparison with state-of-the-art techniques. Conclusion: This is the first paper to demonstrate how to make use of method expansion to address the representation sparseness problem for method-level fine-grained bug localization. © 2019",Call dependency; Method-level bug localization; Query expansion; Semantic similarity; Temporal proximity,"121, 135",,Information and Software Technology,Article,Scopus
832,,ConPredictor: Concurrency Defect Prediction in Real-World Applications,"Yu T., Wen W., Han X., Hayes J.H.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041176093&doi=10.1109%2fTSE.2018.2791521&partnerID=40&md5=2455a90a9de502285e1be319b57b76ff,10.1109/TSE.2018.2791521,"Concurrent programs are difficult to test due to their inherent non-determinism. To address this problem, testing often requires the exploration of thread schedules of a program; this can be time-consuming when applied to real-world programs. Software defect prediction has been used to help developers find faults and prioritize their testing efforts. Prior studies have used machine learning to build such predicting models based on designed features that encode the characteristics of programs. However, research has focused on sequential programs; to date, no work has considered defect prediction for concurrent programs, with program characteristics distinguished from sequential programs. In this paper, we present ConPredictor, an approach to predict defects specific to concurrent programs by combining both static and dynamic program metrics. Specifically, we propose a set of novel static code metrics based on the unique properties of concurrent programs. We also leverage additional guidance from dynamic metrics constructed based on mutation analysis. Our evaluation on four large open source projects shows that ConPredictor improved both within-project defect prediction and cross-project defect prediction compared to traditional features. © 1976-2012 IEEE.",Concurrency; defect prediction; software metrics; software quality,"558, 575",,IEEE Transactions on Software Engineering,Article,Scopus
833,,Text Filtering and Ranking for Security Bug Report Prediction,"Peters F., Tun T.T., Yu Y., Nuseibeh B.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040055397&doi=10.1109%2fTSE.2017.2787653&partnerID=40&md5=b2fbc6e19d7ac30be1f95881d9e59446,10.1109/TSE.2017.2787653,"Security bug reports can describe security critical vulnerabilities in software products. Bug tracking systems may contain thousands of bug reports, where relatively few of them are security related. Therefore finding unlabelled security bugs among them can be challenging. To help security engineers identify these reports quickly and accurately, text-based prediction models have been proposed. These can often mislabel security bug reports due to a number of reasons such as class imbalance, where the ratio of non-security to security bug reports is very high. More critically, we have observed that the presence of security related keywords in both security and non-security bug reports can lead to the mislabelling of security bug reports. This paper proposes FARSEC, a framework for filtering and ranking bug reports for reducing the presence of security related keywords. Before building prediction models, our framework identifies and removes non-security bug reports with security related keywords. We demonstrate that FARSEC improves the performance of text-based prediction models for security bug reports in 90 percent of cases. Specifically, we evaluate it with 45,940 bug reports from Chromium and four Apache projects. With our framework, we mitigate the class imbalance issue and reduce the number of mislabelled security bug reports by 38 percent. © 1976-2012 IEEE.",prediction models; ranking; security bug reports; Security cross words; security related keywords; text filtering; transfer learning,"615, 631",,IEEE Transactions on Software Engineering,Article,Scopus
834,,Automated tongue diagnosis on the smartphone and its applications,"Hu M.-C., Lan K.-C., Fang W.-C., Huang Y.-C., Ho T.-J., Lin C.-P., Yeh M.-H., Raknim P., Lin Y.-H., Cheng M.-H., He Y.-T., Tseng K.-C.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039948351&doi=10.1016%2fj.cmpb.2017.12.029&partnerID=40&md5=a73c6283ab6d6d7631a8d2037f1029b7,10.1016/j.cmpb.2017.12.029,"Tongue features are important objective basis for clinical diagnosis and treatment in both western medicine and Chinese medicine. The need for continuous monitoring of health conditions inspires us to develop an automatic tongue diagnosis system based on built-in sensors of smartphones. However, tongue images taken by smartphone are quite different in color due to various lighting conditions, and it consequently affects the diagnosis especially when we use the appearance of tongue fur to infer health conditions. In this paper, we captured paired tongue images with and without flash, and the color difference between the paired images is used to estimate the lighting condition based on the Support Vector Machine (SVM). The color correction matrices for three kinds of common lights (i.e., fluorescent, halogen and incandescent) are pre-trained by using a ColorChecker-based method, and the corresponding pre-trained matrix for the estimated lighting is then applied to eliminate the effect of color distortion. We further use tongue fur detection as an example to discuss the effect of different model parameters and ColorCheckers for training the tongue color correction matrix under different lighting conditions. Finally, in order to demonstrate the potential use of our proposed system, we recruited 246 patients over a period of 2.5 years from a local hospital in Taiwan and examined the correlations between the captured tongue features and alanine aminotransferase (ALT)/aspartate aminotransferase (AST), which are important bio-markers for liver diseases. We found that some tongue features have strong correlation with AST or ALT, which suggests the possible use of these tongue features captured on a smartphone to provide an early warning of liver diseases. © 2017 Elsevier B.V.",Automatic tongue diagnosis framework on smartphone; Lighting condition estimation; Tongue fur; Tongue fur (white fur) detection; Tongue image color correction,"51, 64",,Computer Methods and Programs in Biomedicine,Article,Scopus
835,,Extracting Quality Attributes from User Stories for Early Architecture Decision Making,"Gilson F., Galster M., Georis F.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066460710&doi=10.1109%2fICSA-C.2019.00031&partnerID=40&md5=787cad6f54f9241571c8b8babc94869c,10.1109/ICSA-C.2019.00031,"Software quality attributes (e.g., security, performance) influence software architecture design decisions, e.g., when choosing technologies, patterns or tactics. As software developers are moving from big upfront design to an evolutionary or emerging design, the architecture of a system evolves as more functionality is added. In agile software development, functional user requirements are often expressed as user stories. Quality attributes might be implicitly referenced in user stories. To support a more systematic analysis and reasoning about quality attributes in agile development projects, this paper explores how to automatically identify quality attributes from user stories. This could help better understand relevant quality attributes (and potential architectural key drivers) before analysing product backlogs and domains in detail and provides the 'bigger picture' of potential architectural drivers for early architecture decision making. The goal of this paper is to present our vision and preliminary work towards understanding whether user stories do include information about quality attributes at all, and if so, how we can identify such information in an automated manner. © 2019 IEEE.",agile software development; decision making; machine learning; natural language processing; software architecture,"129, 136",,"Proceedings - 2019 IEEE International Conference on Software Architecture - Companion, ICSA-C 2019",Conference Paper,Scopus
836,,Ideas on improving software artifact reuse via traceability and self-awareness,"Tinnes C., Biesdorf A., Hohenstein U., Matthes F.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073028390&doi=10.1109%2fSST.2019.00013&partnerID=40&md5=e2f3de090b445be08ce9cc162fa86e76,10.1109/SST.2019.00013,We describe our vision towards automatic software and system development and argue that reusing knowledge from existing projects as well as traceability between corresponding artifacts are important steps towards this vision. We furthermore list barriers that are currently experienced with software artifact reuse and traceability in industry and suggest some ideas to overcome these barriers. © 2019 IEEE.,Architecture Knowledge Management; Natural Language Processing; Self Aware Systems; Software Reuse; Traceability,"13, 16",,"Proceedings - 2019 IEEE/ACM 10th International Workshop on Software and Systems Traceability, SST 2019",Conference Paper,Scopus
837,,Learning units-of-measure from scientific code,"Danish M., Allamanis M., Brockschmidt M., Rice A., Orchard D.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072929708&doi=10.1109%2fSE4Science.2019.00013&partnerID=40&md5=950f7cd3cc76277fd47d395022a8fa95,10.1109/SE4Science.2019.00013,"CamFort is our multi-purpose tool for lightweight analysis and verification of scientific Fortran code. One core feature provides units-of-measure verification (dimensional analysis) of programs, where users partially annotate programs with units-of-measure from which our tool checks consistency and infers any missing specifications. However, many users find it onerous to provide units-of-measure information for existing code, even in part. We have noted however that there are often many common patterns and clues about the intended units-of-measure contained within variable names, comments, and surrounding code context. In this work-in-progress paper, we describe how we are adapting our approach, leveraging machine learning techniques to reconstruct units-of-measure information automatically thus saving programmer effort and increasing the likelihood of adoption. © 2019 IEEE.",Machine learning; Units-of-measure; Verification,"43, 46",,"Proceedings - 2019 IEEE/ACM 14th International Workshop on Software Engineering for Science, SE4Science 2019",Conference Paper,Scopus
838,,Automated software vulnerability assessment with concept drift,"Le T.H.M., Sabir B., Babar M.A.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072343960&doi=10.1109%2fMSR.2019.00063&partnerID=40&md5=02e77bdc8ceaba2e8abbe2bccf72543b,10.1109/MSR.2019.00063,"Software Engineering researchers are increasingly using Natural Language Processing (NLP) techniques to automate Software Vulnerabilities (SVs) assessment using the descriptions in public repositories. However, the existing NLP-based approaches suffer from concept drift. This problem is caused by a lack of proper treatment of new (out-of-vocabulary) terms for the evaluation of unseen SVs over time. To perform automated SVs assessment with concept drift using SVs' descriptions, we propose a systematic approach that combines both character and word features. The proposed approach is used to predict seven Vulnerability Characteristics (VCs). The optimal model of each VC is selected using our customized time-based cross-validation method from a list of eight NLP representations and six well-known Machine Learning models. We have used the proposed approach to conduct large-scale experiments on more than 100,000 SVs in the National Vulnerability Database (NVD). The results show that our approach can effectively tackle the concept drift issue of the SVs' descriptions reported from 2000 to 2018 in NVD even without retraining the model. In addition, our approach performs competitively compared to the existing word-only method. We also investigate how to build compact concept-drift-aware models with much fewer features and give some recommendations on the choice of classifiers and NLP representations for SVs assessment. © 2019 IEEE.",Machine learning; Mining software repositories; Multi-class classification; Natural language processing; Software vulnerability,"371, 382",,IEEE International Working Conference on Mining Software Repositories,Conference Paper,Scopus
839,,Semantic source code models using identifier embeddings,"Efstathiou V., Spinellis D.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072315217&doi=10.1109%2fMSR.2019.00015&partnerID=40&md5=79beddf032d8d1616e1a843ca437fb6c,10.1109/MSR.2019.00015,"The emergence of online open source repositories in the recent years has led to an explosion in the volume of openly available source code, coupled with metadata that relate to a variety of software development activities. As an effect, in line with recent advances in machine learning research, software maintenance activities are switching from symbolic formal methods to data-driven methods. In this context, the rich semantics hidden in source code identifiers provide opportunities for building semantic representations of code which can assist tasks of code search and reuse. To this end, we deliver in the form of pretrained vector space models, distributed code representations for six popular programming languages, namely, Java, Python, PHP, C, C++, and C#. The models are produced using fastText, a state-of-the-art library for learning word representations. Each model is trained on data from a single programming language; the code mined for producing all models amounts to over 13.000 repositories. We indicate dissimilarities between natural language and source code, as well as variations in coding conventions in between the different programming languages we processed. We describe how these heterogeneities guided the data preprocessing decisions we took and the selection of the training parameters in the released models. Finally, we propose potential applications of the models and discuss limitations of the models. © 2019 IEEE.",Code Semantics; Fasttext; Semantic Similarity; Vector Space Models,"29, 33",,IEEE International Working Conference on Mining Software Repositories,Conference Paper,Scopus
840,,DeepPerf: Performance Prediction for Configurable Software with Deep Sparse Neural Network,"Ha H., Zhang H.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072296070&doi=10.1109%2fICSE.2019.00113&partnerID=40&md5=86756357c108de5a150d4a5e8f30dac1,10.1109/ICSE.2019.00113,"Many software systems provide users with a set of configuration options and different configurations may lead to different runtime performance of the system. As the combination of configurations could be exponential, it is difficult to exhaustively deploy and measure system performance under all possible configurations. Recently, several learning methods have been proposed to build a performance prediction model based on performance data collected from a small sample of configurations, and then use the model to predict system performance under a new configuration. In this paper, we propose a novel approach to model highly configurable software system using a deep feedforward neural network (FNN) combined with a sparsity regularization technique, e.g. the L1 regularization. Besides, we also design a practical search strategy for automatically tuning the network hyperparameters efficiently. Our method, called DeepPerf, can predict performance values of highly configurable software systems with binary and/or numeric configuration options at much higher prediction accuracy with less training data than the state-of-the art approaches. Experimental results on eleven public real-world datasets confirm the effectiveness of our approach. © 2019 IEEE.",deep sparse feedforward neural network; highly configurable systems; software performance prediction; sparsity regularization,"1095, 1106",,Proceedings - International Conference on Software Engineering,Conference Paper,Scopus
841,,CRADLE: Cross-Backend Validation to Detect and Localize Bugs in Deep Learning Libraries,"Pham H.V., Lutellier T., Qi W., Tan L.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072280039&doi=10.1109%2fICSE.2019.00107&partnerID=40&md5=fd74844eb6293f8fffe1fa54b9a2a002,10.1109/ICSE.2019.00107,"Deep learning (DL) systems are widely used in domains including aircraft collision avoidance systems, Alzheimer's disease diagnosis, and autonomous driving cars. Despite the requirement for high reliability, DL systems are difficult to test. Existing DL testing work focuses on testing the DL models, not the implementations (e.g., DL software libraries) of the models. One key challenge of testing DL libraries is the difficulty of knowing the expected output of DL libraries given an input instance. Fortunately, there are multiple implementations of the same DL algorithms in different DL libraries. Thus, we propose CRADLE, a new approach that focuses on finding and localizing bugs in DL software libraries. CRADLE (1) performs cross-implementation inconsistency checking to detect bugs in DL libraries, and (2) leverages anomaly propagation tracking and analysis to localize faulty functions in DL libraries that cause the bugs. We evaluate CRADLE on three libraries (TensorFlow, CNTK, and Theano), 11 datasets (including ImageNet, MNIST, and KGS Go game), and 30 pre-trained models. CRADLE detects 12 bugs and 104 unique inconsistencies, and highlights functions relevant to the causes of inconsistencies for all 104 unique inconsistencies. © 2019 IEEE.",bugs detection; cross-implementation testing; deep learning software testing; software testing,"1027, 1038",,Proceedings - International Conference on Software Engineering,Conference Paper,Scopus
842,,Resource-Aware Program Analysis Via Online Abstraction Coarsening,"Heo K., Oh H., Yang H.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072276957&doi=10.1109%2fICSE.2019.00027&partnerID=40&md5=dd4e7df3b8d5b241d6d52bd09e63c363,10.1109/ICSE.2019.00027,"We present a new technique for developing a resource-aware program analysis. Such an analysis is aware of constraints on available physical resources, such as memory size, tracks its resource use, and adjusts its behaviors during fixpoint computation in order to meet the constraint and achieve high precision. Our resource-aware analysis adjusts behaviors by coarsening program abstraction, which usually makes the analysis consume less memory and time until completion. It does so multiple times during the analysis, under the direction of what we call a controller. The controller constantly intervenes in the fixpoint computation of the analysis and decides how much the analysis should coarsen the abstraction. We present an algorithm for learning a good controller automatically from benchmark programs. We applied our technique to a static analysis for C programs, where we control the degree of flow-sensitivity to meet a constraint on peak memory consumption. The experimental results with 18 real-world programs show that our algorithm can learn a good controller and the analysis with this controller meets the constraint and utilizes available memory effectively. © 2019 IEEE.",learning; resource constraint; static analysis,"94, 104",,Proceedings - International Conference on Software Engineering,Conference Paper,Scopus
843,,Leveraging small software engineering data sets with pre-trained neural networks,"Robbes R., Janes A.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072066773&doi=10.1109%2fICSE-NIER.2019.00016&partnerID=40&md5=5f19c0dfa4e10d6cc48ffd239ccc05b8,10.1109/ICSE-NIER.2019.00016,"Many software engineering data sets, particularly those that demand manual labelling for classification, are necessarily small. As a consequence, several recent software engineering papers have cast doubt on the effectiveness of deep neural networks for classification tasks, when applied to these data sets. We provide initial evidence that recent advances in Natural Language Processing, that allow neural networks to leverage large amount of unlabelled data in a pre-training phase, can significantly improve performance. © 2019 IEEE.",Data sets; Deep learning; Transfer learning,"29, 32",,"Proceedings - 2019 IEEE/ACM 41st International Conference on Software Engineering: New Ideas and Emerging Results, ICSE-NIER 2019",Conference Paper,Scopus
844,,Multifaceted Automated Analyses for Variability-Intensive Embedded Systems,"Lazreg S., Cordy M., Collet P., Heymans P., Mosser S.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072045046&doi=10.1109%2fICSE.2019.00092&partnerID=40&md5=0ad1c7551cb6ec6baf51dd1484e4e5ea,10.1109/ICSE.2019.00092,"Embedded systems, like those found in the automotive domain, must comply with stringent functional and non-functional requirements. To fulfil these requirements, engineers are confronted with a plethora of design alternatives both at the software and hardware level, out of which they must select the optimal solution wrt. possibly-antagonistic quality attributes (e.g. cost of manufacturing vs. speed of execution). We propose a model-driven framework to assist engineers in this choice. It captures high-level specifications of the system in the form of variable dataflows and configurable hardware platforms. A mapping algorithm then derives the design space, i.e. the set of compatible pairs of application and platform variants, and a variability-aware executable model, which encodes the functional and non-functional behaviour of all viable system variants. Novel verification algorithms then pinpoint the optimal system variants efficiently. The benefits of our approach are evaluated through a real-world case study from the automotive industry. © 2019 IEEE.",Embedded system design engineering; model checking; multi objective optimization; non functional property; variability modeling,"854, 865",,Proceedings - International Conference on Software Engineering,Conference Paper,Scopus
845,,Towards zero knowledge learning for cross language API mappings,Bui N.,2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071855101&doi=10.1109%2fICSE-Companion.2019.00054&partnerID=40&md5=b18e559c9eeb447e2b7a3c0c00af5af3,10.1109/ICSE-Companion.2019.00054,"Programmers often need to migrate programs from one language or platform to another in order to implement functionality, instead of rewriting the code from scratch. However, most techniques proposed to identify API mappings across languages and facilitate automated program translation require manually curated parallel corpora that contain already mapped API seeds or functionally-equivalent code using the APIs in two different languages so that the techniques can have an anchor to map APIs. To alleviate the need of curating parallel data and to generalize the applicability of program translation techniques, we develop a new automated approach for identifying API mappings across languages based on the idea of unsupervised domain adaption via Generative Adversarial Network (GAN) and an additional refinement procedure that can transform two vector spaces to align the API vectors in the two spaces without the need of manually provided anchors. We show that our approach can identify API mappings more accurately than Api2Api without the need of curated parallel seeds. © 2019 IEEE.",Adversarial learning; Cross language; Generative adversarial network; Skip gram,"123, 125",,"Proceedings - 2019 IEEE/ACM 41st International Conference on Software Engineering: Companion, ICSE-Companion 2019",Conference Paper,Scopus
846,,Cross-project Defect Prediction Method Based on Feature Transfer and Instance Transfer [基于特征迁移和实例迁移的跨项目缺陷预测方法],"Ni C., Chen X., Liu W.-S., Gu Q., Huang Q.-G., Li N.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071586994&doi=10.13328%2fj.cnki.jos.005712&partnerID=40&md5=4cc66fcd1378d95957fe8692011b4072,10.13328/j.cnki.jos.005712,"In real software development, a project, which needs defect prediction, may be a new project or maybe has less training data. A simple solution is to use training data from other projects (i.e., source projects) to construct the model, and use the trained model to perform prediction on the current project (i.e., target project). However, datasets among different projects may have large distribution difference. To solve this problem, a novel two phase cross-project defect prediction method FeCTrA is proposed, which considers both feature transfer and instance transfer. In the feature transfer phase, FeCTrA uses cluster analysis to select features, which have high distribution similarity between the source project and the target project. In the instance transfer phase, FeCTrA utilizes TrAdaBoost, which selects relevant instances from the source project when give some labeled instances in the target project. To verify the effectiveness of FeCTrA, Relink and AEEEM datasets are choosen as the experimental subjects and F1 as the performance measure. Firstly, it is found that FeCTrA outperforms single phase methods, which only consider feature transfer or instance transfer. Then after comparing with state-of-the-art baseline methods (i.e., TCA+, Peters filter, Burak filter, and DCPDP), the performance of FeCTrA improves 23%, 7.2%, 9.8%, and 38.2% on Relink dataset and the performance of FeCTrA improves 96.5%, 108.5%, 103.6%, and 107.9% on AEEEM dataset. Finally, the influence of factors in FeCTrA is analyzed and a guideline to effectively use this method is provided. © Copyright 2019, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Cross-project defect prediction; Feature transfer; Instance transfer; Software defect prediction; Software quality assurance; Transfer learning,"1308, 1329",,Ruan Jian Xue Bao/Journal of Software,Article,Scopus
847,,Approach of Bug Reports Classification Based on Cost Extreme Learning Machine [基于代价极速学习机的软件缺陷报告分类方法],"Zhang T.-L., Chen R., Yang X., Zhu H.-Y.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071571654&doi=10.13328%2fj.cnki.jos.005725&partnerID=40&md5=64635536d6677c837f93c62f4bf5c103,10.13328/j.cnki.jos.005725,"Bug is an unavoidable problem in the development of all software systems. For developers of software system, bug report is a powerful tool for fixing bugs. However, manual recognition on bug reports tends to be time-consuming and not economical. It thus becomes significant to advance the automated classification approach to provide clear guidelines on how to assign a reasonable severity to a reported bug. In this study, several algrithoms are proposed based on extreme learning machine to automatically classify bug reports. Concretely, this study focuses on three problems in the field of bug report classification. The first one is the imbalanced class distribution in bug report dataset; the second is the insufficient labeled sample in bug report dataset; the last is the limited training data available. In order to solve these issues, three methods are proposed based on cost-sensitive supervised classification, semi-supervised learning, and sample transferring, respectively. Extensive experiments on real bug report datasets are conducted, and the results demonstrate the practicability and effectiveness of the proposed methods. © Copyright 2019, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Extreme learning machine; Sample transferring approach; Semi-supervised learning approach; Software bug report; Supervised classification method,"1386, 1406",,Ruan Jian Xue Bao/Journal of Software,Article,Scopus
848,,Fault Cause Identification Method for Aircraft Equipment Based on Maintenance Log [基于维修日志的飞机设备故障原因判别方法],"Wang R.-G., Wu J., Liu C., Yang H.-Y.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071544140&doi=10.13328%2fj.cnki.jos.005730&partnerID=40&md5=25f6885a943d2f5d48b8c33f8d058177,10.13328/j.cnki.jos.005730,"In the process of aircraft maintenance, the aviation maintenance company has accumulated a large number of empirical maintenance log data. Machine learning methods can be used to help maintenance staff to make correct fault diagnosis decisions, using this type of maintenance log reasonably. Firstly, according to the particularity of the maintenance log, an iterative fault diagnosis process is proposed. Secondly, based on the traditional text feature extraction technology, the text feature extraction method based on convolution neural network (CNN) with the information in the domain is proposed, which is used in the case of small sample size. The method uses the target vector to train word vector to get more adequate text features. Finally, the random forest (RF) model is used in combination with other fault characteristics to determine the cause of aircraft equipment failure. The convolutional neural network aims at the cause of the failure, and pre-trains the word vector in the fault phenomenon to obtain a text feature that better reflects the field. Compared with other text feature extraction methods, the method obtains better results in the case of small sample size. At the same time, the convolutional neural network and random forest model are applied to the identification of aircraft equipment failure, and compared with other text feature extraction methods and machine learning prediction models, which illustrates the rationality and necessity of the method of text feature extraction and the method of fault cause identification. © Copyright 2019, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Convolutional neural network; Fault diagnosis; Maintenance log; Random forest,"1375, 1385",,Ruan Jian Xue Bao/Journal of Software,Article,Scopus
849,,Machine learning meets quantitative planning: Enabling self-adaptation in autonomous robots,"Jamshidi P., Camara J., Schmerl B., Kaestner C., Garlan D.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071135281&doi=10.1109%2fSEAMS.2019.00015&partnerID=40&md5=0acd3a03b678c9f403a61351d3834e2c,10.1109/SEAMS.2019.00015,"Modern cyber-physical systems (e.g., robotics systems) are typically composed of physical and software components, the characteristics of which are likely to change over time. Assumptions about parts of the system made at design time may not hold at run time, especially when a system is deployed for long periods (e.g., over decades). Self-adaptation is designed to find reconfigurations of systems to handle such run-time inconsistencies. Planners can be used to find and enact optimal reconfigurations in such an evolving context. However, for systems that are highly configurable, such planning becomes intractable due to the size of the adaptation space. To overcome this challenge, in this paper we explore an approach that (a) uses machine learning to find Pareto-optimal configurations without needing to explore every configuration and (b) restricts the search space to such configurations to make planning tractable. We explore this in the context of robot missions that need to consider task timeliness and energy consumption. An independent evaluation shows that our approach results in high-quality adaptation plans in uncertain and adversarial environments. © 2019 IEEE.",artificial intelligence; Machine learning; quantitative planning; robotics systems; self-adaptive systems,"39, 50",,ICSE Workshop on Software Engineering for Adaptive and Self-Managing Systems,Conference Paper,Scopus
850,,All versus one: An empirical comparison on retrained and incremental machine learning for modeling performance of adaptable software,Chen T.,2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071087966&doi=10.1109%2fSEAMS.2019.00029&partnerID=40&md5=8e6b24a1794c18335509f5180553c690,10.1109/SEAMS.2019.00029,"Given the ever-increasing complexity of adaptable software systems and their commonly hidden internal information (e.g., software runs in the public cloud), machine learning based performance modeling has gained momentum for evaluating, understanding and predicting software performance, which facilitates better informed self-adaptations. As performance data accumulates during the run of the software, updating the performance models becomes necessary. To this end, there are two conventional modeling methods: the retrained modeling that always discard the old model and retrain a new one using all available data; or the incremental modeling that retains the existing model and tunes it using one newly arrival data sample. Generally, literature on machine learning based performance modeling for adaptable software chooses either of those methods according to a general belief, but they provide insufficient evidences or references to justify their choice. This paper is the first to report on a comprehensive empirical study that examines both modeling methods under distinct domains of adaptable software, 5 performance indicators, 8 learning algorithms and settings, covering a total of 1,360 different conditions. Our findings challenge the general belief, which is shown to be only partially correct, and reveal some of the important, statistically significant factors that are often overlooked in existing work, providing evidence-based insights on the choice. © 2019 IEEE.",machine learning; Performance modeling; self-adaptive system; software runtime,"157, 168",,ICSE Workshop on Software Engineering for Adaptive and Self-Managing Systems,Conference Paper,Scopus
851,,Machine Learning Methods for Detecting and Monitoring Extremist Information on the Internet,"Mashechkin I.V., Petrovskiy M.I., Tsarev D.V., Chikunov M.N.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067263360&doi=10.1134%2fS0361768819030058&partnerID=40&md5=bb790f4396cfdbd9cccd44579752e3d4,10.1134/S0361768819030058,"Abstract: In this paper, we employ machine learning methods to solve the problem of countering terrorism and extremism by using information from the Internet. This problem involves retrieving electronic messages, documents, and web resources that potentially contain information of terrorist or extremist nature, identifying the structure of user groups and online communities that disseminate this information, monitoring and modeling information flows in these communities, as well as assessing threats and predicting risks based on monitoring results. We propose some original language-independent algorithms for pattern-based information retrieval, thematic modeling, and prediction of message flow characteristics, as well as assessment and prediction of potential risk coming from members of online communities by using data on the structure of relations in these communities, which makes it possible to detect potentially dangerous users even without full access to the content they distribute, e.g., through private channels and chat rooms. © 2019, Pleiades Publishing, Ltd.",,"99, 115",,Programming and Computer Software,Article,Scopus
852,,An automatic diagnostic network using skew-robust adversarial discriminative domain adaptation to evaluate the severity of depression,"Sun B., Zhang Y., He J., Xiao Y., Xiao R.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060283072&doi=10.1016%2fj.cmpb.2019.01.006&partnerID=40&md5=2f674f3fe911b0403b3512b428d364ee,10.1016/j.cmpb.2019.01.006,"Background and Objective: Deep learning provides an automatic and robust solution to depression severity evaluation. However, despite it is powerful, there is a trade-off between robust performance and the cost of manual annotation. Methods: Motivated by knowledge evolution and domain adaptation, we propose a deep evaluation network using skew-robust adversarial discriminative domain adaptation (SRADDA), which adaptively shifts its domain from a large-scale Twitter dataset to a small-scale depression interview dataset for evaluating the severity of depression. Results: Without top-down selection, SRADDA-based severity evaluation network achieves regression errors of 6.38 (Root Mean Square Error,RMSE)and 4.93 (Mean Absolute Error,MAE), which outperforms baselines provided by the Audio/Visual Emotion Challenge and Workshop(AVEC 2017). However, with top-down selection, the network achieves comparable results (RMSE = 5.13, MAE = 4.08). Conclusions: Results show that SRADDA not only represents features robustly, but also performs comparably to state-of-the-art results on small-scale dataset, DAIC-WOZ. © 2019 Elsevier B.V.",Adversarial learning; Deep learning; Domain adaptation; Knowledge evolution; Skew-robustness,"185, 195",,Computer Methods and Programs in Biomedicine,Article,Scopus
853,,Demystifying Bayesian Inference Workloads,"Wang Y.E., Zhu Y., Ko G.G., Reagen B., Wei G.-Y., Brooks D.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065429870&doi=10.1109%2fISPASS.2019.00031&partnerID=40&md5=f3fa09077b78c41213f283c5fc82b26e,10.1109/ISPASS.2019.00031,"The recent surge of machine learning has motivated computer architects to focus intently on accelerating related workloads, especially in deep learning. Deep learning has been the pillar algorithm that has led the advancement of learning patterns from a vast amount of labeled data, or supervised learning. However, for unsupervised learning, Bayesian methods often work better than deep learning. Bayesian modeling and inference works well with unlabeled or limited data, can leverage informative priors, and has interpretable models. Despite being an important branch of machine learning, Bayesian inference generally has been overlooked by the architecture and systems communities. In this paper, we facilitate the study of Bayesian inference with the development of BayesSuite, a collection of seminal Bayesian inference workloads. We characterize the power and performance profiles of BayesSuite across a variety of current-generation processors and find significant diversity. Manually tuning and deploying Bayesian inference workloads requires deep understanding of the workload characteristics and hardware specifications. To address these challenges and provide high-performance, energy-efficient support for Bayesian inference, we introduce a scheduling and optimization mechanism that can be plugged into a system scheduler. We also propose a computation elision technique that further improves the performance and energy efficiency of the workloads by skipping computations that do not improve the quality of the inference. Our proposed techniques are able to increase Bayesian inference performance by 5.8 × on average over the naive assignment and execution of the workloads. © 2019 IEEE.",Bayesian inference; Machine learning; Workload characterization,"177, 189",,"Proceedings - 2019 IEEE International Symposium on Performance Analysis of Systems and Software, ISPASS 2019",Conference Paper,Scopus
854,,Effects of Image Resolution on Automatic Face Detection,"Haneefl O., Maqbooll S., Siddiquel F., Mahmoodl Z., Khattakl S., Khan G.Z.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064739966&doi=10.1109%2fC-CODE.2019.8680985&partnerID=40&md5=d4b42516c09c75507048b2b514723c8e,10.1109/C-CODE.2019.8680985,"This study presents comparison of four well-known face detection algorithms. Face detection algorithms investigated in this study are, (i) Viola Jones (VJ) face detection algorithm, (ii) Normalized pixel difference (NPD), (iii) Histogram of Oriented Gradients (HOG), and (iv) Skin Color Based (SCB) face detection algorithm. Simulation results reveal that on the LFW database, and for image resolution of 40 × 40 pixels and below, only Skin Color-Based face detection algorithm was able to locate faces with a mean accuracy of 56% at the cost of highest computation complexity. Whereas, for image resolution of 60 × 60 pixels and above, the Viola Jones algorithm has the highest detection accuracy of 61% with average execution time of 0.605 seconds. On real life images that contained up to 64 faces in the input image, the Viola Jones algorithm surpassed the companion algorithms followed by the NPD and the HOG. The SCB algorithm is found to be least effective on real life images. © 2019 IEEE.",AdaBoost; Detection Rate; Face Detection,"231, 236",,"2019 2nd International Conference on Communication, Computing and Digital Systems, C-CODE 2019",Conference Paper,Scopus
855,,Feature Learning of Weight-distribution for Diagnosis of Alzheimer's Disease [用于阿尔茨海默病诊断的权值分布特征学习],"Cheng B., Ding Y., Zhang D.-Q.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073661072&doi=10.13328%2fj.cnki.jos.005371&partnerID=40&md5=226c0f7d2946d748c82b58c0d57b7fe2,10.13328/j.cnki.jos.005371,"In the field of medical imaging analysis using machine learning, the challenge is lack of training sample. In order to solve the problem, a weight-distribution based Lasso (Least absolute shrinkage and selection operator) feature learning model is proposed and applied to early diagnosis of Alzheimer's Disease (AD). Specifically, the proposed diagnosis method is consisted of two components: weight-distribution based Lasso feature selection (WDL) and large margin distribution machine (LDM) for classification. Firstly, in order to capture data distribution information among multimodal features, the WDL feature selection model was built, to improve on the conventional Lasso model via adding a regularization item of weight-distribution. Secondly, in order to achieve better generalization and accuracy on classification, and also to keep complementary information among multimodal features, the LDM algorithm is used for the training of the classifier. To evaluate the effectiveness of the proposed learning model, 202 subjects from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database with multimodal features were employed. Experimental results on the ADNI database show that it can recognize AD from Normal Controls (NC) with 97.5% accuracy, recognize Mild Cognitive Impairment (MCI) from NC with 83.1% accuracy, and recognize progressive MCI (pMCI) patients from stable MCI (sMCI) ones with 84.8% accuracy, which demonstrate that it can significantly improve the performance of early AD diagnosis and achieve feature ranking in terms of discrimination via optimized weight vector. © Copyright 2019, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Alzheimer's disease (AD); Large margin distribution learning; Multimodal; Sparse feature learning; Weight-distribution,"1002, 1014",,Ruan Jian Xue Bao/Journal of Software,Article,Scopus
856,,Coverage-guided learning-assisted grammar-based fuzzing,"Jitsunari Y., Arahori Y.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068380976&doi=10.1109%2fICSTW.2019.00065&partnerID=40&md5=8cb73e8c7c0cb757eb48f1c157cd6a76,10.1109/ICSTW.2019.00065,"Grammar-based fuzzing is known to be an effective technique for checking security vulnerabilities in programs, such as parsers, which take complex structured inputs. Unfortunately, most of existing grammar-based fuzzers require a lot of manual efforts of writing complex input grammars, which hinders their practical use. To address this problem, recently proposed approaches use machine learning to automatically acquire a generative model for structured inputs conforming to a complex grammar. Even such approaches, however, have major limitations: they fail to learn a generative model for instruction sequences, and they cannot achieve good coverage of instruction-parsing code. To overcome such limitations. this paper proposes a collection of techniques for enhancing learning-assisited grammar-based fuzzing. Our approach allows for the learning of a generative model for instruction sequences by training a hybrid character/token-level recursive neural network. In addition, we exploit coverage metrics gathered during previous runs of fuzzing in order to efficiently refine (or fine-tune) the learnt model so that it can make high coverage-inducing new inputs. Our experiments with a real PDF parser show that our approach succeeded in generating new sequences of instructions (in PDF page streams) that induce better code coverage (of the PDF parser) than state-of-the-art learning-assisted grammar-based fuzzers. © 2019 IEEE.",Coverage guided fuzzing; Deep learning; Fuzz testing; Grammar based fuzzing; Machine learning,"275, 280",,"Proceedings - 2019 IEEE 12th International Conference on Software Testing, Verification and Validation Workshops, ICSTW 2019",Conference Paper,Scopus
857,,Deep learning and SURF for automated classification and detection of calcaneus fractures in CT images,"Pranata Y.D., Wang K.-C., Wang J.-C., Idram I., Lai J.-Y., Liu J.-W., Hsieh I.-H.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061973252&doi=10.1016%2fj.cmpb.2019.02.006&partnerID=40&md5=dd9e40d741f74cf32a326c9780bf6dfe,10.1016/j.cmpb.2019.02.006,"Background and objectives: The calcaneus is the most fracture-prone tarsal bone and injuries to the surrounding tissue are some of the most difficult to treat. Currently there is a lack of consensus on treatment or interpretation of computed tomography (CT) images for calcaneus fractures. This study proposes a novel computer-assisted method for automated classification and detection of fracture locations in calcaneus CT images using a deep learning algorithm. Methods: Two types of Convolutional Neural Network (CNN) architectures with different network depths, a Residual network (ResNet) and a Visual geometry group (VGG), were evaluated and compared for the classification performance of CT scans into fracture and non-fracture categories based on coronal, sagittal, and transverse views. The bone fracture detection algorithm incorporated fracture area matching using the speeded-up robust features (SURF) method, Canny edge detection, and contour tracing. Results: Results showed that ResNet was comparable in accuracy (98%) to the VGG network for bone fracture classification but achieved better performance for involving a deeper neural network architecture. ResNet classification results were used as the input for detecting the location and type of bone fracture using SURF algorithm. Conclusions: Results from real patient fracture data sets demonstrate the feasibility using deep CNN and SURF for computer-aided classification and detection of the location of calcaneus fractures in CT images. © 2019 Elsevier B.V.",Calcaneus fracture; Computed tomography image; Convolutional neural networks; Residual network; Visual geometry group,"27, 37",,Computer Methods and Programs in Biomedicine,Article,Scopus
858,,On the Multiple Sources and Privacy Preservation Issues for Heterogeneous Defect Prediction,"Li Z., Jing X.-Y., Zhu X., Zhang H., Xu B., Ying S.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038839109&doi=10.1109%2fTSE.2017.2780222&partnerID=40&md5=caf565baae52bb398ee1cff0c13aba21,10.1109/TSE.2017.2780222,"Heterogeneous defect prediction (HDP) refers to predicting defect-proneness of software modules in a target project using heterogeneous metric data from other projects. Existing HDP methods mainly focus on predicting target instances with single source. In practice, there exist plenty of external projects. Multiple sources can generally provide more information than a single project. Therefore, it is meaningful to investigate whether the HDP performance can be improved by employing multiple sources. However, a precondition of conducting HDP is that the external sources are available. Due to privacy concerns, most companies are not willing to share their data. To facilitate data sharing, it is essential to study how to protect the privacy of data owners before they release their data. In this paper, we study the above two issues in HDP. Specifically, to utilize multiple sources effectively, we propose a multi-source selection based manifold discriminant alignment (MSMDA) approach. To protect the privacy of data owners, a sparse representation based double obfuscation algorithm is designed and applied to HDP. Through a case study of 28 projects, our results show that MSMDA can achieve better performance than a range of baseline methods. The improvement is 3.4-15.315.3 percent in g-measure and 3.0-19.119.1 percent in AUC. © 1976-2012 IEEE.",Heterogeneous defect prediction; manifold discriminant alignment; multiple sources; privacy preservation; source selection; utility,"391, 411",,IEEE Transactions on Software Engineering,Article,Scopus
859,,Learning-Based Recursive Aggregation of Abstract Syntax Trees for Code Clone Detection,"Buch L., Andrzejak A.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064160076&doi=10.1109%2fSANER.2019.8668039&partnerID=40&md5=0df893af833057d41e34f6bbf82de73d,10.1109/SANER.2019.8668039,"Code clone detection remains a crucial challenge in maintaining software projects. Many classic approaches rely on handcrafted aggregation schemes, while recent work uses supervised or unsupervised learning. In this work, we study several aspects of aggregation schemes for code clone detection based on supervised learning. To this aim, we implement an AST-based Recursive Neural Network. Firstly, our ablation study shows the influence of model choices and hyperparameters. We introduce error scaling as a way to effectively and efficiently address the class imbalance problem arising in code clone detection. Secondly, we study the influence of pretrained embeddings representing nodes in ASTs. We show that simply averaging all node vectors of a given AST yields strong baseline aggregation scheme. Further, learned AST aggregation schemes greatly benefit from pretrained node embeddings. Finally, we show the importance of carefully separating training and test data by clone clusters, to reliably measure generalization of models learned with supervision. © 2019 IEEE.",Abstract Syntax Trees; Code Clone Detection; Embeddings; Recursive Neural Network; Siamese Network,"95, 104",,"SANER 2019 - Proceedings of the 2019 IEEE 26th International Conference on Software Analysis, Evolution, and Reengineering",Conference Paper,Scopus
860,,A Neural Model for Method Name Generation from Functional Description,"Gao S., Chen C., Xing Z., Ma Y., Song W., Lin S.-W.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064159567&doi=10.1109%2fSANER.2019.8667994&partnerID=40&md5=1a6a1db8891ee5807046b4078b948b38,10.1109/SANER.2019.8667994,"The names of software artifacts, e.g., method names, are important for software understanding and maintenance, as good names can help developers easily understand others' code. However, the existing naming guidelines are difficult for developers, especially novices, to come up with meaningful, concise and compact names for the variables, methods, classes and files. With the popularity of open source, an enormous amount of project source code can be accessed, and the exhaustiveness and instability of manually naming methods could now be relieved by automatically learning a naming model from a large code repository. Nevertheless, building a comprehensive naming system is still challenging, due to the gap between natural language functional descriptions and method names. Specifically, there are three challenges: how to model the relationship between the functional descriptions and formal method names, how to handle the explosion of vocabulary when dealing with large repositories, and how to leverage the knowledge learned from large repositories to a specific project. To answer these questions, we propose a neural network to directly generate readable method names from natural language description. The proposed method is built upon the encoder-decoder framework with the attention and copying mechanisms. Our experiments show that our method can generate meaningful and accurate method names and achieve significant improvement over the state-of-The-Art baseline models. We also address the cold-start problem using a training trick to utilize big data in Github for specific projects. © 2019 IEEE.",Encoder-Decoder Model; Naming Convention; Transfer Learning,"411, 421",,"SANER 2019 - Proceedings of the 2019 IEEE 26th International Conference on Software Analysis, Evolution, and Reengineering",Conference Paper,Scopus
861,,An Empirical Study of Learning to Rank Techniques for Effort-Aware Defect Prediction,"Yu X., Bennin K.E., Liu J., Keung J.W., Yin X., Xu Z.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064150933&doi=10.1109%2fSANER.2019.8668033&partnerID=40&md5=c2e876b086366987507530c6204ae1d3,10.1109/SANER.2019.8668033,"Effort-Aware Defect Prediction (EADP) ranks software modules based on the possibility of these modules being defective, their predicted number of defects, or defect density by using learning to rank algorithms. Prior empirical studies compared a few learning to rank algorithms considering small number of datasets, evaluating with inappropriate or one type of performance measure, and non-robust statistical test techniques. To address these concerns and investigate the impact of learning to rank algorithms on the performance of EADP models, we examine the practical effects of 23 learning to rank algorithms on 41 available defect datasets from the PROMISE repository using a module-based effort-Aware performance measure (FPA) and a source lines of code (SLOC) based effort-Aware performance measure (Norm(Popt). In addition, we compare the performance of these algorithms when they are trained on a more relevant feature subset selected by the Information Gain feature selection method. In terms of FPA and Norm(Popt), statistically significant differences are observed among these algorithms with BRR (Bayesian Ridge Regression) performing best in terms of FPA, and BRR and LTR (Learning-To-Rank) performing best in terms of Norm (Popt). When these algorithms are trained on a more relevant feature subset selected by Information Gain, LTR and BRR still perform best with significant differences in terms of FPA and Norm(Popt). Therefore, we recommend BRR and LTR for building the EADP model in order to find more defects by inspecting a certain number of modules or lines of codes. © 2019 IEEE.",effort-Aware defect prediction; empirical study; learning to rank; Scott-Knott ESD test,"298, 309",,"SANER 2019 - Proceedings of the 2019 IEEE 26th International Conference on Software Analysis, Evolution, and Reengineering",Conference Paper,Scopus
862,,"Proceedings of the IEEE International Conference on Software Engineering and Service Sciences, ICSESS",[No author name available],2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063642141&partnerID=40&md5=07c3c61ef114e23b4af4fdf6386a42c1,,"The proceedings contain 251 papers. The topics discussed include: design and implementation of a television content expansion system; scalable source code plagiarism detection using source code vectors clustering; using NARX neural network for prediction of urban rail transit passenger flow; effective ADAM-optimized LSTM neural network for electricity price forecasting; image and text correlation judgment based on deep learning; a unique method for detecting grounds in the indoor environment; design of the security mechanism for a BPO cloud computing platform; TailorFix. An automated repair framework for assignment statements; dual discriminator generative adversarial network for single image super-resolution; a recommendation system for cloud services based on knowledge graph; vision-based positioning: related technologies, applications, and research challenges; and transfer learning on convolutional neural networks for dog identification.",,,1178.0,"Proceedings of the IEEE International Conference on Software Engineering and Service Sciences, ICSESS",Conference Review,Scopus
863,,Enhancing Model Performance of Person Re-Indentification on Unknown Target Domain,"Xu R., Fu Y., Liu T., Xiang S.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063642074&doi=10.1109%2fICSESS.2018.8663745&partnerID=40&md5=37b83797f862f22e7b90d26bd8c303e3,10.1109/ICSESS.2018.8663745,"Person re-identification(ReID) is the task that aims at retrieving the same person from the images taken across different cameras. Benefiting from the improvement of deep learning algorithms and the appearance of large datasets, the performance of ReID models has been greatly improved. However, most ReID models focus on a single dataset and their performance will drop dramatically when the train-set and test-set are from different datasets. To improve the generalization ability of the ReID model, this paper proposes a method that takes the advantage of triplet loss and multi-dataset training. And the experiment results show that this method can enhance the model performace in cross dataset usage. © 2018 IEEE.",Convolutional Neural Network; Multi-dataset; Person re-identification; Triplet loss,"666, 669",,"Proceedings of the IEEE International Conference on Software Engineering and Service Sciences, ICSESS",Conference Paper,Scopus
864,,Convolutional Neural Network Architecture for Semaphore Recognition,"Li W., Yang Y., Wang M., Zhang L., Zhu M.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063640793&doi=10.1109%2fICSESS.2018.8663885&partnerID=40&md5=32a6cf2078967402352c6ef81674a832,10.1109/ICSESS.2018.8663885,"This paper proposes convolutional neural networks SRNet and Tiny-SRNet for human semaphore action recognition. SRNet is composed of 5 layers of convolution and 3 layers of fully connected layers. In addition to the first convolution layer., a batch normalization layer is added before each convolution layer. In order to enable deep learning algorithms to be applied to both mobile and embedded platforms., Tiny-SRNet removes the full connected layers in SRNet and replaces them with a convolutional layer and a global average pooling layer. The experimental results show that compared with the mainstream classification models AlexNet., GoogleNet and VGGI6., SRNet achieves the highest recognition rate of 98.9% on the semaphore dataset., and Tiny-SRNet compresses its model size to 1/24 of SRNet with a reduction of 1.7% accuracy. © 2018 IEEE.",BN; semaphore; SRNet; Tiny-SRNet,"559, 562",,"Proceedings of the IEEE International Conference on Software Engineering and Service Sciences, ICSESS",Conference Paper,Scopus
865,,Rotor Winding Image Detection Method Based on Model-Based Transfer Learning,"Jia Y., Zhang X., Chen G.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063635581&doi=10.1109%2fICSESS.2018.8663891&partnerID=40&md5=099905cb1a5823e5f8eb47f01692ad40,10.1109/ICSESS.2018.8663891,"Rotor is a core component of the electric motor. The qualification of rotor winding is one of the core factors for the proper functioning of the rotor, which is still detected by manual operation. Hence, it is important to achieve automatic detection of the rotor windings and enhance the detection accuracy. Recently, convolutional neural network (CNN) has been successfully applied to image recognition., but it requires a large number of labeled samples and there is almost no dataset bias between the target dataset and the source dataset. The challenges of using CNN to recognize rotor winding are that the winding image dataset of different types of rotor exist large dataset bias and the labeled examples are limited. We proposed a new model-based transfer learning method to deal with the challenges. To solve the dataset bias problem., we proposed a new image binarization method to get binary rotor winding images. Using the binary images to train and test model can significantly reduce the interference of dataset bias. Meanwhile., we proposed a method to build model-based transfer learning model which is based on the pre-trained Inception-V3 model trained with the ImageNet dataset., the method is used to solve the problem of limited labeled samples. The comparing experiments show that the model-based transfer learning model trained and tested with binary images significantly outperform existing other models., and can achieve stable and accurate detection of the rotor images. © 2018 IEEE.",binary image; Inception-V3; model-based transfer learning; qualification detection; rotor winding,"675, 679",,"Proceedings of the IEEE International Conference on Software Engineering and Service Sciences, ICSESS",Conference Paper,Scopus
866,,Transfer Learning on Convolutional Neural Networks for Dog Identification,"Tu X., Lai K., Yanushkevich S.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063631684&doi=10.1109%2fICSESS.2018.8663718&partnerID=40&md5=31facabf00fc91049b044602195ac019,10.1109/ICSESS.2018.8663718,"This paper considers application of machine learning in the context of animal identity management for veterinary practice. In this application, electronic medical records of animals would include digital photographs that are used to identify them using image processing and recognition technologies. We investigated how combination of the 'soft' biometrics such as breed, as well as face biometrics of dogs can improve identification of dogs. We apply transfer learning on GoogLeNet to perform the breed classification on the proposed BreedNet, and then to identify individual dogs within the classified breeds, on the proposed DogNet. © 2018 IEEE.",animal biometrics; convolutional neural networks; identification; image processing; machine learning; transfer leaning,"357, 360",,"Proceedings of the IEEE International Conference on Software Engineering and Service Sciences, ICSESS",Conference Paper,Scopus
867,,Improved Deep Belief Network to Feature Extraction in Chinese Text Classification,"Gao J., Yi J., Jia W., Zhao X.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063630494&doi=10.1109%2fICSESS.2018.8663827&partnerID=40&md5=fcb1c4833eea2d766ade63d9d7ce4f5b,10.1109/ICSESS.2018.8663827,"In the field of text classification, the classical text categorization method uses bag-of-words (BOW)model as a classification feature. The BOW model only focuses on the word frequency, but ignores the connection between words, so the BOW model has defects of insufficient expression of the text. The word embedding model maps each individual word to the new vector space, which makes up for the defects of isolated vocabulary and insufficient text expression. As a result, using the word embedding can improve the accuracy of the text classification. In order to avoid the dimension explosion, this paper uses keyword-based word embedding to represent Chinese text. In the feature extraction process, we use the combination of deep belief network and deep Boltzmann machine to extract word vector features of the text. This improved deep belief network feature extraction method further improves the accuracy of text classification. © 2018 IEEE.",deep belief network; deep learning; feature extraction; text classification; word embedding model,"283, 287",,"Proceedings of the IEEE International Conference on Software Engineering and Service Sciences, ICSESS",Conference Paper,Scopus
868,,MRI Brain Image Classification Based on Improved Topographic Sparse Coding,"Wang X., Wang W.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063625241&doi=10.1109%2fICSESS.2018.8663765&partnerID=40&md5=7eaffc31b368932a9b71235d0b747db6,10.1109/ICSESS.2018.8663765,"As the field of neuroimaging grows, imaging technology plays an increasingly important role in the auxiliary diagnosis of brain lesions. An Alzheimer's disease recognition method for magnetic resonance imaging (MRI)based on improved topological sparse coding (ITSC)is proposed. The data source is taken from the ADNI database, after correction, registration, segmentation, smoothing and other operations, the gray matter image of the brain are obtained, and then the improved topological sparse coding model of unsupervised feature learning is used to construct the deep neural network. And the optimization algorithm is used to replace the network model. The valence function is optimized iteratively, the weight matrix is studied and the new feature expression is obtained. Finally, the Softmax classifier and the auxiliary fine-tuning method are used to identify the disease. Compared with principal component analysis and a self-learning neural network, the experimental results show that the proposed method has better recognition performance. © 2018 IEEE.",Alzheimer's disease; greedy algorithm; Structural MRI; topographic sparse coding,"1116, 1119",,"Proceedings of the IEEE International Conference on Software Engineering and Service Sciences, ICSESS",Conference Paper,Scopus
869,,A Transfer Learning Method Based on Residual Block,"Chenhui Y., Chunling C.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063622441&doi=10.1109%2fICSESS.2018.8663835&partnerID=40&md5=9144354ae9f23c22612e1a678d276ca5,10.1109/ICSESS.2018.8663835,"In order to obtain high image representations in limited amount of datasets, a transfer learning method based on residual block is proposed. In this method, we follow a transfer learning approach by increasing the number of layers of the network to extract the higher order statistical features of the image. The main idea is to conduct feature transfer by means of ResNet (Deep Residual Network) model with setting ImageNet dataset as source domain. Firstly, all image data are preprocessed with data enhancement. Then, on the basis of modifying the source model's fully-connected classification layer, the adjustment module-residual block is added to the end of the network. Finally, after training the adjustment module, the deep model is achieved. Through transfer learning and deep feature extraction, the capability of feature recognition that impacted by content differences between source domain and target domain will be improved. Experiments show that our method achieves 97.98% accuracy on MNIST dataset and 90.45% accuracy on CIFAR-l 0 dataset, respectively. The experimental results demonstrate that the performance of our proposed method is significantly better than the existing transfer learning methods. © 2018 IEEE.",deep feature extraction; feature recognition; residual block; ResNet; transfer learning,"807, 810",,"Proceedings of the IEEE International Conference on Software Engineering and Service Sciences, ICSESS",Conference Paper,Scopus
870,,Proteus: Language and Runtime Support for Self-Adaptive Software Development,"Barati S., Bartha F.A., Biswas S., Cartwright R., Duracz A., Fussell D., Hoffmann H., Imes C., Miller J., Mishra N., Arvind, Nguyen D., Palem K.V., Pei Y., Pingali K., Sai R., Wright A., Yang Y.-H., Zhang S.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062223138&doi=10.1109%2fMS.2018.2884864&partnerID=40&md5=411c67a34b1115d0cee2fe5f0e60dc34,10.1109/MS.2018.2884864,[No abstract available],,"73, 82",,IEEE Software,Review,Scopus
871,,Model-Based Adaptation for Robotics Software,"Aldrich J., Garlan D., Kaestner C., Le Goues C., Mohseni-Kabir A., Ruchkin I., Samuel S., Schmerl B., Timperley C.S., Veloso M., Voysey I., Biswas J., Guha A., Holtz J., Camara J., Jamshidi P.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062165823&doi=10.1109%2fMS.2018.2885058&partnerID=40&md5=a8d01278df06a22afef116376a2318d2,10.1109/MS.2018.2885058,"We developed model-based adaptation, an approach that leverages models of software and its environment to enable automated adaptation. The goal of our approach is to build long-lasting software systems that can effectively adapt to changes in their environment. © 1984-2012 IEEE.",,"83, 90",,IEEE Software,Review,Scopus
872,,A two-phase transfer learning model for cross-project defect prediction,"Liu C., Yang D., Xia X., Yan M., Zhang X.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056828109&doi=10.1016%2fj.infsof.2018.11.005&partnerID=40&md5=b881f99f3ea909115bbdfe864ab6aaf3,10.1016/j.infsof.2018.11.005,"Context: Previous studies have shown that a transfer learning model, TCA+ proposed by Nam et al., can significantly improve the performance of cross-project defect prediction (CPDP). TCA+ achieves the improvement by reducing data distribution difference between source (training data) and target (testing data) projects. However, TCA+ is unstable, i.e., its performance varies largely when using different source projects to build prediction models. In practice, it is hard to choose a suitable source project to build the prediction model. Objective: To address the limitation of TCA+, we propose a two-phase transfer learning model (TPTL) for CPDP. Method: In the first phase, we propose a source project estimator (SPE) to automatically choose two source projects with the highest distribution similarity to a target project from candidates. Next, two source projects that are estimated to achieve the highest values of F1-score and cost-effectiveness are selected. In the second phase, we leverage TCA+ to build two prediction models based on the two selected projects and combine their prediction results to further improve the prediction performance. Results: We evaluate TPTL on 42 defect datasets from PROMISE repository, and compare it with two versions of TCA+ (TCA+_Rnd, randomly selecting one source project; TCA+_All, using all alternative source projects), a related source project selection model TDS proposed by Herbold, a state-of-the-art CPDP model leveraging a log transformation (LT) method, and a transfer learning model Dycom with better form of TCA. Experiment results show that, on average across 42 datasets, TPTL respectively improves these baseline models by 19%, 5%, 36%, 27%, and 11% in terms of F1-score; by 64%, 92%, 71%, 11%, and 66% in terms of cost-effectiveness. Conclusion: The proposed TPTL model can solve the instability problem of TCA+, showing substantial improvements over the state-of-the-art and related CPDP models. © 2018",Cross-Project prediction; Defect prediction; Source project selection; Transfer learning,"125, 136",,Information and Software Technology,Article,Scopus
873,,"Performance issues? Hey DevOps, mind the uncertainty","Trubiani C., Jamshidi P., Cito J., Shang W., Jiang Z.M., Borg M.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055138945&doi=10.1109%2fMS.2018.2875989&partnerID=40&md5=49e3d896c36c776dde5e4ef0a9ee250c,10.1109/MS.2018.2875989,DevOps is a novel trend that aims to bridge the gap between software development and operation teams. This article presents an experience report that better identifies performance uncertainties through a case study and provides a step-by-step guide to practitioners for controlling system uncertainties. © 1984-2012 IEEE.,DevOps; Performance Analysis; Software Development; Uncertainty,"110, 117",,IEEE Software,Article,Scopus
874,,Algorithm to Generate Adversarial Examples for Face-spoofing Detection [一种面向人脸活体检测的对抗样本生成算法],"Ma Y.-K., Wu L.-F., Jian M., Liu F.-H., Yang Z.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066076595&doi=10.13328%2fj.cnki.jos.005568&partnerID=40&md5=7d0bce107ee6b47b1925fad8d11a7357,10.13328/j.cnki.jos.005568,"Face-spoofing detection based on deep convolutional neural networks has achieved good performance in recent years. However, deep neural networks are vulnerable to adversarial examples, which will reduce the safety of the face based application systems. Therefore, it is necessary to analyze the mechanism of generating the adversarial examples, so that the face-spoofing detection algorithms will be more robust. Compared with the general classification problems, face-spoofing detection has the smaller inter-class distance, and the perturbation is difficulty to assign. Motivated by the above, this study proposes an approach to generate the adversarial examples for face-spoofing detection by combining the minimum perturbation dimensions and visual concentration. In the proposed approach, perturbation is concentrated on a few pixels in a single component, and the intervals between pixels are constrained-according to the visual concentration. With such constraints, the generated adversarial examples can be perceived by human with low probability. The adversarial examples generated from the proposed approach can defraud the deep neural networks based classifier with only 1.36% changed pixels on average. Furthermore, human vision perception rate of the proposed approach decreases about 20% compared with DeepFool. © Copyright 2019, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Adversarial example; Adversarial perturbation; Convolutional neural network; Face-spoofing detection; Visual concentration,"469, 480",,Ruan Jian Xue Bao/Journal of Software,Article,Scopus
875,,Software defect prediction based on kernel PCA and weighted extreme learning machine,"Xu Z., Liu J., Luo X., Yang Z., Zhang Y., Yuan P., Tang Y., Zhang T.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055260870&doi=10.1016%2fj.infsof.2018.10.004&partnerID=40&md5=a544b2e837eee0848a50de44da2838ac,10.1016/j.infsof.2018.10.004,"Context: Software defect prediction strives to detect defect-prone software modules by mining the historical data. Effective prediction enables reasonable testing resource allocation, which eventually leads to a more reliable software. Objective: The complex structures and the imbalanced class distribution in software defect data make it challenging to obtain suitable data features and learn an effective defect prediction model. In this paper, we propose a method to address these two challenges. Method: We propose a defect prediction framework called KPWE that combines two techniques, i.e., Kernel Principal Component Analysis (KPCA) and Weighted Extreme Learning Machine (WELM). Our framework consists of two major stages. In the first stage, KPWE aims to extract representative data features. It leverages the KPCA technique to project the original data into a latent feature space by nonlinear mapping. In the second stage, KPWE aims to alleviate the class imbalance. It exploits the WELM technique to learn an effective defect prediction model with a weighting-based scheme. Results: We have conducted extensive experiments on 34 projects from the PROMISE dataset and 10 projects from the NASA dataset. The experimental results show that KPWE achieves promising performance compared with 41 baseline methods, including seven basic classifiers with KPCA, five variants of KPWE, eight representative feature selection methods with WELM, 21 imbalanced learning methods. Conclusion: In this paper, we propose KPWE, a new software defect prediction framework that considers the feature extraction and class imbalance issues. The empirical study on 44 software projects indicate that KPWE is superior to the baseline methods in most cases. © 2018 Elsevier B.V.",Feature extraction; Kernel principal component analysis; Nonlinear mapping; Weighted extreme learning machine,"182, 200",,Information and Software Technology,Article,Scopus
876,,A systematic literature review and meta-analysis on cross project defect prediction,"Hosseini S., Turhan B., Gunarathna D.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034218670&doi=10.1109%2fTSE.2017.2770124&partnerID=40&md5=2905340a8fc379c1bd3f852e7fefe2c0,10.1109/TSE.2017.2770124,"Background: Cross project defect prediction (CPDP) recently gained considerable attention, yet there are no systematic efforts to analyse existing empirical evidence. Objective: To synthesise literature to understand the state-of-the-art in CPDP with respect to metrics, models, data approaches, datasets and associated performances. Further, we aim to assess the performance of CPDP versus within project DP models. Method: We conducted a systematic literature review. Results from primary studies are synthesised (thematic, meta-analysis) to answer research questions. Results: We identified 30 primary studies passing quality assessment. Performance measures, except precision, vary with the choice of metrics. Recall, precision, f-measure, and AUC are the most common measures. Models based on Nearest-Neighbour and Decision Tree tend to perform well in CPDP, whereas the popular naïve Bayes yields average performance. Performance of ensembles varies greatly across f-measure and AUC. Data approaches address CPDP challenges using row/column processing, which improve CPDP in terms of recall at the cost of precision. This is observed in multiple occasions including the meta-analysis of CPDP versus WPDP. NASA and Jureczko datasets seem to favour CPDP over WPDP more frequently. Conclusion: CPDP is still a challenge and requires more research before trustworthy applications can take place. We provide guidelines for further research. © 1976-2012 IEEE.",cross project; Defect prediction; fault prediction; meta-analysis; systematic literature review; within project,"111, 147",,IEEE Transactions on Software Engineering,Review,Scopus
877,,Multi-Class Skin Diseases Classification Using Deep Convolutional Neural Network and Support Vector Machine,"Hameed N., Shabut A.M., Hossain M.A.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062696934&doi=10.1109%2fSKIMA.2018.8631525&partnerID=40&md5=baaab1a4d6cc9e6bf3cf982e26d4f5e0,10.1109/SKIMA.2018.8631525,"Globally, skin diseases are the fourth leading cause of non-fatal disease burden. Both high and low-income countries suffer from this burden; indicates the prevention of skin diseases should be prioritised. In this research work, an intelligent diagnosis scheme is proposed for multi-class skin lesion classification. The proposed scheme is implemented using a hybrid approach i.e. using deep convolution neural network and error-correcting output codes (ECOC) support vector machine (SVM). The proposed scheme is designed, implemented and tested to classify skin lesion image into one of five categories, i.e. healthy, acne, eczema, benign, or malignant melanoma. Experiments were performed on 9,144 images obtained from different sources. AlexNET, a pre-trained CNN model was used to extract the features. For classification, the ECOC SVM classifier was used. Using ECOC SVM, the overall accuracy achieved is 86.21%. 10-fold cross validation technique was used to avoid overfitting. The results indicate that features obtained from the convolutional neural network are capable of enhancing the classification performance of multiple skin lesions. © 2018 IEEE.",Acne classification; Convolutional neural network; Eczema classification; Error-correcting output codes model; Melanoma classification; Skin lesion classification; Skin lesion detection; Support vector machine,,,"International Conference on Software, Knowledge Information, Industrial Management and Applications, SKIMA",Conference Paper,Scopus
878,,Decoding LDPC Codes on Binary Erasure Channels using Deep Recurrent Neural-Logic Layers,"Payani A., Fekri F.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062429697&doi=10.1109%2fISTC.2018.8625326&partnerID=40&md5=98324e5ebce043a87b1c928e3e9c8ec2,10.1109/ISTC.2018.8625326,"Iterative algorithms based on belief propagation can perform very close to optimal ML decoding for long LDPC codes. However, for short LDPC codes over Binary Erasure Channels (BEC) their performance drops significantly. In this paper, we introduce an iterative LDPC decoder over BEC by using our novel deep recurrent neural logic networks that learns Boolean logic algebra. It turned out that the neural logic network is capable of learning discrete algorithmic tasks and suitable for decoding LDPC codes. We show that the proposed decoding method can outperform the belief propagation algorithm for short LDPC code while using significantly fewer number of iterations. We further demonstrate that the proposed model is able to generalize very well. In other words, the model that was trained for specific settings such as channel erasure, parity check matrix, and code length, when tested under various other settings, still performed almost as if it was trained for those new settings. © 2018 IEEE.",,,,"International Symposium on Turbo Codes and Iterative Information Processing, ISTC",Conference Paper,Scopus
879,,Software fault prediction using deep learning algorithms,"Qasem O.A., Akour M.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096965570&doi=10.4018%2fIJOSSP.2019100101&partnerID=40&md5=312068fbb1d0be35facf72fb6beced6d,10.4018/IJOSSP.2019100101,"Software faults prediction (SFP) processes can be used for detecting faulty constructs at early stages of the development lifecycle, in addition to its being used in several phases of the development process. Machine learning (ML) is widely used in this area. One of the most promising subsets from ML is deep learning that achieves remarkable performance in various areas. Two deep learning algorithms are used in this paper, the Multi-layer perceptrons (MLPs) and Convolutional Neural Network (CNN). In order to evaluate the studied algorithms, four commonly used datasets from NASA are used i.e. (PC1, KC1, KC2 and CM1). The experiment results show how the CNN algorithm achieves prediction superiority of the MLP algorithm. The accuracy and detection rate measurements when using CNN has reached the standard ratio respectively as follows: PC1 97.7% - 73.9%, KC1 100% - 100%, KC2 99.3% - 99.2% and CM1 97.3% - 82.3%. This study provides promising results in using the deep learning for software fault prediction research. Copyright © 2019, IGI Global.",Convolutional Neural Network (CNN); Deep Learning Algorithms; Fault Prediction; Machine Learning (ML); Multi-Layer Perceptrons (MLP),"1, 19",,International Journal of Open Source Software and Processes,Article,Scopus
880,,Towards quality assurance in repaired models with PARMOREL,"Barriga A., Rutle A., Heldal R.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093821909&partnerID=40&md5=30c8e6427d888dd8954c918fe26949dd,,"Due to the importance of models in the software engineering process, it is crucial to keep them free of errors and assure their quality. We present PARMOREL, a framework for personalized and automatic model repair, which uses reinforcement learning to find the best sequence of actions for repairing a broken model according to preferences chosen by the user. In this paper, we present a proposal for integrating quality assurance into PARMOREL. We describe an architecture that would allow PARMOREL to learn to automatically repair models with high quality. © Actas de las 24th Jornadas de Ingenieria del Software y Bases de Datos, JISBD 2019. All rights reserved.",Model repair; Quality; Reinforcement learning,,,"Actas de las 24th Jornadas de Ingenieria del Software y Bases de Datos, JISBD 2019",Conference Paper,Scopus
881,,Software change prediction: A systematic review and future guidelines,"Malhotra R., Khanna M.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075447665&doi=10.5277%2fe-Inf190107&partnerID=40&md5=6f92522a5c9f35833238ed1909fae61f,10.5277/e-Inf190107,"Background: The importance of Software Change Prediction (SCP) has been emphasized by several studies. Numerous prediction models in literature claim to effectively predict change-prone classes in software products. These models help software managers in optimizing resource usage and in developing good quality, easily maintainable products. Aim: There is an urgent need to compare and assess these numerous SCP models in order to evaluate their effectiveness. Moreover, one also needs to assess the advancements and pitfalls in the domain of SCP to guide researchers and practitioners. Method: In order to fulfill the above stated aims, we conduct an extensive literature review of 38 primary SCP studies from January 2000 to June 2019. Results: The review analyzes the different set of predictors, experimental settings, data analysis techniques, statistical tests and the threats involved in the studies, which develop SCP models. Conclusion: Besides, the review also provides future guidelines to researchers in the SCP domain, some of which include exploring methods for dealing with imbalanced training data, evaluation of search-based algorithms and ensemble of algorithms for SCP amongst others. © 2019 Wroclaw University of Science and Technology. All rights reserved.",Change-proneness; Machine learning; Software quality; Systematic review,"227, 259",,E-Informatica Software Engineering Journal,Article,Scopus
882,,Improving code generation from descriptive text by combining deep learning and syntax rules,"Tang X., Wang Z., Qi J., Li Z.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071390647&doi=10.18293%2fSEKE2019-170&partnerID=40&md5=f109811ea84fa9ae6dfc4d14bea7fbae,10.18293/SEKE2019-170,"Code generation is a model-driven engineering approach that enables developers to generate source code automatically and achieves extremely high development productivity. Specifically, generating code from a descriptive text reduces the time and expense of software development significantly. However, the performance of existing methods is not satisfying, since they are either of low accuracy (lack of specifics of the generated code) or too complicated (lack of efficiency in training). In this work, we proposed three novel methods by combining neural architectures and syntax rules, aiming at explicitly capturing the syntactical characteristics of target code. First, we proposed three models based on the Combination of Deep learning and Syntax rules (CDS models). Then, we evaluated CDS models with BLEU metric by comparing our models with existing methods. The results show that our models outperform existing methods for the challenging code generation task. Finally, we conducted a comparative study between the three CDS models. With further analysis we provided advice on the choice of neural architectures by considering both task accuracy and efficiency. Experimental results show that (1) there is a trade-off between speed and accuracy of the model, and (2) one of our CDS models (i.e., the CDS-POOLING model) outperforms other existing methods for the challenging code generation task. © 2019 Knowledge Systems Institute Graduate School. All rights reserved.",Abstract syntax tree; Code generation; Encoder-decoder; Neural network,"385, 390",,"Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE",Conference Paper,Scopus
883,,TL-GAN: Generative adversarial networks with transfer learning for mode collapse,"Wu X., Feng S., Shi C., Li X., Yin J., Lv J.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071373558&doi=10.18293%2fSEKE2019-160&partnerID=40&md5=ac6834a78715ec2edebbcacd388b063b,10.18293/SEKE2019-160,"Image generation based on the generative adversarial network (GAN) has been widely used in the field of computer vision. It helps generate images similar to the given data by learning their distribution. However, in many tasks, training on small datasets of scenes may lead to mode collapse, such that the generated images are often blurred and almost the same. To solve this problem, we propose a generative adversarial network with transfer learning for mode collapse called TL-GAN. Owing to the size of the training dataset, we introduce transfer learning (VGG pre-training network) to extract more useful features from the underlying pixels and add them to the discriminator, which can be used to calculate the distance between samples, and to provide the discriminator with a new training target. The discriminator thus learns the best features that can distinguish between real data and generated data using the proposed model. This also enhances the learning capability of the generator, which learn further about the distribution of real data. Meanwhile, generator can produce new images more realistic. The results of experiments show that the TL-GAN can guarantee the diversity of samples. A qualitative comparison with several prevalent methods confirmed its effectiveness. © 2019 Knowledge Systems Institute Graduate School. All rights reserved.",Generative adversarial network; Image generation; Mode collapse; Transfer learning; VGG pre-training network,"723, 728",,"Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE",Conference Paper,Scopus
884,,Cross-project defect prediction via transferable deep learning-generated and handcrafted features,"Qiu S., Lu L., Cai Z., Jiang S.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071367948&doi=10.18293%2fSEKE2019-070&partnerID=40&md5=5b05d4727deabd51b7e20557a359540b,10.18293/SEKE2019-070,"Although the machine learning-based software defect prediction (SDP) method has shown promising value in software engineering, yet challenges remain. To improve the performance of SDP, some researchers have used deep learning algorithms to extract the semantic and structural features of the program. However, in more practical cross-project defect prediction (CPDP) tasks, whether deep learning-generated features can be directly used should be explored due to the data distribution shift that usually exists in different projects. In this paper, we propose a Transferable Hybrid Features Learning with Convolutional Neural Network (CNN-THFL) framework to conduct CPDP. Specially, CNN-THFL mines deep learning-generated features from token vectors extracted from programs' abstract syntax trees via convolutional neural network. Furthermore, CNN-THFL learns the transferable joint features simultaneously considering deep learning-generated and handcrafted features by applying a transfer component analysis algorithm. Finally, the features generated by CNN-THFL are fed to the classifier to train a defect prediction model. Extensive experiments verify that CNN-THFL can outperform referential methods on 72 pairs of CPDP tasks formed by 9 open-source projects. © 2019 Knowledge Systems Institute Graduate School. All rights reserved.",Cross-project defect prediction; Semantic feature learning; Software defect prediction; Transfer learning,"431, 436",,"Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE",Conference Paper,Scopus
885,,A novel online supervised hyperparameter tuning procedure applied to cross-company software effort estimation,Minku L.L.,2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062596441&doi=10.1007%2fs10664-019-09686-w&partnerID=40&md5=4479fee57d19f1f80c4d3126a2a5b327,10.1007/s10664-019-09686-w,"Software effort estimation is an online supervised learning problem, where new training projects may become available over time. In this scenario, the Cross-Company (CC) approach Dycom can drastically reduce the number of Within-Company (WC) projects needed for training, saving their collection cost. However, Dycom requires CC projects to be split into subsets. Both the number and composition of such subsets can affect Dycom’s predictive performance. Even though clustering methods could be used to automatically create CC subsets, there are no procedures for automatically tuning the number of clusters over time in online supervised scenarios. This paper proposes the first procedure for that. An investigation of Dycom using six clustering methods and three automated tuning procedures is performed, to check whether clustering with automated tuning can create well performing CC splits. A case study with the ISBSG Repository shows that the proposed tuning procedure in combination with a simple threshold-based clustering method is the most successful in enabling Dycom to drastically reduce (by a factor of 10) the number of required WC training projects, while maintaining (or even improving) predictive performance in comparison with a corresponding WC model. A detailed analysis is provided to understand the conditions under which this approach does or does not work well. Overall, the proposed online supervised tuning procedure was generally successful in enabling a very simple threshold-based clustering approach to obtain the most competitive Dycom results. This demonstrates the value of automatically tuning hyperparameters over time in a supervised way. © 2019, The Author(s).",Concept drift; Cross-company learning; Hyperparameter tuning; Online learning; Software effort estimation; Transfer learning,,,Empirical Software Engineering,Article,Scopus
886,,An active learning approach for improving the accuracy of automated domain model extraction,"Arora C., Sabetzadeh M., Nejati S., Briand L.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060897284&doi=10.1145%2f3293454&partnerID=40&md5=e58d0ed1ca0337597a95b6a571301108,10.1145/3293454,"Domain models are a useful vehicle for making the interpretation and elaboration of natural-language requirements more precise. Advances in natural-language processing (NLP) have made it possible to automatically extract from requirements most of the information that is relevant to domain model construction. However, alongside the relevant information, NLP extracts from requirements a significant amount of information that is superfluous (not relevant to the domain model). Our objective in this article is to develop automated assistance for filtering the superfluous information extracted by NLP during domain model extraction. To this end, we devise an active-learning-based approach that iteratively learns from analysts’ feedback over the relevance and superfluousness of the extracted domain model elements and uses this feedback to provide recommendations for filtering superfluous elements. We empirically evaluate our approach over three industrial case studies. Our results indicate that, once trained, our approach automatically detects an average of ≈45% of the superfluous elements with a precision of ≈96%. Since precision is very high, the automatic recommendations made by our approach are trustworthy. Consequently, analysts can dispose of a considerable fraction – nearly half – of the superfluous elements with minimal manual work. The results are particularly promising, as they should be considered in light of the non-negligible subjectivity that is inherently tied to the notion of relevance. © 2019 Association for Computing Machinery.",Active learning; Case study research; Domain modeling; Natural-language requirements; Requirements engineering,,,ACM Transactions on Software Engineering and Methodology,Article,Scopus
887,,Determining relevant training data for effort estimation using Window-based COCOMO calibration,"Nguyen V., Boehm B., Huang L.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055346555&doi=10.1016%2fj.jss.2018.10.019&partnerID=40&md5=e87e294090b213d620a750df36780a78,10.1016/j.jss.2018.10.019,"Context: A software estimation model is often built using historical project data. As software development practices change over time, however, a model based on past data may not make accurate predictions for a new project. Objectives: We investigate the use of moving windows to determine relevant training data for COCOMO calibration. Method: We present a windowing calibration approach to calibrating COCOMO and assess performance of effort estimation models calibrated using windows and all data. Results: Our results show that calibrating COCOMO using small windows of the most recently completed projects generates superior estimates than using all available historical projects. Large windows tend to produce worse estimates. Conclusions: This study provides empirical evidence to support the use of small windows of projects completed so far to calibrate models when COCOMO-like data is available. Additionally, when the change in software development over time is rapid, the use of windows is more justifiable for improving estimation accuracy. © 2018",COCOMO; Model calibration; Moving windows; Project management; Software estimation; Window-based calibration,"124, 146",,Journal of Systems and Software,Article,Scopus
888,,White blood cells identification system based on convolutional deep neural learning networks,"Shahin A.I., Guo Y., Amin K.M., Sharawi A.A.",2019,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034981078&doi=10.1016%2fj.cmpb.2017.11.015&partnerID=40&md5=aad701f09af83dbfe093559c6e53e66f,10.1016/j.cmpb.2017.11.015,"Background and objectives: White blood cells (WBCs) differential counting yields valued information about human health and disease. The current developed automated cell morphology equipments perform differential count which is based on blood smear image analysis. Previous identification systems for WBCs consist of successive dependent stages; pre-processing, segmentation, feature extraction, feature selection, and classification. There is a real need to employ deep learning methodologies so that the performance of previous WBCs identification systems can be increased. Classifying small limited datasets through deep learning systems is a major challenge and should be investigated. Methods: In this paper, we propose a novel identification system for WBCs based on deep convolutional neural networks. Two methodologies based on transfer learning are followed: transfer learning based on deep activation features and fine-tuning of existed deep networks. Deep acrivation featues are extracted from several pre-trained networks and employed in a traditional identification system. Moreover, a novel end-to-end convolutional deep architecture called “WBCsNet” is proposed and built from scratch. Finally, a limited balanced WBCs dataset classification is performed through the WBCsNet as a pre-trained network. Results: During our experiments, three different public WBCs datasets (2551 images) have been used which contain 5 healthy WBCs types. The overall system accuracy achieved by the proposed WBCsNet is (96.1%) which is more than different transfer learning approaches or even the previous traditional identification system. We also present features visualization for the WBCsNet activation which reflects higher response than the pre-trained activated one. Conclusion: a novel WBCs identification system based on deep learning theory is proposed and a high performance WBCsNet can be employed as a pre-trained network. © 2017",Blood smear image; Deep features visualization; Deep learning; Transfer deep learning; WBCs identification,"69, 80",,Computer Methods and Programs in Biomedicine,Article,Scopus
889,,Visual Feature Combination Approach for Zero-Shot Learning [一种基于视觉特征组合构造的零样本学习方法],"Yang G., Liu J.-L., Li X.-R., Xu J.-P.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074084834&partnerID=40&md5=72c09b1a68378a3fc45e66ab1aafcdd2,,"Zero-Shot learning is an important research in the field of machine learning and image recognition. Zero-Shot learning methods normally use the semantic information among unseen classes and seen classes to transfer the knowledge which is learned from examples of seen classes to unseen classes, so as to recognize and classify the examples of unseen classes. In this study, a zero-shot learning approach based on construction of visual feature combination is proposed. The approach generates many examples of unseen class on visual feature level by the way of feature combination, which is first proposed, and thus transforms zero-shot learning problem to be a traditional classification problem solved by supervised learning. The approach mimics human cognition process of associative memory, and includes four steps: feature-attribute relation extraction, example construction, example screening, and domain adaption. On training examples of seen classes, the relationship between class attributes and dimensions of feature is extracted; on visual feature level, examples of unseen classes are generated by visual feature combination; dissimilarity representation is introduced to filter the generated examples of unseen classes; semi-supervised and unsupervised feature domain adaption are proposed to linearly transform the generated examples of unseen classes to be more effective. The proposed approach shows superior performance on three benchmark datasets (AwA, AwA2, and SUN), especially on dataset AwA, it obtains 82.6% top-1 accuracy which is the best result as far as we know. Experiment results demonstrate the effectiveness and superiority of the proposed approach. © Copyright 2018, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Data preprocessing; Dissimilarity representation; Image classification; Zero-shot learning,"16, 29",,Ruan Jian Xue Bao/Journal of Software,Article,Scopus
890,,Modality Compensation Based Action Recognition [关联模态补偿的视频动作识别算法],"Song S.-J., Liu J.-Y., Li Y.-H., Guo Z.-M.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074074059&partnerID=40&md5=9341f805760ba9eaf0924206f1ff859f,,"With the prevalence of depth cameras, video data of different modalities become more common. Multi-Modal data based human action recognition attracts increasing attention. Different modal data describe human actions from distinct perspectives. How to effectively utilize the complementary information of multi-modal data is a key topic in this area. In this study, we propose a modality compensation based method for action recognition. With RGB/optical flow as source modal data and skeletons as auxiliary modal data, we aim to compensate the feature learning from source modal data, through exploring the common spaces between source and auxiliary modalities. The proposed model is based on deep convolutional neural network (CNN) and long short term memory (LSTM) network to extract spatial and temporal features. With the help of residual learning, a modality adaptation block is proposed to align the distributions of different modalities and achieve modality compensation. To deal with different alignment of source and auxiliary modal data, we propose hierarchical modality adaptation schemes. The proposed model only requires auxiliary modal data in the training process, and is able to improve the recognition performance only with source modal data in the testing phase, which expands the application scenarios of the proposed model. The experiment results illustrate that proposed method outperforms other state-of-the-art approaches. © Copyright 2018, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Action recognition; Deep learning; Modality compensation; Multi-modal data; Residual learning,"1, 15",,Ruan Jian Xue Bao/Journal of Software,Article,Scopus
891,,Image Description Method Based on Generative Adversarial Networks [一种基于生成式对抗网络的图像描述方法],"Xue Z.-Y., Guo P.-Y., Zhu X.-B., Zhang N.-G.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074062369&partnerID=40&md5=9bd696bc779b16e73bdf66f4fe3f1609,,"In recent years, deep learning has gained more and more attention in image description. The existing deep learning methods using CNNs to extract features and RNNs to fold into one sentence. Nevertheless, when dealing with complex images, the feature extraction is inaccurate. And the fixed mode of sentence generation model leads to inconsistent sentences. To solve this problem, this study proposes a method combine channel-wise attention model and GANs, named CACNN-GAN. The channel-wise attention mechanism is added to each conv-layer to extract features, compare with the COCO dataset, and select the top features to generate sentence. Using GANs to generate the sentences, which is generated by the game process between the generator and the discriminator. After that, we can get a sentence generator contains the varied syntax, smooth sentence, and rich vocabulary. Experiments on real datasets illustrates that CACNN-GAN can effectively describe images, and get higher accuracy compared with the state-of-art. © Copyright 2018, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Channel-wise attention model; Convolutional neural network; Generative adversarial networks; Image description,"30, 43",,Ruan Jian Xue Bao/Journal of Software,Article,Scopus
892,,Definition and evaluation of a COSMIC measurement procedure for sizing Web applications in a model-driven development environment,"Abrahão S., De Marco L., Ferrucci F., Gomez J., Gravino C., Sarro F.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050865497&doi=10.1016%2fj.infsof.2018.07.012&partnerID=40&md5=56d586e34b30c8c155fb2fe47a4f215c,10.1016/j.infsof.2018.07.012,"Context. Model-driven development approaches facilitate the production of Web applications. Among them, the Object-Oriented Hypermedia method (OO-H) has been successfully used for the development of industrial Web applications. Similarly to other development approaches, it is important also in this context to put measures in place to support project managers in resource allocation, cost and schedule control, and productivity monitoring. Objective. This motivated us to define a measurement procedure, named OO-HCFP, specifically conceived for OO-H Web applications based on COSMIC, a second-generation functional size measurement method. Method. We present mapping and measurement rules devised to automatically derive size measures from OO-H models. We also carry out an empirical study to evaluate whether our proposed measurement procedure, OO-HCFP, is useful for estimating the effort needed to realise industrial Web applications developed with OO-H. Results. The estimates obtained by using OO-HCFP are more accurate than those obtained by using other measurement approaches based on Function Points and design measures. Conclusions. The proposed approach can be profitably exploited to size Web applications developed with OO-H. Based on our experience, we also provide some guidelines to support the formulation of COSMIC measurement procedures for other model-driven approaches. © 2018 Elsevier B.V.",COSMIC; Functional size measurement; Model-driven development; OO-H method; Web applications,"144, 161",,Information and Software Technology,Article,Scopus
893,,Development and Evaluation of Word Embeddings for Morphologically Rich Languages,"Vasic D., Brajkovic E.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060256891&doi=10.23919%2fSOFTCOM.2018.8555822&partnerID=40&md5=ec9ff349e9cd90e9617002c0db5eab45,10.23919/SOFTCOM.2018.8555822,"Recent advancements in natural language processing (NLP) improved many systems that were relying on natural language to achieve better communication with user using this system. One of the main problem in application of NLP in multiple languages is lack of tools that can be used to develop such systems. Croatian language is highly inflected language from Slavic language family and traditional models used that give great results for English language behave poorly for morphology rich languages. In this article we present model for creating word embeddings for morphologically rich languages such as Croatian. We evaluate the generated word embeddings on newly created word similarity corpus, that is based on English similarity corpus. In the evaluation of word embeddings we compare with two of the best word representation models for English language. We also evaluate our approach with multi-language models such as FastText. The word embeddings created in this article will be used for developing component in training neural models for semantic understanding of sentences written in Croatian language. These language tools can be utilized in many systems where natural language understanding (NLU) and natural language generation (NLG) is needed. In the introduction we give global insight about word embeddings, what are the models for creating such representations and where these representations could be used. In the second section we mention some of the best models for creating word embeddings. In the third section we give a frame-work for development and evaluation of word embeddings for Croatian language. In the conclusion we emphasis the importance of developing tools in Croatian language and announcement of future research. © 2018 University of Split, FESB.",Intelligent tutoring systems; Morphologically rich languages; Natural language processing; Neural network models; Word embeddings,"308, 313",,"2018 26th International Conference on Software, Telecommunications and Computer Networks, SoftCOM 2018",Conference Paper,Scopus
894,,Is one hyperparameter optimizer enough?,"Tu H., Nair V.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058421235&doi=10.1145%2f3278142.3278145&partnerID=40&md5=21e3ec7de6bc26d8b17d485d8933b879,10.1145/3278142.3278145,"Hyperparameter tuning is the black art of automatically finding a good combination of control parameters for a data miner. While widely applied in empirical Software Engineering, there has not been much discussion on which hyperparameter tuner is best for software analytics. To address this gap in the literature, this paper applied a range of hyperparameter optimizers (grid search, random search, differential evolution, and Bayesian optimization) to a defect prediction problem. Surprisingly, no hyperparameter optimizer was observed to be ""best"" and, for one of the two evaluation measures studied here (F-measure), hyperparameter optimization, in 50% of cases, was no better than using default configurations. We conclude that hyperparameter optimization is more nuanced than previously believed. While such optimization can certainly lead to large improvements in the performance of classifiers used in software analytics, it remains to be seen which specific optimizers should be applied to a new dataset. © SWAN 2018 - Proceedings of the 4th ACM SIGSOFT International Workshop on Soft ware Analytics, co-located with FSE 2018.",Defect prediction; Hyperparameter tuning; SBSE,"19, 25",,"SWAN 2018 - Proceedings of the 4th ACM SIGSOFT International Workshop on Software Analytics, co-located with FSE 2018",Conference Paper,Scopus
895,,A semi-supervised deep learning method based on stacked sparse auto-encoder for cancer prediction using RNA-seq data,"Xiao Y., Wu J., Lin Z., Zhao X.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054745104&doi=10.1016%2fj.cmpb.2018.10.004&partnerID=40&md5=85c972f31b9622b95de320bb16a99bd9,10.1016/j.cmpb.2018.10.004,"Background and objective: Cancer has become a complex health problem due to its high mortality. Over the past few decades, with the rapid development of the high-throughput sequencing technology and the application of various machine learning methods, remarkable progress in cancer research has been made based on gene expression data. At the same time, a growing amount of high-dimensional data has been generated, such as RNA-seq data, which calls for superior machine learning methods able to deal with mass data effectively in order to make accurate treatment decision. Methods: In this paper, we present a semi-supervised deep learning strategy, the stacked sparse auto-encoder (SSAE) based classification, for cancer prediction using RNA-seq data. The proposed SSAE based method employs the greedy layer-wise pre-training and a sparsity penalty term to help capture and extract important information from the high-dimensional data and then classify the samples. Results: We tested the proposed SSAE model on three public RNA-seq data sets of three types of cancers and compared the prediction performance with several commonly-used classification methods. The results indicate that our approach outperforms the other methods for all the three cancer data sets in various metrics. Conclusions: The proposed SSAE based semi-supervised deep learning model shows its promising ability to process high-dimensional gene expression data and is proved to be effective and accurate for cancer prediction. © 2018 Elsevier B.V.",Cancer prediction; Deep learning; Gene expression data; Semi-supervised learning; Stacked sparse auto-encoder,"99, 105",,Computer Methods and Programs in Biomedicine,Article,Scopus
896,,Automated detection and classification of liver fibrosis stages using contourlet transform and nonlinear features,"Acharya U.R., Raghavendra U., Koh J.E.W., Meiburger K.M., Ciaccio E.J., Hagiwara Y., Molinari F., Leong W.L., Vijayananthan A., Yaakup N.A., Fabell M.K.B.M., Yeong C.H.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054743325&doi=10.1016%2fj.cmpb.2018.10.006&partnerID=40&md5=743269e5a09acd6111399c89db6d075c,10.1016/j.cmpb.2018.10.006,"Background and objective: Liver fibrosis is a type of chronic liver injury that is characterized by an excessive deposition of extracellular matrix protein. Early detection of liver fibrosis may prevent further growth toward liver cirrhosis and hepatocellular carcinoma. In the past, the only method to assess liver fibrosis was through biopsy, but this examination is invasive, expensive, prone to sampling errors, and may cause complications such as bleeding. Ultrasound-based elastography is a promising tool to measure tissue elasticity in real time; however, this technology requires an upgrade of the ultrasound system and software. In this study, a novel computer-aided diagnosis tool is proposed to automatically detect and classify the various stages of liver fibrosis based upon conventional B-mode ultrasound images. Methods: The proposed method uses a 2D contourlet transform and a set of texture features that are efficiently extracted from the transformed image. Then, the combination of a kernel discriminant analysis (KDA)-based feature reduction technique and analysis of variance (ANOVA)-based feature ranking technique was used, and the images were then classified into various stages of liver fibrosis. Results: Our 2D contourlet transform and texture feature analysis approach achieved a 91.46% accuracy using only four features input to the probabilistic neural network classifier, to classify the five stages of liver fibrosis. It also achieved a 92.16% sensitivity and 88.92% specificity for the same model. The evaluation was done on a database of 762 ultrasound images belonging to five different stages of liver fibrosis. Conclusions: The findings suggest that the proposed method can be useful to automatically detect and classify liver fibrosis, which would greatly assist clinicians in making an accurate diagnosis. © 2018 Elsevier B.V.",Computer-aided diagnosis; Contourlet transform; Liver fibrosis; Probabilistic neural network; Texture features,"91, 98",,Computer Methods and Programs in Biomedicine,Article,Scopus
897,,Cascaded classifiers and stacking methods for classification of pulmonary nodule characteristics,Kaya A.,2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054608582&doi=10.1016%2fj.cmpb.2018.10.009&partnerID=40&md5=a970d91de1a228b26c920554aad77565,10.1016/j.cmpb.2018.10.009,"Background and Objectives: Detection and classification of pulmonary nodules are critical tasks in medical image analysis. The Lung Image Database Consortium (LIDC) database is a widely used resource for small pulmonary nodule classification research. This dataset is comprised of nodule characteristic evaluations and CT scans of patients. Although these characteristics are utilized in several studies, they can be used to improve classification performance. Methods: Numerous methods have been proposed to classify malignancy, but there are not many studies that facilitate nodule characteristics in classification steps. In this study, we use information on nodule characteristics and propose cascaded classification schemes. A group of hand-crafted features and deep features are used to define the nodules. In the first step of the classifier, the nodule characteristics are classified based on individual base classifiers. In the second step, the results of the first level classifier are combined for use in malignancy classification. In addition, stacking methods are applied to improve the performance of the cascaded classifiers. Results: The results confirmed that combining deep and hand-crafted features contribute to classification performance with an 8% improvement in average classification accuracy, 9% improvement in sensitivity, and 3% in specificity. Deep features from a nodule bounding area are more descriptive than the exact nodule region. The best performing cascaded classifier featured a classification accuracy of 84.70%, sensitivity of 67.37%, and specificity of 95.46%. First level stacking demonstrated similar results on classification accuracy and specificity but sensitivity was measured at 75.59%. Stacking on both levels provided the best classification accuracy and specificity with scores of 86.98% and 96.06%, respectively. When the malignancy ratings were grouped, stacking on both levels demonstrated better performance than other methods with a classification accuracy of 88.80%, sensitivity of 88.41%, and specificity of 94.12%. Conclusions: Information on cascading characteristics with image features is beneficial for the classification of the malignancy ratings. Stacking approaches on both levels demonstrate better classification accuracy, but in the context of sensitivity, first level stacking performs better. Grouping the malignancy ratings results in better classification outcomes as in the case of similar studies in the literature. © 2018 Elsevier B.V.",Cascaded classifiers; Nodule characteristic; Pulmonary nodules; Stacking; Transfer learning,"77, 89",,Computer Methods and Programs in Biomedicine,Article,Scopus
898,,Applications of psychological science for actionable analytics,"Chen D., Fu W., Krishna R., Menzies T.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058324185&doi=10.1145%2f3236024.3236050&partnerID=40&md5=35cabe5a4dba3791a11f73e0a25c6d7d,10.1145/3236024.3236050,"According to psychological scientists, humans understand models that most match their own internal models, which they characterize as lists of łheuristicžs (i.e. lists of very succinct rules). One such heuristic rule generator is the Fast-and-Frugal Trees (FFT) preferred by psychological scientists. Despite their successful use in many applied domains, FFTs have not been applied in software analytics. Accordingly, this paper assesses FFTs for software analytics. We ind that FFTs are remarkably efective in that their models are very succinct (5 lines or less describing a binary decision tree) while also outperforming result from very recent, top-level, conference papers. Also, when we restrict training data to operational attributes (i.e., those attributes that are frequently changed by developers), the performance of FFTs are not efected (while the performance of other learners can vary wildly). Our conclusions are two-fold. Firstly, there is much that software analytics community could learn from psychological science. Secondly, proponents of complex methods should always baseline those methods against simpler alternatives. For example, FFTs could be used as a standard baseline learner against which other software analytics tools are compared. © 2018 Association for Computing Machinery.",Decision trees; defect prediction; empirical studies; heuristics; psychological science; software analytics,"456, 467",,ESEC/FSE 2018 - Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering,Conference Paper,Scopus
899,,Predicting node failure in cloud service systems,"Lin Q., Hsieh K., Dang Y., Zhang H., Sui K., Xu Y., Lou J.-G., Li C., Wu Y., Yao R., Chintalapati M., Zhang D.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058275681&doi=10.1145%2f3236024.3236060&partnerID=40&md5=16050d5b6f531a5b28c7651e95e02750,10.1145/3236024.3236060,"In recent years, many traditional software systems have migrated to cloud computing platforms and are provided as online services. The service quality matters because system failures could seriously affect business and user experience. A cloud service system typically contains a large number of computing nodes. In reality, nodes may fail and affect service availability. In this paper, we propose a failure prediction technique, which can predict the failure-proneness of a node in a cloud service system based on historical data, before node failure actually happens. The ability to predict faulty nodes enables the allocation and migration of virtual machines to the healthy nodes, therefore improving service availability. Predicting node failure in cloud service systems is challenging, because a node failure could be caused by a variety of reasons and reflected by many temporal and spatial signals. Furthermore, the failure data is highly imbalanced. To tackle these challenges, we propose MING, a novel technique that combines: 1) a LSTM model to incorporate the temporal data, 2) a Random Forest model to incorporate spatial data; 3) a ranking model that embeds the intermediate results of the two models as feature inputs and ranks the nodes by their failure-proneness, 4) a cost-sensitive function to identify the optimal threshold for selecting the faulty nodes. We evaluate our approach using real-world data collected from a cloud service system. The results confirm the effectiveness of the proposed approach.We have also successfully applied the proposed approach in real industrial practice. © 2018 Association for Computing Machinery.",cloud service systems; Failure prediction; maintenance; node failure; service availability,"480, 490",,ESEC/FSE 2018 - Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering,Conference Paper,Scopus
900,,Learning to sample: Exploiting similarities across environments to learn performance models for configurable systems,"Jamshidi P., Velez M., Kästner C., Siegmund N.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058269084&doi=10.1145%2f3236024.3236074&partnerID=40&md5=265bc241a2ad0ec98769e48c5e3cff40,10.1145/3236024.3236074,"Most software systems provide options that allow users to tailor the system in terms of functionality and qualities. The increased flexibility raises challenges for understanding the configuration space and the effects of options and their interactions on performance and other non-functional properties. To identify how options and interactions affect the performance of a system, several sampling and learning strategies have been recently proposed. However, existing approaches usually assume a fixed environment (hardware, workload, software release) such that learning has to be repeated once the environment changes. Repeating learning and measurement for each environment is expensive and often practically infeasible. Instead, we pursue a strategy that transfers knowledge across environments but sidesteps heavyweight and expensive transferlearning strategies. Based on empirical insights about common relationships regarding (i) influential options, (ii) their interactions, and (iii) their performance distributions, our approach, L2S (Learning to Sample), selects better samples in the target environment based on information from the source environment. It progressively shrinks and adaptively concentrates on interesting regions of the configuration space. With both synthetic benchmarks and several real systems, we demonstrate that L2S outperforms state of the art performance learning and transfer-learning approaches in terms of measurement effort and learning accuracy. © 2018 Association for Computing Machinery.",configurable systems; Software performance; transfer learning,"71, 82",,ESEC/FSE 2018 - Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering,Conference Paper,Scopus
901,,Software engineering challenges of deep learning,"Arpteg A., Brinne B., Crnkovic-Friis L., Bosch J.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057169222&doi=10.1109%2fSEAA.2018.00018&partnerID=40&md5=4f67fe396baebb02964c19140b99a05d,10.1109/SEAA.2018.00018,"Surprisingly promising results have been achieved by deep learning (DL) systems in recent years. Many of these achievements have been reached in academic settings, or by large technology companies with highly skilled research groups and advanced supporting infrastructure. For companies without large research groups or advanced infrastructure, building high-quality production-ready systems with DL components has proven challenging. There is a clear lack of well-functioning tools and best practices for building DL systems. It is the goal of this research to identify what the main challenges are, by applying an interpretive research approach in close collaboration with companies of varying size and type. A set of seven projects have been selected to describe the potential with this new technology and to identify associated main challenges. A set of 12 main challenges has been identified and categorized into the three areas of development, production, and organizational challenges. Furthermore, a mapping between the challenges and the projects is defined, together with selected motivating descriptions of how and why the challenges apply to specific projects. Compared to other areas such as software engineering or database technologies, it is clear that DL is still rather immature and in need of further work to facilitate development of high-quality systems. The challenges identified in this paper can be used to guide future research by the software engineering and DL communities. Together, we could enable a large number of companies to start taking advantage of the high potential of the DL technology. © 2018 IEEE.",Artificial intelligence; Deep learning; Machine learning; Software engineering challenges,"50, 59",,"Proceedings - 44th Euromicro Conference on Software Engineering and Advanced Applications, SEAA 2018",Conference Paper,Scopus
902,,Enhanced feature selection using word embeddings for self-admitted technical debt identification,"Flisar J., Podgorelec V.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057152167&doi=10.1109%2fSEAA.2018.00045&partnerID=40&md5=2515f9dad60877d3a0677a95c896d60f,10.1109/SEAA.2018.00045,"Technical debt (TD) is a term used to describe a trade off between code quality and timely software release. Since technical debt has negative impact on software development, identification of such debt is an important task in the software engineering domain. Sometimes, technical debt is annotated in source code comments. This kind of debt is referred to as self-admitted technical debt (SATD). Recently, some studies have focused on automated detection and classification of SATD using natural language processing methods. However, these methods have only used manually annotated data to train their classifiers. In this paper, we present the results of a performed exploratory study for using large corpus of unlabeled code comments, extracted from open source projects on git-hub, to train word embeddings, in order to improve detection of SATD. Our approach aims to enhance the feature selection method by taking advantage of the pre-trained word embeddings to detect similar features in source code comments. The experimental results show a significant improvement in SATD classification. With achieved 82% of correct predictions of SATD, the method seems to be a good candidate to be adopted in practice. © 2018 IEEE.",Feature selection; Self-admitted technical debt; Text classification; Word embeddings,"230, 233",,"Proceedings - 44th Euromicro Conference on Software Engineering and Advanced Applications, SEAA 2018",Conference Paper,Scopus
903,,An exploratory study of search based training data selection for cross project defect prediction,"Hosseini S., Turhan B.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057150579&doi=10.1109%2fSEAA.2018.00048&partnerID=40&md5=5766626252dad0de4f3fa08fdb718633,10.1109/SEAA.2018.00048,"Context: Search based approaches are gaining attention in cross project defect prediction (CPDP). The complexity of such approaches and existence of various design decisions are important issues to consider. Objective: We aim at investigating factors that can affect the performance of search based selection (SBS) approaches. We study a genetic instance selection approach (GIS) and present an evaluation of design options for search based CPDP. Method: Using an exploratory approach, data from different options of models are gathered and analyzed through ANOVA tests and effect sizes. Results: Both feature sets and validation dataset selection options show small or insignificant impacts on F-measure and precision, unlike the more affected false positive and true negative rates. Size of training data does not seem to be related to significant changes in F-measure and precision and high variability in performance are discouraging evidence for using larger datasets. Fitness function is one of the major factors that impact performance with much larger effect than the choice of validation dataset. Finally, while showing slight impacts, data label changes do not seem to be the top contributor to performance. Conclusions: We conclude that exploratory approaches can be effective for making design decisions in constructing search based CPDP models. Effect of individual tuned learners and their interaction with other affecting parameters and more in depth study of quality affecting factors guided by label changes are directions to investigate. © 2018 IEEE.",Cross project defect prediction; Exploratory search based optimization; Training data selection,"244, 251",,"Proceedings - 44th Euromicro Conference on Software Engineering and Advanced Applications, SEAA 2018",Conference Paper,Scopus
904,,Survey on Bayesian Optimization Methodology and Applications [贝叶斯优化方法和应用综述],"Cui J.-X., Yang B.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059102730&doi=10.13328%2fj.cnki.jos.005607&partnerID=40&md5=d9cf71fac7df7199eaf2e11e1b9b45eb,10.13328/j.cnki.jos.005607,"Designing problems are ubiquitous in science research and industry applications. In recent years, Bayesian optimization, which acts as a very effective global optimization algorithm, has been widely applied in designing problems. By structuring the probabilistic surrogate model and the acquisition function appropriately, Bayesian optimization framework can guarantee to obtain the optimal solution under a few numbers of function evaluations, thus it is very suitable to solve the extremely complex optimization problems in which their objective functions could not be expressed, or the functions are non-convex, multimodal and computational expensive. This paper provides a detailed analysis on Bayesian optimization in methodology and application areas, and discusses its research status and the problems in future researches. This work is hopefully beneficial to the researchers from the related communities. © Copyright 2018, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Acquisition function; Bayesian optimization; Black-box; Global optimization algorithm; Probabilistic surrogate model,"3068, 3090",,Ruan Jian Xue Bao/Journal of Software,Review,Scopus
905,,Jointly Modeling Heterogeneous Social and Content Information for Event Recommendation [联合建模异构社交和内容信息的活动推荐模型],"Wang S.-Q., Wang Z., Li C.-P., Zhao K.-K., Chen H.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059056031&doi=10.13328%2fj.cnki.jos.005284&partnerID=40&md5=2fa08f85d8c63d68e6b402f88d57f871,10.13328/j.cnki.jos.005284,"Event-Based social networks (EBSNs) have experienced rapid growth in people's daily life. Hence, event recommendation plays an important role in helping people discover interesting online events and attend offline activities face to face in the real world. However, event recommendation is quite different from traditional recommender systems, and there are several challenges: (1) One user can only attend a scarce number of events, leading to a very sparse user-event matrix; (2) The response data of users is implicit feedback; (3) Events have their life cycles, so outdated events should not be recommended to users; (4) A large number of new events which are created every day need to be recommended to users in time. To cope with these challenges, this article proposes to jointly model heterogeneous social and content information for event recommendation. This approach explores both the online and offline social interactions and fuses the content of events to model their joint effect on users' decision-making for events. Extensive experiments are conducted to evaluate the performance of the proposed model on Meetup dataset. The experimental results demonstrate that the proposed model outperforms state-of-the-art methods. © Copyright 2018, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Event based social network; Event recommendation; Jointly modeling; Poisson factorization; Social network analysis,"3134, 3149",,Ruan Jian Xue Bao/Journal of Software,Article,Scopus
906,,Single-view 2D CNNs with fully automatic non-nodule categorization for false positive reduction in pulmonary nodule detection,"Eun H., Kim D., Jung C., Kim C.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053065489&doi=10.1016%2fj.cmpb.2018.08.012&partnerID=40&md5=f0d45b1f726e083a8e1472934ff57938,10.1016/j.cmpb.2018.08.012,"Background and Objective: In pulmonary nodule detection, the first stage, candidate detection, aims to detect suspicious pulmonary nodules. However, detected candidates include many false positives and thus in the following stage, false positive reduction, such false positives are reliably reduced. Note that this task is challenging due to 1) the imbalance between the numbers of nodules and non-nodules and 2) the intra-class diversity of non-nodules. Although techniques using 3D convolutional neural networks (CNNs) have shown promising performance, they suffer from high computational complexity which hinders constructing deep networks. To efficiently address these problems, we propose a novel framework using the ensemble of 2D CNNs using single views, which outperforms existing 3D CNN-based methods. Methods: Our ensemble of 2D CNNs utilizes single-view 2D patches to improve both computational and memory efficiency compared to previous techniques exploiting 3D CNNs. We first categorize non-nodules on the basis of features encoded by an autoencoder. Then, all 2D CNNs are trained by using the same nodule samples, but with different types of non-nodules. By extending the learning capability, this training scheme resolves difficulties of extracting representative features from non-nodules with large appearance variations. Note that, instead of manual categorization requiring the heavy workload of radiologists, we propose to automatically categorize non-nodules based on the autoencoder and k-means clustering. Results: We performed extensive experiments to validate the effectiveness of our framework based on the database of the lung nodule analysis 2016 challenge. The superiority of our framework is demonstrated through comparing the performance of five frameworks trained with differently constructed training sets. Our proposed framework achieved state-of-the-art performance (0.922 of the competition performance metric score) with low computational demands (789K of parameters and 1024M of floating point operations per second). Conclusion: We presented a novel false positive reduction framework, the ensemble of single-view 2D CNNs with fully automatic non-nodule categorization, for pulmonary nodule detection. Unlike previous 3D CNN-based frameworks, we utilized 2D CNNs using 2D single views to improve computational efficiency. Also, our training scheme using categorized non-nodules, extends the learning capability of representative features of different non-nodules. Our framework achieved state-of-the-art performance with low computational complexity. © 2018 Elsevier B.V.",Automatic non-nodule categorization; Computer-aided detection; Deep learning; False positive reduction; Pulmonary nodule detection,"215, 224",,Computer Methods and Programs in Biomedicine,Article,Scopus
907,,Transfer learning for classification of cardiovascular tissues in histological images,"Mazo C., Bernal J., Trujillo M., Alegre E.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051947652&doi=10.1016%2fj.cmpb.2018.08.006&partnerID=40&md5=1905d14052eab2e6b0e023c23df3f33f,10.1016/j.cmpb.2018.08.006,"Background and Objective: Automatic classification of healthy tissues and organs based on histology images is an open problem, mainly due to the lack of automated tools. Solutions in this regard have potential in educational medicine and medical practices. Some preliminary advances have been made using image processing techniques and classical supervised learning. Due to the breakthrough performance of deep learning in various areas, we present an approach to recognise and classify, automatically, fundamental tissues and organs using Convolutional Neural Networks (CNN). Methods: We adapt four popular CNNs architectures – ResNet, VGG19, VGG16 and Inception – to this problem through transfer learning. The resulting models are evaluated at three stages. Firstly, all the transferred networks are compared to each other. Secondly, the best resulting fine-tuned model is compared to an ad-hoc 2D multi-path model to outline the importance of transfer learning. Thirdly, the same model is evaluated against the state-of-the-art method, a cascade SVM using LBP-based descriptors, to contrast a traditional machine learning approach and a representation learning one. The evaluation task consists of separating six classes accurately: smooth muscle of the elastic artery, smooth muscle of the large vein, smooth muscle of the muscular artery, cardiac muscle, loose connective tissue, and light regions. The different networks are tuned on 6000 blocks of 100 × 100 pixels and tested on 7500. Results: Our proposal yields F-score values between 0.717 and 0.928. The highest and lowest performances are for cardiac muscle and smooth muscle of the large vein, respectively. The main issue leading to limited classification scores for the latter class is its similarity with the elastic artery. However, this confusion is evidenced during manual annotation as well. Our algorithm reached improvements in F-score between 0.080 and 0.220 compared to the state-of-the-art machine learning approach. Conclusions: We conclude that it is possible to classify healthy cardiovascular tissues and organs automatically using CNNs and that deep learning holds great promise to improve tissue and organs classification. We left our training and test sets, models and source code publicly available to the research community. © 2018 Elsevier B.V.",Cardiovascular system; Fundamental tissues; Histological images; Organs; SVM; Transfer learning,"69, 76",,Computer Methods and Programs in Biomedicine,Article,Scopus
908,,Fast unsupervised nuclear segmentation and classification scheme for automatic allred cancer scoring in immunohistochemical breast tissue images,"Mouelhi A., Rmili H., Ali J.B., Sayadi M., Doghri R., Mrad K.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051670704&doi=10.1016%2fj.cmpb.2018.08.005&partnerID=40&md5=e90b6fff48c6dbae72030c2b816e1196,10.1016/j.cmpb.2018.08.005,"Background and objective: This paper presents an improved scheme able to perform accurate segmentation and classification of cancer nuclei in immunohistochemical (IHC) breast tissue images in order to provide quantitative evaluation of estrogen or progesterone (ER/PR) receptor status that will assist pathologists in cancer diagnostic process. Methods: The proposed segmentation method is based on adaptive local thresholding and an enhanced morphological procedure, which are applied to extract all stained nuclei regions and to split overlapping nuclei. In fact, a new segmentation approach is presented here for cell nuclei detection from the IHC image using a modified Laplacian filter and an improved watershed algorithm. Stromal cells are then removed from the segmented image using an adaptive criterion in order to get fast tumor nuclei recognition. Finally, unsupervised classification of cancer nuclei is obtained by the combination of four common color separation techniques for a subsequent Allred cancer scoring. Results: Experimental results on various IHC tissue images of different cancer affected patients, demonstrate the effectiveness of the proposed scheme when compared to the manual scoring of pathological experts. A statistical analysis is performed on the whole image database between immuno-score of manual and automatic method, and compared with the scores that have reached using other state-of-art segmentation and classification strategies. According to the performance evaluation, we recorded more than 98% for both accuracy of detected nuclei and image cancer scoring over the truths provided by experienced pathologists which shows the best correlation with the expert's score (Pearson's correlation coefficient = 0.993, p-value < 0.005) and the lowest computational total time of 72.3 s/image (±1.9) compared to recent studied methods. Conclusions: The proposed scheme can be easily applied for any histopathological diagnostic process that needs stained nuclear quantification and cancer grading. Moreover, the reduced processing time and manual interactions of our procedure can facilitate its implementation in a real-time device to construct a fully online evaluation system of IHC tissue images. © 2018 Elsevier B.V.",Automatic scoring; Breast cancer; Color separation methods; Immunohistochemistry; Morphological operators; Nuclei segmentation,"37, 51",,Computer Methods and Programs in Biomedicine,Article,Scopus
909,,Transfer Learning with Ensemble of Multiple Feature Representations,"Zhao H., Liu Q., Yang Y.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055831842&doi=10.1109%2fSERA.2018.8477189&partnerID=40&md5=3f51419b2382e3eb25eb843fa4c1b560,10.1109/SERA.2018.8477189,"Supervised learning algorithms are to discover the hidden patterns of the statistics, assuming that the training data and the test data are from the same distribution. There are two challenges in the traditional supervised machine learning. One is that the test data distribution always differs largely from the training data distribution in the real world, while another is that there is usually very few labeled data to train a machine learning model. In such cases, transfer learning, which emphasizes the transfer of the previous knowledge from different but related domains and tasks, is recommended to deal with these problems. Traditional transfer learning methods care more about the data itself rather than the task. In fact, there is no one universal feature representation can perfectly benefit the model training work. But different feature representations can discover some independent latent knowledge from the original data. In this paper, we propose an instance-based transfer learning method, which is a weighted ensemble transfer learning framework with multiple feature representations. In our work, mutual information is applied as the smart weighting schema to measure the weight of each feature representation. Extensive experiments have been conducted on three facial expression recognition data sets: JAFFE, KDEF and FERG-DB. The experimental results demonstrate that our approach achieves better performance than the traditional transfer learning method and the non-transfer learning method. © 2018 IEEE.",Ensemble learning; Multiple feature representation; Mutual information; Transfer learning,"54, 61",,"Proceedings - 2018 IEEE/ACIS 16th International Conference on Software Engineering Research, Management and Application, SERA 2018",Conference Paper,Scopus
910,,A Research and Strategy of Objection Detection on Remote Sensing Image,"Fu Y., Wu F., Zhao J.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055789696&doi=10.1109%2fSERA.2018.8477209&partnerID=40&md5=22211bf898e9b887c2f931d4fef74574,10.1109/SERA.2018.8477209,"Data acquisition from satellite is a challenging task due to the limitation of ground station resource and data transmission capacity. Considering that most of the raw data downloaded to the ground are useless, it is worthy to directly get the results by automatic detection on orbit and only transfer the images that include the target objects, which can filter the useless data efficiently. On orbit automatic detection, satellite computing resources need to be considered, so a smaller and faster model needs to be built. Though enormous object detection methods have been proposed and several application have emerged, a detailed survey on different models about detection accuracy and detection speed as well as memory cost is still lacking. This paper aims to provide a survey on the recent object detection researches and make a strategy to detect on orbit. To further compare the performance among different methods, we conduct an experiment in the same real dataset and compare them from accuracy, speed and memory cost. Following the experiment result, a feasible strategy of object detection for the TZ-1 satellite on-orbit which has a low memory dependency, fast speed and comparable accuracy adapt to its computing resources is proposed. © 2018 IEEE.",Computing Resources; Filter data; Object detection on-orbit; Remote Sensing Image,"42, 47",,"Proceedings - 2018 IEEE/ACIS 16th International Conference on Software Engineering Research, Management and Application, SERA 2018",Conference Paper,Scopus
911,,Classification of Dhamma Esan Characters by Transfer Learning of a Deep Neural Network,"Hnoohom N., Yuenyong S.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057767576&doi=10.1109%2fJCSSE.2018.8457361&partnerID=40&md5=0bc681ba0537a764871fd48caf4ebf66,10.1109/JCSSE.2018.8457361,"We present an image classification of Dhamma Esan characters by fine-tuning the Inception V3 deep neural network trained on the ImageNet dataset. Dhamma Esan is a traditional alphabet used in the north-eastern region of Thailand, primarily written on Corypha leaves for the purpose of recording Buddhist scriptures. Preservation of these historical documents calls for the ability to classify the characters of the alphabet in order to facilitate digital indexing and searching, as well as assist anyone trying to read them. Our dataset consists of over 70,000 Dhamma Esan character images, much larger than any previous work. The result of ten-fold cross-validation showed that our model had 100% accuracy for four folds, and 99.99% for the other six folds. The previous best accuracy reported was 97.77%. We also developed a Dhamma Esan character classification web service where users can upload images of characters and get immediate classification results as well as mapping to the modern Thai alphabet. © 2018 IEEE.",Deep neural network; Dhamma Esan; Image classification; Inception-V3; Transfer learning,,,"Proceeding of 2018 15th International Joint Conference on Computer Science and Software Engineering, JCSSE 2018",Conference Paper,Scopus
912,,Transfer Learning for Leaf Classification with Convolutional Neural Networks,"Esmaeili H., Phoka T.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057743159&doi=10.1109%2fJCSSE.2018.8457364&partnerID=40&md5=9ec30704ca9b5436dde627d934954c17,10.1109/JCSSE.2018.8457364,"Convolutional Neural Network (CNN) is taking a big role in image classification. B ut f ully t raining i mages by using CNN takes a plenty of time and uses a very large data set. This paper will focus on transfer learning, a technique that takes a pre-trained model e.g., Inception, Resnet or MobileNets models then retrains the model from the existing weights for a new classification p roblem. T he r etrain t echnique drastically decreases time spending in the training process and many fewer number of image data is required to yield high accuracy trained networks. This paper considers the problem of leaf image classification t hat t he e xisting a pproaches t ake m uch e ffort to choose various types of imagefeatures for classification. This also reflects p utting b iases b y c hoosing s ome f eatures a nd ignoring the other information in images. This paper will conduct the experiments in accuracy comparison between traditional leaf image classification using image processing techniques and CNN with transfer learning. The result will show that without much knowledge in image processing, the leaf image classification can be achieved with high accuracy using the transfer learning technique. © 2018 IEEE.",,,,"Proceeding of 2018 15th International Joint Conference on Computer Science and Software Engineering, JCSSE 2018",Conference Paper,Scopus
913,,Facial Expression Classification using Deep Extreme Inception Networks,"Raksarikorn T., Kangkachit T.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057737949&doi=10.1109%2fJCSSE.2018.8457396&partnerID=40&md5=f645bba418047e2400da980d82b5df0b,10.1109/JCSSE.2018.8457396,"Facial expression classification p lays c rucial role in human-computer interaction. A large number of automated methods have been proposed since the past decades. Recently, deep learning is broadly applied in computer vision field as well as facial expression classification. The reasons are to avoid complex feature extraction process and obtained satisfied classification p erformance. In this work, w e p ropose a deep convolutional neural networks (CNNs) model, inspired from XCEPTION, to classify seven groups of facial expressions. To efficiently use o f m odel parameters, the model a rchitecture has only 2.2 million parameters which is about 10 times less than XCEPTION. The experimental results on FER-2013 dataset show that our model offers comparable accuracy (0.7169) to the state-of-the-art methods and the upper-bound level of human accuracy ( 0.65-pm 5). In addition, our model uses less number of parameters than the state-of-the-art models and without using extra features and data augmentation. © 2018 IEEE.",Computer vision; Convolutional neural networks; Emotions; Facial expression classification; XCEPTION,,,"Proceeding of 2018 15th International Joint Conference on Computer Science and Software Engineering, JCSSE 2018",Conference Paper,Scopus
914,,Deeproad: GaN-based metamorphic testing and input validation framework for autonomous driving systems,"Zhang M., Zhang Y., Zhang L., Liu C., Khurshid S.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056509092&doi=10.1145%2f3238147.3238187&partnerID=40&md5=1d01dd8945cf7323da3424fc0a43a6ff,10.1145/3238147.3238187,"While Deep Neural Networks (DNNs) have established the fundamentals of image-based autonomous driving systems, they may exhibit erroneous behaviors and cause fatal accidents. To address the safety issues in autonomous driving systems, a recent set of testing techniques have been designed to automatically generate artificial driving scenes to enrich test suite, e.g., generating new input images transformed from the original ones. However, these techniques are insufficient due to two limitations: first, many such synthetic images often lack diversity of driving scenes, and hence compromise the resulting efficacy and reliability. Second, for machine-learning-based systems, a mismatch between training and application domain can dramatically degrade system accuracy, such that it is necessary to validate inputs for improving system robustness. In this paper, we propose DeepRoad, an unsupervised DNN-based framework for automatically testing the consistency of DNN-based autonomous driving systems and online validation. First, DeepRoad automatically synthesizes large amounts of diverse driving scenes without using image transformation rules (e.g. scale, shear and rotation). In particular, DeepRoad is able to produce driving scenes with various weather conditions (including those with rather extreme conditions) by applying Generative Adversarial Networks (GANs) along with the corresponding real-world weather scenes. Second, DeepRoad utilizes metamorphic testing techniques to check the consistency of such systems using synthetic images. Third, DeepRoad validates input images for DNN-based systems by measuring the distance of the input and training images using their VGGNet features. We implement DeepRoad to test three well-recognized DNN-based autonomous driving systems in Udacity self-driving car challenge. The experimental results demonstrate that DeepRoad can detect thousands of inconsistent behaviors for these systems, and effectively validate input images to potentially enhance the system robustness as well. © 2018 Association for Computing Machinery.",Deep neural networks; Input validation; Software testing; Test generation,"132, 142",,ASE 2018 - Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering,Conference Paper,Scopus
915,,Navigating the maze: The impact of configurability in bioinformatics software,"Cashman M., Cohen M.B., Ranjan P., Cottingham R.W.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056507250&doi=10.1145%2f3238147.3240466&partnerID=40&md5=9e4603a73cdd4a65f2cc644da608e4d9,10.1145/3238147.3240466,"The bioinformatics software domain contains thousands of applications for automating tasks such as the pairwise alignment of DNA sequences, building and reasoning about metabolic models or simulating growth of an organism. Its end users range from sophisticated developers to those with little computational experience. In response to their needs, developers provide many options to customize the way their algorithms are tuned. Yet there is little or no automated help for the user in determining the consequences or impact of the options they choose. In this paper we describe our experience working with configurable bioinformatics tools. We find limited documentation and help for combining and selecting options along with variation in both functionality and performance. We also find previously undetected faults. We summarize our findings with a set of lessons learned, and present a roadmap for creating automated techniques to interact with bioinformatics software. We believe these will generalize to other types of scientific software. © 2018 Association for Computing Machinery.",Bioinformatics; Configurability; Software testing,"757, 767",,ASE 2018 - Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering,Conference Paper,Scopus
916,,Perflearner: Learning from bug reports to understand and generate performance test frames,"Han X., Yu T., Lo D.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056500372&doi=10.1145%2f3238147.3238204&partnerID=40&md5=76dac25d5edfeb202b698459a033d6c0,10.1145/3238147.3238204,"Software performance is important for ensuring the quality of software products. Performance bugs, defined as programming errors that cause significant performance degradation, can lead to slow systems and poor user experience. While there has been some research on automated performance testing such as test case generation, the main idea is to select workload values to increase the program execution times. These techniques often assume the initial test cases have the right combination of input parameters and focus on evolving values of certain input parameters. However, such an assumption may not hold for highly configurable real-word applications, in which the combinations of input parameters can be very large. In this paper, we manually analyze 300 bug reports from three large open source projects - Apache HTTP Server, MySQL, and Mozilla Firefox. We found that 1) exposing performance bugs often requires combinations of multiple input parameters, and 2) certain input parameters are frequently involved in exposing performance bugs. Guided by these findings, we designed and evaluated an automated approach, PerfLearner, to extract execution commands and input parameters from descriptions of performance bug reports and use them to generate test frames for guiding actual performance test case generation. © 2018 Association for Computing Machinery.",Performance bugs; Software mining; Software testing,"17, 28",,ASE 2018 - Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering,Conference Paper,Scopus
917,,Autoconfig: Automatic configuration tuning for distributed message systems,"Bao L., Xu Z., Liu X., Fang B.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056490506&doi=10.1145%2f3238147.3238175&partnerID=40&md5=e0dc20dc2fdc72fe284c318a9fe68561,10.1145/3238147.3238175,"Distributed message systems (DMSs) serve as the communication backbone for many real-time streaming data processing applications. To support the vast diversity of such applications, DMSs provide a large number of parameters to configure. However, It overwhelms for most users to configure these parameters well for better performance. Although many automatic configuration approaches have been proposed to address this issue, critical challenges still remain: 1) to train a better and robust performance prediction model using a limited number of samples, and 2) to search for a high-dimensional parameter space efficiently within a time constraint. In this paper, we propose AutoConfig - an automatic configuration system that can optimize producer-side throughput on DMSs. AutoConfig constructs a novel comparison-based model (CBM) that is more robust that the prediction-based model (PBM) used by previous learning-based approaches. Furthermore, AutoConfig uses a weighted Latin hypercube sampling (wLHS) approach to select a set of samples that can provide a better coverage over the high-dimensional parameter space. wLHS allows AutoConfig to search for more promising configurations using the trained CBM. We have implemented AutoConfig on the Kafka platform, and evaluated it using eight different testing scenarios deployed on a public cloud. Experimental results show that our CBM can obtain better results than that of PBM under the same random forests based model. Furthermore, AutoConfig outperforms default configurations by 215.40% on average, and five state-of-the-art configuration algorithms by 7.21%-64.56%. © 2018 Association for Computing Machinery.",Automatic configuration tuning; Comparison-based model; Distributed message system; Weighted Latin hypercube sampling,"29, 40",,ASE 2018 - Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering,Conference Paper,Scopus
918,,Feedback-based integrated prediction: Defect prediction based on feedback from software testing process,"Xiao P., Liu B., Wang S.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047826271&doi=10.1016%2fj.jss.2018.05.029&partnerID=40&md5=9b30455308731581a7c56555b4136cd1,10.1016/j.jss.2018.05.029,"Test resource constraints is a common phenomenon in software testing. Using defect prediction to guide the resource allocation can significantly improve the efficiency and effectiveness of available test resources. However, traditional defect prediction (t-DP) is a static strategy, where the predictor cannot be dynamically adjusted during the software testing process (STP). This paper combines defect prediction with feedback control in STP and proposes a feedback-based defect prediction model, where the test results generated during STP is used as feedback information for on-line adjustment of predictor to optimize the prediction result. In addition, a novel approach called feedback-based integrated prediction (FIP) is proposed to improve the prediction accuracy, where a global predictor and a local predictor are employed to make an integrated prediction using the weight to adjust the effects of predictors at different test stages. A systematic experiment is conducted to investigate the performance of the FIP over 10 public data sets. Results show that FIP has better prediction efficiency and better robustness for external data than the t-DP, especially when the percentage of the test modules is 40%. © 2018 Elsevier Inc.",Defect prediction; Feedback control; Integrated prediction; Software testing; Test resource constraints,"159, 171",,Journal of Systems and Software,Article,Scopus
919,,Heterogeneous Defect Prediction,"Nam J., Fu W., Kim S., Menzies T., Tan L.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023757910&doi=10.1109%2fTSE.2017.2720603&partnerID=40&md5=201c1972cb45ff331caa6491d91665be,10.1109/TSE.2017.2720603,"Many recent studies have documented the success of cross-project defect prediction (CPDP) to predict defects for new projects lacking in defect data by using prediction models built by other projects. However, most studies share the same limitations: it requires homogeneous data; i.e., different projects must describe themselves using the same metrics. This paper presents methods for heterogeneous defect prediction (HDP) that matches up different metrics in different projects. Metric matching for HDP requires a 'large enough' sample of distributions in the source and target projects - which raises the question on how large is 'large enough' for effective heterogeneous defect prediction. This paper shows that empirically and theoretically, 'large enough' may be very small indeed. For example, using a mathematical model of defect prediction, we identify categories of data sets were as few as 50 instances are enough to build a defect prediction model. Our conclusion for this work is that, even when projects use different metric sets, it is possible to quickly transfer lessons learned about defect prediction. © 1976-2012 IEEE.",Defect prediction; heterogeneous metrics; quality assurance; transfer learning,"874, 896",,IEEE Transactions on Software Engineering,Article,Scopus
920,,A Comparative Study to Benchmark Cross-Project Defect Prediction Approaches,"Herbold S., Trautsch A., Grabowski J.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023163956&doi=10.1109%2fTSE.2017.2724538&partnerID=40&md5=31c7facb1cda8458ce7091e93910365a,10.1109/TSE.2017.2724538,"Cross-Project Defect Prediction (CPDP) as a means to focus quality assurance of software projects was under heavy investigation in recent years. However, within the current state-of-the-art it is unclear which of the many proposals performs best due to a lack of replication of results and diverse experiment setups that utilize different performance metrics and are based on different underlying data. Within this article, we provide a benchmark for CPDP. We replicate 24 approaches proposed by researchers between 2008 and 2015 and evaluate their performance on software products from five different data sets. Based on our benchmark, we determined that an approach proposed by Camargo Cruz and Ochimizu (2009) based on data standardization performs best and is always ranked among the statistically significant best results for all metrics and data sets. Approaches proposed by Turhan et al. (2009), Menzies et al. (2011), and Watanabe et al. (2008) are also nearly always among the best results. Moreover, we determined that predictions only seldom achieve a high performance of 0.75 recall, precision, and accuracy. Thus, CPDP still has not reached a point where the performance of the results is sufficient for the application in practice. © 1976-2012 IEEE.",benchmark; comparison; Cross-project defect prediction; replication,"811, 833",,IEEE Transactions on Software Engineering,Article,Scopus
921,,Resolving ambiguities in regulations: Towards achieving the kohlbergian stage of principled morality,"Ghaisas S., Sainani A., Anish P.R.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053925484&doi=10.1145%2f3183428.3183433&partnerID=40&md5=956fc7909469240de8c7c1e6c9606339,10.1145/3183428.3183433,"According to Kohlberg, the final stage of morality is characterized by viewing laws as a means to an end by upholding values such as human dignity and fairness as guiding principles for complying with the essence of the law. Given that purpose of compliance is indeed wellbeing of citizens, software systems should, by design, incorporate these values so that laws are followed in spirit. How can we build software systems that incorporate these values? We present our work on disambiguating Health Insurance Portability and Accountability Act (HIPAA) so as to reduce the potential incidents of breach, thereby upholding of the aforesaid guiding principles of morality. We have employed deep learning based approaches to emulate the human process of disambiguation by integrating information from multiple sources, summarizing it, and augmenting the regulatory text with the additional information. This augmented regulatory text can be used by policy makers and software engineers to achieve compliance in spirit. © 2018 ACM.",Ambiguity Resolution; Compliance; Deep Learning; Principled Morality; Regulations,"57, 60",,"Proceedings - 2018 ACM/IEEE 40th International Conference on Software Engineering: Software Engineering in Society, ICSE-SEIS 2018",Conference Paper,Scopus
922,,Cross-entropy: A new metric for software defect prediction,"Zhang X., Ben K., Zeng J.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052283984&doi=10.1109%2fQRS.2018.00025&partnerID=40&md5=0fadbe3899c25d3c1ef52400a84e5305,10.1109/QRS.2018.00025,"Defect prediction is an active topic in software quality assurance, which can help developers find potential bugs and make better use of resources. To improve prediction performance, this paper introduces cross-entropy, one common measure for natural language, as a new code metric into defect prediction tasks and proposes a framework called Defect-Learner for this process. We first build a recurrent neural network language model to learn regularities in source code from software repository. Based on the trained model, the cross-entropy of each component can be calculated. To evaluate the discrimination for defect-proneness, cross-entropy is compared with 20 widely used metrics on 12 open-source projects. The experimental results show that cross-entropy metric is more discriminative than 50% of the traditional metrics. Besides, we combine cross-entropy with traditional metric suites together for accurate defect prediction. With cross-entropy added, the performance of prediction models is improved by an average of 2.8% in F1-score. © 2018 IEEE.",Code naturalness; Deep learning; Language model; Natural language processing; Software defect prediction,"111, 122",,"Proceedings - 2018 IEEE 18th International Conference on Software Quality, Reliability, and Security, QRS 2018",Conference Paper,Scopus
923,,Research Progress on Emotional Computation Technology Based on Semantic Analysis [基于语义分析的情感计算技术研究进展],"Rao Y., Wu L.-W., Wang Y.-M., Feng C.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055520829&doi=10.13328%2fj.cnki.jos.005564&partnerID=40&md5=fc03ab6a5772a517366c567f9a30ba0a,10.13328/j.cnki.jos.005564,"With the development of machine learning and application of big data, semantic-based emotional computing and analysis technology plays a significant role in the research on human perception, attention, memory, decision-making, and social communication. It affects not only the development in artificial intelligence technology, but also human/machine interaction and smart robot technology, therefore drawing widespread interest from the academic and business communities. In this paper, based on the definition of affection and the analysis of more than 90 emotional models, six vital problems and challenges in emotional computing are summarized as follows: where is emotion stem from and how to represent their essential features; how to analyze and compute the emotion under the multi-model environment; how to measure the influence of external factors on the process of emotional evolution; how to measure individual emotion by various of personalized characteristic; how to measure the crowed psychology and emotion and to analyze the mechanism about propagation dynamics; and how to express the subtle emotion and optimize algorithms. Meanwhile, some theoretical research, technical analysis and practical application are brought up to introduce the current work progress and trend for these technical challenges in order to provide new research clues and directions for further study in the field of the semantic-based emotional computing. © Copyright 2018, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Affective evolutionary computing; Artificial emotion generation; Artificial intelligence; Crowd emotion; Emotional semantic analysis; Opinion mining,"2397, 2426",,Ruan Jian Xue Bao/Journal of Software,Review,Scopus
924,,Multiple-components weights model for cross-project software defect prediction,"Qiu S., Lu L., Jiang S.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051714043&doi=10.1049%2fiet-sen.2017.0111&partnerID=40&md5=e4bf4d873410248e066d4bffd87f96d6,10.1049/iet-sen.2017.0111,"Software defect prediction (SDP) technology is receiving widely attention and most of SDP models are trained on data from the same project. However, at an early phase of the software lifecycle, there are little to no within-project training data to learn an available supervised defect-prediction model. Thus, cross-project defect prediction (CPDP), which is learning a defect predictor for a target project by using labelled data from a source project, has shown promising value in SDP. To better perform the CPDP, most current studies focus on filtering instances or selecting features to weaken the impact of irrelevant cross-project data. Instead, the authors propose a novel multiple-components weights (MCWs) learning model to analyse the varying auxiliary power of multiple components in a source project to construct a more precise ensemble classifiers for a target project. By combining the MCW model with kernel mean matching algorithm, their proposed approach adjusts the sourceinstance weights and source-component weights to jointly alleviate the negative impacts of irrelevant cross-project data. They conducted comprehensive experiments by employing 15 real-world datasets to demonstrate the advantages and effectiveness of their proposed approach. © 2018 The Institution of Engineering and Technology.",,"345, 355",,IET Software,Article,Scopus
925,,Skin lesion segmentation in dermoscopy images via deep full resolution convolutional networks,"Al-masni M.A., Al-antari M.A., Choi M.-T., Han S.-M., Kim T.-S.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047600387&doi=10.1016%2fj.cmpb.2018.05.027&partnerID=40&md5=18812c13cc0b21ec98c5062d3b4aad2c,10.1016/j.cmpb.2018.05.027,"Background and objective: Automatic segmentation of skin lesions in dermoscopy images is still a challenging task due to the large shape variations and indistinct boundaries of the lesions. Accurate segmentation of skin lesions is a key prerequisite step for any computer-aided diagnostic system to recognize skin melanoma. Methods: In this paper, we propose a novel segmentation methodology via full resolution convolutional networks (FrCN). The proposed FrCN method directly learns the full resolution features of each individual pixel of the input data without the need for pre- or post-processing operations such as artifact removal, low contrast adjustment, or further enhancement of the segmented skin lesion boundaries. We evaluated the proposed method using two publicly available databases, the IEEE International Symposium on Biomedical Imaging (ISBI) 2017 Challenge and PH2 datasets. To evaluate the proposed method, we compared the segmentation performance with the latest deep learning segmentation approaches such as the fully convolutional network (FCN), U-Net, and SegNet. Results: Our results showed that the proposed FrCN method segmented the skin lesions with an average Jaccard index of 77.11% and an overall segmentation accuracy of 94.03% for the ISBI 2017 test dataset and 84.79% and 95.08%, respectively, for the PH2 dataset. In comparison to FCN, U-Net, and SegNet, the proposed FrCN outperformed them by 4.94%, 15.47%, and 7.48% for the Jaccard index and 1.31%, 3.89%, and 2.27% for the segmentation accuracy, respectively. Furthermore, the proposed FrCN achieved a segmentation accuracy of 95.62% for some representative clinical benign cases, 90.78% for the melanoma cases, and 91.29% for the seborrheic keratosis cases in the ISBI 2017 test dataset, exhibiting better performance than those of FCN, U-Net, and SegNet. Conclusions: We conclude that using the full spatial resolutions of the input image could enable to learn better specific and prominent features, leading to an improvement in the segmentation performance. © 2018 Elsevier B.V.",Deep learning; Dermoscopy; Full resolution convolutional network (FrCN); Melanoma; Skin lesion segmentation,"221, 231",,Computer Methods and Programs in Biomedicine,Article,Scopus
926,,Generalized fused group lasso regularized multi-task feature learning for predicting cognitive outcomes in Alzheimers disease,"Cao P., Liu X., Liu H., Yang J., Zhao D., Huang M., Zaiane O.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047096493&doi=10.1016%2fj.cmpb.2018.04.028&partnerID=40&md5=9961afa0c182547b97768739e69640bb,10.1016/j.cmpb.2018.04.028,"Objective: Alzheimers disease (AD) is characterized by gradual neurodegeneration and loss of brain function, especially for memory during early stages. Regression analysis has been widely applied to AD research to relate clinical and biomarker data such as predicting cognitive outcomes from Magnetic Resonance Imaging (MRI) measures. Recently, the multi-task feature learning (MTFL) methods have been widely studied to predict cognitive outcomes and select the discriminative feature subset from MRI features by incorporating inherent correlations among multiple clinical cognitive measures. However, the existing MTFL assumes the correlation among all the tasks is uniform, and the task relatedness is modeled by encouraging a common subset of features with neglecting the inherent structure of tasks and MRI features. Methods: In this paper, we proposed a generalized fused group lasso (GFGL) regularization to model the underlying structures, involving (1) a graph structure within tasks and (2) a group structure among the image features. Then, we present a multi-task learning framework (called GFGL–MTFL), combining the ℓ2, 1-norm with the GFGL regularization, to model the flexible structures. Results: Through empirical evaluation and comparison with different baseline methods and the state-of-the-art MTL methods on data from Alzheimer's Disease Neuroimaging Initiative (ADNI) database, we illustrate that the proposed GFGL–MTFL method outperforms other methods in terms of both Mean Squared Error (nMSE) and weighted correlation coefficient (wR). Improvements are statistically significant for most scores (tasks). Conclusions: The experimental results with real and synthetic data demonstrate that incorporating the two prior structures by the generalized fused group lasso norm into the multi task feature learning can improve the prediction performance over several state-of-the-art competing methods, and the estimated correlation of the cognitive functions and the identification of cognition relevant imaging markers are clinically and biologically meaningful. © 2018 Elsevier B.V.",Alzheimer's disease; Biomarker identification; Fused lasso; Group lasso; Multi-task learning; Regression; Sparse learning,"19, 45",,Computer Methods and Programs in Biomedicine,Article,Scopus
927,,Cross project defect prediction using class distribution estimation and oversampling,"Limsettho N., Bennin K.E., Keung J.W., Hata H., Matsumoto K.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046137081&doi=10.1016%2fj.infsof.2018.04.001&partnerID=40&md5=efff2f31a08c29811167a7ffb0cd0b69,10.1016/j.infsof.2018.04.001,"Context: Cross-project defect prediction (CPDP) which uses dataset from other projects to build predictors has been recently recommended as an effective approach for building prediction models that lack historical or sufficient local datasets. Class imbalance and distribution mismatch between the source and target datasets associated with real-world defect datasets are known to have a negative impact on prediction performance. Objective: To alleviate the negative effects of class imbalance and distribution mismatch on performance of CPDP models by using Class Distribution Estimation and Synthetic Minority Oversampling Technique. A novel approach called Class Distribution Estimation with Synthetic Minority Oversampling Technique (CDE-SMOTE) is proposed to optimize and improve the CPDP performance and avoid excessive oversampling. Method: The proposed CDE-SMOTE employs CDE to estimate the class distribution of the target project. SMOTE is then used to modify the class distribution of the training data until the distribution becomes the reverse of the approximated class distribution of the target project. Four comprehensive experiments are conducted on 14 open source software projects. Results: The proposed approach improves the overall performance of CPDP models when compared to the performance of other CPDP approaches. Significant improvements are observed in 63% of the test cases according to the Wilcoxon signed-rank tests with 16.421%, 29.687% and 20.259% improvements in terms of Balance, G-measure, and F-measure, respectively. Application of CDE-SMOTE on NN-filtered datasets significantly improved prediction performance. Conclusions: CDE-SMOTE mitigates the class imbalance and distribution mismatch problems and also helps prevents excessive oversampling that results in performance degradation of prediction models. This approach is thus recommended for CPDP studies in software engineering. © 2018 Elsevier B.V.",Class distribution estimation; Class imbalance learning; Cross-Project defect prediction; Oversampling; Software fault prediction,"87, 102",,Information and Software Technology,Article,Scopus
928,,A Top-k Learning to Rank Approach to Cross-Project Software Defect Prediction,"Wang F., Huang J., Ma Y.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066808975&doi=10.1109%2fAPSEC.2018.00048&partnerID=40&md5=88db18292b9f59abd784342646e35eb9,10.1109/APSEC.2018.00048,"Cross-project defect prediction (CPDP) has recently attracted increasing attention in the field of Software Engineering. Most of the previous studies, which treated it as a binary classification problem or a regression problem, are not practical for software testing activities. To provide developers with a more valuable ranking of the most severe entities (e.g., classes and modules), in this paper, we propose a top-k learning to rank (LTR) approach in the scenario of CPDP. In particular, we first convert the number of defects into graded relevance to a specific query according to the three-sigma rule; then, we put forward a new data resampling method called SMOTE-PENN to tackle the imbalanced data problem. An empirical study on the PROMISE dataset shows that SMOTE-PENN outperforms the other six competitive resampling algorithms and RankNet performs the best for the proposed approach framework. Thus, our work could lay a foundation for efficient search engines for top-ranked defective entities in real software testing activities without local historical data for a target project. © 2018 IEEE.",relevance; resampling; top-k ranking; transfer learning; Wilcoxon signed-rank test,"335, 344",,"Proceedings - Asia-Pacific Software Engineering Conference, APSEC",Conference Paper,Scopus
929,,Detecting Duplicate Bug Reports with Convolutional Neural Networks,"Xie Q., Wen Z., Zhu J., Gao C., Zheng Z.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066793313&doi=10.1109%2fAPSEC.2018.00056&partnerID=40&md5=3351a62b4dca9d8c003682c6a890d6dc,10.1109/APSEC.2018.00056,"Bug tracking systems are widely used to track bugs from users during the lifecycle of software systems for reliability maintainence. When software systems have a large user base, which is common in practice, different users may encounter a same bug and then generate many duplicate bug reports. In a large project, each bug report is usually assigned to a different developer or team to parallelize the bug debugging and fixing activities. The presence of duplicate bug reports thus leads to many unnecessary efforts of developers spending on debugging a same issue. To speed up the bug fixing process and save the cost of developers, there is a high demand for automated detection of duplicate bug reports. In this paper, we explore the use of powerful deep learning techniques, including word embedding and Convolution Neural Networks, to calculate the similarity between a pair of bug reports and thus identify possible duplicates. In contrast to previous work that consider only common words between bug descriptions for lexical similarity computation, our approach is able to better capture semantic similarity between words. We further improve traditional CNN models by combining some domain-specific features extracted from bug reports. Evaluation results on the bug reports from four popular open-source projects show that DBR-CNN has made a significant improvement on duplicate detection accuracy over traditional approaches. © 2018 IEEE.",bug reports; CNN; deep learning; duplicate detection; Software reliability,"416, 425",,"Proceedings - Asia-Pacific Software Engineering Conference, APSEC",Conference Paper,Scopus
930,,Automated EEG-based screening of depression using deep convolutional neural network,"Acharya U.R., Oh S.L., Hagiwara Y., Tan J.H., Adeli H., Subha D.P.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046376921&doi=10.1016%2fj.cmpb.2018.04.012&partnerID=40&md5=ed5b73eb146d216ef7c863681531b9e8,10.1016/j.cmpb.2018.04.012,"In recent years, advanced neurocomputing and machine learning techniques have been used for Electroencephalogram (EEG)-based diagnosis of various neurological disorders. In this paper, a novel computer model is presented for EEG-based screening of depression using a deep neural network machine learning approach, known as Convolutional Neural Network (CNN). The proposed technique does not require a semi-manually-selected set of features to be fed into a classifier for classification. It learns automatically and adaptively from the input EEG signals to differentiate EEGs obtained from depressive and normal subjects. The model was tested using EEGs obtained from 15 normal and 15 depressed patients. The algorithm attained accuracies of 93.5% and 96.0% using EEG signals from the left and right hemisphere, respectively. It was discovered in this research that the EEG signals from the right hemisphere are more distinctive in depression than those from the left hemisphere. This discovery is consistent with recent research and revelation that the depression is associated with a hyperactive right hemisphere. An exciting extension of this research would be diagnosis of different stages and severity of depression and development of a Depression Severity Index (DSI). © 2018 Elsevier B.V.",Convolutional neural network; Deep learning; Depression; EEG; Electroencephalogram,"103, 113",,Computer Methods and Programs in Biomedicine,Article,Scopus
931,,Deep learning for healthcare applications based on physiological signals: A review,"Faust O., Hagiwara Y., Hong T.J., Lih O.S., Acharya U.R.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045708084&doi=10.1016%2fj.cmpb.2018.04.005&partnerID=40&md5=5de75f3e63f2180ce1ead0f3ef54005e,10.1016/j.cmpb.2018.04.005,"Background and objective: We have cast the net into the ocean of knowledge to retrieve the latest scientific research on deep learning methods for physiological signals. We found 53 research papers on this topic, published from 01.01.2008 to 31.12.2017. Methods: An initial bibliometric analysis shows that the reviewed papers focused on Electromyogram(EMG), Electroencephalogram(EEG), Electrocardiogram(ECG), and Electrooculogram(EOG). These four categories were used to structure the subsequent content review. Results: During the content review, we understood that deep learning performs better for big and varied datasets than classic analysis and machine classification methods. Deep learning algorithms try to develop the model by using all the available input. Conclusions: This review paper depicts the application of various deep learning algorithms used till recently, but in future it will be used for more healthcare areas to improve the quality of diagnosis. © 2018 Elsevier B.V.",Deep learning; Electrocardiogram; Electroencephalogram; Electromyogram; Electrooculogram; Physiological signals,"1, 13",,Computer Methods and Programs in Biomedicine,Review,Scopus
932,,"DIANNE: a modular framework for designing, training and deploying deep neural networks on heterogeneous distributed infrastructure","Coninck E.D., Bohez S., Leroux S., Verbelen T., Vankeirsbilck B., Simoens P., Dhoedt B.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044594132&doi=10.1016%2fj.jss.2018.03.032&partnerID=40&md5=7e088d3da4fc61f55c1e8b70b2ef9250,10.1016/j.jss.2018.03.032,"Deep learning has shown tremendous results on various machine learning tasks, but the nature of the problems being tackled and the size of state-of-the-art deep neural networks often require training and deploying models on distributed infrastructure. DIANNE is a modular framework designed for dynamic (re)distribution of deep learning models and procedures. Besides providing elementary network building blocks as well as various training and evaluation routines, DIANNE focuses on dynamic deployment on heterogeneous distributed infrastructure, abstraction of Internet of Things (IoT) sensors, integration with external systems and graphical user interfaces to build and deploy networks, while retaining the performance of similar deep learning frameworks. In this paper the DIANNE framework is proposed as an all-in-one solution for deep learning, enabling data and model parallelism though a modular design, offloading to local compute power, and the ability to abstract between simulation and real environment. © 2018 Elsevier Inc.",Artificial neural networks; Distributed applications; Internet of Things; Machine learning,"52, 65",,Journal of Systems and Software,Article,Scopus
933,,NUMA-Caffe: NUMA-Aware Deep Learning Neural Networks,"Roy P., Song S.L., Krishnamoorthy S., Vishnu A., Sengupta D., Liu X.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066942115&doi=10.1145%2f3199605&partnerID=40&md5=0582fd29f647885f1ca8642a94b1307c,10.1145/3199605,"Convolution Neural Networks (CNNs), a special subcategory of Deep Learning Neural Networks (DNNs), have become increasingly popular in industry and academia for their powerful capability in pattern classification, image processing, and speech recognition. Recently, they have been widely adopted in High Performance Computing (HPC) environments for solving complex problems related to modeling, runtime prediction, and big data analysis. Current state-of-the-art designs for DNNs on modern multi- and many-core CPU architectures, such as variants of Caffe, have reported promising performance in speedup and scalability, comparable with the GPU implementations. However, modern CPU architectures employ Non-Uniform Memory Access (NUMA) technique to integrate multiple sockets, which incurs unique challenges for designing highly efficient CNN frameworks. Without a careful design, DNN frameworks can easily suffer from long memory latency due to a large number of memory accesses to remote NUMA domains, resulting in poor scalability. To address this challenge, we propose NUMA-aware multi-solver-based CNN design, named NUMA-Caffe, for accelerating deep learning neural networks on multi- and many-core CPU architectures. NUMA-Caffe is independent of DNN topology, does not impact network convergence rates, and provides superior scalability to the existing Caffe variants. Through a thorough empirical study on four contemporary NUMA-based multi- and many-core architectures, our experimental results demonstrate that NUMA-Caffe significantly outperforms the state-of-the-art Caffe designs in terms of both throughput and scalability. © 2018 ACM.",Deep learning; neural network; NUMA; stochastic gradient descent,,,ACM Transactions on Architecture and Code Optimization,Article,Scopus
934,,Gastric Pathology Image Recognition Based on Deep Residual Networks,"Liu B., Yao K., Huang M., Zhang J., Li Y., Li R.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055544149&doi=10.1109%2fCOMPSAC.2018.10267&partnerID=40&md5=bb7283491e3c313ea9db261fc0d3067a,10.1109/COMPSAC.2018.10267,"Gastric cancer is a malignant neoplasm with a high mortality rate in the world. Nearly one million new cases occur each year. The most important measure to diagnose gastric cancer is the detection and treatment of diseases early. Gastric cancer detection is currently performed by pathologists reviewing large expanses of biological tissues, but this process is labor intensive and error-prone. In this paper, a framework for automatically detection of tumors in gastric pathology image (slide) has been proposed based on deep learning. A deep residual network with 50 layers is built by identity mapping on a dataset of pathology images. The proposed method makes the training of models easier and improves the generalization performance. Finally, the experimental results show that the F-score of our method achieves 96%. The research in auto-classification of gastric pathology images has great value for gastric cancer detection in clinical medicine. © 2018 IEEE.",Classification; Deep residual network; Gastric cancer; Image recognition,"408, 412",,Proceedings - International Computer Software and Applications Conference,Conference Paper,Scopus
935,,Machine Learning Assisted High-Definition Map Creation,Jiao J.,2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055456406&doi=10.1109%2fCOMPSAC.2018.00058&partnerID=40&md5=502c6879023b973daab473f4be285d33,10.1109/COMPSAC.2018.00058,"In recent years, autonomous driving technologies have attracted broad and enormous interests from both academia and industry and are under rapid development. High-Definition (HD) Maps are widely used as an indispensable component of an autonomous vehicle system by researchers and practitioners. HD Maps are digital maps that contain highly precise, fresh and comprehensive geometric information as well as semantics of the road network and surrounding environment. They provide critical inputs to almost all other components of autonomous vehicle systems, including localization, perception, prediction, motion planning, vehicle control etc. Traditionally, it is very laborious and costly to build HD Maps, requiring a significant amount of manual annotation work. In this paper, we first introduce the characteristics and layers of HD Maps; then we provide a formal summary of the workflow of HD Map creation; and most importantly, we present the machine learning techniques being used by the industry to minimize the amount of manual work in the process of HD Map creation. © 2018 IEEE.",Autonomous Vehicle System; HD Map Creation; High-Definition Map,"367, 373",,Proceedings - International Computer Software and Applications Conference,Conference Paper,Scopus
936,,Cross-Project Change-Proneness Prediction,"Liu C., Yang D., Xia X., Yan M., Zhang X.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055440637&doi=10.1109%2fCOMPSAC.2018.00017&partnerID=40&md5=f2c8270b95c3b4f3c7bf885ce6cb48fe,10.1109/COMPSAC.2018.00017,"Software change-proneness prediction (whether or not class files in a project will be changed in the next release) can help software developers to focus on preventive actions to reduce maintenance costs, and managers to allocate resources more effectively. Prior studies found that change-proneness prediction works well if there is sufficient amount of training data to build a model. However, it is not feasible for projects with limited historical data especially for new projects. To address this issue, cross-project change-proneness prediction, which builds a prediction model by using data in another project (i.e., source project), and predicts the change-proneness in a target project, is proposed. Considering there are a large number of source projects, one challenge for cross-project change-proneness prediction is that given a target project, how to automatically select a source project which could show good prediction accuracy on it. In this paper, we propose a selective cross-project (SCP) model for change-proneness prediction. SCP automatically finds the source project which has the similar data distribution with the target project by measuring distribution similarity between source and target projects. We evaluate SCP by conducting an empirical study on 14 open source projects. We compare it with 2 most related change-proneness models, including RCP (Random Cross-Project prediction) proposed by Malhotra and Bansal, and CLAMI+ developed by Yan et al. Experiment results show that SCP improves RCP and CLAMI+ by 25.34% and 4.30% in terms of AUC respectively; and by 171.42% and 172.31% in terms of cost-effectiveness, respectively. © 2018 IEEE.",Change-Proneness; Cross-Project Prediction; Maintainability; Project Selection,"64, 73",,Proceedings - International Computer Software and Applications Conference,Conference Paper,Scopus
937,,Transfer Learning Method for Very Deep CNN for Text Classification and Methods for its Evaluation,"Moriya S., Shibata C.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054785762&doi=10.1109%2fCOMPSAC.2018.10220&partnerID=40&md5=f7c3ac25376798dc0755b2f92978a01f,10.1109/COMPSAC.2018.10220,"In recent years, it has become possible to perform text classification with high accuracy by using convolutional neural networks (CNNs). Zhang et al. decomposed words into characters and classified texts using a CNN with relatively deep layers to obtain excellent classification results. However, it is often difficult to prepare a sufficient number of labeled samples for solving real-world text-classification problems. One method for handling this problem is transfer learning, which uses a network tuned for an arbitrary task as the initial network for a target task. While transfer learning is known to be effective for image recognition, for tasks in natural language processing, such as document classification, it has not yet been shown for what types of data and to what extent transfer learning is effective. In this paper, we first introduce a character-level CNN adopting the structure of a residual network to construct a network with deeper layers for Japanese text classification. We then demonstrate that we can improve classification accuracy by performing transfer learning between two particular datasets. Additionally, we propose an approach to evaluate the effectiveness of transfer learning and use it to evaluate our model. © 2018 IEEE.",CNN; Residual network; Text classification; Transfer learning,"153, 158",,Proceedings - International Computer Software and Applications Conference,Conference Paper,Scopus
938,,Progress on approaches to software defect prediction,"Li Z., Jing X.-Y., Zhu X.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048126698&doi=10.1049%2fiet-sen.2017.0148&partnerID=40&md5=bbd0af983690495a950e3b1e53076a4e,10.1049/iet-sen.2017.0148,"Software defect prediction is one of the most popular research topics in software engineering. It aims to predict defect-prone software modules before defects are discovered, therefore it can be used to better prioritise software quality assurance effort. In recent years, especially for recent 3 years, many new defect prediction studies have been proposed. The goal of this study is to comprehensively review, analyse and discuss the state-of-the-art of defect prediction. The authors survey almost 70 representative defect prediction papers in recent years (January 2014-April 2017), most of which are published in the prominent software engineering journals and top conferences. The selected defect prediction papers are summarised to four aspects: machine learning-based prediction algorithms, manipulating the data, effort-aware prediction and empirical studies. The research community is still facing a number of challenges for building methods and many research opportunities exist. The identified challenges can give some practical guidelines for both software engineering researchers and practitioners in future software defect prediction. © The Institution of Engineering and Technology 2018.",,"161, 175",,IET Software,Review,Scopus
939,,Tissue classification and segmentation of pressure injuries using convolutional neural networks,"Zahia S., Sierra-Sosa D., Garcia-Zapirain B., Elmaghraby A.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043369305&doi=10.1016%2fj.cmpb.2018.02.018&partnerID=40&md5=aab242960976e1122e2b5ddd9e25baca,10.1016/j.cmpb.2018.02.018,"Background and Objectives: This paper presents a new approach for automatic tissue classification in pressure injuries. These wounds are localized skin damages which need frequent diagnosis and treatment. Therefore, a reliable and accurate systems for segmentation and tissue type identification are needed in order to achieve better treatment results. Methods: Our proposed system is based on a Convolutional Neural Network (CNN) devoted to performing optimized segmentation of the different tissue types present in pressure injuries (granulation, slough, and necrotic tissues). A preprocessing step removes the flash light and creates a set of 5x5 sub-images which are used as input for the CNN network. The network output will classify every sub-image of the validation set into one of the three classes studied. Results: The metrics used to evaluate our approach show an overall average classification accuracy of 92.01%, an average total weighted Dice Similarity Coefficient of 91.38%, and an average precision per class of 97.31% for granulation tissue, 96.59% for necrotic tissue, and 77.90% for slough tissue. Conclusions: Our system has been proven to make recognition of complicated structures in biomedical images feasible. © 2018 Elsevier B.V.",Convolutional neural networks; Deep learning; Image segmentation; Pressure injuries; Tissue type classification,"51, 58",,Computer Methods and Programs in Biomedicine,Article,Scopus
940,,Cost-sensitive transfer kernel canonical correlation analysis for heterogeneous defect prediction,"Li Z., Jing X.-Y., Wu F., Zhu X., Xu B., Ying S.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027526319&doi=10.1007%2fs10515-017-0220-7&partnerID=40&md5=4867acb726f45d231580c14341719c9f,10.1007/s10515-017-0220-7,"Cross-project defect prediction (CPDP) refers to predicting defects in a target project using prediction models trained from historical data of other source projects. And CPDP in the scenario where source and target projects have different metric sets is called heterogeneous defect prediction (HDP). Recently, HDP has received much research interest. Existing HDP methods only consider the linear correlation relationship among the features (metrics) of the source and target projects, and such models are insufficient to evaluate nonlinear correlation relationship among the features. So these methods may suffer from the linearly inseparable problem in the linear feature space. Furthermore, existing HDP methods do not take the class imbalance problem into consideration. Unfortunately, the imbalanced nature of software defect datasets increases the learning difficulty for the predictors. In this paper, we propose a new cost-sensitive transfer kernel canonical correlation analysis (CTKCCA) approach for HDP. CTKCCA can not only make the data distributions of source and target projects much more similar in the nonlinear feature space, where the learned features have favorable separability, but also utilize the different misclassification costs for defective and defect-free classes to alleviate the class imbalance problem. We perform the Friedman test with Nemenyi’s post-hoc statistical test and the Cliff’s delta effect size test for the evaluation. Extensive experiments on 28 public projects from five data sources indicate that: (1) CTKCCA significantly performs better than the related CPDP methods; (2) CTKCCA performs better than the related state-of-the-art HDP methods. © 2017, Springer Science+Business Media, LLC.",Class imbalance; Cost-sensitive learning; Heterogeneous defect prediction; Kernel canonical correlation analysis; Transfer learning,"201, 245",,Automated Software Engineering,Article,Scopus
941,,Ensemble techniques for software change prediction: A preliminary investigation,"Catolino G., Ferrucci F.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048880030&doi=10.1109%2fMALTESQUE.2018.8368455&partnerID=40&md5=706224c2c7a787beb949fefc9df1c76b,10.1109/MALTESQUE.2018.8368455,"Predicting the classes more likely to change in the future helps developers to focus on the more critical parts of a software system, with the aim of preventively improving its maintainability. The research community has devoted a lot of effort in the definition of change prediction models, i.e., models exploiting a machine learning classifier to relate a set of independent variables to the change-proneness of classes. Besides the good performances of such models, key results of previous studies highlight how classifiers tend to perform similarly even though they are able to correctly predict the change-proneness of different code elements, possibly indicating the presence of some complementarity among them. In this paper, we aim at analyzing the extent to which ensemble methodologies, i.e., machine learning techniques able to combine multiple classifiers, can improve the performances of change-prediction models. Specifically, we empirically compared the performances of three ensemble techniques (i.e., Boosting, Random Forest, and Bagging) with those of standard machine learning classifiers (i.e., Logistic Regression and Naive Bayes). The study was conducted on eight open source systems and the results showed how ensemble techniques, in some cases, perform better than standard machine learning approaches, even if the differences among them is small. This requires the need of further research aimed at devising effective methodologies to ensemble different classifiers. © 2018 IEEE.",Change Prediction; Empirical Study; Ensemble Techniques,"25, 30",,"2018 IEEE International Workshop on Machine Learning Techniques for Software Quality Evaluation, MaLTeSQuE 2018 - Proceedings",Conference Paper,Scopus
942,,Data-driven search-based software engineering,"Nair V., Agrawal A., Chen J., Fu W., Mathew G., Menzies T., Minku L., Wagner M., Yu Z.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051662725&doi=10.1145%2f3196398.3196442&partnerID=40&md5=89992a451317682b1ca5a06e629919e7,10.1145/3196398.3196442,"This paper introduces Data-Driven Search-based Software Engineering (DSE), which combines insights from Mining Software Repositories (MSR) and Search-based Software Engineering (SBSE). While MSR formulates software engineering problems as data mining problems, SBSE reformulate Software Engineering (SE) problems as optimization problems and use meta-heuristic algorithms to solve them. Both MSR and SBSE share the common goal of providing insights to improve software engineering. The algorithms used in these two areas also have intrinsic relationships. We, therefore, argue that combining these two fields is useful for situations (a) which require learning from a large data source or (b) when optimizers need to know the lay of the land to find better solutions, faster. This paper aims to answer the following three questions: (1) What are the various topics addressed by DSE?, (2) What types of data are used by the researchers in this area?, and (3) What research approaches do researchers use? The paper briefly sets out to act as a practical guide to develop new DSE techniques and also to serve as a teaching resource. This paper also presents a resource (tiny.cc/data-se) for exploring DSE. The resource contains 89 artifacts which are related to DSE, divided into 13 groups such as requirements engineering, software product lines, software processes. All the materials in this repository have been used in recent software engineering papers; i.e., for all this material, there exist baseline results against which researchers can comparatively assess their new ideas. © 2018 ACM.",,"341, 352",,Proceedings - International Conference on Software Engineering,Conference Paper,Scopus
943,,Word embeddings for the software engineering domain,"Efstathiou V., Chatzilenas C., Spinellis D.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051661163&doi=10.1145%2f3196398.3196448&partnerID=40&md5=2a7324a25c3c9b2722fec72a8a978fc2,10.1145/3196398.3196448,"The software development process produces vast amounts of textual data expressed in natural language. Outcomes from the natural language processing community have been adapted in software engineering research for leveraging this rich textual information; these include methods and readily available tools, often furnished with pre-trained models. State of the art pre-trained models however, capture general, common sense knowledge, with limited value when it comes to handling data specific to a specialized domain. There is currently a lack of domain-specific pre-trained models that would further enhance the processing of natural language artefacts related to software engineering. To this end, we release a word2vec model trained over 15GB of textual data from Stack Overflow posts. We illustrate how the model disambiguates polysemous words by interpreting them within their software engineering context. In addition, we present examples of fine-grained semantics captured by the model, that imply transferability of these results to diverse, targeted information retrieval tasks in software engineering and motivate for further reuse of the model. © 2018 ACM.",natural language processing; skip-gram; stack overflow; word2vec,"38, 41",,Proceedings - International Conference on Software Engineering,Conference Paper,Scopus
944,,Cross version defect prediction with representative data via sparse subset selection,"Xu Z., Li S., Tang Y., Luo X., Zhang T., Liu J., Xu J.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051631927&doi=10.1145%2f3196321.3196331&partnerID=40&md5=a5b38173f8193159b6e81e2bb10a1399,10.1145/3196321.3196331,"Software defect prediction aims at detecting the defect-prone software modules by mining historical development data from software repositories. If such modules are identified at the early stage of the development, it can save large amounts of resources. Cross Version Defect Prediction (CVDP) is a practical scenario by training the classification model on the historical data of the prior version and then predicting the defect labels of modules of the current version. However, software development is a constantly-evolving process which leads to the data distribution differences across versions within the same project. The distribution differences will degrade the performance of the classification model. In this paper, we approach this issue by leveraging a state-of-the-art Dissimilarity-based Sparse Subset Selection (DS3) method. This method selects a representative module subset from the prior version based on the pairwise dissimilarities between the modules of two versions and assigns each module of the current version to one of the representative modules. These selected modules can well represent the modules of the current version, thus mitigating the distribution differences. We evaluate the effectiveness of DS3 for CVDP performance on total 40 cross-version pairs from 56 versions of 15 projects with three traditional and two effort-aware indicators. The extensive experiments show that DS3 outperforms three baseline methods, especially in terms of two effort-aware indicators. © 2018 ACM.",cross version defect prediction; pairwise dissimilarities; representative data; sparse subset selection,"132, 143",,Proceedings - International Conference on Software Engineering,Conference Paper,Scopus
945,,Bayesian hierarchical modelling for tailoring metric thresholds,Ernst N.A.,2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051627131&doi=10.1145%2f3196398.3196443&partnerID=40&md5=f560d3307d058f36b449603b98649731,10.1145/3196398.3196443,"Software is highly contextual. While there are cross-cutting 'global' lessons, individual software projects exhibit many 'local' properties. This data heterogeneity makes drawing local conclusions from global data dangerous. A key research challenge is to construct locally accurate prediction models that are informed by global characteristics and data volumes. Previous work has tackled this problem using clustering and transfer learning approaches, which identify locally similar characteristics. This paper applies a simpler approach known as Bayesian hierarchical modeling. We show that hierarchical modeling supports cross-project comparisons, while preserving local context. To demonstrate the approach, we conduct a conceptual replication of an existing study on setting software metrics thresholds. Our emerging results show our hierarchical model reduces model prediction error compared to a global approach by up to 50%. © 2018 ACM.",hierarchical models; metrics thresholds; probabilistic programming,"587, 591",,Proceedings - International Conference on Software Engineering,Conference Paper,Scopus
946,,A learning approach to enhance assurances for real-time self-adaptive systems,"Rodrigues A., Caldas R.D., Rodrigues G.N., Vogel T., Pelliccione P.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051544613&doi=10.1145%2f3194133.3194147&partnerID=40&md5=c00e660b05bf136109aaeb1500783060,10.1145/3194133.3194147,"The assurance of real-time properties is prone to context variability. Providing such assurance at design time would require to check all the possible context and system variations or to predict which one will be actually used. Both cases are not viable in practice since there are too many possibilities to foresee. Moreover, the knowledge required to fully provide the assurance for self-adaptive systems is only available at runtime and therefore difficult to predict at early development stages. Despite all the efforts on assurances for self-adaptive systems at design or runtime, there is still a gap on verifying and validating real-time constraints accounting for context variability. To fill this gap, we propose a method to provide assurance of self-adaptive systems, at design- and runtime, with special focus on real-time constraints. We combine off-line requirements elicitation and model checking with on-line data collection and data mining to guarantee the system's goals, both functional and non-functional, with fine tuning of the adaptation policies towards the optimization of quality attributes. We experimentally evaluate our method on a simulated prototype of a Body Sensor Network system (BSN) implemented in OpenDaVINCI. The results of the validation are promising and show that our method is effective in providing evidence that support the provision of assurance. © 2018 ACM.",assurance evidence; data mining; goal-oriented; learning approach; real-time systems; self-adaptive systems,"206, 216",,Proceedings - International Conference on Software Engineering,Conference Paper,Scopus
947,,Adapting a system with noisy outputs with statistical guarantees,"Gerostathopoulos I., Prehofer C., Bures T.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051495625&doi=10.1145%2f3194133.3194152&partnerID=40&md5=8adc16ada7658469f58e413676b1c34d,10.1145/3194133.3194152,"Many complex systems are intrinsically stochastic in their behavior which complicates their control and optimization. Current self-adaptation and self-optimization approaches are not tailored to systems that have (i) complex internal behavior that is unrealistic to model explicitly, (ii) noisy outputs, (iii) high cost of bad adaptation decisions, i.e. systems that are both hard and risky to adapt at runtime. In response, we propose to model the system to be adapted as black box and apply state-of-the-art optimization techniques combined with statistical guarantees. Our main contribution is a framework that combines runtime optimization with guarantees obtained from statistical testing and with a method for handling cost of bad adaptation decisions. We evaluate the feasibility of our approach by applying it on an existing traffic navigation self-adaptation exemplar. © 2018 ACM.",experimentation cost; self-adaptation; statistical guarantees,"58, 68",,Proceedings - International Conference on Software Engineering,Conference Paper,Scopus
948,,Distributed deep reinforcement learning on the cloud for autonomous driving,"Spryn M., Sharma A., Parkar D., Shrimal M.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051115558&doi=10.1145%2f3194085.3194088&partnerID=40&md5=21d55da08c7c9831038234660c4e52af,10.1145/3194085.3194088,"This paper proposes an architecture for leveraging cloud computing technology to reduce training time for deep reinforcement learning models for autonomous driving by distributing the training process across a pool of virtual machines. By parallelizing the training process, careful design of the reward function and use of techniques like transfer learning, we demonstrate a decrease in training time for our example autonomous driving problem from 140 hours to less than 1 hour. We go over our network architecture, job distribution paradigm, reward function design and report results from experiments on small sized cluster (1 - 6 training nodes) of machines. We also discuss the limitations of our approach when trying to scale up to massive clusters. © 2018 ACM.",Autonomous driving; Cloud computing; Deep reinforcement learning; Distributed machine learning; Simulation,"16, 22",,Proceedings - International Conference on Software Engineering,Conference Paper,Scopus
949,,Resolving ambiguities in regulations: Towards achieving the kohlbergian stage of principled morality,"Ghaisas S., Sainani A., Anish P.R.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054661796&doi=10.1145%2f3183428.3183433&partnerID=40&md5=83cc50e256d6e567524d256c5ddeddb4,10.1145/3183428.3183433,"According to Kohlberg, the final stage of morality is characterized by viewing laws as a means to an end by upholding values such as human dignity and fairness as guiding principles for complying with the essence of the law. Given that purpose of compliance is indeed wellbeing of citizens, software systems should, by design, incorporate these values so that laws are followed in spirit. How can we build software systems that incorporate these values? We present our work on disambiguating Health Insurance Portability and Accountability Act (HIPAA) so as to reduce the potential incidents of breach, thereby upholding of the aforesaid guiding principles of morality. We have employed deep learning based approaches to emulate the human process of disambiguation by integrating information from multiple sources, summarizing it, and augmenting the regulatory text with the additional information. This augmented regulatory text can be used by policy makers and software engineers to achieve compliance in spirit. © 2018 ACM.",ambiguity resolution; compliance; deep learning; principled morality; regulations,"57, 60",,Proceedings - International Conference on Software Engineering,Conference Paper,Scopus
950,,"What is the connection between issues, bugs, and enhancements?: Lessons learned from 800+ software projects","Krishna R., Agrawal A., Rahman A., Sobran A., Menzies T.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049695139&doi=10.1145%2f3183519.3183548&partnerID=40&md5=b28c37741d584ad6f90f13cb1cd0ae5f,10.1145/3183519.3183548,"Agile teams juggle multiple tasks so professionals are often assigned to multiple projects, especially in service organizations that monitor and maintain large suites of software for a large user base. If we could predict changes in project conditions change, then managers could better adjust the staff allocated to those projects. This paper builds such a predictor using data from 832 open source and proprietary projects. Using a time series analysis of the last 4 months of issues, we can forecast how many bug reports and enhancement requests will be generated the next month. The forecasts made in this way only require a frequency count of these issue reports (and do <u>not</u> require an historical record of bugs found in the project). That is, this kind of predictive model is very easy to deploy within a project. We hence strongly recommend this method for forecasting future issues, enhancements, and bugs in a project. © 2018 ACM.",Bugs; Collaborations; Issues; Time series analysis,"306, 315",,Proceedings - International Conference on Software Engineering,Conference Paper,Scopus
951,,SATD detector: A text-mining-based self-Admitted technical debt detection tool,"Liu Z., Huang Q., Xia X., Shihab E., Lo D., Li S.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049683556&doi=10.1145%2f3183440.3183478&partnerID=40&md5=fc9b4f29011dee78dffc65dcb86362ba,10.1145/3183440.3183478,"In software projects, technical debt metaphor is used to describe the situation where developers and managers have to accept compromises in long-Term software quality to achieve short-Term goals. There are many types of technical debt, and self-Admitted technical debt (SATD) was proposed recently to consider debt that is introduced intentionally (e.g., through temporaryfi x) and admitted by developers themselves. Previous work has shown that SATD can be successfully detected using source code comments. However, most current state-of-The-Art approaches identify SATD comments through pattern matching, which achieve high precision but very low recall. That means they may miss many SATD comments and are not practical enough. In this paper, we propose SATD Detector, a tool that is able to (i) automatically detect SATD comments using text mining and (ii) highlight, list and manage detected comments in an integrated development environment (IDE). This tool consists of a Java library and an Eclipse plug-in. The Java library is the back-end, which provides command-line interfaces and Java APIs to re-Train the text mining model using users' data and automatically detect SATD comments using either the build-in model or a user-specified model. The Eclipse plug-in, which is the front-end, first leverages our pre-Trained composite classifier to detect SATD comments, and then highlights and marks these detected comments in the source code editor of Eclipse. In addition, the Eclipse plug-in provides a view in IDE which collects all detected comments for management. Demo URL: https://youtu.be/sn4gU2qhGm0 Java library download: https://git.io/vNdnY Eclipse plug-in download: https://goo.gl/ZzjBzp. © 2018 Authors.",Eclipse plug-in; SATD detection; Self-Admitted technical debt,"9, 12",,Proceedings - International Conference on Software Engineering,Conference Paper,Scopus
952,,A novel biomedical image indexing and retrieval system via deep preference learning,"Pang S., Orgun M.A., Yu Z.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041721081&doi=10.1016%2fj.cmpb.2018.02.003&partnerID=40&md5=83daf49352eb3580b74fd70ba252d771,10.1016/j.cmpb.2018.02.003,"Background and Objectives: The traditional biomedical image retrieval methods as well as content-based image retrieval (CBIR) methods originally designed for non-biomedical images either only consider using pixel and low-level features to describe an image or use deep features to describe images but still leave a lot of room for improving both accuracy and efficiency. In this work, we propose a new approach, which exploits deep learning technology to extract the high-level and compact features from biomedical images. The deep feature extraction process leverages multiple hidden layers to capture substantial feature structures of high-resolution images and represent them at different levels of abstraction, leading to an improved performance for indexing and retrieval of biomedical images. Methods: We exploit the current popular and multi-layered deep neural networks, namely, stacked denoising autoencoders (SDAE) and convolutional neural networks (CNN) to represent the discriminative features of biomedical images by transferring the feature representations and parameters of pre-trained deep neural networks from another domain. Moreover, in order to index all the images for finding the similarly referenced images, we also introduce preference learning technology to train and learn a kind of a preference model for the query image, which can output the similarity ranking list of images from a biomedical image database. To the best of our knowledge, this paper introduces preference learning technology for the first time into biomedical image retrieval. Results: We evaluate the performance of two powerful algorithms based on our proposed system and compare them with those of popular biomedical image indexing approaches and existing regular image retrieval methods with detailed experiments over several well-known public biomedical image databases. Based on different criteria for the evaluation of retrieval performance, experimental results demonstrate that our proposed algorithms outperform the state-of-the-art techniques in indexing biomedical images. Conclusions: We propose a novel and automated indexing system based on deep preference learning to characterize biomedical images for developing computer aided diagnosis (CAD) systems in healthcare. Our proposed system shows an outstanding indexing ability and high efficiency for biomedical image retrieval applications and it can be used to collect and annotate the high-resolution images in a biomedical database for further biomedical image research and applications. © 2018 Elsevier B.V.",Biomedical image retrieval; Convolutional neural network; Deep learning; Preference learning,"53, 69",,Computer Methods and Programs in Biomedicine,Article,Scopus
953,,Incorporating depth into both CNN and CRF for indoor semantic segmentation,"Jiang J., Zhang Z., Huang Y., Zheng L.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047020164&doi=10.1109%2fICSESS.2017.8342970&partnerID=40&md5=dd9cc308aa5ed9991a7cf955c99b9e43,10.1109/ICSESS.2017.8342970,"To improve segmentation performance, a novel neural network architecture (termed DFCN-DCRF) is proposed, which combines an RGB-D fully convolutional neural network (DFCN) with a depth-sensitive fully-connected conditional random field (DCRF). First, a DFCN architecture which fuses depth information into the early layers and applies dilated convolution for later contextual reasoning is designed. Then, a depth-sensitive fully-connected conditional random field (DCRF) is proposed and combined with the previous DFCN to refine the preliminary result. Comparative experiments show that the proposed DFCN-DCRF achieves competitive performance compared with state-of-the-art methods. © 2017 IEEE.",conditional random fields; convolutional neural networks; RGB-D; semantic segmentation; transfer learning,"525, 530",,"Proceedings of the IEEE International Conference on Software Engineering and Service Sciences, ICSESS",Conference Paper,Scopus
954,,Cross-version defect prediction via hybrid active learning with kernel principal component analysis,"Xu Z., Liu J., Luo X., Zhang T.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051016836&doi=10.1109%2fSANER.2018.8330210&partnerID=40&md5=7dda65f6bf969160f559d55c389cf118,10.1109/SANER.2018.8330210,"As defects in software modules may cause product failure and financial loss, it is critical to utilize defect prediction methods to effectively identify the potentially defective modules for a thorough inspection, especially in the early stage of software development lifecycle. For an upcoming version of a software project, it is practical to employ the historical labeled defect data of the prior versions within the same project to conduct defect prediction on the current version, i.e., Cross-Version Defect Prediction (CVDP). However, software development is a dynamic evolution process that may cause the data distribution (such as defect characteristics) to vary across versions. Furthermore, the raw features usually may not well reveal the intrinsic structure information behind the data. Therefore, it is challenging to perform effective CVDP. In this paper, we propose a two-phase CVDP framework that combines Hybrid Active Learning and Kernel PCA (HALKP) to address these two issues. In the first stage, HALKP uses a hybrid active learning method to select some informative and representative unlabeled modules from the current version for querying their labels, then merges them into the labeled modules of the prior version to form an enhanced training set. In the second stage, HALKP employs a non-linear mapping method, kernel PCA, to extract representative features by embedding the original data of two versions into a high-dimension space. We evaluate the HALKP framework on 31 versions of 10 projects with three prevalent performance indicators. The experimental results indicate that HALKP achieves encouraging results with average F-measure, g-mean and Balance of 0.480, 0.592 and 0.580, respectively and significantly outperforms nearly all baseline methods. © 2018 IEEE.",,"209, 220",,"25th IEEE International Conference on Software Analysis, Evolution and Reengineering, SANER 2018 - Proceedings",Conference Paper,Scopus
955,,Extracting features from requirements: Achieving accuracy and automation with neural networks,"Li Y., Schulze S., Saake G.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050960039&doi=10.1109%2fSANER.2018.8330243&partnerID=40&md5=d68384739d44ff932daa9a50bddc4813,10.1109/SANER.2018.8330243,"Analyzing and extracting features and variability from different artifacts is an indispensable activity to support systematic integration of single software systems and Software Product Line (SPL). Beyond manually extracting variability, a variety of approaches, such as feature location in source code and feature extraction in requirements, has been proposed for automating the identification of features and their variation points. While requirements contain more complete variability information and provide traceability links to other artifacts, current techniques exhibit a lack of accuracy as well as a limited degree of automation. In this paper, we propose an unsupervised learning structure to overcome the abovementioned limitations. In particular, our technique consists of two steps: First, we apply Laplacian Eigenmaps, an unsupervised dimensionality reduction technique, to embed text requirements into compact binary codes. Second, requirements are transformed into a matrix representation by looking up a pre-trained word embedding. Then, the matrix is fed into CNN to learn linguistic characteristics of the requirements. Furthermore, we train CNN by matching the output of CNN with the pre-trained binary codes. Initial results show that accuracy is still limited, but that our approach allows to automate the entire process. © 2018 IEEE.",,"477, 481",,"25th IEEE International Conference on Software Analysis, Evolution and Reengineering, SANER 2018 - Proceedings",Conference Paper,Scopus
956,,Evaluating Pred(p) and standardized accuracy criteria in software development effort estimation,"Idri A., Abnane I., Abran A.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045686530&doi=10.1002%2fsmr.1925&partnerID=40&md5=3a3d72a1a9b32d7a365db2168b5e3a45,10.1002/smr.1925,"Software development effort estimation (SDEE) plays a primary role in software project management. But choosing the appropriate SDEE technique remains elusive for many project managers and researchers. Moreover, the choice of a reliable estimation accuracy measure is crucial because SDEE techniques behave differently given different accuracy measures. The most widely used accuracy measures in SDEE are those based on magnitude of relative error (MRE) such as mean/median MRE (MMRE/MedMRE) and prediction at level p (Pred(p)), which counts the number of observations where an SDEE technique gave MREs lower than p. However, MRE has proven to be an unreliable accuracy measure, favoring SDEE techniques that underestimate. Consequently, an unbiased measure called standardized accuracy (SA) has been proposed. This paper deals with the Pred(p) and SA measures. We investigate (1) the consistency of Pred(p) and SA as accuracy measures and SDEE technique selectors, and (2) the relationship between Pred(p) and SA. The results suggest that Pred(p) is less biased towards underestimates and generally selects the same best technique as SA. Moreover, SA and Pred(p) measure different aspects of technique performance, and SA may be used as a predictor of Pred(p) by means of the 3 association rules. Copyright © 2017 John Wiley & Sons, Ltd.",accuracy measure; MMRE; Pred(p); software development effort estimation; standardized accuracy,,,Journal of Software: Evolution and Process,Article,Scopus
957,,Efficient computational model for classification of protein localization images using Extended Threshold Adjacency Statistics and Support Vector Machines,"Tahir M., Jan B., Hayat M., Shah S.U., Amin M.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041549482&doi=10.1016%2fj.cmpb.2018.01.021&partnerID=40&md5=a687277c76a2ed7f17b396c1aecdce34,10.1016/j.cmpb.2018.01.021,"Background and objective: Discriminative and informative feature extraction is the core requirement for accurate and efficient classification of protein subcellular localization images so that drug development could be more effective. The objective of this paper is to propose a novel modification in the Threshold Adjacency Statistics technique and enhance its discriminative power. Methods: In this work, we utilized Threshold Adjacency Statistics from a novel perspective to enhance its discrimination power and efficiency. In this connection, we utilized seven threshold ranges to produce seven distinct feature spaces, which are then used to train seven SVMs. The final prediction is obtained through the majority voting scheme. The proposed ETAS-SubLoc system is tested on two benchmark datasets using 5-fold cross-validation technique. Results: We observed that our proposed novel utilization of TAS technique has improved the discriminative power of the classifier. The ETAS-SubLoc system has achieved 99.2% accuracy, 99.3% sensitivity and 99.1% specificity for Endogenous dataset outperforming the classical Threshold Adjacency Statistics technique. Similarly, 91.8% accuracy, 96.3% sensitivity and 91.6% specificity values are achieved for Transfected dataset. Conclusions: Simulation results validated the effectiveness of ETAS-SubLoc that provides superior prediction performance compared to the existing technique. The proposed methodology aims at providing support to pharmaceutical industry as well as research community towards better drug designing and innovation in the fields of bioinformatics and computational biology. The implementation code for replicating the experiments presented in this paper is available at: https://drive.google.com/file/d/0B7IyGPObWbSqRTRMcXI2bG5CZWs/view?usp=sharing © 2018 Elsevier B.V.",Ensemble learning; Fluorescence microscopy; Protein images; Subcellular localization; Threshold Adjacency Statistics,"205, 215",,Computer Methods and Programs in Biomedicine,Article,Scopus
958,,Deep Convolutional Neural Networks for breast cancer screening,"Chougrad H., Zouaki H., Alheyane O.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041413978&doi=10.1016%2fj.cmpb.2018.01.011&partnerID=40&md5=2cd7703179032adb05fa3ab23fcd3a05,10.1016/j.cmpb.2018.01.011,"Background and objective: Radiologists often have a hard time classifying mammography mass lesions which leads to unnecessary breast biopsies to remove suspicions and this ends up adding exorbitant expenses to an already burdened patient and health care system. Methods: In this paper we developed a Computer-aided Diagnosis (CAD) system based on deep Convolutional Neural Networks (CNN) that aims to help the radiologist classify mammography mass lesions. Deep learning usually requires large datasets to train networks of a certain depth from scratch. Transfer learning is an effective method to deal with relatively small datasets as in the case of medical images, although it can be tricky as we can easily start overfitting. Results: In this work, we explore the importance of transfer learning and we experimentally determine the best fine-tuning strategy to adopt when training a CNN model. We were able to successfully fine-tune some of the recent, most powerful CNNs and achieved better results compared to other state-of-the-art methods which classified the same public datasets. For instance we achieved 97.35% accuracy and 0.98 AUC on the DDSM database, 95.50% accuracy and 0.97 AUC on the INbreast database and 96.67% accuracy and 0.96 AUC on the BCDR database. Furthermore, after pre-processing and normalizing all the extracted Regions of Interest (ROIs) from the full mammograms, we merged all the datasets to build one large set of images and used it to fine-tune our CNNs. The CNN model which achieved the best results, a 98.94% accuracy, was used as a baseline to build the Breast Cancer Screening Framework. To evaluate the proposed CAD system and its efficiency to classify new images, we tested it on an independent database (MIAS) and got 98.23% accuracy and 0.99 AUC. Conclusion: The results obtained demonstrate that the proposed framework is performant and can indeed be used to predict if the mass lesions are benign or malignant. © 2018 Elsevier B.V.",Breast cancer; Breast mass lesion classification; Computer-aided Diagnosis; Convolutional Neural Network; Deep learning; Transfer learning,"19, 30",,Computer Methods and Programs in Biomedicine,Article,Scopus
959,,Simultaneous detection and classification of breast masses in digital mammograms via a deep learning YOLO-based CAD system,"Al-masni M.A., Al-antari M.A., Park J.-M., Gi G., Kim T.-Y., Rivera P., Valarezo E., Choi M.-T., Han S.-M., Kim T.-S.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041410686&doi=10.1016%2fj.cmpb.2018.01.017&partnerID=40&md5=fbde2b90821e2568a51d3b8a21d6dc5c,10.1016/j.cmpb.2018.01.017,"Background and objective: Automatic detection and classification of the masses in mammograms are still a big challenge and play a crucial role to assist radiologists for accurate diagnosis. In this paper, we propose a novel Computer-Aided Diagnosis (CAD) system based on one of the regional deep learning techniques, a ROI-based Convolutional Neural Network (CNN) which is called You Only Look Once (YOLO). Although most previous studies only deal with classification of masses, our proposed YOLO-based CAD system can handle detection and classification simultaneously in one framework. Methods: The proposed CAD system contains four main stages: preprocessing of mammograms, feature extraction utilizing deep convolutional networks, mass detection with confidence, and finally mass classification using Fully Connected Neural Networks (FC-NNs). In this study, we utilized original 600 mammograms from Digital Database for Screening Mammography (DDSM) and their augmented mammograms of 2,400 with the information of the masses and their types in training and testing our CAD. The trained YOLO-based CAD system detects the masses and then classifies their types into benign or malignant. Results: Our results with five-fold cross validation tests show that the proposed CAD system detects the mass location with an overall accuracy of 99.7%. The system also distinguishes between benign and malignant lesions with an overall accuracy of 97%. Conclusions: Our proposed system even works on some challenging breast cancer cases where the masses exist over the pectoral muscles or dense regions. © 2018 Elsevier B.V.",Breast cancer; Computer Aided Diagnosis; Deep learning; Mass detection and classification; You Only Look Once (YOLO),"85, 94",,Computer Methods and Programs in Biomedicine,Article,Scopus
960,,An efficient method for uncertainty propagation in robust software performance estimation,"Aleti A., Trubiani C., van Hoorn A., Jamshidi P.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041298151&doi=10.1016%2fj.jss.2018.01.010&partnerID=40&md5=77ccc1d5979956d0e84c3703513442e9,10.1016/j.jss.2018.01.010,"Software engineers often have to estimate the performance of a software system before having full knowledge of the system parameters, such as workload and operational profile. These uncertain parameters inevitably affect the accuracy of quality evaluations, and the ability to judge if the system can continue to fulfil performance requirements if parameter results are different from expected. Previous work has addressed this problem by modelling the potential values of uncertain parameters as probability distribution functions, and estimating the robustness of the system using Monte Carlo-based methods. These approaches require a large number of samples, which results in high computational cost and long waiting times. To address the computational inefficiency of existing approaches, we employ Polynomial Chaos Expansion (PCE) as a rigorous method for uncertainty propagation and further extend its use to robust performance estimation. The aim is to assess if the software system is robust, i.e., it can withstand possible changes in parameter values, and continue to meet performance requirements. PCE is a very efficient technique, and requires significantly less computations to accurately estimate the distribution of performance indices. Through three very different case studies from different phases of software development and heterogeneous application domains, we show that PCE can accurately (> 97%) estimate the robustness of various performance indices, and saves up to 225 h of performance evaluation time when compared to Monte Carlo Simulation. © 2018 Elsevier Inc.",Polynomial chaos expansion; Software performance engineering; Uncertainty propagation,"222, 235",,Journal of Systems and Software,Article,Scopus
961,,Software defect prediction using stacked denoising autoencoders and two-stage ensemble learning,"Tong H., Liu B., Wang S.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039040169&doi=10.1016%2fj.infsof.2017.11.008&partnerID=40&md5=45ae2449ede74ba56df29aa76b96b6c2,10.1016/j.infsof.2017.11.008,"Context: Software defect prediction (SDP) plays an important role in allocating testing resources reasonably, reducing testing costs, and ensuring software quality. However, software metrics used for SDP are almost entirely traditional features compared with deep representations (DPs) from deep learning. Although stacked denoising autoencoders (SDAEs) are powerful for feature learning and have been successfully applied in other fields, to the best of our knowledge, it has not been investigated in the field of SDP. Meanwhile, class-imbalance is still a pressing problem needing to be addressed. Objective: In this paper, we propose a novel SDP approach, SDAEsTSE, which takes advantages of SDAEs and ensemble learning, namely the proposed two-stage ensemble (TSE). Method: Our method mainly includes two phases: the deep learning phase and two-stage ensemble (TSE) phase. We first use SDAEs to extract the DPs from the traditional software metrics, and then a novel ensemble learning approach, TSE, is proposed to address the class-imbalance problem. Results: Experiments are performed on 12 NASA datasets to demonstrate the effectiveness of DPs, the proposed TSE, and SDAEsTSE, respectively. The performance is evaluated in terms of F-measure, the area under the curve (AUC), and Matthews correlation coefficient (MCC). Generally, DPs, TSE, and SDAEsTSE contribute to significantly higher performance compared with corresponding traditional metrics, classic ensemble methods, and benchmark SDP models. Conclusions: It can be concluded that (1) deep representations are promising for SDP compared with traditional software metrics, (2) TSE is more effective for addressing the class-imbalance problem in SDP compared with classic ensemble learning methods, and (3) the proposed SDAEsTSE is significantly effective for SDP. © 2017 Elsevier B.V.",Deep learning; Ensemble learning; Software defect prediction; Software metrics; Stacked denoising autoencoders,"94, 111",,Information and Software Technology,Article,Scopus
962,,Improving Bug Localization with an Enhanced Convolutional Neural Network,"Xiao Y., Keung J., Mi Q., Bennin K.E.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045951289&doi=10.1109%2fAPSEC.2017.40&partnerID=40&md5=543761dfcfc22ffd812c83a7a57ed60b,10.1109/APSEC.2017.40,"Background: Localizing buggy files automatically speeds up the process of bug fixing so as to improve the efficiency and productivity of software quality teams. There are other useful semantic information available in bug reports and source code, but are mostly underutilized by existing bug localization approaches. Aims: We propose DeepLocator, a novel deep learning based model to improve the performance of bug localization by making full use of semantic information. Method: DeepLocator is composed of an enhanced CNN (Convolutional Neural Network) proposed in this study considering bug-fixing experience, together with a new rTF-IDuF method and pretrained word2vec technique. DeepLocator is then evaluated on over 18,500 bug reports extracted from AspectJ, Eclipse, JDT, SWT and Tomcat projects. Results: The experimental results show that DeepLocator achieves 9.77% to 26.65% higher Fmeasure than the conventional CNN and 3.8% higher MAP than a state-of-the-art method HyLoc using less computation time. Conclusion: DeepLocator is capable of automatically connecting bug reports to the corresponding buggy files and successfully achieves better performance based on a deep understanding of semantics in bug reports and source code. © 2017 IEEE.",bug localization; convolutional neural network; deep learning; semantic information; TF-IDF; word2vec,"338, 347",,"Proceedings - Asia-Pacific Software Engineering Conference, APSEC",Conference Paper,Scopus
963,,Transfer Learning for Cross-Platform Software Crowdsourcing Recommendation,"Yan S., Shen B., Mo W., Li N.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045917363&doi=10.1109%2fAPSEC.2017.33&partnerID=40&md5=f0e2feaf8ba52d1b4cec7353b1cb607f,10.1109/APSEC.2017.33,"Recently, with the development of software crowdsourcing industry, an increasing number of users joined the software crowdsourcing platforms to publish software project tasks or to seek proper work opportunities. One of competitive functions of these platforms is to recommend proficient projects to developers. However, in such recommender system, there exists a serious platform cold-start problem, especially for new software crowdsourcing platforms, as they usually have too little cumulative data to support accurate model training and prediction. This paper focuses on solving the platform cold-start problem in software crowdsourcing recommendation system by transfer learning technologies. We proposed a novel cross-platform recommendation method for new software crowdsourcing platforms, whose idea is trying to transfer data and knowledge from other mature software crowdsourcing platforms (source domains) to solve the insufficient recommendation model training problem in a new platform (target domain). The proposed method maps different kinds of features both in the source domain and the target domain after a certain transformation and combination to a latent space by learning the correspondences between features. Specifically, our method is an instance of content-based recommendation, which uses tags and keywords extracted from project description in crowdsourcing platforms as features, and then set weights for each feature to reflect its importance. Then, Weight-SCL is proposed to merge and distinguish tag features and keyword features before doing feature mapping and data migration to implement knowledge transformation. Finally, we use the data from two famous software crowdsourcing platform as dataset, and a series of experiments are conducted to evaluate the performance of the multi-source recommendation system in comparison with the baseline methods, and get 1.2X performance promotion. © 2017 IEEE.",Cold-start Problem; Recommender Systems; Software Crowdsourcing; Tranfer Learning,"269, 278",,"Proceedings - Asia-Pacific Software Engineering Conference, APSEC",Conference Paper,Scopus
964,,An Empirical Study on Real Bugs for Machine Learning Programs,"Sun X., Zhou T., Li G., Hu J., Yang H., Li B.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045889703&doi=10.1109%2fAPSEC.2017.41&partnerID=40&md5=4052827dd8dd59a3745b29f1ed998f5c,10.1109/APSEC.2017.41,"Due to the availability of various open source Machine Learning (ML) tools and libraries, developers nowadays can easily implement their purposes by just invoking machine learning APIs without knowing the details of the algorithm. However, the owners of ML tools and libraries usually pay more attention to the correctness and functionality of their algorithm, while spending much less effort on maintaining their code and keeping their code at a high quality level. Considering the popularity of machine learning in today's world, low quality ML tools and libraries can have a huge impact on the software products that use ML algorithms. So in this paper, we conduct an empirical study on real machine learning bugs to examine their patterns and how they evolve over time. We collect three popular machine learning projects on Github, and manually analyzed 329 closed bugs from the perspectives of their bug category, fix pattern, fix scale, fix duration, and type of software maintenance. The results show that (1) there are seven categories of bugs in machine learning programs; (2) twelve different fix patterns are commonly used to fix the bugs; (3) 63.83% of the patches belong to micro-scale-fix and small-scale-fix, and 68.39% of the bugs are fixed within one month; (4) 47.77% of the bug fixes belong to corrective activity from the view of software maintenance. © 2017 IEEE.",bug; bug fix; empirical study; machine learning programs,"348, 357",,"Proceedings - Asia-Pacific Software Engineering Conference, APSEC",Conference Paper,Scopus
965,,Optimizing convolutional neural networks on the sunway TaihuLight supercomputer,"Zhao W., Fu H., Fang J., Zheng W., Gan L., Yang G.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045202025&doi=10.1145%2f3177885&partnerID=40&md5=54d75bf290495fe03e58b1e539c70cfb,10.1145/3177885,"The Sunway TaihuLight supercomputer is powered by SW26010, a new 260-core processor designed with on-chip fusion of heterogeneous cores. In this article, we present our work on optimizing the training process of convolutional neural networks (CNNs) on the Sunway TaihuLight supercomputer. Specifically, a highly efficient library (swDNN) and a customized Caffe framework (swCaffe) are proposed. Architecture-oriented optimization methods targeting the many-core architecture of SW26010 are introduced and are able to achieve 48× speedup for the convolution routine in swDNN and 4× speedup for the complete training process of the VGG-16 network using swCaffe, compared to the unoptimized algorithm and framework. Compared to the cuDNN library and the Caffe framework based on the NVIDIA K40m GPU, the proposed swDNN library and swCaffe framework on SW26010 have nearly half the performance of K40m in single -precision and have 3.6× and 1.8× speedup over K40m in double precision, respectively. © 2018 ACM.",Convolutional neural network; Deep learning; Heterogeneous many-core architecture; Sunway taihulight supercomputer,,,ACM Transactions on Architecture and Code Optimization,Article,Scopus
966,,The unreasonable effectiveness of software analytics,Menzies T.,2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043720906&doi=10.1109%2fMS.2018.1661323&partnerID=40&md5=eb66bda7d5cfbc29479bb6d05df4585b,10.1109/MS.2018.1661323,"In theory, software analytics shouldn't work because software project behavior shouldn't be predictable. However, it does. Why? © 1984-2012 IEEE.",Redirections; software analytics; software development; software engineering,"96, 98",,IEEE Software,Article,Scopus
967,,Heterogeneous fault prediction with cost-sensitive domain adaptation,"Li Z., Jing X.-Y., Zhu X.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041028667&doi=10.1002%2fstvr.1658&partnerID=40&md5=c184bde4ae73ef5cad60f7e8b83bb027,10.1002/stvr.1658,"In the early phases of software testing, projects may have only limited historical defect data. Learning prediction model with such insufficient training data will limit the efficacy of learned predictor. In practice, there are usually many publicly available fault prediction datasets. Recently, heterogeneous fault prediction (HFP) has been proposed. However, existing HFP models do not investigate how to use mixed project data to predict target. Furthermore, defect data are often imbalanced. The imbalanced data distribution of source usually leads to serious misclassification of fault-prone instances, which will degrade the predictor's performance. Existing HFP methods do not consider the class imbalance problem in the training stages. In this paper, we propose a novel Cost-sensitive Label and Structure-consistent Unilateral Projection (CLSUP) approach for HFP. CLSUP can not only make better use of the within-project and cross-project data but also alleviate the class imbalance problem by setting different misclassification costs for fault-prone and non–fault-prone instances. Extensive experiments on 30 projects demonstrate the effectiveness of CLSUP. Copyright © 2018 John Wiley & Sons, Ltd.",class imbalance; cost-sensitive learning; heterogeneous domain adaptation; heterogeneous fault prediction; mixed project; software quality assurance,,,Software Testing Verification and Reliability,Article,Scopus
968,,Symtosis: A liver ultrasound tissue characterization and risk stratification in optimized deep learning paradigm,"Biswas M., Kuppili V., Edla D.R., Suri H.S., Saba L., Marinhoe R.T., Sanches J.M., Suri J.S.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038880889&doi=10.1016%2fj.cmpb.2017.12.016&partnerID=40&md5=c1d0b9373848146ed3a5c4edb270c8b3,10.1016/j.cmpb.2017.12.016,[No abstract available],,"165, 177",,Computer Methods and Programs in Biomedicine,Article,Scopus
969,,Investigating the impact of fault data completeness over time on predicting class fault-proneness,"Al Dallal J., Morasca S.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034575078&doi=10.1016%2fj.infsof.2017.11.001&partnerID=40&md5=7dfcfc87371443b4ba6d56c4e0a942f6,10.1016/j.infsof.2017.11.001,"Context: The adequacy of fault-proneness prediction models in representing the relationship between the internal quality of classes and their fault-proneness relies on several factors. One of these factors is the completeness of the fault data. A fault-proneness prediction model that is built using fault data collected during testing or after a relatively short period of time after release may be inadequate and may not be reliable enough in predicting faulty classes. Objective: We empirically study the relationship between the time interval since a system is released and the performance of the fault-proneness prediction models constructed based on the fault data reported within the time interval. Method: We construct prediction models using fault data collected at several time intervals since a system has been released and study the performance of the models in representing the relationship between the internal quality of classes and their fault-proneness. In addition, we empirically explore the relationship between the performance of a prediction model and the percentage of increase in the number of classes detected faulty (PIF) over time. Results: Our results show evidence in favor of the expectation that predictions models that use more complete fault data, to a certain extent, more adequately represent the expected relationship between the internal quality of classes and their fault-proneness and have better performance. A threshold based on the PIF value can be used as an indicator for deciding when to stop collecting fault data. When reaching this threshold, collecting additional fault data will not significantly improve the prediction ability of the constructed model. Conclusion: When constructing fault-proneness prediction models, researchers and software engineers are advised to rely on systems that have relatively long maintenance histories. Researchers and software engineers can use the PIF value as an indicator for deciding when to stop collecting fault data. © 2017 Elsevier B.V.",Class fault-proneness; Fault data; Internal quality attributes; Object-oriented software; Quality measures,"86, 105",,Information and Software Technology,Article,Scopus
970,,A benchmark study on the effectiveness of search-based data selection and feature selection for cross project defect prediction,"Hosseini S., Turhan B., Mäntylä M.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021334527&doi=10.1016%2fj.infsof.2017.06.004&partnerID=40&md5=4e57210961284e77b2deb3aa00421a22,10.1016/j.infsof.2017.06.004,"Context: Previous studies have shown that steered training data or dataset selection can lead to better performance for cross project defect prediction(CPDP). On the other hand, feature selection and data quality are issues to consider in CPDP. Objective: We aim at utilizing the Nearest Neighbor (NN)-Filter, embedded in genetic algorithm to produce validation sets for generating evolving training datasets to tackle CPDP while accounting for potential noise in defect labels. We also investigate the impact of using different feature sets. Method: We extend our proposed approach, Genetic Instance Selection (GIS), by incorporating feature selection in its setting. We use 41 releases of 11 multi-version projects to assess the performance GIS in comparison with benchmark CPDP (NN-filter and Naive-CPDP) and within project (Cross-Validation(CV) and Previous Releases(PR)). To assess the impact of feature sets, we use two sets of features, SCM+OO+LOC(all) and CK+LOC(ckloc) as well as iterative info-gain subsetting(IG) for feature selection. Results: GIS variant with info gain feature selection is significantly better than NN-Filter (all,ckloc,IG) in terms of F1 (p=values≪0.001, Cohen's d={0.621,0.845,0.762}) and G (p=values≪0.001, Cohen's d={0.899,1.114,1.056}), and Naive CPDP (all,ckloc,IG) in terms of F1 (p=values≪0.001, Cohen's d={0.743,0.865,0.789}) and G (p=values≪0.001, Cohen's d={1.027,1.119,1.050}). Overall, the performance of GIS is comparable to that of within project defect prediction (WPDP) benchmarks, i.e. CV and PR. In terms of multiple comparisons test, all variants of GIS belong to the top ranking group of approaches. Conclusions: We conclude that datasets obtained from search based approaches combined with feature selection techniques is a promising way to tackle CPDP. Especially, the performance comparison with the within project scenario encourages further investigation of our approach. However, the performance of GIS is based on high recall in the expense of a loss in precision. Using different optimization goals, utilizing other validation datasets and other feature selection techniques are possible future directions to investigate. © 2017 Elsevier B.V.",Cross project defect prediction; Genetic algorithms; Instance selection; Search based optimization; Training data selection,"296, 312",,Information and Software Technology,Article,Scopus
971,,Tackling class overlap and imbalance problems in software defect prediction,"Chen L., Fang B., Shang Z., Tang Y.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991409678&doi=10.1007%2fs11219-016-9342-6&partnerID=40&md5=5863cb3da721b0f0ecde38abb4218d4e,10.1007/s11219-016-9342-6,"Software defect prediction (SDP) is a promising solution to save time and cost in the software testing phase for improving software quality. Numerous machine learning approaches have proven effective in SDP. However, the unbalanced class distribution in SDP datasets could be a problem for some conventional learning methods. In addition, class overlap increases the difficulty for the predictors to learn the defective class accurately. In this study, we propose a new SDP model which combines class overlap reduction and ensemble imbalance learning to improve defect prediction. First, the neighbor cleaning method is applied to remove the overlapping non-defective samples. The whole dataset is then randomly under-sampled several times to generate balanced subsets so that multiple classifiers can be trained on these data. Finally, these individual classifiers are assembled with the AdaBoost mechanism to build the final prediction model. In the experiments, we investigated nine highly unbalanced datasets selected from a public software repository and confirmed that the high rate of overlap between classes existed in SDP data. We assessed the performance of our proposed model by comparing it with other state-of-the-art methods including conventional SDP models, imbalance learning and data cleaning methods. Test results and statistical analysis show that the proposed model provides more reasonable defect prediction results and performs best in terms of G-mean and AUC among all tested models. © 2016, Springer Science+Business Media New York.",Class imbalance; Class overlap; Machine learning; Software defect prediction,"97, 125",,Software Quality Journal,Article,Scopus
972,,Duplex output software effort estimation model with self-guided interpretation,"Mensah S., Keung J., Bosu M.F., Bennin K.E.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030449177&doi=10.1016%2fj.infsof.2017.09.010&partnerID=40&md5=7143271386c886fe1486c3472eac8b50,10.1016/j.infsof.2017.09.010,"Context Software effort estimation (SEE) plays a key role in predicting the effort needed to complete software development task. However, the conclusion instability across learners has affected the implementation of SEE models. This instability can be attributed to the lack of an effort classification benchmark that software researchers and practitioners can use to facilitate and interpret prediction results. Objective To ameliorate the conclusion instability challenge by introducing a classification and self-guided interpretation scheme for SEE. Method We first used the density quantile function to discretise the effort recorded in 14 datasets into three classes (high, low and moderate) and built regression models for these datasets. The results of the regression models were an effort estimate, termed output 1, which was then classified into an effort class, termed output 2. We refer to the models generated in this study as duplex output models as they return two outputs. The introduced duplex output models trained with the leave-one-out cross validation and evaluated with MAE, BMMRE and adjusted R2, can be used to predict both the software effort and the class of software effort estimate. Robust statistical tests (Welch's t-test and Kruskal-Wallis H-test) were used to examine the statistical significant differences in the models’ prediction performances. Results We observed the following: (1) the duplex output models not only predicted the effort estimates, they also offered a guide to interpreting the effort expended; (2) incorporating the genetic search algorithm into the duplex output model allowed the sampling of relevant features for improved prediction accuracy; and (3) ElasticNet, a hybrid regression, provided superior prediction accuracy over the ATLM, the state-of-the-art baseline regression. Conclusion The results show that the duplex output model provides a self-guided benchmark for interpreting estimated software effort. ElasticNet can also serve as a baseline model for SEE. © 2017 Elsevier B.V.",Duplex output; Effort classification; Effort estimation; Multiple regression models,"1, 13",,Information and Software Technology,Article,Scopus
973,,Deep-stress capsule video endoscopy image enhancement,"Mohammed A., Pedersen M., Hovde Ø., Yildirim S.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061066942&doi=10.2352%2fissn.2169-2629.2018.26.247&partnerID=40&md5=3fa64cfc5f0589c324076f72609b2438,10.2352/issn.2169-2629.2018.26.247,"This paper proposes a unified framework for capsule video endoscopy image enhancement with an objective to enhance the diagnostic values of these images. The proposed method is based on a hybrid approach of deep learning and classical image processing techniques. Given an input image, it is decomposed spatially into multi-layer features. We estimate the base layer with pre-trained deep edge aware filters that are learned on the flicker dataset. The detail layers are estimated by the spatio-temporal retinex-inspired envelope with a stochastic sampling technique. The enhanced image is computed by a convex linear combination of the base and the detail layers giving detailed and shadow surface enhanced image. To show its potential, performance comparison between with and without the proposed image enhancement technique is shown using several video images obtained from capsule endoscopy for different parts of the digestive organ. Moreover, different learned filters such as Bilateral and L0 norm are compared for enhancement using an objective image quality metric, BRISQUE, to show the generality of the proposed method. © 2018, Society for Imaging Science and Technology.",,"247, 252",,Final Program and Proceedings - IS and T/SID Color Imaging Conference,Conference Paper,Scopus
974,,Reviving traditional image quality metrics using CNNs,"Amirshahi S.A., Pedersen M., Beghdadi A.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061036847&doi=10.2352%2fissn.2169-2629.201s.26.241&partnerID=40&md5=0549f5227491323a414bd72fb5ff5c81,10.2352/issn.2169-2629.201s.26.241,"Objective Image Quality Metrics (IQMs) are introduced with the goal of modeling the perceptual quality scores given by observers to an image. In this study we use a pre-trained Convolutional Neural Network (CNN) model to extract feature maps at different convolutional layers of the test and reference image. We then compare the feature maps using traditional IQMs such as: SSIM, MSE, and PSNR. Experimental results on four benchmark datasets show that our proposed approach can increase the accuracy of these IQMs by an average of 23%. Compared to 11 other state-of-the-art IQMs, our proposed approach can either outperform or perform as good as the mentioned 11 metrics. We can show that by linking traditional IQMs and pre-trained CNN models we are able to evaluate image quality with a high accuracy. © 2018, Society for Imaging Science and Technology.",,"241, 246",,Final Program and Proceedings - IS and T/SID Color Imaging Conference,Conference Paper,Scopus
975,,Revisting the impact of regression models for predicting the number of defects,"Wu M., Ye S., Li C., Ma Z., Fu Z.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056892291&doi=10.18293%2fSEKE2018-068&partnerID=40&md5=c5275b1e860c2d80de0708712f721f97,10.18293/SEKE2018-068,"Predicting the number of faults in software modules can be more helpful instead of predicting the modules being faulty or non-faulty. Chen et al. (SEKE 397-402, 2015) and Rathore et al. (Soft Computing 21: 7417-7434, 2017) empirically investigate the feasibility of some regression algorithms for predicting the number of defects. The experimental results showed that the decision tree regression algorithm performed best in terms of average absolute error (AAE), average relative error (ARE) and root mean square error (RMSE). However, they did not consider the imbalanced data distribution problem in defect datasets and employed improper performance measures for evaluating the regression models to evaluate the performance of models for predicting the number of defects. Hence, we revisit the impact of different regression algorithms for predicting the number of defects using Fault-Percentile-Average (FPA) as the performance measure. The experiments on 31 datasets from PROMISE repository show that the prediction performance of models for predicting the number of defects built by different regression algorithms are various, and the gradient boosting regression algorithm and the Bayesian ridge regression algorithm can achieve better performance. © 2018 Universitat zu Koln. All rights reserved.",Data imbalance; Fault-Percentile-Average; Predicting the number of defects; Regression algorithm,"427, 432",,"Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE",Conference Paper,Scopus
976,,How far we have progressed in the journey? An examination of cross-project defect prediction,"Zhou Y., Yang Y., Lu H., Chen L., Li Y., Zhao Y., Qian J., Xu B.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054209479&doi=10.1145%2f3183339&partnerID=40&md5=5594d430ea3d0df9061ebf5903a3fde7,10.1145/3183339,"Background. Recent years have seen an increasing interest in cross-project defect prediction (CPDP), which aims to apply defect prediction models built on source projects to a target project. Currently, a variety of (complex) CPDP models have been proposed with a promising prediction performance. Problem. Most, if not all, of the existing CPDP models are not compared against those simple module size models that are easy to implement and have shown a good performance in defect prediction in the literature. Objective.We aim to investigate how far we have really progressed in the journey by comparing the performance in defect prediction between the existing CPDP models and simple module size models. Method.We first use module size in the target project to build two simple defect prediction models, Manual- Down and ManualUp, which do not require any training data from source projects. ManualDown considers a larger module as more defect-prone, while ManualUp considers a smaller module as more defect-prone. Then, we take the following measures to ensure a fair comparison on the performance in defect prediction between the existing CPDP models and the simple module size models: using the same publicly available data sets, using the same performance indicators, and using the prediction performance reported in the original cross-project defect prediction studies. Result. The simple module size models have a prediction performance comparable or even superior to most of the existing CPDP models in the literature, including many newly proposed models. Conclusion. The results caution us that, if the prediction performance is the goal, the real progress in CPDP is not being achieved as it might have been envisaged. We hence recommend that future studies should include ManualDown/ManualUp as the baseline models for comparison when developing new CPDP models to predict defects in a complete target project. © 2018 ACM.",Cross-project; Defect prediction; Model; Supervised; Unsupervised,,,ACM Transactions on Software Engineering and Methodology,Article,Scopus
977,,Software Analytics: What's Next?,"Menzies T., Zimmermann T.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049787499&doi=10.1109%2fMS.2018.290111035&partnerID=40&md5=67ece3a1b84217572749f4220cf80921,10.1109/MS.2018.290111035,"Knowing what factors control software projects is very useful because humans might not understand those factors. Developers sometimes develop their own ideas about good and bad software, on the basis of just a few past projects. Using software analytics, we can correct those misconceptions. Software analytics lets software engineers learn about AI techniques. Once they learn those techniques, they can build and ship innovative AI tools. That is, software analytics is the training ground for the next generation of AI-literate software engineers. This article is part of a special issue on software engineering's 50th anniversary. © 1984-2012 IEEE.",data mining; software; software analytics; software development; software engineering,"64, 70",,IEEE Software,Article,Scopus
978,,Survey on Medical Image Computer Aided Detection and Diagnosis Systems,"Zheng G.-Y., Liu X.-B., Han G.-H.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049575563&doi=10.13328%2fj.cnki.jos.005519&partnerID=40&md5=b6212c0201bb3162e582ec0d387e861d,10.13328/j.cnki.jos.005519,"Computer aided detection/diagnosis (CAD) can improve the accuracy of diagnosis, reduce false positive, and provide decision supports for doctors. The main purpose of this paper is to analyze the latest development of computer aided diagnosis tools. Focusing on the top four fatal cancer's incidence positions, major recent publications on CAD applications in different medical imaging areas are reviewed in this survey according to different imaging techniques and diseases. Further more, multidimentional analysis is made on the researches from image data sets, algorithms and evaluation methods. Finally, existing problems, research trend and development direction in the field of medical image CAD system are discussed. © Copyright 2018, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Colorectal cancer; Computer-aided detection (CADe); Computer-aided diagnosis (CADx); Lung cancer; Mammary cancer; Medical image; Prostatic cancer,"1471, 1514",,Ruan Jian Xue Bao/Journal of Software,Review,Scopus
979,,Investigation of Empirical Researches in Software Engineering,"Zhang L., Pu M.-Y., Liu Y.-J., Tian J.-H., Yue T., Jiang J.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049555118&doi=10.13328%2fj.cnki.jos.005520&partnerID=40&md5=02111ae26bb140f11169d60139a11bd7,10.13328/j.cnki.jos.005520,"To depict, understand, evaluate, predict, control, manage or enhance software-related artifacts, researchers and practitioners often rely on empirical methods. Empirical methods have been widely used in software engineering, and they are attracting increasing attention over the years. By conducting a systematic mapping, this paper aims to provide a literature survey of 250 papers published in a typical journal-Empirical Software Engineering, from January 2013 to June 2017. With qualitative and quantitative analysis, this survey reveals the commonly used empirical research methods, research purposes, and the application of the methods in subfields of software engineering, including the solved problems and some new features. The findings also cover the use of open source projects, data source, data collection methods and commonly used mathematical statistics methods. Finally, this paper illustrates validity threats and discusses the future work, opportunity and some open issues of empirical research in the era of big data. © Copyright 2018, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Empirical method; Empirical software engineering; Literature survey,"1422, 1450",,Ruan Jian Xue Bao/Journal of Software,Article,Scopus
980,,Graph Based Neural Network Regression Strategy for Facial Image Super-Resolution,"Huyan K., Fan X., Yu L.-T., Luo Z.-X.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049530703&doi=10.13328%2fj.cnki.jos.005405&partnerID=40&md5=4ce740a3d07a0a5e5fbf8e2618c22b68,10.13328/j.cnki.jos.005405,"Facial image super-resolution (SR) generates a high-resolution (HR) facial image from a low-resolution (LR) facial image. Compared with natural images, facial images are so highly structured that local patches at similar locations across different faces share similar textures. In this paper, a novel graph-based neural network (GNN) regression is proposed to leverage this local structural information for facial image SR. Firstly, the grid representation of an input face image is converted into its corresponding graph representation, and then a shallow neural network is trained for each vertex in the graph in order to regress the SR image. Compared with its grid-based counterpart, the graph representation combines both coordinate affinity and textural similarity. Additionally, the NN weights of a vertex are initialized with those converged ones from its neighbors, resulting fast convergence for training and accurate regression. Experimental comparison with the state-of-the-art SR algorithms including those based on deep convolutional neural networks (DCNN) on two benchmark face sets demonstrate the effectiveness of the proposed method in terms of both qualitative inspections and quantitative metrics. The proposed GNN is not only able to deal with facial SR, but also has the potential to apply to data examples with any irregular topology structure. © Copyright 2018, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Facial image; Graph; Neural network; Regression; Super-resolution,"914, 925",,Ruan Jian Xue Bao/Journal of Software,Article,Scopus
981,,Signed Network Prediction Method Based on the Client-to-Client Distributed Framework,"Zhao K.-K., Zhang J., Zhang L.-F., Li C.-P., Chen H.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049522453&doi=10.13328%2fj.cnki.jos.005447&partnerID=40&md5=fcdb0f437a776f1e19eefb0a55dea7ae,10.13328/j.cnki.jos.005447,"The edges of a network can be divided into positive and negative relationships according to their potential meanings. When the edges of a network are signed with plus or minus signs respectively, a signed network can be formed. Signed networks are widely used in many fields such as sociology, informatics and biology. Hence, the sign prediction problem in signed networks has become one of research hot spots. In large dataset, the scalability of sign prediction algorithm is still a great challenge. There are many related works in the distributed design of signed network prediction methods, however, the computation efficiency is still limited by the fundamental server/client framework. This paper proposes client to client distributed framework (C2CDF). Compared with traditional server/client framework, C2CDF is a completely new client-to-client framework which can release the bandwidth pressure by abandoning the server node and allowing the communications between the client nodes. The Experiments on sign prediction in signed social networks, prediction in click-through rate and prediction in forest type show that C2CDF is a general approach which can not only be applied in sign prediction in signed network but also be used in the other prediction areas. In these three datasets, C2CDF can achieve better performance than FM inferred by the traditional SGD algorithm. C2CDF also achieves a 2.3-3.3x speed-up over the method implemented under the server/client framework while obtains a better accuracy performance than the method compared against. © Copyright 2018, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Client-to-client distributed framework; Server/client framework; Sign prediction; Signed network; Stochastic gradient Langevin dynamics (SGLD),"614, 626",,Ruan Jian Xue Bao/Journal of Software,Article,Scopus
982,,Algorithms of Boltzmann Machines Based on Weight Uncertainty,"Ding S.-F., Zhang J., Shi Z.-Z.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049501709&doi=10.13328%2fj.cnki.jos.005263&partnerID=40&md5=aed6119efc0709f5604f24ff5f8a117d,10.13328/j.cnki.jos.005263,"Based on the restricted Boltzmann machine (RBM), which is a probabilistic graphical model, deep learning models contain deep belief net (DBN) and deep Boltzmann machine (DBM). The overfitting problems commonly exist in neural networks and RBM models. In order to alleviate the overfitting problem, this paper introduces weight random variables to the conventional RBM model and, then builds weight uncertainty deep models based on maximum likelihood estimation. In the experimental section, the paper verifies the effectiveness of the weight uncertainty RBM. In order to improve the image recognition ability, the paper introduces the spike-and-slab RBM (ssRBM) to weight uncertainty RBM and then builds the deep models. The experiments show that the deep models based on weight random variables are effective. © Copyright 2018, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",DBM (deep Boltzmann machine); DBN (deep belief net); RBM (restricted Boltzmann machine); Weight uncertainty,"1131, 1142",,Ruan Jian Xue Bao/Journal of Software,Article,Scopus
983,,Multimodal Emotion Recognition in Multi-Cultural Conditions,"Chen S.-Z., Wang S., Jin Q.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049498930&doi=10.13328%2fj.cnki.jos.005412&partnerID=40&md5=58b10d0ba324ded0f5516c0fa567dded,10.13328/j.cnki.jos.005412,"Automatic emotion recognition is a challenging task with a wide range of applications. This paper addresses the problem of emotion recognition in multi-cultural conditions. Different multi-modal features are extracted from audio and visual modalities, and the emotion recognition performance is compared between hand-crafted features and automatically learned features from deep neural networks. Multimodal feature fusion is also explored to combine different modalities. The CHEAVD Chinese multimodal emotion dataset and AFEW English multimodal emotion dataset are utilized to evaluate the proposed methods. The importance of the culture factor for emotion recognition through cross-culture emotion recognition is demonstrated, and then three different strategies, including selecting corresponding emotion model for different cultures, jointly training with multi-cultural datasets, and embedding features from multi-cultural datasets into the same emotion space, are developed to improve the emotion recognition performance in the multi-cultural environment. The embedding strategy separates the culture influence from original features and can generate more discriminative emotion features, resulting in best performance for acoustic and multimodal emotion recognition. © Copyright 2018, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Acoustic emotion feature; Deepconvolutional neural networks; Emotion recognition; Facial expression feature; Multi-cultural condition; Multimodal fusion,"1060, 1070",,Ruan Jian Xue Bao/Journal of Software,Article,Scopus
984,,Survey of Deep Neural Network Model Compression,"Lei J., Gao X., Song J., Wang X.-L., Song M.-L.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049464636&doi=10.13328%2fj.cnki.jos.005428&partnerID=40&md5=5a79dfdff4a05f188c5d553fb3b3123a,10.13328/j.cnki.jos.005428,"Deep neural networks have continually surpassed traditional methods on a variety of computer vision tasks. Though deep neural networks are very powerful, the large number of weights consumes considerable storage and calculation time, making it hard to deploy on resource-constrained hardware platforms such as mobile system. The number of weights in deep neural networks represents the complexity to an extent, but not all the weights contribute to the performance according to recent researches. Specifically, some weights are redundant and even decrease the performance. This survey offers a systematic summarization of existing research achievements of the domestic and foreign researchers in recent years in the aspects of network pruning, network distillation, and network decomposition. Furthermore, comparisons of compression performance are provided on several public deep neural networks. Finally, a perspective of future work and challenges in this research area are discussed. © Copyright 2018, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Deep neural network; Network compression; Network decomposition; Network distillation; Network pruning,"251, 266",,Ruan Jian Xue Bao/Journal of Software,Review,Scopus
985,,Optimization of Deep Convolutional Neural Network for Large Scale Image Classification,"Bai C., Huang L., Chen J.-N., Pan X., Chen S.-Y.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046848046&doi=10.13328%2fj.cnki.jos.005404&partnerID=40&md5=3d83774b9443563884c1f44041bb7f76,10.13328/j.cnki.jos.005404,"Features from different levels should be extracted from images for more accurate image classification. Deep learning is used more and more in large scale image classification. This paper proposes a deep learning framework based on deep convolutional neural network that can be applied for the large scale image classification. The proposed framework has modified the framework and the internal structure of the classical deep convolutional neural network AlexNet to improve the feature representation ability of the network. Furthermore, this framework has the ability of learning image features and binary hash simultaneously by introducing the hidden layer in the full-connection layer. The proposal has been validated in showing significance improvement through the serial experiments in three commonly used databases. Lastly, different effects of different optimization methods are analyzed. © Copyright 2018, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Activation function; Deep conventional neural network; Hash coding; Image classification; Pooling,"1029, 1038",,Ruan Jian Xue Bao/Journal of Software,Article,Scopus
986,,An ensemble deep learning based approach for red lesion detection in fundus images,"Orlando J.I., Prokofyeva E., del Fresno M., Blaschko M.B.",2018,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031774716&doi=10.1016%2fj.cmpb.2017.10.017&partnerID=40&md5=e7968151bce8d8d26468e95751395743,10.1016/j.cmpb.2017.10.017,"Background and objectives: Diabetic retinopathy (DR) is one of the leading causes of preventable blindness in the world. Its earliest sign are red lesions, a general term that groups both microaneurysms (MAs) and hemorrhages (HEs). In daily clinical practice, these lesions are manually detected by physicians using fundus photographs. However, this task is tedious and time consuming, and requires an intensive effort due to the small size of the lesions and their lack of contrast. Computer-assisted diagnosis of DR based on red lesion detection is being actively explored due to its improvement effects both in clinicians consistency and accuracy. Moreover, it provides comprehensive feedback that is easy to assess by the physicians. Several methods for detecting red lesions have been proposed in the literature, most of them based on characterizing lesion candidates using hand crafted features, and classifying them into true or false positive detections. Deep learning based approaches, by contrast, are scarce in this domain due to the high expense of annotating the lesions manually. Methods: In this paper we propose a novel method for red lesion detection based on combining both deep learned and domain knowledge. Features learned by a convolutional neural network (CNN) are augmented by incorporating hand crafted features. Such ensemble vector of descriptors is used afterwards to identify true lesion candidates using a Random Forest classifier. Results: We empirically observed that combining both sources of information significantly improve results with respect to using each approach separately. Furthermore, our method reported the highest performance on a per-lesion basis on DIARETDB1 and e-ophtha, and for screening and need for referral on MESSIDOR compared to a second human expert. Conclusions: Results highlight the fact that integrating manually engineered approaches with deep learned features is relevant to improve results when the networks are trained from lesion-level annotated data. An open source implementation of our system is publicly available at https://github.com/ignaciorlando/red-lesion-detection. © 2017 Elsevier B.V.",Deep learning; Diabetic retinopathy; Fundus images; Red lesion detection,"115, 127",,Computer Methods and Programs in Biomedicine,Article,Scopus
987,,Video-based Chinese sign language recognition using convolutional neural network,"Yang S., Zhu Q.",2017,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049213888&doi=10.1109%2fICCSN.2017.8230247&partnerID=40&md5=1be1ce4820afea3171a95b44e6f6d0ff,10.1109/ICCSN.2017.8230247,"Convolutional neural network (CNN) has been widely used in computer vision tasks recently and achieved remarkable success. This paper presents a novel video-based recognition approach using CNN for Chinese Sign Language (CSL). The proposed method extracts upper body images directly from videos, and employs a pre-Training convolutional network model to recognize the gesture in the image. The new method simplifies the hand-shape segmentation, and avoid information loss in feature extraction. We evaluate the method on our self-built dataset includes 40 daily vocabularies, and show that the proposed approach has good performance on sign language recognition task, with accuracy reaching to 99%. © 2017 IEEE.",convolutional neural network (CNN); deep learning; sign language recognition; skin-color model,"929, 934",,"2017 9th IEEE International Conference on Communication Software and Networks, ICCSN 2017",Conference Paper,Scopus
988,,An approach for intelligent spectrum allocating for multi-RATs networks,"Li Y., Zhang Z., Li F., Feng Y., Chen L.",2017,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049095737&doi=10.1109%2fICCSN.2017.8230112&partnerID=40&md5=7566206b8ce1ea21fc99ca4a31d49194,10.1109/ICCSN.2017.8230112,"The explosive growth in data traffic is resulting in a spectrum crunch forcing many wireless network operators to look forward refarming their 2G Global System (GSM) and 3G Universal Mobile Telecommunications System (UMTS) spectrum and deploy more spectrally efficient Long Term Evolution (LTE). However, mobile network operators face a challenge when it comes to spectrum refarming because 2G or 3G technologies are still widely used for voice services. And 2G and 3G equipment have long life cycles, e.g. smart meters, and it is expensive to migrate these devices to newer technology since a truck roll will typically be required to the site where a device is deployed. Nevertheless, operators are keen to either force their 2G or 3G customers to migrate so that they can refarm the spectrum or set aside a portion of the 2G or 3G spectrum for continuing operating 2G or 3G and only refarm the rest for LTE. Due to this requirement, future wireless networks should be able to dynamically allocate resources to maintain the quality of service and promote the efficient use of radio spectrum. In this paper, we propose a new intelligent method for allocating spectrum between radio access technologies (RATs) such as GSM, UMTS and LTE. We designed a dynamic spectrum allocation scheme to increase the capacity of high traffic volume RAT without hurt other RATs' performance as well as maximize the spectrum efficiency of the whole network. © 2017 IEEE.",multi-RATs; neural network; prediction model; spectrum allocation,"234, 238",,"2017 9th IEEE International Conference on Communication Software and Networks, ICCSN 2017",Conference Paper,Scopus
989,,The Significant Effects of Data Sampling Approaches on Software Defect Prioritization and Classification,"Bennin K.E., Keung J., Monden A., Phannachitta P., Mensah S.",2017,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042378748&doi=10.1109%2fESEM.2017.50&partnerID=40&md5=fab2f0ea9701469886e54f708fc639c5,10.1109/ESEM.2017.50,"Context: Recent studies have shown that performance of defect prediction models can be affected when data sampling approaches are applied to imbalanced training data for building defect prediction models. However, the magnitude (degree and power) of the effect of these sampling methods on the classification and prioritization performances of defect prediction models is still unknown. Goal: To investigate the statistical and practical significance of using resampled data for constructing defect prediction models. Method: We examine the practical effects of six data sampling methods on performances of five defect prediction models. The prediction performances of the models trained on default datasets (no sampling method) are compared with that of the models trained on resampled datasets (application of sampling methods). To decide whether the performance changes are significant or not, robust statistical tests are performed and effect sizes computed. Twenty releases of ten open source projects extracted from the PROMISE repository are considered and evaluated using the AUC, pd, pf and G-mean performance measures. Results: There are statistical significant differences and practical effects on the classification performance (pd, pf and G-mean) between models trained on resampled datasets and those trained on the default datasets. However, sampling methods have no statistical and practical effects on defect prioritization performance (AUC) with small or no effect values obtained from the models trained on the resampled datasets. Conclusions: Existing sampling methods can properly set the threshold between buggy and clean samples, while they cannot improve the prediction of defect-proneness itself. Sampling methods are highly recommended for defect classification purposes when all faulty modules are to be considered for testing. © 2017 IEEE.",Defect prediction; Empirical software engineering; Imbalanced data; Sampling methods; Statistical significance,"364, 373",,International Symposium on Empirical Software Engineering and Measurement,Conference Paper,Scopus
990,,Training Data Selection for Cross-Project Defection Prediction: Which Approach Is Better?,"Bin Y., Zhou K., Lu H., Zhou Y., Xu B.",2017,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042353624&doi=10.1109%2fESEM.2017.49&partnerID=40&md5=253c5cc47d1383cee535e8f7ce64d6d0,10.1109/ESEM.2017.49,"Background: Many relevancy filters have been proposed to select training data for building cross-project defect prediction (CPDP) models. However, up to now, there is no consensus about which relevancy filter is better for CPDP. Goal: In this paper, we conduct a thorough experiment to compare nine relevancy filters proposed in the recent literature. Method: Based on 33 publicly available data sets, we compare not only the retaining ratio of the original training data and the overlapping degree among the retained data but also the prediction performance of the resulting CPDP models under the ranking and classification scenarios. Results: In terms of retaining ratio and overlapping degree, there are important differences among these filters. According to the defect prediction performance, global filter always stays in the first level. Conclusions: For practitioners, it appears that there is no need to filter source project data, as this may lead to better defect prediction results. © 2017 IEEE.",cross-project; Defect prediction; filter; model,"354, 363",,International Symposium on Empirical Software Engineering and Measurement,Conference Paper,Scopus
991,,Improving Cross-Company Defect Prediction with Data Filtering,"Yu X., Liu J., Peng W., Peng X.",2017,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041186510&doi=10.1142%2fS0218194017400046&partnerID=40&md5=7649d4fce417e1233b7d3f4109ca633f,10.1142/S0218194017400046,"Defect prediction aims to estimate software reliability via learning from historical defect data. Cross-company defect prediction (CCDP) is a practical way that trains a prediction model by exploiting one or multiple projects of a source company and then applies the model to the target company. Unfortunately, larger irrelevant cross-company (CC) data usually makes it difficult to build a CCDP model with high performance. To address such issues, this paper proposes a data filtering method based on agglomerative clustering (DFAC) for CCDP. First, DFAC combines within-company (WC) instances and CC instances and uses agglomerative clustering algorithm to group these instances. Second, DFAC selects subclusters which consist of at least one WC instance, and collects the CC instances in the selected subclusters into a new CC data. Compared with existing data filter methods, the experiment results from 15 public PROMISE datasets show that DFAC increases the pd value, reduces the pf value and achieves higher G-measure value. © 2017 World Scientific Publishing Company.",agglomerative clustering; cross-company defect prediction; data filter; Defect prediction,"1427, 1438",,International Journal of Software Engineering and Knowledge Engineering,Conference Paper,Scopus
992,,Which type of metrics are useful to deal with class imbalance in software defect prediction?,Öztürk M.M.,2017,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022048269&doi=10.1016%2fj.infsof.2017.07.004&partnerID=40&md5=cc4e719dfed54f03d36be730e274533a,10.1016/j.infsof.2017.07.004,"Context There are various ways to cope with class imbalance problem which is one of the main issues of software defect prediction. Sampling algorithms are implemented on both industrial and open-source software defect prediction data sets by practitioners to wipe out imbalanced data points. Sampling algorithms, up-to-date, have been employed either static or process code metrics. Objective In this study, sampling algorithms including Virtual, SMOTE, and HSDD (hybrid sampling for defect data sets) are explored using static code and quality metrics together. Our goal is not only to lead practitioners to decide the type of the metrics in defect prediction but also provide useful information for developers to design less defective software projects. Method We ran sampling experiments with three sampling algorithms on ten data sets (from GitHub). Feature selection is applied on large features of the data sets. Using five classifiers, the performance of the data sets after sampling is compared with initial data sets. Regression analyzes are implemented on quality metrics to find the most influential metrics for detecting defect proneness. Results Regardless of the type of the sampling, prediction performances are similar. Quality metrics surpassed static code metrics with respect to training times and prediction accuracies. Conclusion Using quality metrics yields better prediction results rather than static code metrics in imbalanced data sets. As the count of project cloning increases, the number of defects decreases. Thus, approaches, related to the class imbalance, should be evaluated not only in terms of static code metrics but also for quality metrics. © 2017",Class imbalance; Defect prediction; Process metrics; Static code metrics,"17, 29",,Information and Software Technology,Article,Scopus
993,,Automated change-prone class prediction on unlabeled dataset using unsupervised method,"Yan M., Zhang X., Liu C., Xu L., Yang M., Yang D.",2017,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021967431&doi=10.1016%2fj.infsof.2017.07.003&partnerID=40&md5=e706a1f99a571869a18c49328a99a6cc,10.1016/j.infsof.2017.07.003,"Context Software change-prone class prediction can enhance software decision making activities during software maintenance (e.g., resource allocating). Researchers have proposed many change-prone class prediction approaches and most are effective on labeled datasets (projects with historical labeled data). These approaches usually build a supervised model by learning from historical labeled data. However, a major challenge is that this typical change-prone prediction setting cannot be used for unlabeled datasets (e.g., new projects or projects with limited historical data). Although the cross-project prediction is a solution on unlabeled dataset, it needs the prior labeled data from other projects and how to select the appropriate training project is a difficult task. Objective We aim to build a change-prone class prediction model on unlabeled datasets without the need of prior labeled data. Method We propose to tackle this task by adopting a state-of-art unsupervised method, namely CLAMI. In addition, we propose a novel unsupervised approach CLAMI+ by extending CLAMI. The key idea is to enable change-prone class prediction on unlabeled dataset by learning from itself. Results The experiments among 14 open source projects show that the unsupervised methods achieve comparable results to the typical supervised within-project and cross-project prediction baselines in average and the proposed CLAMI+ slightly improves the CLAMI method in average. Conclusion Our method discovers that it is effective for building change-prone class prediction model by using unsupervised method. It is convenient for practical usage in industry, since it does not need prior labeled data. © 2017 Elsevier B.V.",Change-prone prediction; Software maintenance; Unlabeled dataset; Unsupervised prediction,"1, 16",,Information and Software Technology,Article,Scopus
994,,Data Transformation in Cross-project Defect Prediction,"Zhang F., Keivanloo I., Zou Y.",2017,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017427285&doi=10.1007%2fs10664-017-9516-2&partnerID=40&md5=b1f7d8ff3de156a349f51c29c1c3d21f,10.1007/s10664-017-9516-2,"Software metrics rarely follow a normal distribution. Therefore, software metrics are usually transformed prior to building a defect prediction model. To the best of our knowledge, the impact that the transformation has on cross-project defect prediction models has not been thoroughly explored. A cross-project model is built from one project and applied on another project. In this study, we investigate if cross-project defect prediction is affected by applying different transformations (i.e., log and rank transformations, as well as the Box-Cox transformation). The Box-Cox transformation subsumes log and other power transformations (e.g., square root), but has not been studied in the defect prediction literature. We propose an approach, namely Multiple Transformations (MT), to utilize multiple transformations for cross-project defect prediction. We further propose an enhanced approach MT+ to use the parameter of the Box-Cox transformation to determine the most appropriate training project for each target project. Our experiments are conducted upon three publicly available data sets (i.e., AEEEM, ReLink, and PROMISE). Comparing to the random forest model built solely using the log transformation, our MT+ approach improves the F-measure by 7, 59 and 43% for the three data sets, respectively. As a summary, our major contributions are three-fold: 1) conduct an empirical study on the impact that data transformation has on cross-project defect prediction models; 2) propose an approach to utilize the various information retained by applying different transformation methods; and 3) propose an unsupervised approach to select the most appropriate training project for each target project. © 2017, Springer Science+Business Media New York.",Box-cox; Data transformation; Defect prediction; Software metrics,"3186, 3218",,Empirical Software Engineering,Article,Scopus
995,,ASE 2017 - Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering,[No author name available],2017,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041631420&partnerID=40&md5=f2ddd5ff6c725ab070c1197c99cf1e46,,The proceedings contain 114 papers. The topics discussed include: crowd intelligence enhances automated mobile testing; EHBDroid: beyond GUI testing for Android applications; sketch-guided GUI test generation for mobile applications; the impact of continuous integration on other software development practices: a large-scale empirical study; perceived language complexity in GitHub issue discussions and their effect on issue resolution; are developers aware of the architectural impact of their changes?; automatically generating commit messages from Diffs using neural machine translation; improving missing issue-commit link recovery using positive and unlabeled data; iCoq: regression proof selection for large-scale verification projects; more effective interpolations in software model checking; proof-based coverage metrics for formal verification; static detection of asymptotic resource side-channel vulnerabilities in web applications; PAD: programming third-party web advertisement censorship; detecting information flow by mutating input data; SimplyDroid: efficient event sequence simplification for android application; towards robust instruction-level trace alignment of binary code; mining implicit design templates for actionable code reuse; improved query reformulation for concept location using CodeRank and document structures; understanding feature requests by leveraging fuzzy method and linguistic analysis; software performance self-adaptation through efficient model predictive control; and transfer learning for performance modeling of configurable systems: an exploratory analysis.,,,1046.0,ASE 2017 - Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering,Conference Review,Scopus
996,,Learning effective changes for software projects,Krishna R.,2017,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041438445&doi=10.1109%2fASE.2017.8115719&partnerID=40&md5=323394afb6be4d2b3cbfd620b12f53d1,10.1109/ASE.2017.8115719,"The primary motivation of much of software analytics is decision making. How to make these decisions? Should one make decisions based on lessons that arise from within a particular project? Or should one generate these decisions from across multiple projects? This work is an attempt to answer these questions. Our work was motivated by a realization that much of the current generation software analytics tools focus primarily on prediction. Indeed prediction is a useful task, but it is usually followed by 'planning' about what actions need to be taken. This research seeks to address the planning task by seeking methods that support actionable analytics by offering clear guidance on what to do. Specifically, we propose XTREE and BELLTREE algorithms for generating a set of actionable plans within and across projects. Each of these plans, if followed will improve the quality of the software project. © 2017 IEEE.",bellwethers; defect prediction; Planning,"1002, 1005",,ASE 2017 - Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering,Conference Paper,Scopus
997,,Transfer learning for performance modeling of configurable systems: An exploratory analysis,"Jamshidi P., Siegmund N., Velez M., Kastner C., Patel A., Agarwal Y.",2017,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041373067&doi=10.1109%2fASE.2017.8115661&partnerID=40&md5=b192f5e2fe628c444c8ebdf92d387499,10.1109/ASE.2017.8115661,"Modern software systems provide many configuration options which significantly influence their non-functional properties. To understand and predict the effect of configuration options, several sampling and learning strategies have been proposed, albeit often with significant cost to cover the highly dimensional configuration space. Recently, transfer learning has been applied to reduce the effort of constructing performance models by transferring knowledge about performance behavior across environments. While this line of research is promising to learn more accurate models at a lower cost, it is unclear why and when transfer learning works for performance modeling. To shed light on when it is beneficial to apply transfer learning, we conducted an empirical study on four popular software systems, varying software configurations and environmental conditions, such as hardware, workload, and software versions, to identify the key knowledge pieces that can be exploited for transfer learning. Our results show that in small environmental changes (e.g., homogeneous workload change), by applying a linear transformation to the performance model, we can understand the performance behavior of the target environment, while for severe environmental changes (e.g., drastic workload change) we can transfer only knowledge that makes sampling more efficient, e.g., by reducing the dimensionality of the configuration space. © 2017 IEEE.",Performance analysis; transfer learning,"497, 508",,ASE 2017 - Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering,Conference Paper,Scopus
998,,An initial investigation of protocol customization,"Hong D.K., Chen Q.A., Morley Mao Z.",2017,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037126960&doi=10.1145%2f3141235.3141236&partnerID=40&md5=2132ef0a9cf2603ae03771c5de7064b2,10.1145/3141235.3141236,"Attacks exploiting design or implementation! aws of particular features in popular protocols are becoming prevalent and have led to severe security impacts on a majority of software systems. Protocol customization as a general approach to specialize a standard protocol holds signi""cant promise in reducing such attack surfaces in common protocols. In this work, we perform an initial investigation of applying protocol customization practices to reduce the attack surface of standard protocols. Our characterization study on 20 medium or high-impact common vulnerability exposures (CVEs) published in recent years indicates that some forms of customization have been supported in existing protocol software, but were implemented with huge manual e#ort and in an ad-hoc manner. More systematic and automated ways of protocol customization are awaited to generalize common customization practices across protocols. To work towards this goal, we identify key research challenges for the support of systematic and su- ciently automated protocol customization through real-world case study on popular protocol software, and propose an access control framework as a principled solution to unify existing protocol customization practices. We also present a preliminary design of a protocol customization system based on this design principle. Preliminary evaluation results demonstrate that our proposed system supports common customization practices for a majority of real-world protocol vulnerabilities in a systematic way. © 2017 Association for Computing Machinery.",,"57, 64",,"FEAST 2017 - Proceedings of the 2017 Workshop on Forming an Ecosystem Around Software Transformation, co-located with CCS 2017",Conference Paper,Scopus
999,,The utility challenge of privacy-preserving data-sharing in cross-company defect prediction An empirical study of the CLIFF&MORPH algorithm,"Fan Y., Lv C., Zhang X., Zhou G., Zhou Y.",2017,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040597683&doi=10.1109%2fICSME.2017.57&partnerID=40&md5=b4e0ca81ddf1a8d12e02e36eb9a9fbbb,10.1109/ICSME.2017.57,"In practice, the data owners of source projects may need to share data without disclosing sensitive information. Therefore, privacy-preserving data-sharing becomes an important topic in cross-company defect prediction (CCDP). In this context, the challenge is how to achieve a high privacy-preserving level while ensuring the utility of the shared privatized data for CCDP. CLIFF&MORPH is a recently proposed state-of-the-art privacy-preserving data-sharing algorithm for CCDP. It has been reported that the CLIFF&MORPH CCDP model produces a promising defect prediction performance. However, we find that ManualDown, a simple (unsupervised) module size model, built on the target projects has a comparable or even better defect prediction performance. Since ManualDown does not require any source project data to build the model, it is free of the privacy-preserving data-sharing challenges for CCDP. This means that, for practitioners, the motivation of applying privacy-preserving data-sharing algorithms to CCDP could not be well justified if the utility challenge is not addressed. We analyze the implications of our findings and outline the directions for future research. In particular, we strongly suggest that future studies at least use ManualDown as a baseline model for comparison to help develop practical privacy-preserving datasharing algorithms for CCDP. © 2017 IEEE.",Cross-project; Defect prediction; Model; Privacy,"80, 90",,"Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",Conference Paper,Scopus
1000,,Heterogeneous defect prediction through multiple kernel learning and ensemble learning,"Li Z., Jing X.-Y., Zhu X., Zhang H.",2017,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040542582&doi=10.1109%2fICSME.2017.19&partnerID=40&md5=f3658dd73e0ed8d0c8e4b331db4b16e6,10.1109/ICSME.2017.19,"Heterogeneous defect prediction (HDP) aims to predict defect-prone software modules in one project using heterogeneous data collected from other projects. Recently, several HDP methods have been proposed. However, these methods do not sufficiently incorporate the two characteristics of the defect prediction data: (1) data could be linearly inseparable, and (2) data could be highly imbalanced. These two data characteristics make it challenging to build an effective HDP model. In this paper, we propose a novel Ensemble Multiple Kernel Correlation Alignment (EMKCA) based approach to HDP, which takes into consideration the two characteristics of the defect prediction data. Specifically, we first map the source and target project data into high dimensional kernel space through multiple kernel leaning, where the defective and non-defective modules can be better separated. Then, we design a kernel correlation alignment method to make the data distribution of the source and target projects similar in the kernel space. Finally, we integrate multiple kernel classifiers with ensemble learning to relieve the influence caused by class imbalance problem, which can improve the accuracy of the defect prediction model. Consequently, EMKCA owns the advantages of both multiple kernel learning and ensemble learning. Extensive experiments on 30 public projects show that EMKCA outperforms the related competing methods. © 2017 IEEE.",Class imbalance; Ensemble learning; Heterogeneous defect prediction; Kernel correlation alignment; Linearly inseparable; Multiple kernel learning,"91, 102",,"Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",Conference Paper,Scopus
1001,,Online Transfer Learning from Multiple Sources Based on Local Classification Accuracy,"Tang S.-Q., Wen Y.-M., Qin Y.-X.",2017,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037811280&doi=10.13328%2fj.cnki.jos.005352&partnerID=40&md5=fafa9113afa01d6a02b9a9b5b668815a,10.13328/j.cnki.jos.005352,"In recent years, transfer learning has gained more and more attention. However, most of the existing online transfer learning methods transfer knowledge from a single source, and it is hard to make effective transfer learning when the similarity between source domain and target domain is low. To solve this problem, this paper proposes a multi-source online transfer learning method, LC-MSOTL, based on local classification accuracy. LC-MSOTL stores multiple classifiers each trained on a different source, computes the distance between the new arrived sample and its k-nearest neighbor samples in the target domain as well as the local classification accuracies of each source domain classifier on the k-nearest neighbor samples, selects the classifier with the highest local classification accuracy from source domain classifiers and combines it with the target domain classifier, so as to realize the knowledge transfer from multi-source domains to a target domain. Experiments on artificial datasets and real datasets illustrates that LC-MSOTL can effectively transfer knowledge selectively from multi-source domains, and can get higher classification accuracy compared with the single source online transfer learning algorithm OTL. © Copyright 2017, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Local classification accuracy; Multi sources online transfer; Online learning; Transfer learning,"2940, 2960",,Ruan Jian Xue Bao/Journal of Software,Article,Scopus
1002,,Research Progress on Cognitive-Oriented Multi-Source Data Learning Theory and Algorithm,"Yang L., Yu J., Liu Y., Zhan D.-C.",2017,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037810985&doi=10.13328%2fj.cnki.jos.005348&partnerID=40&md5=6d5a01305af786289e02bae45becce6b,10.13328/j.cnki.jos.005348,"In the age of big data, learning from multi-source data plays an important role in many real applications. To date, plenty of multi-source data learning algorithms have been proposed, however, they pay little attention to the fundamental theoretic laws. Meanwhile, it is hard for the classical machine learning theories to govern all learning systems, and to further provide a theoretical support for multi-source learning algorithms. From the perspective of knowledge acquisition through learning, a survey is given on the research progress of three key problems: the human cognitive mechanism, three classical machine learning theories (such as computational learning theory, statistical learning theory, and probabilistic graphical model), and the design of multi-source learning algorithms. Future theoretical research issues of multi-source data learning also presented and investigated. © Copyright 2017, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Coginitive psychology; Featurespace; Pattern classification; Statistical learning theory,"2971, 2991",,Ruan Jian Xue Bao/Journal of Software,Review,Scopus
1003,,Transductive Discriminative Dictionary Learning Approach for Zero-Shot Classification,"Ji Z., Sun T., Yu Y.-L.",2017,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037810709&doi=10.13328%2fj.cnki.jos.005338&partnerID=40&md5=4dd13d4c2528cb45fd2f7f0964ecdc71,10.13328/j.cnki.jos.005338,"Zero-Shot classification aims at recognizing instances from unseen categories that have no training instances in the training stage. To address this task, most existing approaches resort to class semantic information to transfer knowledge from the seen classes to the unseen ones. In this paper, a transductive dictionary learning approach is proposed to facilitate the task in two steps. A discriminative dictionary learning model is first proposed for constructing the relations between the visual modality and the class semantic modality with the labeled seen instances. Then a transductive modified model is used to alleviate the domain shift issue caused by the disjointness between the seen classes and the unseen classes. Experimental results on three benchmark datasets (AwA, CUB and SUN) demonstrate the effectiveness and superiority of the proposed approach. © Copyright 2017, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Dictionary learning; Image classification; Transductive learning; Zero-shot classification,"2961, 2970",,Ruan Jian Xue Bao/Journal of Software,Article,Scopus
1004,,Comments on ScottKnottESD in response to an empirical comparison of model validation techniques for defect prediction models,Herbold S.,2017,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029188984&doi=10.1109%2fTSE.2017.2748129&partnerID=40&md5=89ed09f31178c53fa459e291a0672366,10.1109/TSE.2017.2748129,"In this article, we discuss the ScottKnottESD test, which was proposed in a recent paper 'An Empirical Comparison of Model Validation Techniques for Defect Prediction Models' that was published in this journal. We discuss the implications and the empirical impact of the proposed normality correction of ScottKnottESD and come to the conclusion that this correction does not necessarily lead to the fulfillment of the assumptions of the original Scott-Knott test and may cause problems with the statistical analysis. © 1976-2012 IEEE.",log transformation; Scott-knott test; statistics,"1091, 1094",,IEEE Transactions on Software Engineering,Article,Scopus
1005,,Research patterns and trends in software effort estimation,"Sehra S.K., Brar Y.S., Kaur N., Sehra S.S.",2017,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020853907&doi=10.1016%2fj.infsof.2017.06.002&partnerID=40&md5=d7bb2515ad81a0037a63628ac8372db5,10.1016/j.infsof.2017.06.002,"Context Software effort estimation (SEE) is most crucial activity in the field of software engineering. Vast research has been conducted in SEE resulting into a tremendous increase in literature. Thus it is of utmost importance to identify the core research areas and trends in SEE which may lead the researchers to understand and discern the research patterns in large literature dataset. Objective To identify unobserved research patterns through natural language processing from a large set of research articles on SEE published during the period 1996 to 2016. Method A generative statistical method, called Latent Dirichlet Allocation (LDA), applied on a literature dataset of 1178 articles published on SEE. Results As many as twelve core research areas and sixty research trends have been revealed; and the identified research trends have been semantically mapped to associate core research areas. Conclusions This study summarises the research trends in SEE based upon a corpus of 1178 articles. The patterns and trends identified through this research can help in finding the potential research areas. © 2017 Elsevier B.V.",Latent Dirichlet allocation; Research trends; Software effort estimation,"1, 21",,Information and Software Technology,Review,Scopus
1006,,A feature matching and transfer approach for cross-company defect prediction,"Yu Q., Jiang S., Zhang Y.",2017,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022069038&doi=10.1016%2fj.jss.2017.06.070&partnerID=40&md5=f0bc3ce293daa6a5eef02ac9a3dd5837,10.1016/j.jss.2017.06.070,"Software defect prediction has drawn much attention of researchers in software engineering. Traditional defect prediction methods aim to build the prediction model based on historical data. For a new project or a project with limited historical data, we cannot build a good prediction model. Therefore, researchers have proposed the cross-project defect prediction (CPDP) and cross-company defect prediction (CCDP) methods to share the historical data among different projects. However, the features of cross-company datasets are often heterogeneous, which may affect the feasibility of CCDP. To address the heterogeneous features of CCDP, this paper presents a feature matching and transfer (FMT) approach. First, we conduct feature selection for the source project and get the distribution curves of selected features. Similarly, we also get the distribution curves of all features in the target project. Second, according to the ‘distance’ of different distribution curves, we design a feature matching algorithm to convert the heterogeneous features into the matched features. Finally, we can achieve feature transfer from the source project to the target project. All experiments are conducted on 16 datasets from NASA and PROMISE, and the results show that FMT is effective for CCDP. © 2017 Elsevier Inc.",Feature matching; Feature transfer; Heterogeneous features; Software defect prediction,"366, 378",,Journal of Systems and Software,Article,Scopus
1007,,Negative results for software effort estimation,"Menzies T., Yang Y., Mathew G., Boehm B., Hihn J.",2017,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84996477418&doi=10.1007%2fs10664-016-9472-2&partnerID=40&md5=a5c2261b69cb80c79137913361b0b70c,10.1007/s10664-016-9472-2,"More than half the literature on software effort estimation (SEE) focuses on comparisons of new estimation methods. Surprisingly, there are no studies comparing state of the art latest methods with decades-old approaches. Accordingly, this paper takes five steps to check if new SEE methods generated better estimates than older methods. Firstly, collect effort estimation methods ranging from “classical” COCOMO (parametric estimation over a pre-determined set of attributes) to “modern” (reasoning via analogy using spectral-based clustering plus instance and feature selection, and a recent “baseline method” proposed in ACM Transactions on Software Engineering). Secondly, catalog the list of objections that lead to the development of post-COCOMO estimation methods. Thirdly, characterize each of those objections as a comparison between newer and older estimation methods. Fourthly, using four COCOMO-style data sets (from 1991, 2000, 2005, 2010) and run those comparisons experiments. Fifthly, compare the performance of the different estimators using a Scott-Knott procedure using (i) the A12 effect size to rule out “small” differences and (ii) a 99 % confident bootstrap procedure to check for statistically different groupings of treatments. The major negative result of this paper is that for the COCOMO data sets, nothing we studied did any better than Boehms original procedure. Hence, we conclude that when COCOMO-style attributes are available, we strongly recommend (i) using that data and (ii) use COCOMO to generate predictions. We say this since the experiments of this paper show that, at least for effort estimation, how data is collected is more important than what learner is applied to that data. © 2016, Springer Science+Business Media New York.",A12; Bootstrap sampling; CART; Clustering; COCOMO; Effect size; Effort estimation; Feature selection; Nearest neighbor; Prototype generation,"2658, 2683",,Empirical Software Engineering,Article,Scopus
1008,,Wrist Player: A Smartwatch Gesture Controller for Smart TVs,"Luna M.M., Carvalho T.P., Soares F.A.A.M.N., Nascimento H.A.D., Costa R.M.",2017,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032864598&doi=10.1109%2fCOMPSAC.2017.266&partnerID=40&md5=fb267da794efa5e615b2b40cd7ee603e,10.1109/COMPSAC.2017.266,"Emerging technology on mobile and wearable market, smartwatches have embedded movement sensors whose potential is yet to be fully explored. This paper proposes an interaction method with smart TVs via gestures performed by person's wrist using a smartwatch. Detailed architecture and implementation for a complete prototype, named Wrist Player, is presented. A user study is also conducted, in order to evaluate the prototype performance and the user's interest on the proposal. Results show that the method works very well, with participants reporting having a good experience with the prototype. We present our insights on the concept, challenges faced in our research and ideas for future studies. © 2017 IEEE.",Gesture Recognition; Human Computer Interaction; Internet of Things; Movement Sensors; smart TV; smartwatch,"336, 341",,Proceedings - International Computer Software and Applications Conference,Conference Paper,Scopus
1009,,Precision-enhanced image attribute prediction model,"Hu C., Miao J., Su Z., Shi X., Chen Q., Luo X.",2017,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032364781&doi=10.1109%2fTrustcom%2fBigDataSE%2fICESS.2017.324&partnerID=40&md5=1000bffc2c78b089251a1227ba508497,10.1109/Trustcom/BigDataSE/ICESS.2017.324,"High-precision attribute prediction is a challenging issue due to the complex object and scene variations. Targeting on enhancing attribute prediction precision, we propose an Enhanced Attribute Prediction-Latent Dirichlet Allocation (EAP-LDA) model to address this issue. EAP-LDA model enhances the attribute prediction precision in two steps: classification adaptation and prediction enhancement. In classification adaptation, we transfer image low-level features to mid-level features (attributes) by the SVM classifiers, which are trained using the low-level features extracted from images. In prediction enhancement, we first exploit its advantages in extracting and analyzing the topic information between image samples and attributes by the LDA topic model. We then use a strategy to search the nearest neighbor image collection from test datasets by KNN. Finally, we evaluate the accuracy onHAT datasets and demonstrate significant improvement over the baseline algorithm. © 2017 IEEE.",Attribute Prediction; Classification Adaptation; Prediction Enhancement,"866, 872",,"Proceedings - 16th IEEE International Conference on Trust, Security and Privacy in Computing and Communications, 11th IEEE International Conference on Big Data Science and Engineering and 14th IEEE International Conference on Embedded Software and Systems, Trustcom/BigDataSE/ICESS 2017",Conference Paper,Scopus
1010,,FeSCH: A Feature Selection Method using Clusters of Hybrid-data for Cross-Project Defect Prediction,"Ni C., Liu W., Gu Q., Chen X., Chen D.",2017,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031909129&doi=10.1109%2fCOMPSAC.2017.127&partnerID=40&md5=206c06c460278d8af25031380d63a60e,10.1109/COMPSAC.2017.127,"Cross project defect prediction (CPDP) is a challenging task since the predictor built on the source projects can hardly generalize well to the target project. Previous studies have shown that both feature mapping and feature selection can alleviate the differences between the source and target projects. In this paper, we propose a novel method FeSCH (Feature Selection using Clusters of Hybrid-data). In particular it includes two phases. The first is the feature clustering phase, which uses a density-based clustering method DPC to group highly co-related features into clusters. The second is the feature selection phase, which selects beneficial features from each cluster. We design three ranking strategies to choose appropriate features. During the empirical studies, we design experiments based on real-world software projects, and evaluate the prediction performance of FeSCH by analyzing the influence of ranking strategies. The experimental results show that FeSCH can outperform three baseline methods (i.e., WPDP, ALL, and TCA+) in most cases, and its performance is independent of the used classifiers. © 2017 IEEE.",Cross-project Defect Prediction; Feature Clustering; Feature Selection; Software Defect Prediction,"51, 56",,Proceedings - International Computer Software and Applications Conference,Conference Paper,Scopus
1011,,Which models of the past are relevant to the present? A software effort estimation approach to exploiting useful past models,"Minku L.L., Yao X.",2017,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007505021&doi=10.1007%2fs10515-016-0209-7&partnerID=40&md5=356f9b84a5fd55b6f6fd63b9a5fbc9bf,10.1007/s10515-016-0209-7,"Software Effort Estimation (SEE) models can be used for decision-support by software managers to determine the effort required to develop a software project. They are created based on data describing projects completed in the past. Such data could include past projects from within the company that we are interested in (WC projects) and/or from other companies (cross-company, i.e., CC projects). In particular, the use of CC data has been investigated in an attempt to overcome limitations caused by the typically small size of WC datasets. However, software companies operate in non-stationary environments, where changes may affect the typical effort required to develop software projects. Our previous work showed that both WC and CC models of the past can become more or less useful over time, i.e., they can sometimes be helpful and sometimes misleading. So, how can we know if and when a model created based on past data represents well the current projects being estimated? We propose an approach called Dynamic Cross-company Learning (DCL) to dynamically identify which WC or CC past models are most useful for making predictions to a given company at the present. DCL automatically emphasizes the predictions given by these models in order to improve predictive performance. Our experiments comparing DCL against existing WC and CC approaches show that DCL is successful in improving SEE by emphasizing the most useful past models. A thorough analysis of DCL’s behaviour is provided, strengthening its external validity. © 2016, The Author(s).",Cross-company learning; Machine learning; Model-based software effort estimation; Non-stationary environments; Online learning,"499, 542",,Automated Software Engineering,Article,Scopus
1012,,Transferring context-dependent test inputs,"Reichstaller A., Knapp A.",2017,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029449160&doi=10.1109%2fQRS.2017.16&partnerID=40&md5=b3f0778e1040b3992c4f52a35898be94,10.1109/QRS.2017.16,"We consider the question of how to treat existing, context-based test inputs when contextual conditions change. Simply ignoring the voided inputs reduces confidence in the correctness of the system under test (SuT). Instead, we suggest to adjust the parameters of those inputs to the new conditions in a way that retains their original intention. This often comprises behavioral assumptions, e.g., because of coverage or risk considerations. Transferred test inputs should consequently trigger similar behavior of the SuT within the new environment as the original ones did in the old. We formalize this claim by a distance function on test inputs which compares the expected reactions of the SuT. The more similar the responses, the closer the test inputs. The proposed metric can thus be used for guiding test input transfer. In addition to a recursive definition, we present an algorithm that utilizes neural models to estimate the metric by simply observing a given simulation which sketches the intended behavior of the SuT. As this approach seems to specifically match the prerequisites when testing proactive systems, motivation and first experiments consider a simplified instance of those: an exemplary smart vacuum system. © 2017 IEEE.",Autonomous Systems; Distance Metric; Machine Learning; Proactive Systems; Software Testing; Test Transfer,"65, 72",,"Proceedings - 2017 IEEE International Conference on Software Quality, Reliability and Security, QRS 2017",Conference Paper,Scopus
1013,,Cross-project defect prediction using a credibility theory based naive bayes classifier,"Poon W.N., Bennin K.E., Huang J., Phannachitta P., Keung J.W.",2017,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029431103&doi=10.1109%2fQRS.2017.53&partnerID=40&md5=ad18f8dd468d85731b2431c73457faf5,10.1109/QRS.2017.53,"Several defect prediction models proposed are effective when historical datasets are available. Defect prediction becomes difficult when no historical data exist. Cross-project defect prediction (CPDP), which uses projects from other sources/companies to predict the defects in the target projects proposed in recent studies has shown promising results. However, the performance of most CPDP approaches are still beyond satisfactory mainly due to distribution mismatch between the source and target projects. In this study, a credibility theory based Naïve Bayes (CNB) classifier is proposed to establish a novel reweighting mechanism between the source projects and target projects so that the source data could simultaneously adapt to the target data distribution and retain its own pattern. Our experimental results show that the feasibility of the novel algorithm design and demonstrate the significant improvement in terms of the performance metrics considered achieved by CNB over other CPDP approaches. © 2017 IEEE.",Credibility theory; Cross-project defect prediction; Naive Bayes classifier; Quality assurance; Software engineering; Transfer learning,"434, 441",,"Proceedings - 2017 IEEE International Conference on Software Quality, Reliability and Security, QRS 2017",Conference Paper,Scopus
1014,,Investigating the significance of bellwether effect to improve software effort estimation,"Mensah S., Keung J., MacDonell S.G., Bosu M.F., Bennin K.E.",2017,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029421585&doi=10.1109%2fQRS.2017.44&partnerID=40&md5=e93c5a62dc7ea453ad2d22c291e635aa,10.1109/QRS.2017.44,"Bellwether effect refers to the existence of exemplary projects (called the Bellwether) within a historical dataset to be used for improved prediction performance. Recent studies have shown an implicit assumption of using recently completed projects (referred to as moving window) for improved prediction accuracy. In this paper, we investigate the Bellwether effect on software effort estimation accuracy using moving windows. The existence of the Bellwether was empirically proven based on six postulations. We apply statistical stratification and Markov chain methodology to select the Bellwether moving window. The resulting Bellwether moving window is used to predict the software effort of a new project. Empirical results show that Bellwether effect exist in chronological datasets with a set of exemplary and recently completed projects representing the Bellwether moving window. Result from this study has shown that the use of Bellwether moving window with the Gaussian weighting function significantly improve the prediction accuracy. © 2017 IEEE.",Bellwether Effect; Bellwether moving window; Chronological dataset; Markov chains,"340, 351",,"Proceedings - 2017 IEEE International Conference on Software Quality, Reliability and Security, QRS 2017",Conference Paper,Scopus
1015,,An empirical analysis of three-stage data-preprocessing for analogy-based software effort estimation on the ISBSG data,"Huang J., Li Y.-F., Keung J.W., Yu Y.T., Chan W.K.",2017,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029416717&doi=10.1109%2fQRS.2017.54&partnerID=40&md5=4804ed57e0cc70cf95dc052c4a0d5c7c,10.1109/QRS.2017.54,"Analogy-based software effort estimation is a method to estimate the project cost of an unseen project based on analogies against previous projects sharing selected features. The validity of the selected features depends on many factors, and one of most crucial factors is the effectiveness of the datapreprocessing techniques applied to the datasets of the previous projects. In this paper, we report the first controlled experiment that studies the class of three-stage data-preprocessing techniques with stages of missing data imputation, data normalization, and feature selection for analogy-based effort estimation. We conducted our investigation on the ISBSG data. The experimental results show that three-stage data-preprocessing techniques have significant impacts on the resultant effort estimation accuracy. The results also indicate that the combined use of Z-Score normalization, kNN imputation and mutual information based feature weighting can be an effective choice for analogy-based effort estimation. © 2017 IEEE.",Analogy-based effort estimation; Data normalization; Data-preprocessing; Feature selection; Missing data imputation,"442, 449",,"Proceedings - 2017 IEEE International Conference on Software Quality, Reliability and Security, QRS 2017",Conference Paper,Scopus
1016,,Combinatorial Methods of Feature Selection for Cell Image Classification,"Vilkomir S., Wang J., Thai N.L., Ding J.",2017,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034416393&doi=10.1109%2fQRS-C.2017.18&partnerID=40&md5=18b3d38b4370ea95949fa07e48382b91,10.1109/QRS-C.2017.18,"Feature selection is a major task in machine learning for selecting the most impactful features that will result in better accuracy and performance. Experiment-based feature selection is one of the main approaches to this task. However, this approach is not feasible in many cases due to the large number of experiments needed in order to effectively select the optimized feature set. In this paper, we propose the use of an experimentbased feature selection guided by combinatorial methods such as t-way coverage. This approach can significantly reduce the required number of experiments in a study while selecting the best feature set to achieve a high level of accuracy.We conducted a case study of the feature selection process for a Support Vector Machine (SVM) classification of biomedical images for cell typing. Three thousand labeled images were used in the experimental SVM classification, and 32 features were evaluated for each image. We considered feature sets of different sizes ranging from 6 to 32, and, for each size, we generated t- way combinations of features for t from 2 to 6. The accuracy of each combinatorial combination was evaluated, which allowed us to select the optimal set of features for use in the classification process. The experiment results show that the proposed approach is an effective way to conduct feature selection and that it can be adapted easily for feature selection in other machine learning algorithms. © 2017 IEEE.",combinatorial method; diffraction image; feature selection; GLCM; machine learning; support vector machine,"55, 60",,"Proceedings - 2017 IEEE International Conference on Software Quality, Reliability and Security Companion, QRS-C 2017",Conference Paper,Scopus
1017,,Less is more: Minimizing code reorganization using XTREE,"Krishna R., Menzies T., Layman L.",2017,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016444613&doi=10.1016%2fj.infsof.2017.03.012&partnerID=40&md5=8b759acaa01c5136355d900e55157249,10.1016/j.infsof.2017.03.012,"Context: Developers use bad code smells to guide code reorganization. Yet developers, textbooks, tools, and researchers disagree on which bad smells are important. How can we offer reliable advice to developers about which bad smells to fix? Objective: To evaluate the likelihood that a code reorganization to address bad code smells will yield improvement in the defect-proneness of the code. Method: We introduce XTREE, a framework that analyzes a historical log of defects seen previously in the code and generates a set of useful code changes. Any bad smell that requires changes outside of that set can be deprioritized (since there is no historical evidence that the bad smell causes any problems). Evaluation: We evaluate XTREE's recommendations for bad smell improvement against recommendations from previous work (Shatnawi, Alves, and Borges) using multiple data sets of code metrics and defect counts. Results: Code modules that are changed in response to XTREE's recommendations contain significantly fewer defects than recommendations from previous studies. Further, XTREE endorses changes to very few code metrics, so XTREE requires programmers to do less work. Further, XTREE's recommendations are more responsive to the particulars of different data sets. Finally XTREE's recommendations may be generalized to identify the most crucial factors affecting multiple datasets (see the last figure in paper). Conclusion: Before undertaking a code reorganization based on a bad smell report, use a framework like XTREE to check and ignore any such operations that are useless; i.e. ones which lack evidence in the historical record that it is useful to make that change. Note that this use case applies to both manual code reorganizations proposed by developers as well as those conducted by automatic methods. © 2017 Elsevier B.V.",Bad smells; Decision trees; Performance prediction,"53, 66",,Information and Software Technology,Article,Scopus
1018,,Global vs. local models for cross-project defect prediction: A replication study,"Herbold S., Trautsch A., Grabowski J.",2017,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992223488&doi=10.1007%2fs10664-016-9468-y&partnerID=40&md5=5c27ff1dfe9467e8ef9c0eea69e2d9ff,10.1007/s10664-016-9468-y,"Although researchers invested significant effort, the performance of defect prediction in a cross-project setting, i.e., with data that does not come from the same project, is still unsatisfactory. A recent proposal for the improvement of defect prediction is using local models. With local models, the available data is first clustered into homogeneous regions and afterwards separate classifiers are trained for each homogeneous region. Since the main problem of cross-project defect prediction is data heterogeneity, the idea of local models is promising. Therefore, we perform a conceptual replication of the previous studies on local models with a focus on cross-project defect prediction. In a large case study, we evaluate the performance of local models and investigate their advantages and drawbacks for cross-project predictions. To this aim, we also compare the performance with a global model and a transfer learning technique designed for cross-project defect predictions. Our findings show that local models make only a minor difference in comparison to global models and transfer learning for cross-project defect prediction. While these results are negative, they provide valuable knowledge about the limitations of local models and increase the validity of previously gained research results. © 2016, Springer Science+Business Media New York.",Cross-project; Defect prediction; Local models,"1866, 1902",,Empirical Software Engineering,Article,Scopus
1019,,Semantically Enhanced Software Traceability Using Deep Learning Techniques,"Guo J., Cheng J., Cleland-Huang J.",2017,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027712504&doi=10.1109%2fICSE.2017.9&partnerID=40&md5=4e6adac484ff619efc3ec042383c62cb,10.1109/ICSE.2017.9,"In most safety-critical domains the need for traceability is prescribed by certifying bodies. Trace links are generally created among requirements, design, source code, test cases and other artifacts, however, creating such links manually is time consuming and error prone. Automated solutions use information retrieval and machine learning techniques to generate trace links, however, current techniques fail to understand semantics of the software artifacts or to integrate domain knowledge into the tracing process and therefore tend to deliver imprecise and inaccurate results. In this paper, we present a solution that uses deep learning to incorporate requirements artifact semantics and domain knowledge into the tracing solution. We propose a tracing network architecture that utilizes Word Embedding and Recurrent Neural Network (RNN) models to generate trace links. Word embedding learns word vectors that represent knowledge of the domain corpus and RNN uses these word vectors to learn the sentence semantics of requirements artifacts. We trained 360 different configurations of the tracing network using existing trace links in the Positive Train Control domain and identified the Bidirectional Gated Recurrent Unit (BI-GRU) as the best model for the tracing task. BI-GRU significantly out-performed state-of-The-Art tracing methods including the Vector Space Model and Latent Semantic Indexing. © 2017 IEEE.",Deep Learning; Recurrent Neural Network; Semantic Representation; Traceability,"3, 14",,"Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",Conference Paper,Scopus
1020,,Learning to Prioritize Test Programs for Compiler Testing,"Chen J., Bai Y., Hao D., Xiong Y., Zhang H., Xie B.",2017,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020744779&doi=10.1109%2fICSE.2017.70&partnerID=40&md5=6ceaf3fd0dcce959b887ec4acc6c24ab,10.1109/ICSE.2017.70,"Compiler testing is a crucial way of guaranteeing the reliability of compilers (and software systems in general). Many techniques have been proposed to facilitate automated compiler testing. These techniques rely on a large number of test programs (which are test inputs of compilers) generated by some test-generation tools (e.g., CSmith). However, these compiler testing techniques have serious efficiency problems as they usually take a long period of time to find compiler bugs. To accelerate compiler testing, it is desirable to prioritize the generated test programs so that the test programs that are more likely to trigger compiler bugs are executed earlier. In this paper, we propose the idea of learning to test, which learns the characteristics of bug-revealing test programs from previous test programs that triggered bugs. Based on the idea of learning to test, we propose LET, an approach to prioritizing test programs for compiler testing acceleration. LET consists of a learning process and a scheduling process. In the learning process, LET identifies a set of features of test programs, trains a capability model to predict the probability of a new test program for triggering compiler bugs and a time model to predict the execution time of a test program. In the scheduling process, LET prioritizes new test programs according to their bug-revealing probabilities in unit time, which is calculated based on the two trained models. Our extensive experiments show that LET significantly accelerates compiler testing. In particular, LET reduces more than 50% of the testing time in 24.64% of the cases, and reduces between 25% and 50% of the testing time in 36.23% of the cases. © 2017 IEEE.",,"700, 711",,"Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",Conference Paper,Scopus
1021,,"Proceedings - 2017 IEEE/ACM 12th International Symposium on Software Engineering for Adaptive and Self-Managing Systems, SEAMS 2017",[No author name available],2017,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027181085&partnerID=40&md5=8fe9e421a6f8ad6ba836c48cbb1ffae3,,The proceedings contain 21 papers. The topics discussed include: runtime monitoring and resolution of probabilistic obstacles to system goals; transfer learning for improving model predictions in highly configurable software; self-adaptive learning in decentralized combinatorial optimization - a design paradigm for sharing economies; delivering elastic containerized cloud applications to enable DevOps; decision-making with cross-entropy for self-adaptation; self-adaptation based on big data analytics: a model problem and tool; towards a formal framework for hybrid planning in self-adaptation; intelligent ensembles - a declarative group description language and java framework; and self-adaptive video encoder: comparison of multiple adaptation strategies made simple.,,,216.0,"Proceedings - 2017 IEEE/ACM 12th International Symposium on Software Engineering for Adaptive and Self-Managing Systems, SEAMS 2017",Conference Review,Scopus
1022,,Transfer Learning for Improving Model Predictions in Highly Configurable Software,"Jamshidi P., Velez M., Kästner C., Siegmund N., Kawthekar P.",2017,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027096954&doi=10.1109%2fSEAMS.2017.11&partnerID=40&md5=81e9779436473b86dc87ed8ee228c52c,10.1109/SEAMS.2017.11,"Modern software systems are built to be used in dynamic environments using configuration capabilities to adapt to changes and external uncertainties. In a self-adaptation context, we are often interested in reasoning about the performance of the systems under different configurations. Usually, we learn a black-box model based on real measurements to predict the performance of the system given a specific configuration. However, as modern systems become more complex, there are many configuration parameters that may interact and we end up learning an exponentially large configuration space. Naturally, this does not scale when relying on real measurements in the actual changing environment. We propose a different solution: Instead of taking the measurements from the real system, we learn the model using samples from other sources, such as simulators that approximate performance of the real system at low cost. We define a cost model that transform the traditional view of model learning into a multi-objective problem that not only takes into account model accuracy but also measurements effort as well. We evaluate our cost-aware transfer learning solution using real-world configurable software including (i) a robotic system, (ii) 3 different stream processing applications, and (iii) a NoSQL database system. The experimental results demonstrate that our approach can achieve (a) a high prediction accuracy, as well as (b) a high model reliability. © 2017 IEEE.",Highly configurable software; machine learning; self-adaptive system; self-optimization; transfer learning,"31, 41",,"Proceedings - 2017 IEEE/ACM 12th International Symposium on Software Engineering for Adaptive and Self-Managing Systems, SEAMS 2017",Conference Paper,Scopus
1023,,Enhanced Deep Automatic Image Annotation Based on Data Equalization,"Zhou M.-K., Ke X., Du M.-Z.",2017,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031317411&doi=10.13328%2fj.cnki.jos.005112&partnerID=40&md5=a342d105cc0eb15749049b146fc0eebc,10.13328/j.cnki.jos.005112,"Automatic image annotation is a challenging research problem involving lots of tags and various features. Aiming at the problem that the image annotation based on the traditional shallow machine learning algorithm has low efficiency and is difficult to apply to complex classification task, this paper proposes an automatic image annotation algorithm based on stacked auto-encoder (SAE) to improve both efficiency and effectiveness of annotation. In this paper, two types of strategies are proposed to solve the main problem of unbalanced data in image annotation. For the annotation model itself, to improve the annotation effect of low frequency tags, a balanced and stacked auto-encoder (B-SAE) that can enhance training for low frequency tags is proposed. Based on this model, a robust balanced and stacked auto-encoder algorithm (RB-SAE) is proposed to increase the annotation stability through enhanced training by group in sub B-SAE model. This strategy ensures that the model itself has a strong ability to deal with the unbalanced data. For the annotation process, taking the unknown image as the starting point, the local equilibrium dataset of the unknown image is constructed, and the high and low frequency attribute of the image is discriminated to determine the different annotation process. The local semantic propagation algorithm (SP) annotates the low frequency images and the RB-SAE algorithm annotates the high frequency images. The framework of attribute discrimination annotation (ADA) is formed to improve the overall image annotation effect. This strategy ensures that the labeling process has a strong ability to deal with unbalanced data. Experimental results generated from three public data sets show that many indicators in the presented model are all improved comparing with the previous models. © Copyright 2017, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Balance data; Deep learning; Image annotation; SAE (stacked auto-encoder); Semantic propagation,"1862, 1880",,Ruan Jian Xue Bao/Journal of Software,Article,Scopus
1024,,Pareto-optimal multi-objective dimensionality reduction deep auto-encoder for mammography classification,"Taghanaki S.A., Kawahara J., Miles B., Hamarneh G.",2017,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018513373&doi=10.1016%2fj.cmpb.2017.04.012&partnerID=40&md5=110b5ebed1c1c3315ff22ffd1af9f631,10.1016/j.cmpb.2017.04.012,"Background and objective Feature reduction is an essential stage in computer aided breast cancer diagnosis systems. Multilayer neural networks can be trained to extract relevant features by encoding high-dimensional data into low-dimensional codes. Optimizing traditional auto-encoders works well only if the initial weights are close to a proper solution. They are also trained to only reduce the mean squared reconstruction error (MRE) between the encoder inputs and the decoder outputs, but do not address the classification error. The goal of the current work is to test the hypothesis that extending traditional auto-encoders (which only minimize reconstruction error) to multi-objective optimization for finding Pareto-optimal solutions provides more discriminative features that will improve classification performance when compared to single-objective and other multi-objective approaches (i.e. scalarized and sequential). Methods In this paper, we introduce a novel multi-objective optimization of deep auto-encoder networks, in which the auto-encoder optimizes two objectives: MRE and mean classification error (MCE) for Pareto-optimal solutions, rather than just MRE. These two objectives are optimized simultaneously by a non-dominated sorting genetic algorithm. Results We tested our method on 949 X-ray mammograms categorized into 12 classes. The results show that the features identified by the proposed algorithm allow a classification accuracy of up to 98.45%, demonstrating favourable accuracy over the results of state-of-the-art methods reported in the literature. Conclusions We conclude that adding the classification objective to the traditional auto-encoder objective and optimizing for finding Pareto-optimal solutions, using evolutionary multi-objective optimization, results in producing more discriminative features. © 2017 Elsevier B.V.",Auto-encoder; Breast cancer; Computer aided diagnosis; Feature reduction; Multi-objective optimization,"85, 93",,Computer Methods and Programs in Biomedicine,Article,Scopus
1025,,Cross-project and within-project semi-supervised software defect prediction problems study using a unified solution,"Wu F., Jing X.-Y., Dong X., Cao J., Xu M., Zhang H., Ying S., Xu B.",2017,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026725782&doi=10.1109%2fICSE-C.2017.72&partnerID=40&md5=910f9c69cff4d397aa4a22c6584be1a5,10.1109/ICSE-C.2017.72,"When there exists not enough historical defect data for building accurate prediction model, semi-supervised defect prediction (SSDP) and cross-project defect prediction (CPDP) are two feasible solutions. Existing CPDP methods assume that the available source data is well labeled. However, due to expensive human efforts for labeling a large amount of defect data, usually, we can only make use of the suitable unlabeled source data to help build the prediction model. We call CPDP in this scenario as cross-project semi-supervised defect prediction (CSDP). As to within-project semi-supervised defect prediction (WSDP), although some WSDP methods have been developed in recent years, there still exists much room for improvement. In this paper, we aim to provide an effective solution for both CSDP and WSDP problems. We introduce the semi-supervised dictionary learning technique, an effective machine learning technique, into defect prediction and propose a semi-supervised structured dictionary learning (SSDL) approach for CSDP and WSDP. SSDL can make full use of the useful information in limited labeled defect data and a large amount of unlabeled data. Experiments on two public datasets indicate that SSDL can obtain better prediction performance than related SSDP methods in the CSDP scenario. © 2017 IEEE.",Cross-project semi-supervised defect prediction; Semi-supervised structured dictionary learning; Within-project semi-supervised defect prediction,"195, 197",,"Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering Companion, ICSE-C 2017",Conference Paper,Scopus
1026,,Sentiment analysis on social media using morphological sentence pattern model,"Han Y., Kim K.K.",2017,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026657883&doi=10.1109%2fSERA.2017.7965710&partnerID=40&md5=bee3bf3ba8365e3d02353f945959e2f5,10.1109/SERA.2017.7965710,"social media became popular than ever as people are willing to share their emotions and opinions or to participate in social networking. Accordingly, the understanding of social media usage became important. The sentiment analysis is emerged as one of useful methods to analyze emotional stats expressed in textual data including social media data. However, this method still presents some limitations, particularly with an accuracy issue. For example, our previous sentiment analysis used a probability model and needed to adopt human-coded train-sets to maintain an acceptable accuracy level (89%). To overcome and improve this weakness, we propose an automated sentiment analysis in this paper by using the morphological sentence pattern model. We found that this new approach presented in this paper allowed us to achieve a higher level of accuracy (91.2%). The movie reviews were used for this analysis from IMDb, Rotten Tomatoes, Metacritic, YouTube and Twitter. © 2017 IEEE.",Aspect based approach; Morphological patterns; Natural language processing; Sentiment analysis; Social media,"79, 84",,"Proceedings - 2017 15th IEEE/ACIS International Conference on Software Engineering Research, Management and Applications, SERA 2017",Conference Paper,Scopus
1027,,Semi-Supervised Ensemble Learning Approach for Cross-Project Defect Prediction,"He J.-Y., Meng Z.-P., Chen X., Wang Z., Fan X.-Y.",2017,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027340108&doi=10.13328%2fj.cnki.jos.005228&partnerID=40&md5=49c3c1ee1eb1de5f4d40e120b378f4a0,10.13328/j.cnki.jos.005228,"Software defect prediction can help developers to optimize the distribution of test resources by predicting whether or not a software module is defect-prone. Most defect prediction researches focus on within-project defect prediction which needs sufficient training data from the same project. However, in real software development, a project which needs defect prediction is always new or without any historical data. Therefore cross-project defect prediction becomes a hot topic which uses training data from several projects and performs prediction on another one. The main research challenges in cross-project defect prediction are the variety of distribution from source project to target project and class imbalance problem among datasets. Inspired by search based software engineering, this paper proposes a search based semi-supervised ensemble learning approach S3EL. By adjusting the ratio of distribution in training dataset, several Naïve Bayes classifiers are built as the base learners, then a small amount of labeled target instances and genetic algorithm are used to combine these base classifiers as a final prediction model. S3EL is compared with other up-to-date classical cross-project defect prediction approaches (such as Burak filter, Peters filter, TCA+, CODEP and HYDRA) on AEEEM and Promise dataset. Final results show that S3EL has the best prediction performance in most cases under the F1 measure. © Copyright 2017, Institute of Software, the Chinese Academy of Sciences. All rights reserved.","Cross-project defect prediction; Genetic algorithm; Naïve Bayes; Semi-supervised learning, ensemble learning","1455, 1473",,Ruan Jian Xue Bao/Journal of Software,Article,Scopus
1028,,A two-step convolutional neural network based computer-aided detection scheme for automatically segmenting adipose tissue volume depicting on CT images,"Wang Y., Qiu Y., Thai T., Moore K., Liu H., Zheng B.",2017,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015984007&doi=10.1016%2fj.cmpb.2017.03.017&partnerID=40&md5=6e593436e248418003bb95653c00cefa,10.1016/j.cmpb.2017.03.017,"Accurately assessment of adipose tissue volume inside a human body plays an important role in predicting disease or cancer risk, diagnosis and prognosis. In order to overcome limitation of using only one subjectively selected CT image slice to estimate size of fat areas, this study aims to develop and test a computer-aided detection (CAD) scheme based on deep learning technique to automatically segment subcutaneous fat areas (SFA) and visceral fat areas (VFA) depicting on volumetric CT images. A retrospectively collected CT image dataset was divided into two independent training and testing groups. The proposed CAD framework consisted of two steps with two convolution neural networks (CNNs) namely, Selection-CNN and Segmentation-CNN. The first CNN was trained using 2,240 CT slices to select abdominal CT slices depicting SFA and VFA. The second CNN was trained with 84,000 pixel patches and applied to the selected CT slices to identify fat-related pixels and assign them into SFA and VFA classes. Comparing to the manual CT slice selection and fat pixel segmentation results, the accuracy of CT slice selection using the Selection-CNN yielded 95.8%, while the accuracy of fat pixel segmentation using the Segmentation-CNN was 96.8%. This study demonstrated the feasibility of applying a new deep learning based CAD scheme to automatically recognize abdominal section of human body from CT scans and segment SFA and VFA from volumetric CT data with high accuracy or agreement with the manual segmentation results. © 2017 Elsevier B.V.",Computer-aided detection (CAD); Convolution neural network (CNN); Deep learning; Segmentation of adipose tissue; Subcutaneous fat area (SFA); Visceral fat area (VFA),"97, 104",,Computer Methods and Programs in Biomedicine,Article,Scopus
1029,,An Improved SDA Based Defect Prediction Framework for Both Within-Project and Cross-Project Class-Imbalance Problems,"Jing X.-Y., Wu F., Dong X., Xu B.",2017,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018998019&doi=10.1109%2fTSE.2016.2597849&partnerID=40&md5=97abb3bd85e7a195b1f5df1163dd976c,10.1109/TSE.2016.2597849,"Background. Solving the class-imbalance problem of within-project software defect prediction (SDP) is an important research topic. Although some class-imbalance learning methods have been presented, there exists room for improvement. For cross-project SDP, we found that the class-imbalanced source usually leads to misclassification of defective instances. However, only one work has paid attention to this cross-project class-imbalance problem. Objective. We aim to provide effective solutions for both within-project and cross-project class-imbalance problems. Method. Subclass discriminant analysis (SDA), an effective feature learning method, is introduced to solve the problems. It can learn features with more powerful classification ability from original metrics. For within-project prediction, we improve SDA for achieving balanced subclasses and propose the improved SDA (ISDA) approach. For cross-project prediction, we employ the semi-supervised transfer component analysis (SSTCA) method to make the distributions of source and target data consistent, and propose the SSTCA+ISDA prediction approach. Results. Extensive experiments on four widely used datasets indicate that: 1) ISDA-based solution performs better than other state-of-the-art methods for within-project class-imbalance problem; 2) SSTCA+ISDA proposed for cross-project class-imbalance problem significantly outperforms related methods. Conclusion. Within-project and cross-project class-imbalance problems greatly affect prediction performance, and we provide a unified and effective prediction framework for both problems. © 2017 IEEE.",Cross-Project class-imbalance; Improved subclass discriminant analysis (ISDA); ISDA based defect prediction framework; Software defect prediction (SDP); Within-Project class-imbalance,"321, 339",,IEEE Transactions on Software Engineering,Article,Scopus
1030,,Investigating the use of moving windows to improve software effort prediction: a replicated study,"Lokan C., Mendes E.",2017,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983399519&doi=10.1007%2fs10664-016-9446-4&partnerID=40&md5=e88a5c1827701593e5165f8cda1f8835,10.1007/s10664-016-9446-4,"To date most research in software effort estimation has not taken chronology into account when selecting projects for training and validation sets. A chronological split represents the use of a project’s starting and completion dates, such that any model that estimates effort for a new project p only uses as its training set projects that have been completed prior to p’s starting date. A study in 2009 (“S3”) investigated the use of chronological split taking into account a project’s age. The research question investigated was whether the use of a training set containing only the most recent past projects (a “moving window” of recent projects) would lead to more accurate estimates when compared to using the entire history of past projects completed prior to the starting date of a new project. S3 found that moving windows could improve the accuracy of estimates. The study described herein replicates S3 using three different and independent data sets. Estimation models were built using regression, and accuracy was measured using absolute residuals. The results contradict S3, as they do not show any gain in estimation accuracy when using windows for effort estimation. This is a surprising result: the intuition that recent data should be more helpful than old data for effort estimation is not supported. Several factors, which are discussed in this paper, might have contributed to such contradicting results. Some of our future work entails replicating this work using other datasets, to understand better when using windows is a suitable choice for software companies. © 2016, Springer Science+Business Media New York.",Chronological splitting; Effort estimation; Moving window; Regression-based estimation models,"716, 767",,Empirical Software Engineering,Article,Scopus
1031,,"Proceedings - Asia-Pacific Software Engineering Conference, APSEC",[No author name available],2017,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017674294&partnerID=40&md5=42476fcdd9eaaf582c5c6ee4ddd1f6f2,,The proceedings contain 58 papers. The topics discussed include: task recommendation with developer social network in software crowdsourcing; EXPSOL: recommending online threads for exception-related bug reports; retrieving design pattern usage examples using domain matching; LibSift: automated detection of third-party libraries in Android applications; does the role matter? an investigation of the code quality of casual contributors in GitHub; a model checking based approach for containment checking of UML sequence diagrams; model driven software security architecture of systems-of-systems; analytical study of cognitive layered approach for understanding security requirements using problem domain ontology; a map of threats to validity of systematic literature reviews in software engineering; heterogeneous cross-company effort estimation through transfer learning; an algorithmic-based change effort estimation model for software development; achieving high code coverage in Android UI testing via automated widget exercising; testing android apps via guided gesture event generation; model-based API-call constraint checking for automotive control software; and minimalist qualitative models for model checking cyber-physical feature coordination.,,,420.0,"Proceedings - Asia-Pacific Software Engineering Conference, APSEC",Conference Review,Scopus
1032,,A novel end-to-end classifier using domain transferred deep convolutional neural networks for biomedical images,"Pang S., Yu Z., Orgun M.A.",2017,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011883667&doi=10.1016%2fj.cmpb.2016.12.019&partnerID=40&md5=3461c46640f53b42561a301ee3e549bd,10.1016/j.cmpb.2016.12.019,"Background and objectives Highly accurate classification of biomedical images is an essential task in the clinical diagnosis of numerous medical diseases identified from those images. Traditional image classification methods combined with hand-crafted image feature descriptors and various classifiers are not able to effectively improve the accuracy rate and meet the high requirements of classification of biomedical images. The same also holds true for artificial neural network models directly trained with limited biomedical images used as training data or directly used as a black box to extract the deep features based on another distant dataset. In this study, we propose a highly reliable and accurate end-to-end classifier for all kinds of biomedical images via deep learning and transfer learning. Methods We first apply domain transferred deep convolutional neural network for building a deep model; and then develop an overall deep learning architecture based on the raw pixels of original biomedical images using supervised training. In our model, we do not need the manual design of the feature space, seek an effective feature vector classifier or segment specific detection object and image patches, which are the main technological difficulties in the adoption of traditional image classification methods. Moreover, we do not need to be concerned with whether there are large training sets of annotated biomedical images, affordable parallel computing resources featuring GPUs or long times to wait for training a perfect deep model, which are the main problems to train deep neural networks for biomedical image classification as observed in recent works. Results With the utilization of a simple data augmentation method and fast convergence speed, our algorithm can achieve the best accuracy rate and outstanding classification ability for biomedical images. We have evaluated our classifier on several well-known public biomedical datasets and compared it with several state-of-the-art approaches. Conclusions We propose a robust automated end-to-end classifier for biomedical images based on a domain transferred deep convolutional neural network model that shows a highly reliable and accurate performance which has been confirmed on several public biomedical image datasets. © 2017 Elsevier Ireland Ltd",Biomedical image classification; Convolutional neural network; Data augmentation; Deep learning; Transfer learning,"283, 293",,Computer Methods and Programs in Biomedicine,Article,Scopus
1033,,A transfer cost-sensitive boosting approach for cross-project defect prediction,"Ryu D., Jang J.-I., Baik J.",2017,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940560563&doi=10.1007%2fs11219-015-9287-1&partnerID=40&md5=2ad1716f272557c5d754eb93f9e8540c,10.1007/s11219-015-9287-1,"Software defect prediction has been regarded as one of the crucial tasks to improve software quality by effectively allocating valuable resources to fault-prone modules. It is necessary to have a sufficient set of historical data for building a predictor. Without a set of sufficient historical data within a company, cross-project defect prediction (CPDP) can be employed where data from other companies are used to build predictors. In such cases, a transfer learning technique, which extracts common knowledge from source projects and transfers it to a target project, can be used to enhance the prediction performance. There exists the class imbalance problem, which causes difficulties for the learner to predict defects. The main impacts of imbalanced data under cross-project settings have not been investigated in depth. We propose a transfer cost-sensitive boosting method that considers both knowledge transfer and class imbalance for CPDP when given a small amount of labeled target data. The proposed approach performs boosting that assigns weights to the training instances with consideration of both distributional characteristics and the class imbalance. Through comparative experiments with the transfer learning and the class imbalance learning techniques, we show that the proposed model provides significantly higher defect detection accuracy while retaining better overall performance. As a result, a combination of transfer learning and class imbalance learning is highly effective for improving the prediction performance under cross-project settings. The proposed approach will help to design an effective prediction model for CPDP. The improved defect prediction performance could help to direct software quality assurance activities and reduce costs. Consequently, the quality of software can be managed effectively. © 2015, Springer Science+Business Media New York.",Boosting; Class imbalance; Cost-sensitive learning; Cross-project defect prediction; Software defect prediction; Transfer learning,"235, 272",,Software Quality Journal,Article,Scopus
1034,,An introduction to data analytics for software security,"Ben Othmane L., Brucker A.D., Dashevskyi S., Tsalovski P.",2017,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052714345&doi=10.1201%2fb20962&partnerID=40&md5=90e6501cf03e25077c153e3f22f6d6c2,10.1201/b20962,"Secure software development, e. g., following processes similar to Microsoft’s Security Development Lifecycle (SDL) [16], is considered to be an important part of developing secure software. On the one hand, such processes require a significant effort and, on the other hand, they generate (potentially) a large amount of data—both on the process level (e. g., process descriptions and regulations) where reported, as well as on the technical level (e. g., results of static code analysis tools). © 2018 by Taylor & Francis Group, LLC.",,"69, 94",,Empirical Research for Software Security: Foundations and Experience,Book Chapter,Scopus
1035,,Analysis of material representation of manga line drawings using convolutional neural networks,"Horiuchi T., Saito Y., Hirai K.",2017,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042358598&doi=10.2352%2fJ.ImagingSci.Technol.2017.61.4.040404&partnerID=40&md5=779ef0030f00fa34691483c929fb6ae0,10.2352/J.ImagingSci.Technol.2017.61.4.040404,"Visual perception of materials that make up objects has been gaining increasing interest. Most previous studies on visual material-category perception have used stimuli with rich information, e.g., color, shape, and texture. This article analyzes the image features of the material representations in Japanese ""manga"" comics, which are composed of line drawings and are typically printed in black and white. In this study, the authors first constructed a manga-material database by collecting 799 material images that gave consistent material impressions to observers. The manga-material data from the database were used to fully train ""CaffeNet,"" a convolutional neural network (CNN). Then, the authors visualized training-image patches corresponding to the top-n activations for filters in each convolution layer. From the filter visualization, they found that the filters reacted gradually to complicated features, moving from the input layer to the output layer. Some filters were constructed to represent specific features unique to manga comics. Furthermore, materials in natural photographic images were classified using the constructed CNN, and a modest classification accuracy of 63% was obtained. This result suggests that material-perception features for natural images remain in the manga line-drawing representations. © 2017 Society for Imaging Science and Technology.",,"48, 57",,Final Program and Proceedings - IS and T/SID Color Imaging Conference,Conference Paper,Scopus
1036,,Combing data filter and data sampling for cross- Company defect prediction: An empricial study,"Yu X., Wu M., Zhang Y., Fu M.",2017,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029537837&doi=10.18293%2fSEKE2017-134&partnerID=40&md5=0e9dae36febdc2a6e80d17e8868eccac,10.18293/SEKE2017-134,"Cross-company defect prediction (CCDP) is a practical way that trains a prediction model by exploiting one or multiple projects of a source company and then applies the model to target company. Unfortunately, larger irrelevant crosscompany (CC) data usually makes it difficult to build a prediction model with high performance. On the other hand, the CC data has the highly imbalanced nature between the defectiveprone and non-defective classes, which will degrade the performance of CCDP. To address such issues, this paper proposes an approach, in which data sampling is combined with data filter, to overcome these problems. Data sampling seeks a more balanced dataset through the addition or removal of instances, while data filter is a process of filtering out the irrelevant CC data so that the performance of CCDP models can be improved. We employ two data filtering methods called NN filter and DBSCAN filter combined with SMOTE (Synthetic Minority Oversampling Technique) and RUS (Random Under- Sampling). Eight different approaches would be produced when combing these four techniques: 1- NN filter performed prior to RUS; 2- NN filter performed after RUS; 3- NN filter performed prior to SMOTE; 4- NN filter performed after SMOTE; 5- DBSCAN filter performed prior to RUS; 6- DBSCAN filter performed after RUS; 7- DBSCAN filter performed prior to SMOTE; 8- DBSCAN filter performed after SMOTE. The empirical study was carried out on 15 publicly available project datasets. The experimental results demonstrate that NN filter performed prior to RUS (Approach 1) performs better than the other seven approaches.",Cross-company defect prediction; Data filter; Data sampling; Software defect prediction,"301, 306",,"Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE",Conference Paper,Scopus
1037,,A data filtering method based on agglomerative clustering,"Yu X., Zhang J., Zhou P., Liu J.",2017,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029509244&doi=10.18293%2fSEKE2017-043&partnerID=40&md5=8cac78845d24b2a0d1d0a3919c09ac6d,10.18293/SEKE2017-043,"Cross-company defect prediction (CCDP) is a practical way that trains a prediction model by exploiting one or multiple projects of a source company and then applies the model to target company. Unfortunately, larger irrelevant crosscompany (CC) data usually makes it difficult to build a crosscompany defect prediction model with high performance. To address such issues, this paper proposes a data filtering method based on Agglomerative Clustering (DFAC) for cross-company defect prediction. First, DFAC combines within-company instances and cross-company instances and uses Agglomerative clustering algorithms to group these instances. Second, DFAC selects sub-clusters which consist at least one WC instance, and collects the CC instances in the selected sub-clusters into a new CC data. Compared with existing data filter methods, the experimental results on 15 public PROMISE datasets show that DFAC increases PD value, reduces PF value and achieves higher G-measure and AUC values.",Agglomerative clustering; Cross-company defect prediction; Data filter; Software defect prediction,"392, 397",,"Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE",Conference Paper,Scopus
1038,,Is the number of faults helpful for cross-company defect prediction?,"Jing Y., Zhang J., Liu J.",2017,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029489192&doi=10.18293%2fSEKE2017-034&partnerID=40&md5=f0e25ed3c0e796cc90f333f290ccb216,10.18293/SEKE2017-034,"In the field of Cross-company Defect Prediction (CCDP), how to deal with the data to make it more accurately predict the cross-company software defects is the focus problem we need to consider. Now a mainstream idea is to determine the weight of the training data based on the similarity between the training data and the test set, and then build the model on the basis of these weighted data. However, sometimes, when we deal with some problems with imbalance class, directly using the weight calculated above may lead to errors. Because a large number of non-defective instance's weight accumulation will lead to the defective instance's weight has a little impact on the final result. This is why we need to consider the addition of the number of defects. Considering the number of defects will effectively eliminate the impact of a large number of non-defective instance's weight accumulation. Therefore, we propose a Transfer-learning Naïve Bayes model considering the number of defective information(NTNB). The method consists of two major stages: Weight the data and build the prediction model. In the stage of weighting the data, we not only consider the similarity between the data but also consider the number of defects to get the final weights for data. And we conducted a set of comparative experiments on six open crosscompany datasets. The results show that considering the number of defects information can effectively avoid some defective instance is misjudged as non-defective instance, and improve the accuracy of prediction in some unbalanced problems.",Cross-company defect prediction; NTNB; Software defect prediction; Transfer learning,"111, 116",,"Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE",Conference Paper,Scopus
1039,,A stratification and sampling model for bellwether moving window,"Mensah S., Keung J., Bosu M., Bennin K., Kudjo P.K.",2017,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029477709&doi=10.18293%2fSEKE2017-126&partnerID=40&md5=8b7c2cf2dfff06a6b4621dba033cd83a,10.18293/SEKE2017-126,"An effective method for finding the relevant number (window size) and the elapsed time (window age) of recently completed projects has proven elusive in software effort estimation. Although these two parameters significantly affect the prediction accuracy, there is no effective method to stratify and sample chronological projects to improve prediction performance of software effort estimation models. Exemplary projects (Bellwether) representing the training set have been empirically validated to improve the prediction accuracy in the domain of software defect prediction. However, the concept of Bellwether and its effect have not been empirically proven in software effort estimation as a method of selecting exemplary/relevant projects with defined window size and age. In view of this, we introduce a novel method for selecting relevant and recently completed projects referred to as Bellwether moving window for improving the software effort prediction accuracy. We first sort and cluster a pool of N projects and apply statistical stratification based on Markov chain modeling to select the Bellwether moving window. We evaluate the proposed approach using the baseline Automatically Transformed Linear Model on the ISBSG dataset. Results show that (1) Bellwether effect exist in software effort estimation dataset, (2) the Bellwether moving window with a window size of 82 to 84 projects and window age of 1.5 to 2 years resulted in an improved prediction accuracy than the traditional approach.",Bellwether effect; Chronological dataset; Markov chains; Window age; Window size,"481, 486",,"Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE",Conference Paper,Scopus
1040,,Using class imbalance learning for cross-company defect prediction,"Yu X., Zhou M., Chen X., Deng L., Wang L.",2017,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029468964&doi=10.18293%2fSEKE2017-035&partnerID=40&md5=b18f2b81878d6630858664043057cb33,10.18293/SEKE2017-035,"Cross-company defect prediction (CCDP) is a practical way that trains a prediction model by exploiting one or multiple projects of a source company and then applies the model to target company. Unfortunately, the performance of such CCDP models is susceptible to the high imbalanced nature between the defect-prone and non-defect classes of CC data. Class imbalance learning is applied to alleviate this issue. Because many class imbalance learning methods have been proposed, there is an imperative need to analyze and compare the performance of these methods for CCDP. Although prior empirical studies have proven AdaBoost.NC algorithm achieves the best performance for defect prediction. This observation leads us to conduct a careful empirical study the issues of if and how class imbalance learning methods can benefit cross-company defect prediction. We investigate different types of class imbalance learning methods, including under-sampling technique, over-sampling technique and over sampling followed by under-sampling technique on the cross-company defect prediction performance over 15 publicly available datasets. Experimental results show that under-sampling technique achieves the best overall performance in terms of the gmeasure among those methods we studied.1.",Class imbalance learning; Cross-company defect prediction; Software defect prediction,"117, 122",,"Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE",Conference Paper,Scopus
1041,,Multi-agent deep reinforcement learning for task allocation in dynamic environment,"Noureddine D.B., Gharbi A., Ahmed S.B.",2017,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029316469&doi=10.5220%2f0006393400170026&partnerID=40&md5=c4dfddba80c3e2ec133e9450c69af515,10.5220/0006393400170026,"The task allocation problem in a distributed environment is one of the most challenging problems in a multiagent system. We propose a new task allocation process using deep reinforcement learning that allows cooperating agents to act automatically and learn how to communicate with other neighboring agents to allocate tasks and share resources. Through learning capabilities, agents will be able to reason conveniently, generate an appropriate policy and make a good decision. Our experiments show that it is possible to allocate tasks using deep Q-learning and more importantly show the performance of our distributed task allocation approach. Copyright © 2017 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.",Communication; Deep reinforcement learning; Distributed environment; Multi-agent system; Task allocation,"17, 26",,ICSOFT 2017 - Proceedings of the 12th International Conference on Software Technologies,Conference Paper,Scopus
1042,,ECLogger: Cross-project catch-block logging prediction using ensemble of classifiers,"Lal S., Sardana N., Sureka A.",2017,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018709870&doi=10.5277%2fe-Inf170101&partnerID=40&md5=0c2029784c95af257bb9edbb6d17ff80,10.5277/e-Inf170101,"Background: Software developers insert log statements in the source code to record program execution information. However, optimizing the number of log statements in the source code is challenging. Machine learning based within-project logging prediction tools, proposed in previous studies, may not be suitable for new or small software projects. For such software projects, we can use cross-project logging prediction. Aim: The aim of the study presented here is to investigate cross-project logging prediction methods and techniques. Method: The proposed method is ECLogger, which is a novel, ensemble-based, cross-project, catch-block logging prediction model. In the research We use 9 base classifiers were used and combined using ensemble techniques. The performance of ECLogger was evaluated on on three open-source Java projects: Tomcat, CloudStack and Hadoop. Results: ECLoggerBagging, ECLoggerAverageVote, and ECLoggerMajorityVote show a considerable improvement in the average Logged F-measure (LF) on 3, 5, and 4 source → target project pairs, respectively, compared to the baseline classifiers. ECLoggerAverageVote performs best and shows improvements of 3.12% (average LF) and 6.08% (average ACC - Accuracy). Conclusion: The classifier based on ensemble techniques, such as bagging, average vote, and majority vote outperforms the baseline classifier. Overall, the ECLoggerAverageVote model performs best. The results show that the CloudStack project is more generalizable than the other projects.",Classification; Debugging; Ensemble logging; Machine learning; Source code analysis; Tracing,"7, 38",,E-Informatica Software Engineering Journal,Article,Scopus
1043,,Automatic land cover classification of geo-tagged field photos by deep learning,"Xu G., Zhu X., Fu D., Dong J., Xiao X.",2017,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012969636&doi=10.1016%2fj.envsoft.2017.02.004&partnerID=40&md5=892e7f8635203626fe9a2c978c6e071e,10.1016/j.envsoft.2017.02.004,"With more and more crowdsourcing geo-tagged field photos available online, they are becoming a potentially valuable source of information for environmental studies. However, the labelling and recognition of these photos are time-consuming. To utilise such information, a land cover type recognition model for field photos was proposed based on the deep learning technique. This model combines a pre-trained convolutional neural network (CNN) as the image feature extractor and the multinomial logistic regression model as the feature classifier. The pre-trained CNN model Inception-v3 was used in this study. The labelled field photos from the Global Geo-Referenced Field Photo Library (http://eomf.ou.edu/photos) were chosen for model training and validation. The results indicated that our recognition model achieved an acceptable accuracy (48.40% for top-1 prediction and 76.24% for top-3 prediction) of land cover classification. With accurate self-assessment of confidence, the model can be applied to classify numerous online geo-tagged field photos for environmental information extraction. © 2017 Elsevier Ltd",Convolutional neural network; Crowdsourced photos; Deep learning; Land cover; Multinomial logistic regression; Transfer learning,"127, 134",,Environmental Modelling and Software,Article,Scopus
1044,,An Empirical Comparison of Model Validation Techniques for Defect Prediction Models,"Tantithamthavorn C., McIntosh S., Hassan A.E., Matsumoto K.",2017,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009874260&doi=10.1109%2fTSE.2016.2584050&partnerID=40&md5=8e0362aaaa6056608661217e88e5498f,10.1109/TSE.2016.2584050,"Defect prediction models help software quality assurance teams to allocate their limited resources to the most defect-prone modules. Model validation techniques, such as k-fold cross-validation, use historical data to estimate how well a model will perform in the future. However, little is known about how accurate the estimates of model validation techniques tend to be. In this paper, we investigate the bias and variance of model validation techniques in the domain of defect prediction. Analysis of 101 public defect datasets suggests that 77 percent of them are highly susceptible to producing unstable results-selecting an appropriate model validation technique is a critical experimental design choice. Based on an analysis of 256 studies in the defect prediction literature, we select the 12 most commonly adopted model validation techniques for evaluation. Through a case study of 18 systems, we find that single-repetition holdout validation tends to produce estimates with 46-229 percent more bias and 53-863 percent more variance than the top-ranked model validation techniques. On the other hand, out-of-sample bootstrap validation yields the best balance between the bias and variance of estimates in the context of our study. Therefore, we recommend that future defect prediction studies avoid single-repetition holdout validation, and instead, use out-of-sample bootstrap validation. © 1976-2012 IEEE.",bootstrap validation; cross validation; Defect prediction models; holdout validation; model validation techniques,"1, 18",,IEEE Transactions on Software Engineering,Article,Scopus
1045,,An ID-based Dynamic Authenticated Group Key Agreement Scheme with Optimal Round Complexity from Pairings,"Li F., Xie D., Gao W., Wang X.A., Yan J.",2016,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011018196&doi=10.1109%2fCISIS.2016.63&partnerID=40&md5=af7f3121f4a05efa90f019011ebdae4e,10.1109/CISIS.2016.63,This paper presents an identity-based dynamic authenticated group key agreement (DAGKA) protocol with following advantages. The algorithms of Setup and Join has only one round communication. There is no message exchange among members in Leave algorithm. Joining members cannot know previous session keys and leaving members cannot know future session keys. Its AKE-security is proved under Decisional Bilinear Diffie-Hellman (DBDH) assumption. The protocol can resist key control attack and has forward security. © 2016 IEEE.,authentication; group key agreement; identity based cryptography; provable security,"468, 472",,"Proceedings - 2016 10th International Conference on Complex, Intelligent, and Software Intensive Systems, CISIS 2016",Conference Paper,Scopus
1046,,Applying assemble clustering algorithm and fault prediction to test case prioritization,"Xiao L., Miao H., Zhuang W., Chen S.",2016,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010628676&doi=10.1109%2fSATE.2016.25&partnerID=40&md5=b92aef9666a32fff279a84fc4f9b3190,10.1109/SATE.2016.25,"Cluster application is proposed as an efficient approach to improve test case prioritization, Test case in a same cluster are considered to have similar behaviors. In the process of cluster test case, the selection of test case feature and the clusters number have great influence on the clustering results. but to date almost clustering algorithm to improve test case prioritization are selected random clusters number and clustering result are based on one or a few of the code features, the paper propose a new prioritization techniques that not only consider the best clusters number but also produce the best clustering result based on test case multidimensional feature. After clustering, considering the inter-cluster prioritization and intra-cluster prioritization,in order to improve the effectiveness of our approach, the fault prediction value of code corresponding to the test case is used as one of a prioritization weight. Finally,we implemented an empirical studies using an industrial software to illustrate the effectiveness of the test case prioritization techniques. © 2016 IEEE.",Assemble clustering algorithm; Fault prediction; Test case prioritization; The best clusters number,"108, 116",,"Proceedings - 2016 International Conference on Software Analysis, Testing and Evolution, SATE 2016",Conference Paper,Scopus
1047,,Which is more important for cross-project defect prediction: Instance or feature?,"Yu Q., Jiang S., Qian J.",2016,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010619423&doi=10.1109%2fSATE.2016.22&partnerID=40&md5=0c6edad1a0c6f27f3f0899a40359cb07,10.1109/SATE.2016.22,"Software defect prediction plays an important role in software testing. We can build the prediction model based on historical data. However, for a new project, we cannot be able to build a good prediction model due to lack of historical data. Therefore, researchers have proposed the cross-project defect prediction (CPDP) methods to share the historical data among different projects. In practice, there may be the problems of instance distribution differences and feature redundancy in cross-project datasets. To investigate which is more important for CPDP, instance or feature, we take instance filter and feature selection as examples to show their efficiency for CPDP. Our experiments are conducted on NASA and PROMISE datasets, and the results indicate that feature selection performs better than instance filter in improving the performance of CPDP. We can conclude that feature could be more important than instance for CPDP. © 2016 IEEE.",Cross-project defect prediction; Feature selection; Instance filter; Software testing,"90, 95",,"Proceedings - 2016 International Conference on Software Analysis, Testing and Evolution, SATE 2016",Conference Paper,Scopus
1048,,Cost-sensitive local collaborative representation for software defect prediction,"Wu F., Jing X.-Y., Dong X., Cao J., Xu B., Ying S.",2016,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010509593&doi=10.1109%2fSATE.2016.24&partnerID=40&md5=6a478a2b162f64f679a4dd457fa01f57,10.1109/SATE.2016.24,"Recently, representative sparse representation based classifiers, namely dictionary learning and collaborative representation based classifier (CRC), has been introduced into software defect prediction (SDP) and demonstrated to be effective for SDP. The dictionary learning based SDP method needs relatively large computational cost, while collaborative representation based method can significantly reduce the computational cost and achieve comparable prediction effects as the former. In this paper, we aim to preserve the desirable efficiency of collaborative representation based SDP method and further improve its prediction effect. We propose a cost-sensitive local collaborative representation (CLCR) approach for SDP. CLCR firstly efficiently finds the neighboring modules of a given test (query) module using CRC. Then CLCR represents the test module as a linear combination of its neighbors and uses the representation error for prediction. To solve the class-imbalance problem, CLCR further incorporates the cost-sensitive factor into the representation coefficients in the prediction phase. Experiments on five projects of the NASA dataset demonstrate the effectiveness of the proposed approach as compared with several related SDP methods. © 2016 IEEE.",Collaborative representation classification (CRC); Cost-sensitive local collaborative representation (CLCR); Software defect prediction (SDP),"102, 107",,"Proceedings - 2016 International Conference on Software Analysis, Testing and Evolution, SATE 2016",Conference Paper,Scopus
1049,,Exploiting Correlation Subspace to Predict Heterogeneous Cross-Project Defects,"Cheng M., Wu G., Wan H., You G., Yuan M., Jiang M.",2016,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013271810&doi=10.1142%2fS0218194016710017&partnerID=40&md5=297ca9f78869c1b13d6b8f86c60cdacc,10.1142/S0218194016710017,"Cross-project defect prediction trains a prediction model using historical data from source projects and applies the model to target projects. Most previous efforts assumed the cross-project data have the same metrics set, which means the metrics used and the size of metrics set are the same. However, this assumption may not hold in practical scenarios. In addition, software defect datasets have the class-imbalance problem which increases the difficulty for the learner to predict defects. In this paper, we advance canonical correlation analysis by deriving a joint feature space for associating cross-project data. We also propose a novel support vector machine algorithm which incorporates the correlation transfer information into classifier design for cross-project prediction. Moreover, we take different misclassification costs into consideration to make the classification inclining to classify a module as a defective one, alleviating the impact of imbalanced data. The experimental results show that our method is more effective compared to state-of-the-art methods. © 2016 World Scientific Publishing Company.",canonical correlation analysis; class imbalance; Defect prediction; heterogeneous metrics; support vector machine,"1571, 1580",,International Journal of Software Engineering and Knowledge Engineering,Conference Paper,Scopus
1050,,Improving the performance of defect prediction based on evolution data,"Wang D.-D., Wang Q.",2016,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006304301&doi=10.13328%2fj.cnki.jos.004869&partnerID=40&md5=b8e449ce0ca38dd177a69c318a3ea8e7,10.13328/j.cnki.jos.004869,"It is an undisputed fact that software continues to evolve. Software evolution is caused by requirement changes which often result in injection of defects. Existing defect prediction techniques mainly focus on utilizing the attributes of software work products, such as documents, source codes and test cases, to predict defects. Consider an evolving software as a species and its development process as a natural species' evolutionary process, the injection of defects may have the characters of a species and will be impacted by its evolution. A great many of researchers have studied the process of software evolution and proposed some evolution related metrics. In this study, a set of new metrics is first proposed based on evolutionary history to characterize software evolution process, and then a case study on building defect prediction models is presented. Experiments on six well-known open source projects achieved good performance, demonstrating the effectiveness of the proposed metrics. © Copyright 2016, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Defect prediction; Evolution metrics; Software evolution,"3014, 3029",,Ruan Jian Xue Bao/Journal of Software,Article,Scopus
1051,,Exploring the complementarity of THz pulse imaging and DCE-MRIs: Toward a unified multi-channel classification and a deep learning framework,"Yin X.-X., Zhang Y., Cao J., Wu J.-L., Hadjiloucas S.",2016,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988811953&doi=10.1016%2fj.cmpb.2016.08.026&partnerID=40&md5=1a85c88965d12106032a38b35cf70d37,10.1016/j.cmpb.2016.08.026,"We provide a comprehensive account of recent advances in biomedical image analysis and classification from two complementary imaging modalities: terahertz (THz) pulse imaging and dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI). The work aims to highlight underlining commonalities in both data structures so that a common multi-channel data fusion framework can be developed. Signal pre-processing in both datasets is discussed briefly taking into consideration advances in multi-resolution analysis and model based fractional order calculus system identification. Developments in statistical signal processing using principal component and independent component analysis are also considered. These algorithms have been developed independently by the THz-pulse imaging and DCE-MRI communities, and there is scope to place them in a common multi-channel framework to provide better software standardization at the pre-processing de-noising stage. A comprehensive discussion of feature selection strategies is also provided and the importance of preserving textural information is highlighted. Feature extraction and classification methods taking into consideration recent advances in support vector machine (SVM) and extreme learning machine (ELM) classifiers and their complex extensions are presented. An outlook on Clifford algebra classifiers and deep learning techniques suitable to both types of datasets is also provided. The work points toward the direction of developing a new unified multi-channel signal processing framework for biomedical image analysis that will explore synergies from both sensing modalities for inferring disease proliferation. © 2016 Elsevier Ireland Ltd",Complex extreme learning machine; Deep learning; MRI; Multiclass classification; THz imaging; Wavelet analysis,"87, 114",,Computer Methods and Programs in Biomedicine,Review,Scopus
1052,,Efficient design space exploration by knowledge transfer,"Li D., Wang S., Yao S., Liu Y.-H., Cheng Y., Sun X.-H.",2016,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006870205&doi=10.1145%2f2968456.2968457&partnerID=40&md5=b899b6fb8f0ce41a5eda9f91b4012b66,10.1145/2968456.2968457,"Due to the exponentially increasing size of design space of microprocessors and time-consuming simulations, predictive models have been widely employed in design space exploration (DSE). Traditional approaches mostly build a program-specific predictor that needs a large number of program-specific samples. Thus considerable simulation cost is required for each program. In this paper, we study the novel problem of transferring knowledge from the labeled samples of previous programs to help predict the responses of the new target program whose labeled samples are very sparse. Inspired by the recent advances of transfer learning, we propose a transfer learning based DSE framework TrDSE to build a more efficient and effective predictive model for the target program with only a few simulations by borrowing knowledge from previous programs. Specifically, TrDSE includes two phases: 1) clustering the programs based on the proposed orthogonal array sampling and the distribution related features, and 2) with the guidance of clustering results, predicting the responses of configurations in design space of the target program by a transfer learning based regression algorithm. We evaluate the proposed TrDSE on the benchmarks of SPEC CPU 2006 suite. The results demonstrate that the proposed framework is more efficient and effective than state-of-art DSE techniques. © 2016 ACM.",Design space exploration; Knowledge transfer; Processor design,,,"2016 International Conference on Hardware/Software Codesign and System Synthesis, CODES+ISSS 2016",Conference Paper,Scopus
1053,,General game playing: A research field for exploring machine intelligence in games,"Zhang H.-F., Liu D.-Y., Li W.-X.",2016,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012257789&doi=10.13328%2fj.cnki.jos.004928&partnerID=40&md5=2cf041a1db3ee024ebe862b8b44a2d79,10.13328/j.cnki.jos.004928,"General game playing (GGP) is a research field for improving the general gaming intelligence of machines. It is different from specific game playing in that GGP players do not know game rules before the game begins, which makes them independent from human experience on specific games. Until now, GGP researchers have deeply explored many problems, such as game representation, search algorithm, and state evaluation. Also, some efforts have been made on knowledge transfer. The progress of GGP research represents the development of artificial general intelligence to some extent, which makes it remarkable. © Copyright 2016, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Artificial intelligence; Game; General; Knowledge representation; Logic,"2814, 2827",,Ruan Jian Xue Bao/Journal of Software,Article,Scopus
1054,,Transfer affinity propagation clustering algorithm,"Hang W.-L., Jiang Y.-Z., Liu J.-F., Wang S.-T.",2016,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012240138&doi=10.13328%2fj.cnki.jos.004921&partnerID=40&md5=e75ed9d86b398fd9acebff6a02790b84,10.13328/j.cnki.jos.004921,"The main limitation of most traditional clustering methods is that they cannot effectively deal with the insufficient datasets in target domain. It is desirable to develop new cluster algorithms which can leverage useful information in the source domain to guide the clustering performance in the target domain so that appropriate number of clusters and high quality clustering result can be obtained in this situation. In this paper, a clustering algorithm called transfer affinity propagation (TAP) is proposed for the insufficient dataset scenarios. The new algorithm improves the clustering performance when the distribution of source and target domains are similar. The basic idea of TAP is to modify the update rules about two message propagations, used in affinity propagation (AP), through leveraging statistical property and geometric structure together. With the corresponding factor graph, TAP indeed can be applied to cluster in AP-like transfer learning, i.e., TAP can abstract the knowledge of source domains through the two tricks to enhance the learning of target domain even if the data in the current scene are not adequate. Extensive experiments demonstrate that the proposed algorithm outperforms traditional algorithms in situations of insufficient data. © Copyright 2016, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Affinity propagation (AP); Cluster method; Geometric structure; Insufficient data; Statistical property; Transfer learning,"2796, 2813",,Ruan Jian Xue Bao/Journal of Software,Article,Scopus
1055,,A pre-training strategy for convolutional neural network applied to Chinese digital gesture recognition,"Li Y., Yang Y., Chen Y., Zhu M.",2016,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994478024&doi=10.1109%2fICCSN.2016.7586597&partnerID=40&md5=462f1914735eb19957e23fb523f39167,10.1109/ICCSN.2016.7586597,"In this paper, we present an approach to classify Chinese digital gesture based on convolutional neural network (CNN). Principal Component Analysis (PCA) is employed to learn convolution kernels as the pre-training strategy. The learned convolution kernels are used for extracting features instead of the random convolution kernels. The convolutional layers can be directly implemented without any further training, such as Back Propagation (BP). For better understanding, we name the proposed architecture for PCA-based Convolutional Neural Network (PCNN). The dataset is divided into six gesture classes including 14500 gesture images, with 12000 images for training and 2500 images for testing. We examine the robustness of the PCNN against noises and distortions. In addition, the MNIST database of handwritten digits is employed to assess the suitability of the PCNN. Different from the CNN, the PCNN reduces the high computational cost of convolution kernels training. About one-fifth of the training time is shortened. The result shows that our approach classifies six gesture classes with 99.92% accuracy. Multiple experiments manifest the PCNN serving as an efficient approach for image processing and object recognition. © 2016 IEEE.",Chinese digital gesture recognition; convolution kernels; convolutional neural network; principal component analysis,"620, 624",,"Proceedings of 2016 8th IEEE International Conference on Communication Software and Networks, ICCSN 2016",Conference Paper,Scopus
1056,,HYDRA: Massively compositional model for cross-project defect prediction,"Xia X., Lo D., Pan S.J., Nagappan N., Wang X.",2016,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994895286&doi=10.1109%2fTSE.2016.2543218&partnerID=40&md5=b74ffeea7d3a9ca9d33936dcbe9dd19c,10.1109/TSE.2016.2543218,"Most software defect prediction approaches are trained and applied on data from the same project. However, often a new project does not have enough training data. Cross-project defect prediction, which uses data from other projects to predict defects in a particular project, provides a new perspective to defect prediction. In this work, we propose a HYbrid moDel Reconstruction Approach (HYDRA) for cross-project defect prediction, which includes two phases: genetic algorithm (GA) phase and ensemble learning (EL) phase. These two phases create a massive composition of classifiers. To examine the benefits of HYDRA, we perform experiments on 29 datasets from the PROMISE repository which contains a total of 11,196 instances (i.e., Java classes) labeled as defective or clean. We experiment with logistic regression as the underlying classification algorithm of HYDRA. We compare our approach with the most recently proposed cross-project defect prediction approaches: TCA+ by Nam et al., Peters filter by Peters et al., GP by Liu et al., MO by Canfora et al., and CODEP by Panichella et al. Our results show that HYDRA achieves an average F1-score of 0.544. On average, across the 29 datasets, these results correspond to an improvement in the F1-scores of 26.22 , 34.99, 47.43, 28.61, and 30.14 percent over TCA+, Peters filter, GP, MO, and CODEP, respectively. In addition, HYDRA on average can discover 33 percent of all bugs if developers inspect the top 20 percent lines of code, which improves the best baseline approach (TCA+) by 44.41 percent. We also find that HYDRA improves the F1-score of Zero-R which predict all the instances to be defective by 5.42 percent, but improves Zero-R by 58.65 percent when inspecting the top 20 percent lines of code. In practice, Zero-R can be hard to use since it simply predicts all of the instances to be defective, and thus developers have to inspect all of the instances to find the defective ones. Moreover, we notice the improvement of HYDRA over other baseline approaches in terms of F1-score and when inspecting the top 20 percent lines of code are substantial, and in most cases the improvements are significant and have large effect sizes across the 29 datasets. © 1976-2012 IEEE.",Cross-project defect prediction; ensemble learning; genetic algorithm; transfer learning,"977, 998",,IEEE Transactions on Software Engineering,Article,Scopus
1057,,Towards building a universal defect prediction model with rank transformed predictors,"Zhang F., Mockus A., Keivanloo I., Zou Y.",2016,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941350779&doi=10.1007%2fs10664-015-9396-2&partnerID=40&md5=cfc68c31b20ae580a4981d9fa18d20ff,10.1007/s10664-015-9396-2,"Software defects can lead to undesired results. Correcting defects costs 50 % to 75 % of the total software development budgets. To predict defective files, a prediction model must be built with predictors (e.g., software metrics) obtained from either a project itself (within-project) or from other projects (cross-project). A universal defect prediction model that is built from a large set of diverse projects would relieve the need to build and tailor prediction models for an individual project. A formidable obstacle to build a universal model is the variations in the distribution of predictors among projects of diverse contexts (e.g., size and programming language). Hence, we propose to cluster projects based on the similarity of the distribution of predictors, and derive the rank transformations using quantiles of predictors for a cluster. We fit the universal model on the transformed data of 1,385 open source projects hosted on SourceForge and GoogleCode. The universal model obtains prediction performance comparable to the within-project models, yields similar results when applied on five external projects (one Apache and four Eclipse projects), and performs similarly among projects with different context factors. At last, we investigate what predictors should be included in the universal model. We expect that this work could form a basis for future work on building a universal model and would lead to software support tools that incorporate it into a regular development workflow. © 2015, Springer Science+Business Media New York.",Context factors; Defect prediction; Large-scale; Rank transformation; Software quality; Universal defect prediction model,"2107, 2145",,Empirical Software Engineering,Article,Scopus
1058,,Too much automation? the bellwether effect and its implications for transfer learning,"Krishna R., Menzies T., Fu W.",2016,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989206867&doi=10.1145%2f2970276.2970339&partnerID=40&md5=959e16066254267048c55abd1fb21f06,10.1145/2970276.2970339,"""Transfer learning"": is the process of translating quality predictors learned in one data set to another. Transfer learning has been the subject of much recent research. In practice, that research means changing models all the time as transfer learners continually exchange new models to the current project. This paper offers a very simple ""bellwether"" transfer learner. Given N data sets, we find which one produces the best predictions on all the others. This ""bellwether"" data set is then used for all subsequent predictions (or, until such time as its predictions start failing-at which point it is wise to seek another bellwether). Bellwethers are interesting since they are very simple to find (just wrap a for-loop around standard data miners). Also, they simplify the task of making general policies in SE since as long as one bellwether remains useful, stable conclusions for N data sets can be achieved just by reasoning over that bellwether. From this, we conclude (1) this bellwether method is a useful (and very simple) transfer learning method; (2) ""bellwethers"" are a baseline method against which future transfer learners should be compared; (3) sometimes, when building increasingly complex automatic methods, researchers should pause and compare their supposedly more sophisticated method against simpler alternatives. © 2016 ACM.",Data Mining; Defect Prediction; Transfer learning,"122, 131",,ASE 2016 - Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering,Conference Paper,Scopus
1059,,ASE 2016 - Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering,[No author name available],2016,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989203030&partnerID=40&md5=034a914967eadf087040a1340a5686af,,The proceedings contain 97 papers. The topics discussed include: an empirical investigation into the nature of test smells; evaluating non-adequate test-case reduction; optimizing customized program coverage; what makes killing a mutant hard; test case permutation to improve execution time; predicting semantically linkable knowledge in developer online forums via convolutional neural network; testing advanced driver assistance systems using multi-objective search and neural networks; privacy preserving via interval covering based subclass division and manifold learning based bidirectional obfuscation for effort estimation; deep learning code fragments for code clone detection; automatically recommending code reviewers based on their expertise: an empirical comparison; evaluating the evaluations of code recommender systems: a reality check; and too much automation? the bellwether effect and its implications for transfer learning.,,,912.0,ASE 2016 - Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering,Conference Review,Scopus
1060,,Privacy preserving via interval covering based subclass division and manifold learning based bi-directional obfuscation for effort estimation,"Qi F., Jing X.-Y., Zhu X., Wu F., Cheng L.",2016,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989196127&doi=10.1145%2f2970276.2970302&partnerID=40&md5=9fbde1ddb580947fb0ec249c8b9fcfdc,10.1145/2970276.2970302,"When a company lacks local data in hand, engineers can build an effort model for the effort estimation of a new project by utilizing the training data shared by other companies. However, one of the most important obstacles for data sharing is the privacy concerns of software development organizations. In software engineering, most of existing privacy-preserving works mainly focus on the defect prediction, or debugging and testing, yet the privacypreserving data sharing problem has not been well studied in effort estimation. In this paper, we aim to provide data owners with an effective approach of privatizing their data before release. We firstly design an Interval Covering based Subclass Division (ICSD) strategy. ICSD can divide the target data into several subclasses by digging a new attribute (i.e., class label) from the effort data. And the obtained class label is beneficial to maintaining the distribution of the target data after obfuscation. Then, we propose a manifold learning based bi-directional data obfuscation (MLBDO) algorithm, which uses two nearest neighbors, which are selected respectively from the previous and next subclasses by utilizing the manifold learning based nearest neighbor selector, as the disturbances to obfuscate the target sample. We call the entire approach as ICSD & MLBDO. Experimental results on seven public effort datasets show that: 1) ICSD&MLBDO can guarantee the privacy and maintain the utility of obfuscated data. 2) ICSD&MLBDO can achieve better privacy and utility than the compared privacy-preserving methods. © 2016 ACM.",Effort estimation; Locality preserving projection; Privacy-preserving; Subclass division,"75, 86",,ASE 2016 - Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering,Conference Paper,Scopus
1061,,Privacy Preservation in Affect-Driven Personalization,"Addo I.D., Madiraju P., Ahamed S.I., Chu W.C.",2016,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988038773&doi=10.1109%2fCOMPSAC.2016.168&partnerID=40&md5=15587d2ea4a0f04aed3e8c7a950b7b04,10.1109/COMPSAC.2016.168,"Knowing exactly how a given audience feels about some content in addition to what mood they are in, can affect the type of user experience that is delivered in the form of personalized advertising and content delivery. As we continue to mine rich data sources and draw insights from emotion analytics, we often leave in our wake, a major privacy gap pertaining to the human subjects involved in our studies. Yes, the goal is to derive insights from a user's emotional state in a bid to select personalized content or predict what the end user is likely to find attractive at a given point in time. Yet, this has to be done in a way that is ubiquitous and above all preserves the privacy of the subjects in question. This literature details the visual privacy gaps in collecting and analyzing emotion data in support of personalized advertising and, consequently, offers a reference framework for preserving end-user privacy throughout the emotion analytics lifecycle. © 2016 IEEE.",Affect-Detection; Affect-Driven Personalization; Collective Intelligence; Computational Advertising; Emotion Analytics; Privacy Preservation; Reference Architecture,"400, 405",,Proceedings - International Computer Software and Applications Conference,Conference Paper,Scopus
1062,,Heterogeneous Cross-Company Effort Estimation through Transfer Learning,"Tong S., He Q., Chen Y., Yang Y., Shen B.",2016,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018467034&doi=10.1109%2fAPSEC.2016.033&partnerID=40&md5=d301135574febd1ee9952f5024db007c,10.1109/APSEC.2016.033,"Software effort estimation is vital but challenging activity during software development. In many small or medium-sized companies, such challenges are stemmed from historical data shortage. The problem can be solved by leveraging cross-company data for effort estimation. While in practice, cross-company effort estimation may not be easy to take because the cross-company data for effort estimation can be heterogenous. In this paper, we propose a novel approach named Mixture of Canonical Correlation Analysis and Restricted Boltzmann Machines (MCR) to address data heterogeneity issue in cross-company effort estimation. The essential ideas in MCR are (1) to present a unified metric representing heterogenous effort estimation data; and (2) to combine Canonical Correlation Analysis and Restricted Boltzmann Machines method to estimate effort in heterogenous cross-company effort estimation. The MCR approach is evaluated on 5 public datasets in PROMISE repository. The evaluation results show that: (1) for estimations with partially different metrics, the MCR approach outperforms within-company effort estimator KNN with a decrease in MMRE by 0.60, an increase in PRED(25) by 0.16, and a decrease in MdMRE by 0.19; (2) for estimations with totally different metrics, the MCR approach outperforms within-company effort estimator KNN with a decrease in MMRE by 0.49, an increase in PRED(25) by 0.08, and a decrease in MdMRE by 0.10. © 2016 IEEE.",Canonical Correlation Analysis; Heterogenous Data; Restricted Boltzmann Machines; Software Effort Estimation; Transfer Learning,"169, 176",,"Proceedings - Asia-Pacific Software Engineering Conference, APSEC",Conference Paper,Scopus
1063,,Road vehicle detection and classification based on Deep Neural Network,"Zhang Z., Xu C., Feng W.",2016,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016968906&doi=10.1109%2fICSESS.2016.7883158&partnerID=40&md5=281128f429b32b001293d383913a53f7,10.1109/ICSESS.2016.7883158,"The deep learning is a growing multi-layer neural network learning algorithm in the field of machine learning in recent years. Firstly, this paper analyzes the superiority of the deep learning at the aspect of feature extraction. Aimed at the lack of feature expression capacity and curse of dimensionality results from excessive feature dimensions of shallow learning, this paper proposes that using deep learning can extract high-lever features from low-lever features though its given layer structure. Secondly, the deep learning algorithm is applied in the case of road vehicle detection. Based on the traditional method, such as neural network the deep learning structure is further studied to increase the performance of feature extraction and classification recognition. Also, some tests are run in the Matlab software. The tests results show that with the increasing the amount of the data, the mean error and misclassification rate gradually decrease, so this algorithm based on the neural network has good superiority and adaptability of the deep learning. Finally, this paper proposes some suggestions for the improvement of the algorithm and prospects the development direction of the deep learning in the field of machine learning and artificial intelligence. © 2016 IEEE.",Classification; Deep neural network; Mode recognition; Road vehicle detection,"675, 678",,"Proceedings of the IEEE International Conference on Software Engineering and Service Sciences, ICSESS",Conference Paper,Scopus
1064,,"Assessment and cross-product prediction of software product line quality: accounting for reuse across products, over multiple releases","Devine T., Goseva-Popstojanova K., Krishnan S., Lutz R.R.",2016,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960376244&doi=10.1007%2fs10515-014-0160-4&partnerID=40&md5=1fccfaf3a5cbacdae47ca41160aded31,10.1007/s10515-014-0160-4,"The goals of cross-product reuse in a software product line (SPL) are to mitigate production costs and improve the quality. In addition to reuse across products, due to the evolutionary development process, a SPL also exhibits reuse across releases. In this paper, we empirically explore how the two types of reuse—reuse across products and reuse across releases—affect the quality of a SPL and our ability to accurately predict fault proneness. We measure the quality in terms of post-release faults and consider different levels of reuse across products (i.e., common, high-reuse variation, low-reuse variation, and single-use packages), over multiple releases. Assessment results showed that quality improved for common, low-reuse variation, and single-use packages as they evolved across releases. Surprisingly, within each release, among preexisting (‘old’) packages, the cross-product reuse did not affect the change and fault proneness. Cross-product predictions based on pre-release data accurately ranked the packages according to their post-release faults and predicted the 20 % most faulty packages. The predictions benefited from data available for other products in the product line, with models producing better results (1) when making predictions on smaller products (consisting mostly of common packages) rather than on larger products and (2) when trained on larger products rather than on smaller products. © 2014, Springer Science+Business Media New York.",Assessment; Cross-product prediction; Cross-product reuse; Cross-release reuse; Fault proneness prediction; Longitudinal study; Software product lines,"253, 302",,Automated Software Engineering,Article,Scopus
1065,,MICHAC: Defect prediction via feature selection based on Maximal Information Coefficient with Hierarchical Agglomerative Clustering,"Xu Z., Xuan J., Liu J., Cui X.",2016,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988850457&doi=10.1109%2fSANER.2016.34&partnerID=40&md5=f54e924df9c77de6c447aa4fabcbcaba,10.1109/SANER.2016.34,"Defect prediction aims to estimate software reliability via learning from historical defect data. A defect prediction method identifies whether a software module is defect-prone or not according to metrics that are mined from software projects. These metric values, also known as features, may involve irrelevance and redundancy, which will hurt the performance of defect prediction methods. Existing work employs feature selection to preprocess defect data to filter out useless features. In this paper, we propose a novel feature selection framework, MICHAC, short for defect prediction via Maximal Information Coefficient with Hierarchical Agglomerative Clustering. MICHAC consists of two major stages. First, MICHAC employs maximal information coefficient to rank candidate features to filter out irrelevant ones; second, MICHAC groups features with hierarchical agglomerative clustering and selects one feature from each resulted group to remove redundant features. We evaluate our proposed method on 11 widelystudied NASA projects and four open-source AEEEM projects using three different classifiers with four performance metrics (precision, recall, F-measure, and AUC). Comparison with five existing methods demonstrates that MICHAC is effective in selecting features in defect prediction. © 2016 IEEE.",Defect prediction; Feature selection; Maximal information coefficient,"370, 381",,"2016 IEEE 23rd International Conference on Software Analysis, Evolution, and Reengineering, SANER 2016",Conference Paper,Scopus
1066,,Automatically learning semantic features for defect prediction,"Wang S., Liu T., Tan L.",2016,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971385322&doi=10.1145%2f2884781.2884804&partnerID=40&md5=6ce4bb6f1752563bff564db00236fe9f,10.1145/2884781.2884804,"Software defect prediction, which predicts defective code regions, can help developers find bugs and prioritize their testing efforts. To build accurate prediction models, previous studies focus on manually designing features that encode the characteristics of programs and exploring different machine learning algorithms. Existing traditional features often fail to capture the semantic differences of programs, and such a capability is needed for building accurate prediction models. To bridge the gap between programs' semantics and defect prediction features, this paper proposes to leverage a powerful representation-learning algorithm, deep learning, to learn semantic representation of programs automatically from source code. Specifically, we leverage Deep Belief Network (DBN) to automatically learn semantic features from token vectors extracted from programs' Abstract Syntax Trees (ASTs). Our evaluation on ten open source projects shows that our automatically learned semantic features significantly improve both within-project defect prediction (WPDP) and cross-project defect prediction (CPDP) compared to traditional features. Our semantic features improve WPDP on average by 14.7% in precision, 11.5% in recall, and 14.2% in F1. For CPDP, our semantic features based approach outperforms the state-of-the-art technique TCA+ with traditional features by 8.9% in F1. © 2016 ACM.",,"297, 308",,Proceedings - International Conference on Software Engineering,Conference Paper,Scopus
1067,,Cross-project defect prediction using a connectivity-based unsupervised classifier,"Zhang F., Zheng Q., Zou Y., Hassan A.E.",2016,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971384470&doi=10.1145%2f2884781.2884839&partnerID=40&md5=99fa47405d6c038892383a59fdd3652f,10.1145/2884781.2884839,"Defect prediction on projects with limited historical data has attracted great interest from both researchers and practitioners. Cross-project defect prediction has been the main area of progress by reusing classifiers from other projects. However, existing approaches require some degree of homogeneity (e.g., a similar distribution of metric values) between the training projects and the target project. Satisfying the homogeneity requirement often requires significant effort (currently a very active area of research). An unsupervised classifier does not require any training data, therefore the heterogeneity challenge is no longer an issue. In this paper, we examine two types of unsupervised classifiers: a) distance-based classifiers (e.g., k-means); and b) connectivity-based classifiers. While distance-based unsupervised classifiers have been previously used in the defect prediction literature with disappointing performance, connectivity-based classifiers have never been explored before in our community. We compare the performance of unsupervised classifiers versus supervised classifiers using data from 26 projects from three publicly available datasets (i.e., AEEEM, NASA, and PROMISE). In the cross-project setting, our proposed connectivity-based classifier (via spectral clustering) ranks as one of the top classifiers among five widely-used supervised classifiers (i.e., random forest, naive Bayes, logistic regression, decision tree, and logistic model tree) and five unsupervised classifiers (i.e., k-means, partition around medoids, fuzzy C-means, neural-gas, and spectral clustering). In the within-project setting (i.e., models are built and applied on the same project), our spectral classifier ranks in the second tier, while only random forest ranks in the first tier. Hence, connectivity-based unsupervised classifiers offer a viable solution for cross and within project defect predictions. © 2016 ACM.",,"309, 320",,Proceedings - International Conference on Software Engineering,Conference Paper,Scopus
1068,,Feature subset selection for instance filtering methods on cross-project defect prediction,"Porto F., Simao A.",2016,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988329636&partnerID=40&md5=9f2238a5dd3d605ba453c438de2cf6bc,,"The defect prediction models can be a good tool on organizing the project's test resources. However, not all companies maintain an appropriate set of historical defect data. In this case, the companies can build an appropriate dataset from known external projects. This approach, called Cross-project Defect Prediction (CPDP), solves the lack of defect data, although introduces heterogeneity on data. This heterogeneity can compromises the performance of CPDP models. Recently, filtering methods were proposed in order to decrease the heterogeneity of data by selecting the most similar instances from the training dataset. This similarity between instances is calculated based on the available features of the dataset. On this study, we propose that using only the most relevant features on this calculation can result in more accurate filtered datasets and better prediction performances. We present an empirical evaluation of different methods used for selecting the most relevant features. We evaluate different configurations of four Feature Selection (FS) methods and two metric subsets. We used 36 versions of 11 open source projects on experiments. The results indicate that the defect prediction performance can be improved by using the evaluated approach. In addition, we investigated which methods present better performances. The results do not indicate a method with general better performances, i.e., the most appropriate method for a project can vary according to the project characteristics.",Code metrics; Cross-project defect prediction; Feature selection; Instance filtering; Network metrics; Software quality assurance,"171, 184",,CIBSE 2016 - XIX Ibero-American Conference on Software Engineering,Conference Paper,Scopus
1069,,Cross-organism learning method to discover new gene functionalities,"Domeniconi G., Masseroli M., Moro G., Pinoli P.",2016,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960104134&doi=10.1016%2fj.cmpb.2015.12.002&partnerID=40&md5=74f43bf74b4d41d8db2969383b8d4458,10.1016/j.cmpb.2015.12.002,"Background: Knowledge of gene and protein functions is paramount for the understanding of physiological and pathological biological processes, as well as in the development of new drugs and therapies. Analyses for biomedical knowledge discovery greatly benefit from the availability of gene and protein functional feature descriptions expressed through controlled terminologies and ontologies, i.e., of gene and protein biomedical controlled annotations. In the last years, several databases of such annotations have become available; yet, these valuable annotations are incomplete, include errors and only some of them represent highly reliable human curated information. Computational techniques able to reliably predict new gene or protein annotations with an associated likelihood value are thus paramount. Methods: Here, we propose a novel cross-organisms learning approach to reliably predict new functionalities for the genes of an organism based on the known controlled annotations of the genes of another, evolutionarily related and better studied, organism. We leverage a new representation of the annotation discovery problem and a random perturbation of the available controlled annotations to allow the application of supervised algorithms to predict with good accuracy unknown gene annotations. Taking advantage of the numerous gene annotations available for a well-studied organism, our cross-organisms learning method creates and trains better prediction models, which can then be applied to predict new gene annotations of a target organism. Results: We tested and compared our method with the equivalent single organism approach on different gene annotation datasets of five evolutionarily related organisms (Homo sapiens, Mus musculus, Bos taurus, Gallus gallus and Dictyostelium discoideum). Results show both the usefulness of the perturbation method of available annotations for better prediction model training and a great improvement of the cross-organism models with respect to the single-organism ones, without influence of the evolutionary distance between the considered organisms. The generated ranked lists of reliably predicted annotations, which describe novel gene functionalities and have an associated likelihood value, are very valuable both to complement available annotations, for better coverage in biomedical knowledge discovery analyses, and to quicken the annotation curation process, by focusing it on the prioritized novel annotations predicted. © 2015 Elsevier Ireland Ltd.",Biomolecular annotation prediction; Data representation; Discrete matrix completion; Gene ontology; Knowledge discovery; Transfer learning,"20, 34",,Computer Methods and Programs in Biomedicine,Article,Scopus
1070,,CrossPare: A tool for benchmarking cross-project defect predictions,Herbold S.,2016,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964461200&doi=10.1109%2fASEW.2015.8&partnerID=40&md5=0e789b5db64144705bb014fbb382d6f9,10.1109/ASEW.2015.8,"During the last decade, many papers on defect prediction were published. One still for the most part unresolved issue are cross-project defect predictions. Here, the aim is to predict the defects of a project, with data from other projects. Many approaches were suggested and evaluated in recent years. However, due to the usage of different implementations and data sets, the comparison between the work is a hard task. Within this paper, we present the tool CrossPare. CrossPare is designed to facilitate benchmarks for cross-project defect predictions. The tool already implements many techniques proposed within the current state of the art of cross-project defect predictions. Moreover, the tool is able to load different data sets that are commonly used for the evaluation of techniques and supports all major performance metrics. Through the usage of CrossPare other reseachers can improve the comparability of their results and possibly also reduce their implementation efforts for new cross-project defect prediction techniques by reusing features already offered by CrossPare. © 2015 IEEE.",benchmark; cross-project; defect prediction,"90, 95",,"Proceedings - 2015 30th IEEE/ACM International Conference on Automated Software Engineering Workshops, ASEW 2015",Conference Paper,Scopus
1071,,Deep learning based large scale handwritten Devanagari character recognition,"Acharya S., Pant A.K., Gyawali P.K.",2016,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964589426&doi=10.1109%2fSKIMA.2015.7400041&partnerID=40&md5=135b6e8c2040f595c8d5fb7b803bffa9,10.1109/SKIMA.2015.7400041,"In this paper, we introduce a new public image dataset for Devanagari script: Devanagari Handwritten Character Dataset (DHCD). Our dataset consists of 92 thousand images of 46 different classes of characters of Devanagari script segmented from handwritten documents. We also explore the challenges in recognition of Devanagari characters. Along with the dataset, we also propose a deep learning architecture for recognition of those characters. Deep Convolutional Neural Network (CNN) have shown superior results to traditional shallow networks in many recognition tasks. Keeping distance with the regular approach of character recognition by Deep CNN, we focus the use of Dropout and dataset increment approach to improve test accuracy. By implementing these techniques in Deep CNN, we were able to increase test accuracy by nearly 1 percent. The proposed architecture scored highest test accuracy of 98.47% on our dataset. © 2015 IEEE.",Computer Vision; Deep Convolutional Neural Network; Deep learning; Devanagari Handwritten Character Dataset; Dropout; Image processing; Optical Character Recognition,,,"SKIMA 2015 - 9th International Conference on Software, Knowledge, Information Management and Applications",Conference Paper,Scopus
1072,,Value-cognitive boosting with a support vector machine for cross-project defect prediction,"Ryu D., Choi O., Baik J.",2016,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84955124239&doi=10.1007%2fs10664-014-9346-4&partnerID=40&md5=658f7b9337d996b0825c5bc6ad123d59,10.1007/s10664-014-9346-4,"It is well-known that software defect prediction is one of the most important tasks for software quality improvement. The use of defect predictors allows test engineers to focus on defective modules. Thereby testing resources can be allocated effectively and the quality assurance costs can be reduced. For within-project defect prediction (WPDP), there should be sufficient data within a company to train any prediction model. Without such local data, cross-project defect prediction (CPDP) is feasible since it uses data collected from similar projects in other companies. Software defect datasets have the class imbalance problem increasing the difficulty for the learner to predict defects. In addition, the impact of imbalanced data on the real performance of models can be hidden by the performance measures chosen. We investigate if the class imbalance learning can be beneficial for CPDP. In our approach, the asymmetric misclassification cost and the similarity weights obtained from distributional characteristics are closely associated to guide the appropriate resampling mechanism. We performed the effect size A-statistics test to evaluate the magnitude of the improvement. For the statistical significant test, we used Wilcoxon rank-sum test. The experimental results show that our approach can provide higher prediction performance than both the existing CPDP technique and the existing class imbalance technique. © 2014, Springer Science+Business Media New York.",Boosting; Class imbalance; Cross-project defect prediction; Transfer learning,"43, 71",,Empirical Software Engineering,Article,Scopus
1073,,CLAMI: Defect prediction on unlabeled datasets,"Nam J., Kim S.",2016,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963818075&doi=10.1109%2fASE.2015.56&partnerID=40&md5=8e44aad9d9cf4a0535891f3746c09f44,10.1109/ASE.2015.56,"Defect prediction on new projects or projects with limited historical data is an interesting problem in software engineering. This is largely because it is difficult to collect defect information to label a dataset for training a prediction model. Cross-project defect prediction (CPDP) has tried to address this problem by reusing prediction models built by other projects that have enough historical data. However, CPDP does not always build a strong prediction model because of the different distributions among datasets. Approaches for defect prediction on unlabeled datasets have also tried to address the problem by adopting unsupervised learning but it has one major limitation, the necessity for manual effort. In this study, we propose novel approaches, CLA and CLAMI, that show the potential for defect prediction on unlabeled datasets in an automated manner without need for manual effort. The key idea of the CLA and CLAMI approaches is to label an unlabeled dataset by using the magnitude of metric values. In our empirical study on seven open-source projects, the CLAMI approach led to the promising prediction performances, 0.636 and 0.723 in average f-measure and AUC, that are comparable to those of defect prediction based on supervised learning. © 2015 IEEE.",,"452, 463",,"Proceedings - 2015 30th IEEE/ACM International Conference on Automated Software Engineering, ASE 2015",Conference Paper,Scopus
1074,,Heterogeneous defect prediction via exploiting correlation subspace,"Cheng M., Wu G., Jiang M., Wan H., You G., Yuan M.",2016,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988446609&doi=10.18293%2fSEKE2016-090&partnerID=40&md5=f4367babf26df4b7cb45e710d3684fc5,10.18293/SEKE2016-090,"Software defect prediction generally builds models from intra-project data. Lack of training data at the early stage of software testing limits the efficiency of prediction in practice. Thereby researchers proposed cross-project defect prediction using the data from other projects. Most previous efforts assumed the cross-project defect data have the same metrics set which means the metrics used and size of metrics set are same in the data of projects. However, in real scenarios, this assumption may not hold. In addition, software defect datasets have the class imbalance problem increasing the difficulty for the learner to predict defects. In this paper, we advance canonical correlation analysis for deriving a joint feature space for associating crossproject data and propose a novel support vector machine algorithm which incorporates the correlation transfer information into classifier design for cross-project prediction. Moreover, we take different misclassification costs into consideration to make the classification inclining to classify a module as a defective one, alleviating the impact of imbalanced data. Experiments on public heterogeneous datasets from different projects show that our method is more effective, compared to state-of-the-art methods. Copyright © 2016 by KSI Research Inc. and Knowledge Systems Institute Graduate School.",Canonical correlation analysis; Class imbalance; Defect prediction; Heterogeneous metrics; Support vector machine,"171, 176",,"Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE",Conference Paper,Scopus
1075,,A multi-source tradaboost approach for cross-company defect prediction,"Yu X., Liu J., Fu M., Ma C., Nie G., Chen X.",2016,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988369891&doi=10.18293%2fSEKE2016-120&partnerID=40&md5=63c7711ef5da368a0a6033e76688055d,10.18293/SEKE2016-120,"Cross-company defect prediction (CCDP) is a practical way that trains a prediction model by exploiting one or multiple projects of a source company and then applies the model to target company. Unfortunately, larger irrelevant crosscompany (CC) data usually makes it difficult to build a prediction model with high performance. On the other hand, brute force leveraging of CC data poorly related to withincompany (WC) data may decrease the prediction model performance. To address such issues, this paper introduces Multi-Source TrAdaBoost algorithm, an effective transfer learning approach to perform CCDP. The core idea of our approach is that: 1) employ limited amount of labeled WC data to weaken the impact of irrelevant CC data; 2) import knowledge not from one but from multiple sources to avoid negative transfer. The experimental results indicate that: 1) our proposed approach achieves the best overall performance among all tested CCDP approaches; 2) only 10% labeled WC data is enough to achieve good performance of CCDP by using our proposed approach. Copyright © 2016 by KSI Research Inc. and Knowledge Systems Institute Graduate School.",Cross-company defect prediction; Multi-source TrAdaBoost; Software defect prediction; Transfer learning,"237, 242",,"Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE",Conference Paper,Scopus
1076,,Survey of static software defect prediction,"Chen X., Gu Q., Liu W.-S., Liu S.-L., Ni C.",2016,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975748199&doi=10.13328%2fj.cnki.jos.004923&partnerID=40&md5=f2433131aa8f9b784d82a7ca2c15afab,10.13328/j.cnki.jos.004923,"Static software defect prediction is an active research topic in the domain of software engineering data mining. The phases of the study include designing novel code or process metrics to characterize the faults in the program modules, constructing software defect prediction model based on the training data gathered after mining software historical repositories, using the trained model to predict potential defect-proneness of program modules. The research on software defect prediction can optimize the allocation of testing resources and improve the quality of software. This paper offers a systematic survey of existing research achievements of the domestic and foreign researchers in recent years. First, a research framework is proposed and three key factors (i.e., metrics, model construction approaches, and issues in datasets) influencing the performance of defect prediction are identified. Next, existing research achievements in these three key factors are discussed in sequence. Then, the existing achievements on a special defect prediction issues (i.e., code change based defect prediction) are summarized. Finally a perspective of the future work in this research area is discussed. © Copyright 2016, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Data preprocessing; Machine learning; Software defect prediction; Software metrics; Software quality assurance,"1, 25",,Ruan Jian Xue Bao/Journal of Software,Article,Scopus
1077,,Optimization of land suitability for food crops using neural network and swarm optimization algorithm,"Layona R., Suharjito, Tunardi Y., Tanoto D.F.",2016,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964285508&doi=10.15866%2firecos.v11i1.7535&partnerID=40&md5=950fca1c7840ed729be3ba29cf473939,10.15866/irecos.v11i1.7535,"Land quality and suitability are factors that affect food crop productivity. Inappropriate or unproductive land will impact to productivity decrease, time-consuming, and profit lost. Determining and evaluating the optimal land suitability can be done through prediction of each production area that is obtained by analyzing land fitness data. This study proposes the implementation of Neural Network and Swarm Optimization Algorithm (Particle Swarm Optimization & Cat Swarm Optimization) to obtain prediction of production. To evaluate the proposed method, this study performed comparative evaluation based on the Mean Square Error (MSE) and accuracy. Based on the experimental results, Cat Swarm Optimization produces minimum error equal of 0.00439 for training phase and 0.10453 for testing phase. In training phase, Cat Swarm Optimization produces higher accuracy (93%) than Particle Swarm Optimization (67%). © 2016 Praise Worthy Prize S.r.l. - All rights reserved.",Cat swarm optimization; Land suitability; Neural network; Particle swarm optimization; Swarm optimization algorithm,"1, 7",,International Review on Computers and Software,Article,Scopus
1078,,How to Make Best Use of Cross-Company Data for Web Effort Estimation?,"Minku L., Sarro F., Mendes E., Ferrucci F.",2015,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961620838&doi=10.1109%2fESEM.2015.7321199&partnerID=40&md5=6434cdaab8d576281bb5a93e85844120,10.1109/ESEM.2015.7321199,"[Context]: The numerous challenges that can hinder software companies from gathering their own data have motivated over the past 15 years research on the use of cross-company (CC) datasets for software effort prediction. Part of this research focused on Web effort prediction, given the large increase worldwide in the development of Web applications. Some of these studies indicate that it may be possible to achieve better performance using CC models if some strategy to make the CC data more similar to the within-company (WC) data is adopted. [Goal]: This study investigates the use of a recently proposed approach called Dycom to assess to what extent Web effort predictions obtained using CC datasets are effective in relation to the predictions obtained using WC data when explicitly mapping the CC models to the WC context. [Method]: Data on 125 Web projects from eight different companies part of the Tukutuku database were used to build prediction models. We benchmarked these models against baseline models (mean and median effort) and a WC base learner that does not benefit of the mapping. We also compared Dycom against a competitive CC approach from the literature (NN-filtering). We report a company-by- company analysis. [Results]: Dycom usually managed to achieve similar or better performance than a WC model while using only half of the WC training data. These results are also an improvement over previous studies that investigated the use of different strategies to adapt CC models to the WC data for Web effort estimation. [Conclusions]: We conclude that the use of Dycom for Web effort prediction is quite promising and in general supports previous results when applying Dycom to conventional software datasets. © 2015 IEEE.",Companies; Databases; Estimation; Optical wavelength conversion; Predictive models; Software; Training,"172, 181",,International Symposium on Empirical Software Engineering and Measurement,Conference Paper,Scopus
1079,,CLuster: Smart clustering of free-hand sketches on largeinteractive surfaces,"Perteneder F., Bresler M., Grossauer E.-M., Leong J., Haller M.",2015,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959299077&doi=10.1145%2f2807442.2807455&partnerID=40&md5=98eb63cb5c0aa57771a24defd954f572,10.1145/2807442.2807455,"Structuring and rearranging free-hand sketches on large interactive surfaces typically requires making multiple stroke selections. This can be both time-consuming and fatiguing in the absence of well-designed selection tools. Investigating the concept of automated clustering, we conducted a background study that highlighted the fact that people have varying perspectives on how elements in sketches can and should be grouped. In response to these diverse user expectations, we present cLuster, a flexible, domain-independent clustering approach for free-hand sketches. Our approach is designed to accept an initial user selection, which is then used to calculate a linear combination of pre-trained perspectives in real-time. The remaining elements are then clustered. An initial evaluation revealed that in many cases, only a few corrections were necessary to achieve the desired clustering results. Finally, we demonstrate the utility of our approach in a variety of application scenarios.",Cluster Analysis; Free-Hand Sketching; Segmentation,"37, 46",,UIST 2015 - Proceedings of the 28th Annual ACM Symposium on User Interface Software and Technology,Conference Paper,Scopus
1080,,Computational intelligence for big data analysis: Current status and future prospect,"Guo P., Wang K., Luo A.-L., Xue M.-Z.",2015,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84948139840&doi=10.13328%2fj.cnki.jos.004900&partnerID=40&md5=779cdec453527ff5cbcc840418966b5d,10.13328/j.cnki.jos.004900,"Big data and its real-world applications have attracted a lot of attention with the explosive growth of data volumes not only in the academic but also in industrial. Big data analysis aimed at mining the potential value of big data has become a popular research topic. Computational intelligence (CI) which is an important research direction of artificial intelligence and information science has been shown to be promising to solve complex problems in scientific research and engineering. CI techniques are expected to provide powerful tools for addressing challenges in big data analytics. This paper surveys the related CI techniques, analyzes the grand challenges brought forth by big data from big data analysis perspectives, and discusses the possible research directions in the future of the big data era. Further, it proposes to conduct the research of big data analysis on scientific big data such as astronomy big data. © Copyright 2015, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Astronomy big data; Big data; Big data analysis; Computational intelligence,"3010, 3025",,Ruan Jian Xue Bao/Journal of Software,Article,Scopus
1081,,Heterogeneous transductive transfer learning algorithm,"Yang L., Jing L.-P., Yu J.",2015,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84948138472&doi=10.13328%2fj.cnki.jos.004892&partnerID=40&md5=c4357444bdcd48bd5844d311053f3cd0,10.13328/j.cnki.jos.004892,"The lack of labeled data affects the performance in target domain. Fortunately, there are ample labeled data in some other related source domains. Transfer learning allows knowledge to be transferred from source domains to target domain. In real applications, such as text-image and cross-language transfer learning, the feature spaces of source and target domains are different, that is heterogeneous transfer learning. This paper focuses on heterogeneous transductive transfer learning (HTTL), an approach to improve the performance of unlabeled data in target domain by using some labeled data in heterogeneous source domains. Since the feature spaces of source domains and target domain are different, the key problem is to learn the mapping functions between the heterogeneous source domains and target domain. This paper proposes to learn the mapping functions by unsupervised matching in the different feature spaces. The data in source domains can be re-represented with the mapping functions and transferred to the target domain. Thus, in target domain, there are some labeled data which come from the source domains. Standard machine learning methods such as support vector machine can be used to train classifiers for predicting the labels of unlabeled data in target domain. Moreover, a probabilistic interpretation is derived to verify the robustness of the presented method over certain noises in the utility matrices. A sample complexity bound is given to indicate how many instances are needed to adequately find the mapping functions. The effectiveness of the proposed approach is verified by experiments on four real-world data sets. © Copyright 2015, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Heterogeneous feature space; Heterogeneous transfer learning; Mapping function; Transductive transfer learning,"2762, 2780",,Ruan Jian Xue Bao/Journal of Software,Article,Scopus
1082,,Using a serious game to complement CPR instruction in a nurse faculty,"Boada I., Rodriguez-Benitez A., Garcia-Gonzalez J.M., Olivet J., Carreras V., Sbert M.",2015,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944280046&doi=10.1016%2fj.cmpb.2015.08.006&partnerID=40&md5=d9b84772724550192bde6f31851bb02a,10.1016/j.cmpb.2015.08.006,"Cardiopulmonary resuscitation (CPR) is a first aid key survival technique used to stimulate breathing and keep blood flowing to the heart. Its effective administration can significantly increase the chances of survival for victims of cardiac arrest. LISSA is a serious game designed to complement CPR teaching and also to refresh CPR skills in an enjoyable way. The game presents an emergency situation in a 3D virtual environment and the player has to save the victim applying the CPR actions. In this paper, we describe LISSA and its evaluation in a population composed of 109 nursing undergraduate students enrolled in the Nursing degree of our university. To evaluate LISSA we performed a randomized controlled trial that compares the classical teaching methodology, composed of self-directed learning for theory plus laboratory sessions with a mannequin for practice, with the one that uses LISSA after self-directed learning for theory and before laboratory sessions with a mannequin. From our evaluation we observed that students using LISSA (Group 2 and 3) gave significantly better learning acquisition scores than those following traditional classes (Group 1). To evaluate the differences between students of these groups we performed a paired samples t-test between Group 1 and 2 (μ1=35, 67, μ2=47, 50 and p&lt;0.05) and between students of Group 1 and 3 (μ1=35, 67, μ3=50, 58 and p&lt;0.05). From these tests we observed that there are significant differences in both cases. We also evaluated student performance of main steps of CPR protocol. Students that use LISSA performed better than the ones that did not use it. © 2015 Elsevier Ireland Ltd.",Cardiopulmonary resuscitation; E-learning; Nursing informatics; Serious games,"282, 291",,Computer Methods and Programs in Biomedicine,Article,Scopus
1083,,Improving Cross-Project Defect Prediction Methods with Data Simplification,"Amasaki S., Kawata K., Yokogawa T.",2015,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958255916&doi=10.1109%2fSEAA.2015.25&partnerID=40&md5=9a8ddbd69855e0321a510b357ce1f062,10.1109/SEAA.2015.25,"Context: Cross-project defect prediction (CPDP) research has been popular and many CPDP methods were proposed. While these methods used cross-project data as is for their inputs, useless or noisy information in the cross-project data can cause the degradation of predictive and computation performance. Removing such information makes the cross-project data simple and it will affect the performance of CPDP methods. Objective: To identify and quantify the effects of the data simplification for CPDP methods. Method: We conducted experiments that compared the predictive performance between CPDP with and without the data simplification. We adopted a data simplification method based on an active learning method proposed for software effort estimation. The experiments adopted 44 versions of OSS projects, four prediction models, and two CPDP methods, namely, Burak-filter and cross-project selection. Results: The data simplification achieved significant improvement in predictive performance for the cross-project selection. It did not improve Burak-filter. Conclusion: The data simplification can be helpful for the cross-project selection in terms of predictive performance and size reduction of cross-project data. © 2015 IEEE.",cross-project defect prediction; data simplification,"96, 103",,"Proceedings - 41st Euromicro Conference on Software Engineering and Advanced Applications, SEAA 2015",Conference Paper,Scopus
1084,,A Reference Architecture for Social Media Intelligence Applications in the Cloud,"Addo I.D., Do D., Ge R., Ahamed S.I.",2015,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962175860&doi=10.1109%2fCOMPSAC.2015.128&partnerID=40&md5=1fe0072349716bcd053b63f8ee906fb0,10.1109/COMPSAC.2015.128,"As the social media upsurge of today continues to mount, opportunities to derive collective intelligence from online social networking (OSN) content sources are inevitably expected to grow. While enterprise organizations and research institutions make a dash for identifying rich insights and opportunities to tap into the millions of conversations and user profile relationships exposed by this new social-influenced big data phenomenon, architectural concerns regarding the storage and processing of large datasets unearthed by OSNs, along with performance, scalability, fault-tolerance, security, privacy, and high-availability solutions have become an area of concern for social media intelligence (SMI) solutions. In this literature, we present a reference architecture, for designing SMI solutions. In addition, we showcase two key case studies for SMI applications built on this architecture. Our selected case studies are focused on the analysis of User-Generated Content (i.e. With Sentiment Analysis in Twitter data) and Social Graph Influence (i.e. In a Facebook-influenced Movie Recommendations solution). We evaluate the 'goodness-of-fit' in applying our model to these case study solutions and present results from our performance evaluation of these cloud-hosted solutions across multiple cloud providers like Amazon AWS, Microsoft Azure and Google Cloud. © 2015 IEEE.",Big Data Analysis; Cloud Architecture; Collective Intelligence; Reference Architecture; Social Data Science; Social Insights; Social Media Intelligence,"906, 913",,Proceedings - International Computer Software and Applications Conference,Conference Paper,Scopus
1085,,Cross-Project Aging Related Bug Prediction,"Qin F., Zheng Z., Bai C., Qiao Y., Zhang Z., Chen C.",2015,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962124868&doi=10.1109%2fQRS.2015.17&partnerID=40&md5=8916a73fb52805ac5176d3c92d564c63,10.1109/QRS.2015.17,"In a long running system, software tends to encounter performance degradation and increasing failure rate during execution, which is called software aging. The bugs contributing to the phenomenon of software aging are defined as Aging Related Bugs (ARBs). Lots of manpower and economic costs will be saved if ARBs can be found in the testing phase. However, due to the low presence probability and reproducing difficulty of ARBs, it is usually hard to predict ARBs within a project. In this paper, we study whether and how ARBs can be located through cross-project prediction. We propose a transfer learning based aging related bug prediction approach (TLAP), which takes advantage of transfer learning to reduce the distribution difference between training sets and testing sets while preserving their data variance. Furthermore, in order to mitigate the severe class imbalance, class imbalance learning is conducted on the transferred latent space. Finally, we employ machine learning methods to handle the bug prediction tasks. The effectiveness of our approach is validated and evaluated by experiments on two real software systems. It indicates that after the processing of TLAP, the performance of ARB bug prediction can be dramatically improved. © 2015 IEEE.",aging related bug; bug prediction; ross-project; software aging; transfer learning,"43, 48",,"Proceedings - 2015 IEEE International Conference on Software Quality, Reliability and Security, QRS 2015",Conference Paper,Scopus
1086,,Heterogeneous cross-company defect prediction by unified metric representation and CCA-based transfer learning,"Jing X., Wu F., Dong X., Qi F., Xu B.",2015,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960419541&doi=10.1145%2f2786805.2786813&partnerID=40&md5=76ea74cc9958aeba858d2cb75708f0bd,10.1145/2786805.2786813,"Cross-company defect prediction (CCDP) learns a prediction model by using training data from one or multiple projects of a source company and then applies the model to the target company data. Existing CCDP methods are based on the assumption that the data of source and target companies should have the same software metrics. However, for CCDP, the source and target company data is usually heterogeneous, namely the metrics used and the size of metric set are different in the data of two companies. We call CCDP in this scenario as heterogeneous CCDP (HCCDP) task. In this paper, we aim to provide an effective solution for HCCDP. We propose a unified metric representation (UMR) for the data of source and target companies. The UMR consists of three types of metrics, i.e., the common metrics of the source and target companies, source-company specific metrics and target-company specific metrics. To construct UMR for source company data, the target-company specific metrics are set as zeros, while for UMR of the target company data, the source-company specific metrics are set as zeros. Based on the unified metric representation, we for the first time introduce canonical correlation analysis (CCA), an effective transfer learning method, into CCDP to make the data distributions of source and target companies similar. Experiments on 14 public heterogeneous datasets from four companies indicate that: 1) for HCCDP with partially different metrics, our approach significantly outperforms state-of-the-art CCDP methods; 2) for HCCDP with totally different metrics, our approach obtains comparable prediction performances in contrast with within-project prediction results. The proposed approach is effective for HCCDP. © 2015 ACM.",Canonical correlation analysis (cca); Common metrics; Company-specific metrics; Heterogeneous cross-company defect prediction (hccdp); Unified metric representation,"496, 507",,"2015 10th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering, ESEC/FSE 2015 - Proceedings",Conference Paper,Scopus
1087,,Heterogeneous defect prediction,"Nam J., Kim S.",2015,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960392256&doi=10.1145%2f2786805.2786814&partnerID=40&md5=e49e90809e8bc6c2b5436d9239cf43d2,10.1145/2786805.2786814,"Software defect prediction is one of the most active research areas in software engineering. We can build a prediction model with defect data collected from a software project and predict defects in the same project, i.e. within-project defect prediction (WPDP). Researchers also proposed crossproject defect prediction (CPDP) to predict defects for new projects lacking in defect data by using prediction models built by other projects. In recent studies, CPDP is proved to be feasible. However, CPDP requires projects that have the same metric set, meaning the metric sets should be identical between projects. As a result, current techniques for CPDP are difficult to apply across projects with heterogeneous metric sets. To address the limitation, we propose heterogeneous defect prediction (HDP) to predict defects across projects with heterogeneous metric sets. Our HDP approach conducts metric selection and metric matching to build a prediction model between projects with heterogeneous metric sets. Our empirical study on 28 subjects shows that about 68% of predictions using our approach outperform or are comparable to WPDP with statistical significance. © 2015 ACM.",Defect prediction; Heterogeneous metrics; Quality assurance,"508, 519",,"2015 10th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering, ESEC/FSE 2015 - Proceedings",Conference Paper,Scopus
1088,,LACE2: Better privacy-preserving data sharing for cross project defect prediction,"Peters F., Menzies T., Layman L.",2015,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951868301&doi=10.1109%2fICSE.2015.92&partnerID=40&md5=a927b15f1141a8c455dcdf3280b172e8,10.1109/ICSE.2015.92,"Before a community can learn general principles, it must share individual experiences. Data sharing is the fundamental step of cross project defect prediction, i.e. the process of using data from one project to predict for defects in another. Prior work on secure data sharing allowed data owners to share their data on a single-party basis for defect prediction via data minimization and obfuscation. However the studied method did not consider that bigger data required the data owner to share more of their data. In this paper, we extend previous work with LACE2 which reduces the amount of data shared by using multi-party data sharing. Here data owners incrementally add data to a cache passed among them and contribute ""interesting"" data that are not similar to the current content of the cache. Also, before data owner i passes the cache to data owner j, privacy is preserved by applying obfuscation algorithms to hide project details. The experiments of this paper show that (a) LACE2 is comparatively less expensive than the single-party approach and (b) the multiparty approach of LACE2 yields higher privacy than the prior approach without damaging predictive efficacy (indeed, in some cases, LACE2 leads to better defect predictors). © 2015 IEEE.",,"801, 811",,Proceedings - International Conference on Software Engineering,Conference Paper,Scopus
1089,,Software Risk Modeling by Clustering Project Metrics,Chang C.-P.,2015,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945941623&doi=10.1142%2fS0218194015500175&partnerID=40&md5=0eccc6efedc77e0545a8427f7dec3fb6,10.1142/S0218194015500175,"Together with the development and integration of software technologies, an increase in the complexity of the software development environment has made identifying software risks challenging. Identifying software risks, which is a critical activity in project management, is challenging because numerous factors may affect software projects. In this paper, an approach to identify software-risk items is proposed in which data collected from past software projects are mined to construct software-risk models. The prediction models obtained can be used to identify potential software risks for subsequent software projects. The advantage of the proposed approach is that the software-risk models can be constructed at an early stage in software projects to facilitate the planning of methods to mitigate software risks. The proposed approach is applied to a business project to demonstrate how software risk items can be identified. © 2015 World Scientific Publishing Company.",clustering; data mining; Software process; software risk,"1053, 1076",,International Journal of Software Engineering and Knowledge Engineering,Article,Scopus
1090,,Local linear coding based on riemannian kernel,"Jiang W., Bi T.-T., Li K.-Q., Yang B.-R.",2015,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938345114&doi=10.13328%2fj.cnki.jos.004714&partnerID=40&md5=ed68c5985888da13f99a247e41062f33,10.13328/j.cnki.jos.004714,"Recent research has shown that better recognition performance can be attained through representing symmetric positive definite matrices as points on Riemannian manifolds for many computer vision tasks. However, most existing algorithms only approximate the Riemannian manifold locally by its tangent space and are incapable of scaling effectively distribution of samples. Inspired by kernel methods, a novel method, called local linear coding based on Riemannian kernel (LLCRK), is proposed and applied successfully to vision classification issues. Firstly, with the aid of recently introduced Riemannian kernel, symmetric positive definite matrices are mapped into the reproducing kernel Hilbert space by kernel method and a mathematical model of sparse coding and Riemannian dictionary learning is constructed by local linear coding theory. Secondly, an efficient algorithm of LLCRK is presented for dictionary learning according to the convex optimization methods. Finally, an iterative updating algorithm is constructed to optimize the objective function, and the test samples are classified by nearest neighbor classifier. Experimental results on three visual classification data sets demonstrate that the proposed algorithm achieves considerable improvement in discrimination accuracy. © Copyright 2015, Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Locality-constrained linear coding; Riemannian manifold; Sparse representation; Symmetric positive definite matrix; Tangent space,"1812, 1823",,Ruan Jian Xue Bao/Journal of Software,Article,Scopus
1091,,Transfer learning in effort estimation,"Kocaguneli E., Menzies T., Mendes E.",2015,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929522024&doi=10.1007%2fs10664-014-9300-5&partnerID=40&md5=5081f1dda9dbe21c2af1c92c8c3005d0,10.1007/s10664-014-9300-5,"When projects lack sufficient local data to make predictions, they try to transfer information from other projects. How can we best support this process? In the field of software engineering, transfer learning has been shown to be effective for defect prediction. This paper checks whether it is possible to build transfer learners for software effort estimation. We use data on 154 projects from 2 sources to investigate transfer learning between different time intervals and 195 projects from 51 sources to provide evidence on the value of transfer learning for traditional cross-company learning problems. We find that the same transfer learning method can be useful for transfer effort estimation results for the cross-company learning problem and the cross-time learning problem. It is misguided to think that: (1) Old data of an organization is irrelevant to current context or (2) data of another organization cannot be used for local solutions. Transfer learning is a promising research direction that transfers relevant cross data between time intervals and domains. © 2014, Springer Science+Business Media New York.",Data mining; Effort estimation; k-NN; Transfer learning,"813, 843",,Empirical Software Engineering,Article,Scopus
1092,,Sparse label propagation: a robust domain adaptation learning method,"Tao J.-W., Chung F.-L., Wang S.-T., Yao Q.-F.",2015,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930068489&doi=10.13328%2fj.cnki.jos.004575&partnerID=40&md5=a88e34b9ee10f96a176428310c4712d3,10.13328/j.cnki.jos.004575,"Sparse representation has received an increasing amount of interest in pattern classification due to its robustness. In this paper, a domain adaptation learning (DAL) approach is explored based on a sparsity preserving model, which assumes that each data point can be sparsely reconstructed. The proposed robust DAL algorithm, called sparse label propagation domain adaptation learning (SLPDAL), propagates the labels from labeled points in the source domain to the unlabeled dataset in the target domain using those sparsely reconstructed objects with sufficient smoothness. SLPDAL consists of three steps. First, it finds an optimal kernel space in which all samples from both source and target domains can be embedded by minimizing the mean discrepancy between these two domains. Then, it computes the best kernel sparse reconstructed coefficients for each data point in the kernel space by using l1-norm minimization. Finally, it propagates the labels of source domain to the target domain by preserving the kernel sparse reconstructed coefficients. The paper also derives an easy way to extend SLPDAL to out-of-sample data and multiple kernel learning respectively. Promising experimental results have been obtained for several DAL problems such as face recognition, visual video detection and text classification tasks. © Copyright 2015, Institute of Software, the Chinese Academy of Sciences. All right reserved.",Domain adaptation learning; Label propagation; Maximum mean discrepancy; Multiple kernel learning; Sparse representation,"977, 1000",,Ruan Jian Xue Bao/Journal of Software,Article,Scopus
1093,,Cross-project build co-change prediction,"Xia X., Lo D., Mcintosh S., Shihab E., Hassan A.E.",2015,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928670632&doi=10.1109%2fSANER.2015.7081841&partnerID=40&md5=0944b50c71bcb44fc511c27aae139471,10.1109/SANER.2015.7081841,"Build systems orchestrate how human-readable source code is translated into executable programs. In a software project, source code changes can induce changes in the build system (aka. build co-changes). It is difficult for developers to identify when build co-changes are necessary due to the complexity of build systems. Prediction of build co-changes works well if there is a sufficient amount of training data to build a model. However, in practice, for new projects, there exists a limited number of changes. Using training data from other projects to predict the build co-changes in a new project can help improve the performance of the build co-change prediction. We refer to this problem as cross-project build co-change prediction. In this paper, we propose CroBuild, a novel cross-project build co-change prediction approach that iteratively learns new classifiers. CroBuild constructs an ensemble of classifiers by iteratively building classifiers and assigning them weights according to its prediction error rate. Given that only a small proportion of code changes are build co-changing, we also propose an imbalance-aware approach that learns a threshold boundary between those code changes that are build co-changing and those that are not in order to construct classifiers in each iteration. To examine the benefits of CroBuild, we perform experiments on 4 large datasets including Mozilla, Eclipse-core, Lucene, and Jazz, comprising a total of 50,884 changes. On average, across the 4 datasets, CroBuild achieves a F1-score of up to 0.408. We also compare CroBuild with other approaches such as a basic model, AdaBoost proposed by Freund et al., and TrAdaBoost proposed by Dai et al. On average, across the 4 datasets, the CroBuild approach yields an improvement in F1-scores of 41.54%, 36.63%, and 36.97% over the basic model, AdaBoost, and TrAdaBoost, respectively. © 2015 IEEE.",Build Co-change Prediction; Cross-project; Imbalance Data; Transfer Learning,"311, 320",,"2015 IEEE 22nd International Conference on Software Analysis, Evolution, and Reengineering, SANER 2015 - Proceedings",Conference Paper,Scopus
1094,,Enhancing medical named entity recognition with an extended segment representation technique,"Keretna S., Lim C.P., Creighton D., Shaban K.B.",2015,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926208562&doi=10.1016%2fj.cmpb.2015.02.007&partnerID=40&md5=95a12eb0fa893ca6bf97e330973e8e96,10.1016/j.cmpb.2015.02.007,"Objective: The objective of this paper is to formulate an extended segment representation (SR) technique to enhance named entity recognition (NER) in medical applications. Methods: An extension to the IOBES (Inside/Outside/Begin/End/Single) SR technique is formulated. In the proposed extension, a new class is assigned to words that do not belong to a named entity (NE) in one context but appear as an NE in other contexts. Ambiguity in such cases can negatively affect the results of classification-based NER techniques. Assigning a separate class to words that can potentially cause ambiguity in NER allows a classifier to detect NEs more accurately; therefore increasing classification accuracy. Results: The proposed SR technique is evaluated using the i2b2 2010 medical challenge data set with eight different classifiers. Each classifier is trained separately to extract three different medical NEs, namely treatment, problem, and test. From the three experimental results, the extended SR technique is able to improve the average F1-measure results pertaining to seven out of eight classifiers. The kNN classifier shows an average reduction of 0.18% across three experiments, while the C4.5 classifier records an average improvement of 9.33%. © 2015 Elsevier Ireland Ltd.",Biomedical text annotation; Biomedical text mining; Information extraction; Natural language processing; Unstructured electronic medical records,"88, 100",,Computer Methods and Programs in Biomedicine,Article,Scopus
1095,,An empirical study on software defect prediction with a simplified metric set,"He P., Li B., Liu X., Chen J., Ma Y.",2015,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921027598&doi=10.1016%2fj.infsof.2014.11.006&partnerID=40&md5=8d9c3dd0f911ea02f763bf4d5b6480e0,10.1016/j.infsof.2014.11.006,"Context Software defect prediction plays a crucial role in estimating the most defect-prone components of software, and a large number of studies have pursued improving prediction accuracy within a project or across projects. However, the rules for making an appropriate decision between within- and cross-project defect prediction when available historical data are insufficient remain unclear. Objective The objective of this work is to validate the feasibility of the predictor built with a simplified metric set for software defect prediction in different scenarios, and to investigate practical guidelines for the choice of training data, classifier and metric subset of a given project. Method First, based on six typical classifiers, three types of predictors using the size of software metric set were constructed in three scenarios. Then, we validated the acceptable performance of the predictor based on Top-k metrics in terms of statistical methods. Finally, we attempted to minimize the Top-k metric subset by removing redundant metrics, and we tested the stability of such a minimum metric subset with one-way ANOVA tests. Results The study has been conducted on 34 releases of 10 open-source projects available at the PROMISE repository. The findings indicate that the predictors built with either Top-k metrics or the minimum metric subset can provide an acceptable result compared with benchmark predictors. The guideline for choosing a suitable simplified metric set in different scenarios is presented in Table 12. Conclusion The experimental results indicate that (1) the choice of training data for defect prediction should depend on the specific requirement of accuracy; (2) the predictor built with a simplified metric set works well and is very useful in case limited resources are supplied; (3) simple classifiers (e.g., Naïve Bayes) also tend to perform well when using a simplified metric set for defect prediction; and (4) in several cases, the minimum metric subset can be identified to facilitate the procedure of general defect prediction with acceptable loss of prediction precision in practice. © 2014 Elsevier B.V. All rights reserved.",Defect prediction; Metric set simplification; Software metrics; Software quality,"170, 190",,Information and Software Technology,Article,Scopus
1096,,Software defect prediction using ensemble learning on selected features,"Laradji I.H., Alshayeb M., Ghouti L.",2015,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84914106409&doi=10.1016%2fj.infsof.2014.07.005&partnerID=40&md5=b57dcaeee29a9a9a316339816f2257f6,10.1016/j.infsof.2014.07.005,"Context: Several issues hinder software defect data including redundancy, correlation, feature irrelevance and missing samples. It is also hard to ensure balanced distribution between data pertaining to defective and non-defective software. In most experimental cases, data related to the latter software class is dominantly present in the dataset. Objective: The objectives of this paper are to demonstrate the positive effects of combining feature selection and ensemble learning on the performance of defect classification. Along with efficient feature selection, a new two-variant (with and without feature selection) ensemble learning algorithm is proposed to provide robustness to both data imbalance and feature redundancy. Method: We carefully combine selected ensemble learning models with efficient feature selection to address these issues and mitigate their effects on the defect classification performance. Results: Forward selection showed that only few features contribute to high area under the receiver-operating curve (AUC). On the tested datasets, greedy forward selection (GFS) method outperformed other feature selection techniques such as Pearson's correlation. This suggests that features are highly unstable. However, ensemble learners like random forests and the proposed algorithm, average probability ensemble (APE), are not as affected by poor features as in the case of weighted support vector machines (W-SVMs). Moreover, the APE model combined with greedy forward selection (enhanced APE) achieved AUC values of approximately 1.0 for the NASA datasets: PC2, PC4, and MC1. Conclusion: This paper shows that features of a software dataset must be carefully selected for accurate classification of defective components. Furthermore, tackling the software data issues, mentioned above, with the proposed combined learning model resulted in remarkable classification performance paving the way for successful quality control. © 2014 Elsevier B.V. All rights reserved.",Data imbalance; Defect prediction; Ensemble learning; Feature redundancy/correlation; Feature selection; Software quality,"388, 402",,Information and Software Technology,Article,Scopus
1097,,Using Bayesian regression and EM algorithm with missing handling for software effort prediction,"Zhang W., Yang Y., Wang Q.",2015,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84913590122&doi=10.1016%2fj.infsof.2014.10.005&partnerID=40&md5=506b34f7a1b7e9d4fbd10615e5939f6d,10.1016/j.infsof.2014.10.005,"Context: Although independent imputation techniques are comprehensively studied in software effort prediction, there are few studies on embedded methods in dealing with missing data in software effort prediction. Objective: We propose BREM (Bayesian Regression and Expectation Maximization) algorithm for software effort prediction and two embedded strategies to handle missing data. Method: The MDT (Missing Data Toleration) strategy ignores the missing data when using BREM for software effort prediction and the MDI (Missing Data Imputation) strategy uses observed data to impute missing data in an iterative manner while elaborating the predictive model. Results: Experiments on the ISBSG and CSBSG datasets demonstrate that when there are no missing values in historical dataset, BREM outperforms LR (Linear Regression), BR (Bayesian Regression), SVR (Support Vector Regression) and M5′ regression tree in software effort prediction on the condition that the test set is not greater than 30% of the whole historical dataset for ISBSG dataset and 25% of the whole historical dataset for CSBSG dataset. When there are missing values in historical datasets, BREM with the MDT and MDI strategies significantly outperforms those independent imputation techniques, including MI, BMI, CMI, MINI and M5′. Moreover, the MDI strategy provides BREM with more accurate imputation for the missing values than those given by the independent missing imputation techniques on the condition that the level of missing data in training set is not larger than 10% for both ISBSG and CSBSG datasets. Conclusion: The experimental results suggest that BREM is promising in software effort prediction. When there are missing values, the MDI strategy is preferred to be embedded with BREM. © 2014 Elsevier B.V. All rights reserved.",Bayesian regression; EM algorithm; Missing imputation; Software effort prediction,"58, 70",,Information and Software Technology,Article,Scopus
1098,,An empirical study on predicting defect numbers,"Chen M., Ma Y.",2015,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969752678&doi=10.18293%2fSEKE2015-132&partnerID=40&md5=1857857b989e2ddfbc88b84fd534d60a,10.18293/SEKE2015-132,"Defect prediction is an important activity to make software testing processes more targeted and efficient. Many methods have been proposed to predict the defect-proneness of software components using supervised classification techniques in within- and cross-project scenarios. However, very few prior studies address the above issue from the perspective of predictive analytics. How to make an appropriate decision among different prediction approaches in a given scenario remains unclear. In this paper, we empirically investigate the feasibility of defect numbers prediction with typical regression models in different scenarios. The experiments on six open-source software projects in PROMISE repository show that the prediction model built with Decision Tree Regression seems to be the best estimator in both of the scenarios, and that for all the prediction models, the results yielded in the cross-project scenario can be comparable to (or sometimes better than) those in the within-project scenario when choosing suitable training data. Therefore, the findings provide a useful insight into defect numbers prediction for those new and inactive projects.",Cross-project scenario; Defect prediction; Predictive analytics; Regression model,"397, 402",,"Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE",Conference Paper,Scopus
1099,,SMOTE with distance threshold for machine learning applied to real-time bidding for online advertising,"McInroy B., Feng W., Hu G.",2015,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964047173&partnerID=40&md5=387e104cdaae16dca38d3c5bbf1d1af2,,"Online advertising has become a de facto marketing tool for businesses and competition for ad space on providers' sites is getting intense. Various methodologies have been developed for successful real time bidding on ad space using the customers' browsing history and behavior data. One of the problems is that the data may be imbalanced caused by rare events and/or fraudulent events. The synthetic minority oversampling technique (SMOTE) is a commonly used method to address this imbalanced data problem. However, SMOTE may create misleading synthetic data points when the original dataset contains outliers. In this paper, we propose a modified SMOTE technique to overcome this problem by introducing a distance threshold to calculate the nearest neighbors in SMOTE.",Imbalanced data; Machine learning; Online advertising; Real-time bidding; SMOTE,"153, 158",,"24th International Conference on Software Engineering and Data Engineering, SEDE 2015",Conference Paper,Scopus
1100,,Negative samples reduction in cross-company software defects prediction,"Chen L., Fang B., Shang Z., Tang Y.",2015,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84932611698&doi=10.1016%2fj.infsof.2015.01.014&partnerID=40&md5=384b8dc11cabf5c02708836c7907da7b,10.1016/j.infsof.2015.01.014,"Context: Software defect prediction has been widely studied based on various machine-learning algorithms. Previous studies usually focus on within-company defects prediction (WCDP), but lack of training data in the early stages of software testing limits the efficiency of WCDP in practice. Thus, recent research has largely examined the cross-company defects prediction (CCDP) as an alternative solution. Objective: However, the gap of different distributions between cross-company (CC) data and withincompany (WC) data usually makes it difficult to build a high-quality CCDP model. In this paper, a novel algorithm named Double Transfer Boosting (DTB) is introduced to narrow this gap and improve the performance of CCDP by reducing negative samples in CC data. Method: The proposed DTB model integrates two levels of data transfer: first, the data gravitation method reshapes the whole distribution of CC data to fit WC data. Second, the transfer boosting method employs a small ratio of labeled WC data to eliminate negative instances in CC data. Results: The empirical evaluation was conducted based on 15 publicly available datasets. CCDP experiment results indicated that the proposed model achieved better overall performance than compared CCDP models. DTB was also compared to WCDP in two different situations. Statistical analysis suggested that DTB performed significantly better than WCDP models trained by limited samples and produced comparable results to WCDP with sufficient training data. Conclusions: DTB reforms the distribution of CC data from different levels to improve the performance of CCDP, and experimental results and analysis demonstrate that it could be an effective model for early software defects detection. © 2015 Elsevier B.V. All rights reserved.",Cross-company defects prediction; Software fault prediction; Transfer learning,"67, 77",,Information and Software Technology,Article,Scopus
1101,,Survey on transfer learning research,"Zhuang F.-Z., Luo P., He Q., Shi Z.-Z.",2015,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924797926&doi=10.13328%2fj.cnki.jos.004631&partnerID=40&md5=6d7a5e5c234f70e9d1116806da931456,10.13328/j.cnki.jos.004631,"In recent years, transfer learning has provoked vast amount of attention and research. Transfer learning is a new machine learning method that applies the knowledge from related but different domains to target domains. It relaxes the two basic assumptions in traditional machine learning: (1) the training (also referred as source domain) and test data (also referred target domain) follow the independent and identically distributed (i.i.d.) condition; (2) there are enough labeled samples to learn a good classification model, aiming to solve the problems that there are few or even not any labeled data in target domains. This paper surveys the research progress of transfer learning and introduces its own works, especially the ones in building transfer learning models by applying generative model on the concept level. Finally, the paper introduces the applications of transfer learning, such as text classification and collaborative filtering, and further suggests the future research direction of transfer learning. ©2015 ISCAS.",Concept learning; Generative model; Independent and identical distribution; Related domain; Transfer learning,"26, 39",,Ruan Jian Xue Bao/Journal of Software,Article,Scopus
1102,,Automated classification of software change messages by semi-supervised Latent Dirichlet Allocation,"Fu Y., Yan M., Zhang X., Xu L., Yang D., Kymer J.D.",2015,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922658213&doi=10.1016%2fj.infsof.2014.05.017&partnerID=40&md5=1f045dd2fca571d8b62f44b94a356a7e,10.1016/j.infsof.2014.05.017,"Context: Topic models such as probabilistic Latent Semantic Analysis (pLSA) and Latent Dirichlet Allocation (LDA) have demonstrated success in mining software repository tasks. Understanding software change messages described by the unstructured nature-language text is one of the fundamental challenges in mining these messages in repositories. Objective: We seek to present a novel automatic change message classification method characterized by semi-supervised topic semantic analysis. Method: In this work, we present a semi-supervised LDA based approach to automatically classify change messages. We use domain knowledge of software changes to make labeled samples which are added to build the semi-supervised LDA model. Next, we verify the cross-project analysis application of our method on three open-source projects. Our method has two advantages over existing software change classification methods: First of all, it mitigates the issue of how to set the appropriate number of latenttopics. We do not have to choose the number of latent topics in our method, because it corresponds to the number of class labels. Second, this approach utilizes the information provided by the label samples in the training set. Results: Our method automatically classified about 85% of the change messages in our experiment and our validation survey showed that 70.56% of the time our automatic classification results were in agreement with developer opinions. Conclusion: Our approach automatically classifies most of the change messages which record the cause of the software change and the method is applicable to cross-project analysis of software change messages. © 2014 Elsevier B.V. All rights reserved.",Change message; LDA; Semi-supervised topic modeling; Software repositories mining,"369, 377",,Information and Software Technology,Conference Paper,Scopus
1103,,Learning to combine multiple ranking metrics for fault localization,"Xuan J., Monperrus M.",2014,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84931094842&doi=10.1109%2fICSME.2014.41&partnerID=40&md5=51b108f987f7c5944858bff717dfc118,10.1109/ICSME.2014.41,"Fault localization is an inevitable step in software debugging. Spectrum-based fault localization consists in computing a ranking metric on execution traces to identify faulty source code. Existing empirical studies on fault localization show that there is no optimal ranking metric for all faults in practice. In this paper, we propose Multric, a learning-based approach to combining multiple ranking metrics for effective fault localization. In Multric, a suspiciousness score of a program entity is a combination of existing ranking metrics. Multric consists two major phases: learning and ranking. Based on training faults, Multric builds a ranking model by learning from pairs of faulty and non-faulty source code elements. When a new fault appears, Multric computes the final ranking with the learned model. Experiments are conducted on 5386 seeded faults in ten open-source Java programs. We empirically compare Multric against four widely-studied metrics and three recently-proposed one. Our experimental results show that Multric localizes faults more effectively than state-of-art metrics, such as Tarantula, Ochiai, and Ample. © 2014 IEEE.",Fault localization; learning to rank; multiple ranking metrics,"191, 200",,"Proceedings - 30th International Conference on Software Maintenance and Evolution, ICSME 2014",Conference Paper,Scopus
1104,,Device adaptive wireless signal feature extraction and localization method,"Gu Y., Jiang X.-L., Liu J.-F., Chen Y.-Q.",2014,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929617982&partnerID=40&md5=573a2f8bf2d376bf4e805eafa7b925ee,,"In recent years, research on Wi-Fi based indoor localization draws increasing attention. However, in practical applications, the localization error caused by device variance is a severe problem. In this paper, a new calibration-free and unsupervised method, SSDR (Signal Strength Difference Ratio) is proposed to solve this issue. Considering the signal variance between training devices and testing devices, SSDR first removes the linear effect of fingerprint to get new features. It then puts forward a distance calculation criterion with AP impact factor according to the effect of AP. Finally SSDR eliminates the variance of devices and realizes indoor localization based on the new features and distance calculation criterion. The experiment deployed in real indoor wireless environment shows, compared with traditional indoor localization methods, the proposed SSDR can increase the indoor localization accuracy by 10%~20%, which greatly improves the practical usability of indoor localization system. ©2014 ISCAS.",AP impact factor; Device variance; Feature fusion; Machine learning; Wi-Fi indoor localization,"12, 20",,Ruan Jian Xue Bao/Journal of Software,Article,Scopus
1105,,Efficient data encoding for convolutional neural network application,"Trinh H.-P., Duranton M., Paindavoine M.",2014,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921287138&doi=10.1145%2f2685394&partnerID=40&md5=4ae6f3a89e58804a3951613c05b9c1a2,10.1145/2685394,"This article presents an approximate data encoding scheme called Significant Position Encoding (SPE). The encoding allows efficient implementation of the recall phase (forward propagation pass) of Convolutional Neural Networks (CNN)-a typical Feed-Forward Neural Network.This implementation uses only7bits data representation and achieves almost the same classification performance compared with the initial network: on MNIST handwriting recognition task, using this data encoding scheme losses only 0.03% in terms of recognition rate (99.27% vs. 99.3%). In terms of storage, we achieve a 12.5% gain compared with an 8 bits fixed-point implementation of the same CNN. Moreover, this data encoding allows efficient implementation of processing unit thanks to the simplicity of scalar product operation-the principal operation in a Feed-Forward Neural Network. © 2014 ACM.",Canonical signed digit; Convolutional neural network; Significant Position Encoding,,,ACM Transactions on Architecture and Code Optimization,Article,Scopus
1106,,A novel deep model for image recognition,"Zhu M., Wu Y.",2014,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910082244&doi=10.1109%2fICSESS.2014.6933585&partnerID=40&md5=c0cac5bb1671a6fbb874b6b1b43d8760,10.1109/ICSESS.2014.6933585,"In this paper we propose a hybrid deep network for image recognition. First we use the sparse autoencoder(SAE) which is a method to extract high-level feature representations of data in an unsupervised way, without any manual feature engineering, and then we perform the classification using the deep belief networks(DBNs), which consist of restricted Boltzmann machine(RBM). Finally, we implement some comparative experiments on image datasets, and the results show that our methods achieved better performance when compared with neural network and other deep learning techniques such as DBNs. © 2014 IEEE.",deep belief network; image recognition; sparse autoencoder,"373, 376",,"Proceedings of the IEEE International Conference on Software Engineering and Service Sciences, ICSESS",Conference Paper,Scopus
1107,,Specific touch gesture on mobile devices to find attractive phrases in news browsing,"Ito S., Yoshida T., Harada F., Shimakawa H.",2014,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928628926&doi=10.1109%2fCOMPSAC.2014.74&partnerID=40&md5=6f6163e43d1c25dd8a4b831ced004805,10.1109/COMPSAC.2014.74,"When smart phone users browse web news articles, they encounter attractive phrases by chance. At that time, they try to obtain information on the smart phones. In order to search the information of the attractive phrases on web pages, they have to manipulate the smart phones to search on the small screen. It causes stresses because of situations such as manipulation errors. Such stresses could be eliminated if the attractive phrases can be identified and be input automatically in order to recommend the web pages with the information of the attractive phrases. Because of the small screens of smart phones, users move the news article area displayed on the screens with touch gestures such as swipe. The history of touch gestures during browsing an article implies the position and the timing users have focused on in the article. This paper proposes a method to identify the areas where attractive phrases have appeared on news articles, in order to enable automatic identification of attractive phrases. We have achieved real-time identification by utilizing the history of touch gestures during a web news browsing. The proposed method shows the history of touch gestures by a gesture trail. It is a graph showing the time series of the vertical coordinate of the displayed article area. When users encounter attractive phrases, they take certain patterns of touch gestures to confirm or read carefully the neighborhood of the attractive phrases. Thus, in the proposed method, the news article area including an attractive phrase is detected by matching the gesture trail in a sliding time window with a pattern obtained by pre-training. We use slow-down and resting patterns approximated by quadratic functions with the parameters defined for individual users. Experiments to identify time windows of attractive phrases on gesture trails has revealed that the highest and the lowest precision ratios are 0.579 and 0.278, respectively. © 2014 IEEE.",mobile device; recommendation; smartphone; touch gesture; web news,"519, 528",,Proceedings - International Computer Software and Applications Conference,Conference Paper,Scopus
1108,,Survey on big data system and analytic technology,"Cheng X.-Q., Jin X.-L., Wang Y.-Z., Guo J.-F., Zhang T.-Y., Li G.-J.",2014,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907666947&doi=10.13328%2fj.cnki.jos.004674&partnerID=40&md5=d12b9f9e6a155bd66b3e2ea312550fd9,10.13328/j.cnki.jos.004674,"This paper first introduces the key features of big data in different processing modes and their typical application scenarios, as well as corresponding representative processing systems. It then summarizes three development trends of big data processing systems. Next, the paper gives a brief survey on system supported analytic technologies and applications (including deep learning, knowledge computing, social computing, and visualization), and summarizes the key roles of individual technologies in big data analysis and understanding. Finally, the paper lays out three grand challenges of big data processing and analysis, i.e., data complexity, computation complexity, and system complexity. Potential ways for dealing with each complexity are also discussed. © Copyright 2014, Institute of Software, the Chinese Academy of Science. All Rights Reserved.",Data analysis; Deep learning; Dig data; Knowledge computing; Social computing; Visualization,"1889, 1908",,Ruan Jian Xue Bao/Journal of Software,Review,Scopus
1109,,How to make best use of cross-company data in software effort estimation?,"Minku L.L., Yao X.",2014,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994193527&doi=10.1145%2f2568225.2568228&partnerID=40&md5=eb4d5b86b6edf905dc71a1edf50a6253,10.1145/2568225.2568228,"Previous works using Cross-Company (CC) data for making Within-Company (WC) Software Effort Estimation (SEE) try to use CC data or models directly to provide predictions in the WC context. So, these data or models are only helpful when they match the WC context well. When they do not, a fair amount of WC training data, which are usually expensive to acquire, are still necessary to achieve good performance. We investigate how to make best use of CC data, so that we can reduce the amount of WC data while maintaining or improving performance in comparison to WC SEE models. This is done by proposing a new framework to learn the relationship between CC and WC projects explicitly, allowing CC models to be mapped to the WC context. Such mapped models can be useful even when the CC models themselves do not match the WC context directly. Our study shows that a new approach instantiating this framework is able not only to use substantially less WC data than a corresponding WC model, but also to achieve similar/better performance. This approach can also be used to provide insight into the behaviour of a company in comparison to others. © 2014 ACM.",cross-company learning; ensembles of learning machines; online learning; Software effort estimation; transfer learning,"446, 456",,Proceedings - International Conference on Software Engineering,Conference Paper,Scopus
1110,,Towards building a universal defect prediction model,"Zhang F., Mockus A., Keivanloo I., Zou Y.",2014,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938817878&doi=10.1145%2f2597073.2597078&partnerID=40&md5=3ac747513c804644e7fd8c10432e4db6,10.1145/2597073.2597078,"To predict files with defects, a suitable prediction model must be built for a software project from either itself (withinproject) or other projects (cross-project). A universal defect prediction model that is built from the entire set of diverse projects would relieve the need for building models for an individual project. A universal model could also be interpreted as a basic relationship between software metrics and defects. However, the variations in the distribution of predictors pose a formidable obstacle to build a universal model. Such variations exist among projects with different context factors (e.g., size and programming language). To overcome this challenge, we propose context-aware rank transformations for predictors. We cluster projects based on the similarity of the distribution of 26 predictors, and derive the rank transformations using quantiles of predictors for a cluster. We then fit the universal model on the transformed data of 1,398 open source projects hosted on SourceForge and GoogleCode. Adding context factors to the universal model improves the predictive power. The universal model obtains prediction performance comparable to the within-project models and yields similar results when applied on five external projects (one Apache and four Eclipse projects). These results suggest that a universal defect prediction model may be an achievable goal. Copyright 2014 ACM.",Bug; Context factors; Defect; Defect prediction; Large scale; Quality; Rank transformation; Universal defect prediction model,"182, 191",,"11th Working Conference on Mining Software Repositories, MSR 2014 - Proceedings",Conference Paper,Scopus
1111,,The moderating effect of organizational learning culture on individual motivation and ERP system assimilation at individual level,"Guo Y., Wang C., Feng Y.",2014,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894475561&doi=10.4304%2fjsw.9.2.365-373&partnerID=40&md5=19e78c6a58d4614f8ba959b090f1f6f2,10.4304/jsw.9.2.365-373,"This study contributes to the enterprise resource planning (ERP) system assimilation theory by providing an enriched understanding about how organizational learning culture influences end users' usage degree in post-implementation stage. Empirical data collected from 141 ERP users showed that both intrinsic motivation and perceived usefulness has positive impact on individual assimilation level of ERP systems. We further observe significant moderating effect of organizational learning culture on the association between individual motivations and individual assimilation level of ERP technology. Our findings highlight the importance of organizational learning culture and provide some guidelines for the firms to facilitate appropriate organizational culture, so as to enhance the intrinsic and extrinsic motivators of ERP users, promote the employees' fully utilization of the ERP software, and achieve business benefits with the assimilation of ERP systems. Theoretical and practical implications of the study are discussed along with its limitations. © 2014 ACADEMY PUBLISHER.",Enterprise resource planning (ERP); Individual level assimilation; Intrinsic motivation; Organizational learning culture; Perceived usefulness,"365, 373",,Journal of Software,Article,Scopus
1112,,E-learning education for academic literacy in computer-mediated communication,"Ha M.-J., Kim H.-C.",2014,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893835425&doi=10.14257%2fijseia.2014.8.1.10&partnerID=40&md5=79a4285e7856111d2dd0bf7a26cd4b38,10.14257/ijseia.2014.8.1.10,"Many educators are gravitating towards the use of learning management systems (LMSs), such as Blackboard, Daedalus Interchange, and Moodle, for managing courses and enhancing student learning. There is thus a growing need to examine second language (L2) learners' academic socialization through their participation in computer-mediated academic literacy practices. By employing a community of practice perspective, the present study was an attempt to demonstrate how learning management systems mediate L2 learners' academic discourse socialization and is closely related to issues of identities and learner agency. The study presented here contributes to the growing body of e-learning research to illustrate and explain the complex and dynamic ways that non-native novice students negotiated their academic participation in their graduate class. © 2014 SERSC.",Blackboard; Computer-mediated communication; E-learning; Learning management systems,"107, 118",,International Journal of Software Engineering and its Applications,Article,Scopus
1113,,Graphical models for probabilistic and causal reasoning,Pearl J.,2014,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054695138&doi=10.1201%2fb16812&partnerID=40&md5=270472662dda84ae8b1d691a0281b76b,10.1201/b16812,[No abstract available],,"44-1, 44-24",,"Computing Handbook, Third Edition: Computer Science and Software Engineering",Book Chapter,Scopus
1114,,Explanation-based learning,DeJong G.,2014,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054607841&doi=10.1201%2fb16812&partnerID=40&md5=437db336932ce72eaa2e4d64ee998013,10.1201/b16812,[No abstract available],,"37-1, 37-20",,"Computing Handbook, Third Edition: Computer Science and Software Engineering",Book Chapter,Scopus
1115,,A multistage algorithm for fricative spotting,"Ruinskiy D., Lavner Y.",2014,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904660614&doi=10.1109%2fPVC.2014.6845421&partnerID=40&md5=f7d5673d8be23c576d7b1fd946e72fbd,10.1109/PVC.2014.6845421,"We present an algorithm for spotting fricative consonants in continuous speech. Fricative spotting can be useful in professional audio applications, where excessive accentuation of these phonemes can degrade the aesthetics of voice recordings, or in applications for the hearing-impaired, where certain manipulations can increase their perception. All stages of our algorithm rely only on features extracted directly from the audio signal and on common classification techniques, making it simple to implement and language-invariant. In the first stage, a linear classifier, pre-trained using the Fisher's Linear Discriminant Analysis (LDA) method, is used to detect fricatives inside speech sentences. In the second stage, the detected phonemes are further analyzed using a decision-tree classifier, attempting to reject false detections. Tested on the full corpus of the TIMIT audio database the algorithm achieved very good detection rates across the entire range of fricative phonemes. © 2014 IEEE.",decision tree; Fricatives; Linear Discriminant Analysis; phoneme spotting,,,"Proceedings - 2014 22nd Annual Pacific Voice Conference - Voice Technology: Software, Hardware Applications, Bioengineering, Health and Performance, PVC 2014",Conference Paper,Scopus
1116,,Better cross company defect prediction,"Peters F., Menzies T., Marcus A.",2013,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889018963&doi=10.1109%2fMSR.2013.6624057&partnerID=40&md5=07b1ea23eb68e688b7a6ea80b446fdf3,10.1109/MSR.2013.6624057,"How can we find data for quality prediction? Early in the life cycle, projects may lack the data needed to build such predictors. Prior work assumed that relevant training data was found nearest to the local project. But is this the best approach? This paper introduces the Peters filter which is based on the following conjecture: When local data is scarce, more information exists in other projects. Accordingly, this filter selects training data via the structure of other projects. To assess the performance of the Peters filter, we compare it with two other approaches for quality prediction. Within-company learning and cross-company learning with the Burak filter (the state-of-the-art relevancy filter). This paper finds that: 1) within-company predictors are weak for small data-sets; 2) the Peters filter+cross-company builds better predictors than both within-company and the Burak filter+cross-company; and 3) the Peters filter builds 64% more useful predictors than both within-company and the Burak filter+cross-company approaches. Hence, we recommend the Peters filter for cross-company learning. © 2013 IEEE.",Cross company; Data mining; Defect prediction,"409, 418",,IEEE International Working Conference on Mining Software Repositories,Conference Paper,Scopus
1117,,Learning from open-source projects: An empirical study on defect prediction,"He Z., Peters F., Menzies T., Yang Y.",2013,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893274382&doi=10.1109%2fESEM.2013.20&partnerID=40&md5=fe464bf4b35d570054671995e0493070,10.1109/ESEM.2013.20,"The fundamental issue in cross project defect prediction is selecting the most appropriate training data for creating quality defect predictors. Another concern is whether historical data of open-source projects can be used to create quality predictors for proprietary projects from a practical point-of-view. Current studies have proposed statistical approaches to finding these training data, however, thus far no apparent effort has been made to study their success on proprietary data. Also these methods apply brute force techniques which are computationally expensive. In this work we introduce a novel data selection procedure which takes into account the similarities between the distribution of the test and potential training data. Additionally we use feature subset selection to increase the similarity between the test and training sets. Our procedure provides a comparable and scalable means of solving the cross project defect prediction problem for creating quality defect predictors. To evaluate our procedure we conducted empirical studies with comparisons to the within company defect prediction and a relevancy filtering method. We found that our proposed method performs relatively better than the filtering method in terms of both computation cost and prediction performance. © 2013 IEEE.",cross-project; data similarity; feature subset selection; instance selection; software defect prediction,"45, 54",,International Symposium on Empirical Software Engineering and Measurement,Conference Paper,Scopus
1118,,A modeling Toolbox and its applications to statistical process modeling,"Ko Y.-D., Shang H.",2013,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887308289&doi=10.4304%2fjsw.8.11.2706-2710&partnerID=40&md5=f25911a5140ba6ed028d51443c4985b1,10.4304/jsw.8.11.2706-2710,"Analyzing data that are measured or collected in processes requires statistical software. Most methods for analysis use a regression approach or its expanded forms such as polynomial and response surface models. However, the available commercial programs are inconvenient and costly for personal use, and require demanding pre-training in using them. A simple and user-friendly interface program adaptable to personal needs is therefore demanded. In this paper, a Matlab Toolbox, named the Regression Modeler (RM), is developed. The structure of the program and procedure of implementation are described. The program is verified using various datasets in process engineering. The toolbox provides users with solutions to regression models, polynomial models and response surface models with fine 2D and 3D plots. © 2013 ACADEMY PUBLISHER.",Matlab toolbox; Multiple regression model; Polynomial model; Regression model; Response surface model,"2706, 2710",,Journal of Software,Article,Scopus
1119,,Predicting more from less: Synergies of learning,"Kocaguneli E., Cukic B., Lu H.",2013,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886653917&doi=10.1109%2fRAISE.2013.6615203&partnerID=40&md5=05fc8a069b1252c9ff486a9ba1346318,10.1109/RAISE.2013.6615203,"Thanks to the ever increasing importance of project data, its collection has been one of the primary focuses of software organizations. Data collection activities have resulted in the availability of massive amounts of data through software data reposi-tories. This is great news for the predictive modeling research in software engineering. However, widely used supervised methods for predictive modeling require labeled data that is relevant to the local context of a project. This requirement cannot be met by many of the available data sets, introducing new challenges for software engineering research. How to transfer data between different contexts? How to handle insufficient number of labeled instances? In this position paper, we investigate synergies between different learning methods (transfer, semi-supervised and active learning) which may overcome these challenges. © 2013 IEEE.",,"42, 48",,"2013 2nd International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering, RAISE 2013 - Proceedings",Conference Paper,Scopus
1120,,Study on relational classification learning bound,"Wang X., Fang B.-X., Zhang H.-L., He H., Zhao L.",2013,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891341980&doi=10.3724%2fSP.J.1001.2013.04468&partnerID=40&md5=66da3d85f91e615d0e8a6387cf74dca8,10.3724/SP.J.1001.2013.04468,"Currently, there is some lack of knowledge about learning bound in relational classification model. In this study, some learning bounds for relational classification are proposed. First, two bounds are deduced for finite and infinite hypothesis space of the relational classification model respectively. Further, a complexity metric called relational dimension is proposed to measure the linking ability of the relational classification model. The relation between the complexity and growth function is proofed, and the learning bound for finite VC dimension and relational dimension is obtained. Afterwards, the condition of learnable, non-trivial, and the feasibility of the bound is analyzed. Finally, the learning progress of relational classification model based on Markov logic network is analyzed with some examples. The experimental result on a real dataset has demonstrated that the proposed bounds are useful in some practical problems. ©Copyright 2013, Institute of Software, the Chinese Academy of Sciences.",Learning bound; Relational classification; Statistical relational learning,"2508, 2521",,Ruan Jian Xue Bao/Journal of Software,Article,Scopus
1121,,"How, and why, process metrics are better","Rahman F., Devanbu P.",2013,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886390635&doi=10.1109%2fICSE.2013.6606589&partnerID=40&md5=88e89836a4d7aff05b0ff683190cc467,10.1109/ICSE.2013.6606589,"Defect prediction techniques could potentially help us to focus quality-assurance efforts on the most defect-prone files. Modern statistical tools make it very easy to quickly build and deploy prediction models. Software metrics are at the heart of prediction models; understanding how and especially why different types of metrics are effective is very important for successful model deployment. In this paper we analyze the applicability and efficacy of process and code metrics from several different perspectives. We build many prediction models across 85 releases of 12 large open source projects to address the performance, stability, portability and stasis of different sets of metrics. Our results suggest that code metrics, despite widespread use in the defect prediction literature, are generally less useful than process metrics for prediction. Second, we find that code metrics have high stasis; they don't change very much from release to release. This leads to stagnation in the prediction models, leading to the same files being repeatedly predicted as defective; unfortunately, these recurringly defective files turn out to be comparatively less defect-dense. © 2013 IEEE.",,"432, 441",,Proceedings - International Conference on Software Engineering,Conference Paper,Scopus
1122,,Support vector regression for large domain adaptation,"Xu M., Wang S.-T., Gu X., Yu L.",2013,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890250902&doi=10.3724%2fSP.J.1001.2013.04375&partnerID=40&md5=acdd9ad0a702e8fb77826785626a6c06,10.3724/SP.J.1001.2013.04375,"Incomplete data collection in regression analysis would lead to low prediction performance, which aises the issue of domain adaptation. It is well known that support vector regression (SVR) is equivalent to center-constrained minimum enclosing ball (CC-MEB). Also in solving the problem of how to effectively transfer the knowledge between the two fields, new theorems reveal that the difference between two probability distributions from two similar domains only depends on the centers of the two domains' minimum enclosing balls. Based on these developments, a fast adaptive-core vector regression (A-CVR) algorithm is proposed for large domain adaptation. The proposed algorithm uses the center of the source domain's CC-MEB to calibrate the center of the target domain's in order to improve the regression performance of the target domain. Experimental results show that the proposed domain adaptive algorithm can make up for the lack of data and greatly improve the performance of the target domain regression. © 2013 ISCAS.",Center-constrained minimum enclosing ball (CC-MEB); Core vector machine (CVM); Domain adaptation; Large data set; Support vector regression (SVR),"2312, 2326",,Ruan Jian Xue Bao/Journal of Software,Article,Scopus
1123,,Active Learning and effort estimation: Finding the essential content of software effort estimation data,"Kocaguneli E., Menzies T., Keung J., Cok D., Madachy R.",2013,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881032800&doi=10.1109%2fTSE.2012.88&partnerID=40&md5=d30ff2470a155bc0e52b8387d48b7a81,10.1109/TSE.2012.88,"Background: Do we always need complex methods for software effort estimation (SEE)? Aim: To characterize the essential content of SEE data, i.e., the least number of features and instances required to capture the information within SEE data. If the essential content is very small, then 1) the contained information must be very brief and 2) the value added of complex learning schemes must be minimal. Method: Our QUICK method computes the euclidean distance between rows (instances) and columns (features) of SEE data, then prunes synonyms (similar features) and outliers (distant instances), then assesses the reduced data by comparing predictions from 1) a simple learner using the reduced data and 2) a state-of-the-art learner (CART) using all data. Performance is measured using hold-out experiments and expressed in terms of mean and median MRE, MAR, PRED(25), MBRE, MIBRE, or MMER. Results: For 18 datasets, QUICK pruned 69 to 96 percent of the training data (median = 89 percent). ({K}=1) nearest neighbor predictions (in the reduced data) performed as well as CART's predictions (using all data). Conclusion: The essential content of some SEE datasets is very small. Complex estimation methods may be overelaborate for such datasets and can be simplified. We offer QUICK as an example of such a simpler SEE method. © 1976-2012 IEEE.",active learning; analogy; k-NN; Software cost estimation,"1040, 1053",,IEEE Transactions on Software Engineering,Article,Scopus
1124,,Comparing collaborative filtering methods based on user-topic ratings,"He T., Du X., Wang W., Chen Z., Liu J.",2013,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937721456&partnerID=40&md5=e84506ac9740967206997f0b8e1d50f6,,"User based collaborative filtering (CF) has been successfully applied into recommender system for years. The main idea of user based CF is to discover communities of users sharing similar interests. However, existing user based CF methods may be inaccurate due to the problem of data sparsity. One possible way to improve it is to append new data sources into user based CF. Tags which are added and generated by users is one of the new sources. In order to utilize tags effectively, user-topic based CF is proposed to extract features behind tags, assign them to topics, and measure users' preferences on these topics. In this paper, we conduct comparisons between two user-topic based CF methods based on different tag-topic relations. Both methods calculate user-topic preferences according to ratings of items and topic weights. Experiments are conducted on the data set of MovieLens. The results show that usertopic based CF method is better than user based CF both in computational efficiency and recommendation effect. The effects are significant especially when each tag belongs to multiple topics. Copyright © 2013 by Knowledge Systems Institute Graduate School.",Collaborative filtering; Recommender systems; Tag; Topic model,"312, 317",,"Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE",Conference Paper,Scopus
1125,,Transfer defect learning,"Nam J., Pan S.J., Kim S.",2013,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886378360&doi=10.1109%2fICSE.2013.6606584&partnerID=40&md5=4cb028aa369f1909438b53cc8e2523a7,10.1109/ICSE.2013.6606584,"Many software defect prediction approaches have been proposed and most are effective in within-project prediction settings. However, for new projects or projects with limited training data, it is desirable to learn a prediction model by using sufficient training data from existing source projects and then apply the model to some target projects (cross-project defect prediction). Unfortunately, the performance of cross-project defect prediction is generally poor, largely because of feature distribution differences between the source and target projects. In this paper, we apply a state-of-the-art transfer learning approach, TCA, to make feature distributions in source and target projects similar. In addition, we propose a novel transfer defect learning approach, TCA+, by extending TCA. Our experimental results for eight open-source projects show that TCA+ significantly improves cross-project prediction performance. © 2013 IEEE.",cross-project defect prediction; empirical software engineering; transfer learning,"382, 391",,Proceedings - International Conference on Software Engineering,Conference Paper,Scopus
1126,,"Recalling the ""imprecision"" of cross-project defect prediction","Rahman F., Posnett D., Devanbu P.",2012,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871348143&doi=10.1145%2f2393596.2393669&partnerID=40&md5=f5fab0b28deffc8355eb383a7b280080,10.1145/2393596.2393669,"There has been a great deal of interest in defect prediction: using prediction models trained on historical data to help focus quality-control resources in ongoing development. Since most new projects don't have historical data, there is interest in cross-project prediction: using data from one project to predict defects in another. Sadly, results in this area have largely been disheartening. Most experiments in cross-project defect prediction report poor performance, using the standard measures of precision, recall and F-score. We argue that these IR-based measures, while broadly applicable, are not as well suited for the quality-control settings in which defect prediction models are used. Specifically, these measures are taken at specific threshold settings (typically thresholds of the predicted probability of defectiveness returned by a logistic regression model). However, in practice, software quality control processes choose from a range of time-and-cost vs quality tradeoffs: how many files shall we test? how many shall we inspect? Thus, we argue that measures based on a variety of tradeoffs, viz., 5%, 10% or 20% of files tested/inspected would be more suitable. We study cross-project defect prediction from this perspective. We find that cross-project prediction performance is no worse than within-project performance, and substantially better than random prediction! © 2012 ACM.",empirical software engineering; fault prediction; inspection,,,"Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering, FSE 2012",Conference Paper,Scopus
1127,,Using brain-computer interfaces in an interactive multimedia application,"Wang A.I., Larsen E.A.",2012,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876529914&doi=10.2316%2fP.2012.790-046&partnerID=40&md5=5e421f01b509c46565a08a322b46fe8a,10.2316/P.2012.790-046,"The paper describes experiences from implementing a simple snake game, which can be controlled by the user's brainwaves using the NeuroSky mindset. The NeuroSky mindset is an inexpensive Brain-Computer Interface (BCI) device allowing developers to process EEG signals that can be used to control a computer. The BCI opens for new ways for humans to interact with computers, and can be used for many purposes such as aids for people with physical disabilities. A major challenge with inexpensive Brain-Computer Interfaces like the NeuroSky mindset is to discover which patterns of the brain signals that are sufficient accurate and reliable to be used to control a game, as well as can be used as real-time input of interactive multimedia applications. Our prototype incorporates all parts of a functioning BCI system, which includes acquiring the EEG signals, processing and classifying the EEG signals, and using the signal classification to control a game. The paper share experiences from implementing a BCI controlled game as well as results of testing the game on users. Our experiments found that in our prototype, the user can control the snake game using EEG signals with above 90% accuracy. Our solution differentiates from other appliances of the NeuroSky mindset that it does not require any mental pre-training for the user.",Brain-computer interface; Game technology; Graphical user interfaces; Multimedia systems; Real time systems,"266, 273",,"Proceedings of the IASTED International Conference on Software Engineering and Applications, SEA 2012",Conference Paper,Scopus
1128,,Boolean autoencoders and hypercube clustering complexity,Baldi P.,2012,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868350896&doi=10.1007%2fs10623-012-9719-x&partnerID=40&md5=f87f88fbcaf057748a16b66ead42ebc5,10.1007/s10623-012-9719-x,"We introduce and study the properties of Boolean autoencoder circuits. In particular, we show that the Boolean autoencoder circuit problem is equivalent to a clustering problem on the hypercube. We show that clustering m binary vectors on the n-dimensional hypercube into k clusters is NP-hard, as soon as the number of clusters scales like m ε(ε &gt; 0), and thus the general Boolean autoencoder problem is also NP-hard. We prove that the linear Boolean autoencoder circuit problem is also NP-hard, and so are several related problems such as: subspace identification over finite fields, linear regression over finite fields, even/odd set intersections, and parity circuits. The emerging picture is that autoencoder optimization is NP-hard in the general case, with a few notable exceptions including the linear cases over infinite fields or the Boolean case with fixed size hidden layer. However learning can be tackled by approximate algorithms, including alternate optimization, suggesting a new class of learning algorithms for deep networks, including deep networks of threshold gates or artificial neurons.© Springer Science+Business Media, LLC 2011.",Autoencoders; Boolean circuits; Clustering; Computational complexity,"383, 403",,"Designs, Codes, and Cryptography",Article,Scopus
1129,,A Java simulator of Rescorla and Wagner's prediction error model and configural cue extensions,"Alonso E., Mondragón E., Fernández A.",2012,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865753966&doi=10.1016%2fj.cmpb.2012.02.004&partnerID=40&md5=c5a6a000aadfbe813a6f297fe1c45900,10.1016/j.cmpb.2012.02.004,"In this paper we present the ""R&W Simulator""(version 3.0), a Java simulator of Rescorla and Wagner's prediction error model of learning. It is able to run whole experimental designs, and compute and display the associative values of elemental and compound stimuli simultaneously, as well as use extra configural cues in generating compound values; it also permits change of the US parameters across phases. The simulator produces both numerical and graphical outputs, and includes a functionality to export the results to a data processor spreadsheet. It is user-friendly, and built with a graphical interface designed to allow neuroscience researchers to input the data in their own ""language"". It is a cross-platform simulator, so it does not require any special equipment, operative system or support program, and does not need installation. The ""R&W Simulator""(version 3.0) is available free. © 2012 Elsevier Ireland Ltd.",Classical conditioning; Compound stimuli; Configural cue; Java simulator; Open-source; Platform independent; Prediction error learning,"346, 355",,Computer Methods and Programs in Biomedicine,Article,Scopus
1130,,Multiple kernel local leaning-based domain adaptation,"Tao J.-W., Wang S.-T.",2012,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867454211&doi=10.3724%2fSP.J.1001.2012.04240&partnerID=40&md5=bf302af35b1b27239edfcfe80ddd3b39,10.3724/SP.J.1001.2012.04240,"Domain adaptation (or cross domain) learning (DAL) aims to learn a robust target classifier for the target domain, which has none or a few labeled samples, by leveraging labeled samples from the source domain (or auxiliary domain). The key challenge in DAL is how to minimize the maximum distribution distance among different domains. To address the considerable change between feature distributions of different domains, this paper proposes a three-stage multiple kernel local learning-based domain adaptation (MKLDA) scheme:1) MKLDA simultaneously learns a reproduced multiple kernel Hilbert space and a initial support vector machine (SVM) by minimizing both the structure risk functional and the maximum mean discrepancy (MMD) between different domains, thus implementing the initial separation of patterns from target domain; 2) By employing the idea of local learning-based method, MKLDA predicts the label of each data point in target domain based on its neighbors and their labels in the kernel Hilbert space learned in 1); And 3) MKLDA learns a robust kernel classifier to classify the unseen data in target domain with training data well predicted in 2). Experimental results on real world problems show the outperformed or comparable effectiveness of the proposed approach compared to related approaches. © 2012 ISCAS.",Domain adaptation learning; Local learning; Maximum mean discrepancy; Multiple kernel learning; Pattern classification,"2297, 2310",,Ruan Jian Xue Bao/Journal of Software,Article,Scopus
1131,,Transfer learning for cross-company software defect prediction,"Ma Y., Luo G., Zeng X., Chen A.",2012,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855652194&doi=10.1016%2fj.infsof.2011.09.007&partnerID=40&md5=ab8c71703506b92827a9a1fc91fb2db6,10.1016/j.infsof.2011.09.007,"Context: Software defect prediction studies usually built models using within-company data, but very few focused on the prediction models trained with cross-company data. It is difficult to employ these models which are built on the within-company data in practice, because of the lack of these local data repositories. Recently, transfer learning has attracted more and more attention for building classifier in target domain using the data from related source domain. It is very useful in cases when distributions of training and test instances differ, but is it appropriate for cross-company software defect prediction? Objective: In this paper, we consider the cross-company defect prediction scenario where source and target data are drawn from different companies. In order to harness cross company data, we try to exploit the transfer learning method to build faster and highly effective prediction model. Method: Unlike the prior works selecting training data which are similar from the test data, we proposed a novel algorithm called Transfer Naive Bayes (TNB), by using the information of all the proper features in training data. Our solution estimates the distribution of the test data, and transfers cross-company data information into the weights of the training data. On these weighted data, the defect prediction model is built. Results: This article presents a theoretical analysis for the comparative methods, and shows the experiment results on the data sets from different organizations. It indicates that TNB is more accurate in terms of AUC (The area under the receiver operating characteristic curve), within less runtime than the state of the art methods. Conclusion: It is concluded that when there are too few local training data to train good classifiers, the useful knowledge from different-distribution training data on feature level may help. We are optimistic that our transfer learning method can guide optimal resource allocation strategies, which may reduce software testing cost and increase effectiveness of software testing process. © 2011 Elsevier B.V. All rights reserved.",Different distribution; Machine learning; Naive Bayes; Software defect prediction; Transfer learning,"248, 256",,Information and Software Technology,Article,Scopus
1132,,Imaginary phone: Learning imaginary interfaces by transferring spatial memory from a familiar device,"Gustafson S., Holz C., Baudisch P.",2011,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80755168363&doi=10.1145%2f2047196.2047233&partnerID=40&md5=d9c71ead13a594a3f0c9514c169f7d4b,10.1145/2047196.2047233,"We propose a method for learning how to use an imaginary interface (i.e., a spatial non-visual interface) that we call ""transfer learning"". By using a physical device (e.g. an iPhone) a user inadvertently learns the interface and can then transfer that knowledge to an imaginary interface. We illustrate this concept with our Imaginary Phone prototype. With it users interact by mimicking the use of a physical iPhone by tapping and sliding on their empty non-dominant hand without visual feedback. Pointing on the hand is tracked using a depth camera and touch events are sent wirelessly to an actual iPhone, where they invoke the corresponding actions. Our prototype allows the user to perform everyday task such as picking up a phone call or launching the timer app and setting an alarm. Imaginary Phone thereby serves as a shortcut that frees users from the necessity of retrieving the actual physical device. We present two user studies that validate the three assumptions underlying the transfer learning method. (1) Users build up spatial memory automatically while using a physical device: participants knew the correct location of 68% of their own iPhone home screen apps by heart. (2) Spatial memory transfers from a physical to an imaginary interface: participants recalled 61% of their home screen apps when recalling app location on the palm of their hand. (3) Palm interaction is precise enough to operate a typical mobile phone: Participants could reliably acquire 0.95cm wide iPhone targets on their palm - sufficiently large to operate any iPhone standard widget. © 2011 ACM.",Design; Experimentation; Human factors,"283, 292",,UIST'11 - Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology,Conference Paper,Scopus
1133,,A simulation study of deep belief network combined with the self-organizing mechanism of adaptive resonance theory,"Wu Y., Cai H.J.",2010,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79951620206&doi=10.1109%2fCISE.2010.5677265&partnerID=40&md5=f221e0967a2257cede9fe10df095d918,10.1109/CISE.2010.5677265,"Computer simulation study of brain neuronal networks is an active academic field. Deep Belief Network (DBN) introduces an effective way of training deep neural networks and the Adaptive Resonance Theory (ART) puts forward a two-layer competitive network emulating human cognitive processes. In our study, we implement a DBN with the mechanism of ART which benefits from DBN's multi-layer structure and ART's self-organizing stable learning mechanism. Our preliminary results show that the optimal number of layers is relevant to the data learned. The correct reconstruction rate decreases slowly with respect to the volume of data stored. ©2010 IEEE.",Adaptive resonance theory; Deep belief networks; Machine learning; Neural network; Pattern recognition,,,"2010 International Conference on Computational Intelligence and Software Engineering, CiSE 2010",Conference Paper,Scopus
1134,,Web wrapper generation using tree alignment and transfer learning,"Xia Y., Zhang S., Yu H.",2010,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956501570&partnerID=40&md5=20989e7f3ae96f1dd4306cf94b38d23e,,"This paper studies the web wrapper generation for web pages of forum, blog and news web sites. While more and more web pages are dynamically generated using a common template populated with data from databases. This paper proposes a novel method that uses tree alignment and transfer learning method to generate the wrapper from this kind of web pages. We present a new tree alignment algorithm to find the best matching structure of the input web pages. A kind of linear regression method is employed to get the weight of different tag-matching. Based on the alignment, we merge the trees into one union tree whose nodes record the statistical information gotten from multiple web pages. We use a transfer learning method to find the most likely content block and use the alignment algorithm to detect the repeat patterns on the union tree. After that, we generate a wrapper to extract data from web pages. Experimental results show that the method can achieve high extraction accuracy and has steady performance.",Alignment; Tree; Wrapper,"410, 415",,"2nd International Conference on Software Engineering and Data Mining, SEDM 2010",Conference Paper,Scopus
1135,,Clustering web images by correlation mining of image-text,"Wu F., Han Y.-H., Zhuang Y.-T., Shao J.",2010,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955110681&doi=10.3724%2fSP.J.1001.2010.03704&partnerID=40&md5=3eaa295bd65a3378249095f6f4028092,10.3724/SP.J.1001.2010.03704,"To cluster the retrieval results of Web image, a framework for the clustering is proposed in this paper. It explores the surrounding text to mine the correlations between words and images and therefore the correlations are used to improve clustering results. Two kinds of correlations, namely word to image and word to word correlations, are mainly considered. As a standard text process technique, tf-idf method cannot measure the correlation of word to image directly. Therefore, this paper proposes to combine tf-idf method with a feature of word, namely visibility, to infer the correlation of word to image. Through LDA model, it defines a topic relevance function to compute the weights of word to word correlations. Finally, complex graph clustering and spectral co-clustering algorithms are used to testify the effect of introducing visibility and topic relevance into image clustering. Encouraging experimental results are reported in this paper. © by Institute of Software, the Chinese Academy of Sciences. All rights reserved.",Complex graph; Graph clustering; Latent Dirichlet allocation; Spectral clustering; Visibility,"1561, 1575",,Ruan Jian Xue Bao/Journal of Software,Article,Scopus
1136,,Bayesian task-level transfer learning for non-linear regression,"Yang P., Tan Q., Ding Y.",2008,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-61849148813&doi=10.1109%2fCSSE.2008.1612&partnerID=40&md5=e01af6747bcf6db8aeeb32e1c58f3e0c,10.1109/CSSE.2008.1612,"Mullti-task learning utilizes labeled data from other ""similar"" tasks and can achieve efficient knowledge-sharing between tasks. Previous research mainly focused on multi-task learning for linear regression. In this paper, a novel Bayesian multi-task learning model for non-linear regression, i.e. HiRBF, is proposed. HiRBF is constructed under a hierarchical Bayesian framework. In the model all tasks are combined in a single RBF network. The input-to-hidden weights are shared between tasks, and the hidden-to-output weights are assumed to be sampled randomly from a certain prior distribution. The HiRBF algorithm is compared with two transfer-unaware approaches. The experiments demonstrate that HiRBF significantly outperforms the others. © 2008 IEEE.",Bayesian hierarchical model; RBF network; Regression; Transfer learning,"62, 65",,"Proceedings - International Conference on Computer Science and Software Engineering, CSSE 2008",Conference Paper,Scopus
1137,,An extension of Fault-Prone filtering using precise training and a dynamic threshold,"Hata H., Mizuno O., Kikuno T.",2008,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-57049183546&doi=10.1145%2f1370750.1370772&partnerID=40&md5=7d947ad3065c547bd9a2c806d024c47d,10.1145/1370750.1370772,"Fault-prone module detection in source code is important for assurance of software quality. Most previous fault-prone detection approaches have been based on software metrics. Such approaches, however, have difficulties in collecting the metrics and in constructing mathematical models based on the metrics. To mitigate such difficulties, we have proposed a novel approach for detecting fault-prone modules using a spam-filtering technique, named Fault-Prone Filtering. In our approach, fault-prone modules are detected in such a way that the source code modules are considered as text files and are applied to the spam filter directly. In practice, we use the training only errors procedure and apply this procedure to fault-prone. Since no pre-training is required, this procedure can be applied to an actual development field immediately.This paper describes an extension of the training only errors procedures. We introduce a precise unit of training, ""modified lines of code,"" instead of methods. In addition, we introduce the dynamic threshold for classification. The result of the experiment shows that our extension leads to twice the precision with about the same recall, and improves 15% on the best F1 measurement. Copyright 2008 ACM.",Fault-prone modules; Spam filter; Text mining,"89, 97",,Proceedings - International Conference on Software Engineering,Conference Paper,Scopus
1138,,Training on errors experiment to detect fault-prone software modules by spam filter,"Mizuno O., Kikuno T.",2007,Scopus,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-37849003535&doi=10.1145%2f1287624.1287683&partnerID=40&md5=0d10e14eb40c926308e396f93039af10,10.1145/1287624.1287683,"The fault-prone module detection in source code is of importance for assurance of software quality. Most of previous fault-prone detection approaches are based on software metrics. Such approaches, however, have difficulties in collecting the metrics and constructing mathematical models based on the metrics. In order to mitigate such difficulties, we propose a novel approach for detecting fault-prone modules using a spam filtering technique, named Fault-Prone Filtering. Because of the increase of needs for spam e-mail detection, the spam filtering technique has been progressed as a convenient and effective technique for text mining. In our approach, fault-prone modules are detected in a way that the source code modules are considered as text files and are applied to the spam filter directly. This paper describes the training on errors procedure to apply fault-prone filtering in practice. Since no pre-training is required, this procedure can be applied to actual development field immediately. In order to show the usefulness of our approach, we conducted an experiment using a large source code repository of Java based open source project. The result of experiment shows that our approach can classify about 85% of software modules correctly. The result also indicates that fault-prone modules can be detected relatively low cost at an early stage. Copyright 2007 ACM.",Fault-prone modules; Spam filter; Text mining,"405, 414",,"6th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering, ESEC/FSE 2007",Conference Paper,Scopus
