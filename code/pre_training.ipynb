{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ImpactPretraining/impact_pre-training/blob/main/code/pre_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6cABHZVpSpmU"
      },
      "outputs": [],
      "source": [
        "# from google.colab import auth\n",
        "# auth.authenticate_user()\n",
        "\n",
        "import os\n",
        "os.environ['USE_AUTH_EPHEM'] = '0'\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "#@title ## Set Your GCS credential\n",
        "project_id = 'literaturereview-358312' #@param {type:\"string\"}\n",
        "bucket_name = 'literature_review' #@param {type:\"string\"}\n",
        "\n",
        "!gcloud config set project {project_id}\n",
        "\n",
        "!pip3 install --upgrade pip\n",
        "!pip install -qU t5==0.9.2\n",
        "!pip install -q tensorflow-text==2.8.0rc0\n",
        "!pip3 install keras==2.7.0\n",
        "!pip3 install gin-config\n",
        "\n",
        "import functools\n",
        "import os\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import t5\n",
        "\n",
        "tf.flags.DEFINE_string('f','','')\n",
        "\n",
        "#Set the base dir(Google cloud bucket)\n",
        "BASE_DIR = \"gs://\" + bucket_name \n",
        "\n",
        "if not BASE_DIR or BASE_DIR == \"gs://\":\n",
        "  raise ValueError(\"You must enter a BASE_DIR.\")\n",
        "ON_CLOUD = True\n",
        "\n",
        "\n",
        "if ON_CLOUD:\n",
        "  import tensorflow_gcs_config\n",
        "  from google.colab import auth\n",
        "  # Set credentials for GCS reading/writing from Colab and TPU.\n",
        "  TPU_TOPOLOGY = \"2x2\"\n",
        "  try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "    TPU_ADDRESS = tpu.get_master()\n",
        "    print('Running on TPU:', TPU_ADDRESS)\n",
        "  except ValueError:\n",
        "    raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "  auth.authenticate_user()\n",
        "  tf.compat.v1.enable_eager_execution(config=None, device_policy=None, execution_mode=None)\n",
        "  tf.config.experimental_connect_to_host(TPU_ADDRESS)\n",
        "  tensorflow_gcs_config.configure_gcs_from_colab_auth()\n",
        "\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "# Improve logging.\n",
        "from contextlib import contextmanager\n",
        "import logging as py_logging\n",
        "\n",
        "if ON_CLOUD:\n",
        "  tf.get_logger().propagate = False\n",
        "  py_logging.root.setLevel('INFO')\n",
        "\n",
        "@contextmanager\n",
        "def tf_verbosity_level(level):\n",
        "  og_level = tf.logging.get_verbosity()\n",
        "  tf.logging.set_verbosity(level)\n",
        "  yield\n",
        "  tf.logging.set_verbosity(og_level)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppqr24dbUfDg"
      },
      "outputs": [],
      "source": [
        "#@title ## Set pre-training dataset path\n",
        "masked_pretraining_dataset_path = \"gs://literature_review/data/pre-training/MLM/MLM.tsv\" #@param {type:\"string\"}\n",
        "\n",
        "nq_tsv_path = {\n",
        "    \"train\": masked_pretraining_dataset_path\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtE9DUzgUtw8"
      },
      "outputs": [],
      "source": [
        "from t5.data import postprocessors as t5_postprocessors\n",
        "from t5.seqio import Feature,SentencePieceVocabulary\n",
        "\n",
        "#@title ## Set tokenizer's model and vocab paths\n",
        "vocab_model_path = 'gs://literature_review/tokenizer/BPE_Model.model' #@param {type:\"string\"} \n",
        "vocab_path = 'gs://literature_review/tokenizer/BPE_Model.vocab' #@param {type:\"string\"}\n",
        "\n",
        "TaskRegistry = t5.data.TaskRegistry\n",
        "TfdsTask = t5.data.TfdsTask\n",
        "\n",
        "def get_default_vocabulary():\n",
        "  return SentencePieceVocabulary(vocab_model_path, 100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oiiUUWHUVI9E"
      },
      "outputs": [],
      "source": [
        "DEFAULT_OUTPUT_FEATURES = {\n",
        "    \"inputs\": Feature(\n",
        "        vocabulary=get_default_vocabulary(), add_eos=False, required=True),\n",
        "\n",
        "    \"targets\": Feature(\n",
        "        vocabulary=get_default_vocabulary(), add_eos=False)\n",
        "}\n",
        "\n",
        "def nq_dataset_fn(split, shuffle_files=True):\n",
        "  # We only have one file for each split.\n",
        "  del shuffle_files\n",
        "\n",
        "  # Load lines from the text file as examples.\n",
        "  ds = tf.data.TextLineDataset(nq_tsv_path[split])\n",
        "  ds = ds.map(\n",
        "      functools.partial(tf.io.decode_csv, record_defaults=[\"string\",\"string\"],\n",
        "                        field_delim=\"\\t\", use_quote_delim=False),\n",
        "      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  ds = ds.map(lambda *ex: dict(zip([\"input\", \"output\"], ex)))\n",
        "  return ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0C-Lpna5VlaX"
      },
      "outputs": [],
      "source": [
        "def preprocessing(ds):\n",
        "  def to_inputs_and_targets(ex):\n",
        "        inputs = tf.strings.join([ex['input']], separator=' ')\n",
        "        class_label = tf.strings.join([ex['output']], separator=' ')\n",
        "        return {'inputs': inputs, 'targets': class_label }\n",
        "  return ds.map(to_inputs_and_targets, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "#Create a new training task\n",
        "t5.data.TaskRegistry.remove('pretraining')\n",
        "t5.data.TaskRegistry.add(\n",
        "    \"pretraining\",\n",
        "    t5.data.Task,\n",
        "    dataset_fn=nq_dataset_fn,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    text_preprocessor=[preprocessing],\n",
        "    output_features = DEFAULT_OUTPUT_FEATURES,\n",
        "    metric_fns=[t5.evaluation.metrics.accuracy],\n",
        ")\n",
        "\n",
        "nq_task = t5.data.TaskRegistry.get(\"pretraining\")\n",
        "ds = nq_task.get_dataset(split=\"train\", sequence_length={\"inputs\": 512, \"targets\": 512})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "peTVBFiZWErv"
      },
      "outputs": [],
      "source": [
        "from mesh_tensorflow.transformer.learning_rate_schedules import learning_rate_schedule_noam\n",
        "\n",
        "MODEL_SIZE = \"small\"  \n",
        "\n",
        "#@title ## Set the output model folder path\n",
        "MODEL_DIR = 'gs://literature_review/models/pre-training/MLM/' #@param {type:\"string\"} \n",
        "\n",
        "model_parallelism, train_batch_size, keep_checkpoint_max = {\n",
        "    \"small\": (1, 256, 16),\n",
        "    \"base\": (2, 128, 8),\n",
        "    \"large\": (8, 64, 4),\n",
        "    \"3B\": (8, 16, 1),\n",
        "    \"11B\": (8, 16, 1)}[MODEL_SIZE]\n",
        "\n",
        "tf.io.gfile.makedirs(MODEL_DIR)\n",
        "from t5 import models\n",
        "\n",
        "model = t5.models.MtfModel(\n",
        "    model_dir=MODEL_DIR,\n",
        "    tpu=TPU_ADDRESS,\n",
        "    tpu_topology=TPU_TOPOLOGY,\n",
        "    model_parallelism=model_parallelism,\n",
        "    batch_size=train_batch_size,\n",
        "    sequence_length={\"inputs\": 512, \"targets\": 512},\n",
        "    learning_rate_schedule = learning_rate_schedule_noam,\n",
        "    save_checkpoints_steps=10000,\n",
        "    keep_checkpoint_max=keep_checkpoint_max if ON_CLOUD else None\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9cnx1dwTW7R2"
      },
      "outputs": [],
      "source": [
        "# We used 156250 TRAIN_STEPS (40 epochs) \n",
        "#@title ## Set the gin file path\n",
        "PATH_GIN_FILE = \"gs://literature_review/utils/operative_config.gin\" #@param {type:\"string\"}\n",
        "import gin\n",
        "with gin.unlock_config():    \n",
        "    gin.parse_config_file(PATH_GIN_FILE)\n",
        "    TRAIN_STEPS = 156250\n",
        "    model.train(\"pretraining\", steps=TRAIN_STEPS)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}