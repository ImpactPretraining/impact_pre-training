{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fine_tuning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ImpactPretraining/impact_pre-training/blob/main/fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPjBRwKu1EOg",
        "outputId": "12556060-04f7-482e-ee78-bd44275f0137"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (22.2.2)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras==2.7.0\n",
            "  Using cached keras-2.7.0-py2.py3-none-any.whl (1.3 MB)\n",
            "Installing collected packages: keras\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.8.0\n",
            "    Uninstalling keras-2.8.0:\n",
            "      Successfully uninstalled keras-2.8.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.2+zzzcolab20220822160911 requires keras<2.9,>=2.8.0rc0, but you have keras 2.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-2.7.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.7/dist-packages (0.5.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on TPU: grpc://10.34.251.50:8470\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import auth\n",
        "# auth.authenticate_user()\n",
        "\n",
        "import os\n",
        "os.environ['USE_AUTH_EPHEM'] = '0'\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "#@title ## Set Your GCS credential\n",
        "project_id = 'literaturereview-358312' #@param {type:\"string\"}\n",
        "bucket_name = 'literature_review' #@param {type:\"string\"}\n",
        "\n",
        "!gcloud config set project {project_id}\n",
        "\n",
        "!pip3 install --upgrade pip\n",
        "!pip install -qU t5==0.9.2\n",
        "!pip install -q tensorflow-text==2.8.0rc0\n",
        "!pip3 install keras==2.7.0\n",
        "!pip3 install gin-config\n",
        "\n",
        "import functools\n",
        "import os\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import t5\n",
        "\n",
        "tf.flags.DEFINE_string('f','','')\n",
        "\n",
        "#Set the base dir(Google cloud bucket)\n",
        "BASE_DIR = \"gs://\" + bucket_name \n",
        "\n",
        "if not BASE_DIR or BASE_DIR == \"gs://\":\n",
        "  raise ValueError(\"You must enter a BASE_DIR.\")\n",
        "ON_CLOUD = True\n",
        "\n",
        "\n",
        "if ON_CLOUD:\n",
        "  import tensorflow_gcs_config\n",
        "  from google.colab import auth\n",
        "  # Set credentials for GCS reading/writing from Colab and TPU.\n",
        "  TPU_TOPOLOGY = \"2x2\"\n",
        "  try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "    TPU_ADDRESS = tpu.get_master()\n",
        "    print('Running on TPU:', TPU_ADDRESS)\n",
        "  except ValueError:\n",
        "    raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "  auth.authenticate_user()\n",
        "  tf.compat.v1.enable_eager_execution(config=None, device_policy=None, execution_mode=None)\n",
        "  tf.config.experimental_connect_to_host(TPU_ADDRESS)\n",
        "  tensorflow_gcs_config.configure_gcs_from_colab_auth()\n",
        "\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "# Improve logging.\n",
        "from contextlib import contextmanager\n",
        "import logging as py_logging\n",
        "\n",
        "if ON_CLOUD:\n",
        "  tf.get_logger().propagate = False\n",
        "  py_logging.root.setLevel('INFO')\n",
        "\n",
        "@contextmanager\n",
        "def tf_verbosity_level(level):\n",
        "  og_level = tf.logging.get_verbosity()\n",
        "  tf.logging.set_verbosity(level)\n",
        "  yield\n",
        "  tf.logging.set_verbosity(og_level)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## training and validation sets\n",
        "\n",
        "#@title ## Set training and valitadion dataset paths\n",
        "training_set_path = 'gs://literature_review/data/fine-tuning/bug-fix/train.tsv' #@param {type:\"string\"} \n",
        "validation_set_path = 'gs://literature_review/data/fine-tuning/bug-fix/val.tsv' #@param {type:\"string\"}\n",
        "\n",
        "nq_tsv_path = {\n",
        "    \"train\": training_set_path,\n",
        "    \"validation\": validation_set_path\n",
        "}\n",
        "\n",
        "!gsutil cp {nq_tsv_path[\"train\"]} ./train.tsv\n",
        "!gsutil cp {nq_tsv_path[\"validation\"]} ./val.tsv\n",
        "\n",
        "data_train = len([line for line in open('./train.tsv', 'r')])\n",
        "data_val = len([line for line in open('./val.tsv', 'r')])\n",
        "\n",
        "num_nq_examples = dict(train=data_train, validation=data_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-U_JygM1en4",
        "outputId": "5050046b-2c9d-41cf-f4f8-0a68954a291b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying gs://literature_review/data/fine-tuning/bug-fix/train.tsv...\n",
            "/ [1 files][ 28.8 MiB/ 28.8 MiB]                                                \n",
            "Operation completed over 1 objects/28.8 MiB.                                     \n",
            "Copying gs://literature_review/data/fine-tuning/bug-fix/val.tsv...\n",
            "/ [1 files][  3.6 MiB/  3.6 MiB]                                                \n",
            "Operation completed over 1 objects/3.6 MiB.                                      \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from t5.data import postprocessors as t5_postprocessors\n",
        "from t5.seqio import Feature,SentencePieceVocabulary\n",
        "\n",
        "#@title ## Set tokenizer's model and vocab paths\n",
        "vocab_model_path = 'gs://literature_review/tokenizer/BPE_Model.model' #@param {type:\"string\"} \n",
        "vocab_path = 'gs://literature_review/tokenizer/BPE_Model.vocab' #@param {type:\"string\"}\n",
        "\n",
        "TaskRegistry = t5.data.TaskRegistry\n",
        "TfdsTask = t5.data.TfdsTask\n",
        "\n",
        "def get_default_vocabulary():\n",
        "  return SentencePieceVocabulary(vocab_model_path, 100)\n",
        "\n",
        "DEFAULT_OUTPUT_FEATURES = {\n",
        "    \"inputs\": Feature(\n",
        "        vocabulary=get_default_vocabulary(), add_eos=True, required=False),\n",
        "\n",
        "    \"targets\": Feature(\n",
        "        vocabulary=get_default_vocabulary(), add_eos=True)\n",
        "}"
      ],
      "metadata": {
        "id": "IfwXi6Ee1uLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## Set fine-tuning task\n",
        "fine_tuning_task = 'bug_fix' #@param [\"bug_fix\", \"code_completion\", \"code_summarization\"]\n",
        "\n",
        "def nq_dataset(split, shuffle_files=True):\n",
        "  # We only have one file for each split.\n",
        "  del shuffle_files\n",
        "\n",
        "  # Load lines from the text file as examples.\n",
        "  ds = tf.data.TextLineDataset(nq_tsv_path[split])\n",
        "  ds = ds.map(\n",
        "      functools.partial(tf.io.decode_csv, record_defaults=[\"string\",\"string\"],\n",
        "                        field_delim=\"\\t\", use_quote_delim=False),\n",
        "      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  \n",
        "  ds = ds.map(lambda *ex: dict(zip([\"input\", \"output\"], ex)))\n",
        "  return ds\n",
        "\n",
        "def preprocessing(ds):\n",
        "  def to_inputs_and_targets(ex):\n",
        "        inputs = tf.strings.join([ex['input']], separator=' ')\n",
        "        class_label = tf.strings.join([ex['output']], separator=' ')\n",
        "        return {'inputs': inputs, 'targets': class_label }\n",
        "    \n",
        "  return ds.map(to_inputs_and_targets, \n",
        "                num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  \n",
        "t5.data.TaskRegistry.remove(fine_tuning_task)\n",
        "t5.data.TaskRegistry.add(\n",
        "    fine_tuning_task,\n",
        "    dataset_fn=nq_dataset_bug_fix,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    text_preprocessor=[bug_fix_preprocessing],\n",
        "    output_features = DEFAULT_OUTPUT_FEATURES,\n",
        "    metric_fns=[t5.evaluation.metrics.accuracy],\n",
        "    num_input_examples=num_nq_examples_bug_fix\n",
        ")\n",
        "\n",
        "nq_task = t5.data.TaskRegistry.get(fine_tuning_task)\n",
        "ds = nq_task.get_dataset(split=\"train\", sequence_length={\"inputs\": 512, \"targets\": 512})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIvNjIMd19Em",
        "outputId": "f01f0d71-06d8-4573-a8ca-3b8c28e453a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Using an uncached FunctionDataset for training is not recommended since it often results in insufficient shuffling on restarts, resulting in overfitting. It is highly recommended that you cache this task before training with it or use a data source that supports lower-level shuffling (e.g., FileDataSource).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def _rate_num_input_examples(task):\n",
        "  if \"train\" in task.splits:\n",
        "    return float(task.num_input_examples(\"train\"))\n",
        "  elif \"validation\" in task.splits:\n",
        "    return float(task.num_input_examples(\"validation\"))\n",
        "  else:\n",
        "    raise ValueError(\"Task %s does not have a train or validation split.\" % (task.name))"
      ],
      "metadata": {
        "id": "aE1gbJ-n3wg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## BUG-FIX\n",
        "t5.data.MixtureRegistry.remove(fine_tuning_task)\n",
        "t5.data.MixtureRegistry.add(\n",
        "    fine_tuning_task,\n",
        "    [fine_tuning_task],\n",
        "    default_rate=_rate_num_input_examples\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Gz_WfLq35fw",
        "outputId": "4720efff-114c-4f52-a4d7-ae2971fccf51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<seqio.dataset_providers.Mixture at 0x7f5c2a098810>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## Select fine-tuning with or without pre-training, pre-trained checkpoint path, output model path\n",
        "fine_tuning = \"fine-tuning_with_pre-training/\" #@param [\"fine-tuning_with_pre-training/\", \"fine-tuning_without_pre-training/\"]\n",
        "\n",
        "# Specify the pre-trained dir (if needed) which must contain:\n",
        "#  - the pre-trained models,\n",
        "#  - the operative_config.gin file \n",
        "#  - the checkpoint files as well\n",
        "PRETRAINED_DIR= 'gs://literature_review/models/pre-training/MLM/' #@param {type:\"string\"} \n",
        "\n",
        "############ output path ############\n",
        "MODEL_DIR = 'gs://literature_review/models/fine-tuning/bug_fix/MLM/' #@param {type:\"string\"} \n",
        "\n",
        "# our T5 selected architecture\n",
        "MODEL_SIZE = \"small\"\n",
        "\n",
        "model_parallelism, train_batch_size, keep_checkpoint_max = {\n",
        "    \"small\": (1, 256, 200),\n",
        "    \"base\": (2, 128, 8),\n",
        "    \"large\": (8, 64, 4),\n",
        "    \"3B\": (8, 16, 1),\n",
        "    \"11B\": (8, 16, 1)}[MODEL_SIZE]\n"
      ],
      "metadata": {
        "id": "tVkaJcu54F-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mesh_tensorflow.transformer.learning_rate_schedules import slanted_triangular \n",
        "from mesh_tensorflow.transformer.learning_rate_schedules import truncated_rsqrt\n",
        "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
        "\n",
        "starter_learning_rate = 0.05\n",
        "end_learning_rate = 0.001\n",
        "decay_steps = 10000\n",
        "\n",
        "learning_rate_fn = PolynomialDecay(\n",
        "    starter_learning_rate,\n",
        "    decay_steps,\n",
        "    end_learning_rate,\n",
        "    power=0.5)\n",
        "\n",
        "# learning rate scheduler\n",
        "selected_learning_rate_scheduler = slanted_triangular\n",
        "PATH_GIN_FILE = 'gs://literature_review/utils/operative_config_slanted.gin'\n",
        "\n",
        "# changed by Sara\n",
        "#@title Select a learning rate scheduler\n",
        "number_of_steps = 300000 #@param {type:\"integer\"}\n",
        "\n",
        "pretraining_steps = 0\n",
        "if fine_tuning == \"fine-tuning_with_pre-training/\":\n",
        "  pretraining_steps = 156250\n",
        "\n",
        "tf.io.gfile.makedirs(MODEL_DIR)\n",
        "\n",
        "from t5 import models\n",
        "\n",
        "model = t5.models.MtfModel(\n",
        "    model_dir=MODEL_DIR,\n",
        "    tpu=TPU_ADDRESS,\n",
        "    tpu_topology=TPU_TOPOLOGY,\n",
        "    model_parallelism=model_parallelism,\n",
        "    batch_size=train_batch_size,\n",
        "    learning_rate_schedule = selected_learning_rate_scheduler,\n",
        "    sequence_length={\"inputs\": 512, \"targets\": 512},\n",
        "    save_checkpoints_steps=10000,\n",
        "    keep_checkpoint_max=keep_checkpoint_max if ON_CLOUD else None,\n",
        "    iterations_per_loop=100,\n",
        ")\n",
        "\n",
        "!gsutil cp {PATH_GIN_FILE}  ./config.gin\n",
        "# modify gin file\n",
        "gin_lines = [line for line in open(\"./config.gin\")]\n",
        "f = open(\"./config.gin\", \"w+\")\n",
        "for i in range(len(gin_lines)):\n",
        "  if i == 196 and fine_tuning == \"fine-tuning_without_pre-training/\":\n",
        "    line = \"slanted_triangular.start_step = 0\\n\"\n",
        "    f.write(line)\n",
        "    continue\n",
        "  if i == 197:\n",
        "    line = \"slanted_triangular.total_train_steps = \" + str(number_of_steps + pretraining_steps) + '\\n'\n",
        "    f.write(line)\n",
        "    continue\n",
        "  f.write(gin_lines[i])\n",
        "f.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhvW1KRc4C-i",
        "outputId": "5fbf003a-7017-4c18-fb9e-22795d56e783"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying gs://literature_review/utils/operative_config_slanted.gin...\n",
            "/ [0 files][    0.0 B/ 11.6 KiB]                                                \r/ [1 files][ 11.6 KiB/ 11.6 KiB]                                                \r\n",
            "Operation completed over 1 objects/11.6 KiB.                                     \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gin\n",
        "\n",
        "if fine_tuning == \"fine-tuning_without_pre-training/\":\n",
        "  # NON PRETRAINED\n",
        "  with gin.unlock_config():    \n",
        "      gin.parse_config_file(\"./config.gin\")\n",
        "      TRAIN_STEPS = number_of_steps\n",
        "      model.train(task, steps=number_of_steps)\n",
        "\n",
        "else:\n",
        "  # PRETRAINED\n",
        "  with gin.unlock_config():\n",
        "      gin.parse_config_file(\"./config.gin\")\n",
        "      #RUN FINE-TUNING\n",
        "      model.finetune(\n",
        "          mixture_or_task_name=task,\n",
        "          pretrained_model_dir=PRETRAINED_DIR,\n",
        "            finetune_steps=number_of_steps\n",
        "      )"
      ],
      "metadata": {
        "id": "3t5bovRK7C1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "UQWX8XXJ5w8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoints = [x for x in range(10000, 500000, 10000)]\n",
        "\n",
        "# Use a larger batch size for evaluation, which requires less memory.\n",
        "model.batch_size = 1024\n",
        "model.eval(\n",
        "    mixture_or_task_name=task,\n",
        "    # -1 will evaluate the last checkpoint, you can also provide \n",
        "    # a list of checkpoints with the following format : [10000, 20000, 30000]\n",
        "    checkpoint_steps=checkpoints,\n",
        "    split=\"validation\"\n",
        "    )"
      ],
      "metadata": {
        "id": "ZkJ9hPAubW8k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3678da9b-20d7-4515-d56d-f108d5e35af1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:system_path_file_exists:gs://literature_review/models/fine-tuning/bug_fix/MI_MLM_NSP_RTD/operative_config.gin\n",
            "ERROR:root:Path not found: gs://literature_review/models/fine-tuning/bug_fix/MI_MLM_NSP_RTD/operative_config.gin\n",
            "INFO:absl:Using checkpoint at step 166250 which is closest to requested step 170000\n",
            "INFO:absl:Using checkpoint at step 176250 which is closest to requested step 180000\n",
            "INFO:absl:Using checkpoint at step 186250 which is closest to requested step 190000\n",
            "INFO:absl:Using checkpoint at step 196250 which is closest to requested step 200000\n",
            "INFO:absl:Using checkpoint at step 206250 which is closest to requested step 210000\n",
            "INFO:absl:Using checkpoint at step 216250 which is closest to requested step 220000\n",
            "INFO:absl:Using checkpoint at step 226250 which is closest to requested step 230000\n",
            "INFO:absl:Using checkpoint at step 236250 which is closest to requested step 240000\n",
            "INFO:absl:Using checkpoint at step 246250 which is closest to requested step 250000\n",
            "INFO:absl:Using checkpoint at step 256250 which is closest to requested step 260000\n",
            "INFO:absl:Using checkpoint at step 266250 which is closest to requested step 270000\n",
            "INFO:absl:Using checkpoint at step 276250 which is closest to requested step 280000\n",
            "INFO:absl:Using checkpoint at step 286250 which is closest to requested step 290000\n",
            "INFO:absl:Using checkpoint at step 296250 which is closest to requested step 300000\n",
            "INFO:absl:Using checkpoint at step 306250 which is closest to requested step 310000\n",
            "INFO:absl:Using checkpoint at step 316250 which is closest to requested step 320000\n",
            "INFO:absl:Using checkpoint at step 326250 which is closest to requested step 330000\n",
            "INFO:absl:Using checkpoint at step 326250 which is closest to requested step 340000\n",
            "INFO:absl:Using checkpoint at step 326250 which is closest to requested step 350000\n",
            "INFO:absl:Using checkpoint at step 326250 which is closest to requested step 360000\n",
            "INFO:absl:Using checkpoint at step 326250 which is closest to requested step 370000\n",
            "INFO:absl:Using checkpoint at step 326250 which is closest to requested step 380000\n",
            "INFO:absl:Using checkpoint at step 326250 which is closest to requested step 390000\n",
            "INFO:absl:Using checkpoint at step 326250 which is closest to requested step 400000\n",
            "INFO:absl:Using checkpoint at step 326250 which is closest to requested step 410000\n",
            "INFO:absl:Using checkpoint at step 326250 which is closest to requested step 420000\n",
            "INFO:absl:Using checkpoint at step 326250 which is closest to requested step 430000\n",
            "INFO:absl:Using checkpoint at step 326250 which is closest to requested step 440000\n",
            "INFO:absl:Using checkpoint at step 326250 which is closest to requested step 450000\n",
            "INFO:absl:Using checkpoint at step 326250 which is closest to requested step 460000\n",
            "INFO:absl:Using checkpoint at step 326250 which is closest to requested step 470000\n",
            "INFO:absl:Using checkpoint at step 326250 which is closest to requested step 480000\n",
            "INFO:absl:Using checkpoint at step 326250 which is closest to requested step 490000\n",
            "WARNING:absl:bug_fix is both a Task and a Mixture, returning Mixture\n",
            "WARNING:absl:bug_fix is both a Task and a Mixture, returning Mixture\n",
            "INFO:absl:Adding task 'bug_fix' with predict metric_fn(s).\n",
            "WARNING:absl:bug_fix is both a Task and a Mixture, returning Mixture\n",
            "INFO:absl:Automatically caching small dataset in memory: 'bug_fix:validation'\n",
            "INFO:absl:Skipping packing/padding for 'bug_fix' since sequence length is None.\n",
            "INFO:absl:Setting sequence lengths to {'inputs': 527, 'targets': 547}\n",
            "INFO:absl:Evaluating checkpoint step: 166250\n",
            "WARNING:absl:bug_fix is both a Task and a Mixture, returning Mixture\n",
            "INFO:absl:Automatically caching small dataset in memory: 'bug_fix:validation'\n",
            "INFO:absl:Padding 'bug_fix' with sequence lengths: {'inputs': 527, 'targets': 547}\n",
            "SimdMeshImpl ignoring devices ['', '', '', '', '', '', '', '']\n",
            "Using default tf glorot_uniform_initializer for variable encoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "From /usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:825: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer Variable.assign which has equivalent behavior in 2.X.\n",
            "INFO:absl:eval/bug_fix/accuracy at step 166250: 12.437\n",
            "INFO:absl:Evaluating checkpoint step: 176250\n",
            "WARNING:absl:bug_fix is both a Task and a Mixture, returning Mixture\n",
            "INFO:absl:Automatically caching small dataset in memory: 'bug_fix:validation'\n",
            "INFO:absl:Padding 'bug_fix' with sequence lengths: {'inputs': 527, 'targets': 547}\n",
            "SimdMeshImpl ignoring devices ['', '', '', '', '', '', '', '']\n",
            "Using default tf glorot_uniform_initializer for variable encoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "INFO:absl:eval/bug_fix/accuracy at step 176250: 12.401\n",
            "INFO:absl:Evaluating checkpoint step: 186250\n",
            "WARNING:absl:bug_fix is both a Task and a Mixture, returning Mixture\n",
            "INFO:absl:Automatically caching small dataset in memory: 'bug_fix:validation'\n",
            "INFO:absl:Padding 'bug_fix' with sequence lengths: {'inputs': 527, 'targets': 547}\n",
            "SimdMeshImpl ignoring devices ['', '', '', '', '', '', '', '']\n",
            "Using default tf glorot_uniform_initializer for variable encoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "INFO:absl:eval/bug_fix/accuracy at step 186250: 12.401\n",
            "INFO:absl:Evaluating checkpoint step: 196250\n",
            "WARNING:absl:bug_fix is both a Task and a Mixture, returning Mixture\n",
            "INFO:absl:Automatically caching small dataset in memory: 'bug_fix:validation'\n",
            "INFO:absl:Padding 'bug_fix' with sequence lengths: {'inputs': 527, 'targets': 547}\n",
            "SimdMeshImpl ignoring devices ['', '', '', '', '', '', '', '']\n",
            "Using default tf glorot_uniform_initializer for variable encoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "INFO:absl:eval/bug_fix/accuracy at step 196250: 12.975\n",
            "INFO:absl:Evaluating checkpoint step: 206250\n",
            "WARNING:absl:bug_fix is both a Task and a Mixture, returning Mixture\n",
            "INFO:absl:Automatically caching small dataset in memory: 'bug_fix:validation'\n",
            "INFO:absl:Padding 'bug_fix' with sequence lengths: {'inputs': 527, 'targets': 547}\n",
            "SimdMeshImpl ignoring devices ['', '', '', '', '', '', '', '']\n",
            "Using default tf glorot_uniform_initializer for variable encoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "INFO:absl:eval/bug_fix/accuracy at step 206250: 12.366\n",
            "INFO:absl:Evaluating checkpoint step: 216250\n",
            "WARNING:absl:bug_fix is both a Task and a Mixture, returning Mixture\n",
            "INFO:absl:Automatically caching small dataset in memory: 'bug_fix:validation'\n",
            "INFO:absl:Padding 'bug_fix' with sequence lengths: {'inputs': 527, 'targets': 547}\n",
            "SimdMeshImpl ignoring devices ['', '', '', '', '', '', '', '']\n",
            "Using default tf glorot_uniform_initializer for variable encoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "INFO:absl:eval/bug_fix/accuracy at step 216250: 12.294\n",
            "INFO:absl:Evaluating checkpoint step: 226250\n",
            "WARNING:absl:bug_fix is both a Task and a Mixture, returning Mixture\n",
            "INFO:absl:Automatically caching small dataset in memory: 'bug_fix:validation'\n",
            "INFO:absl:Padding 'bug_fix' with sequence lengths: {'inputs': 527, 'targets': 547}\n",
            "SimdMeshImpl ignoring devices ['', '', '', '', '', '', '', '']\n",
            "Using default tf glorot_uniform_initializer for variable encoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "INFO:absl:eval/bug_fix/accuracy at step 226250: 12.545\n",
            "INFO:absl:Evaluating checkpoint step: 236250\n",
            "WARNING:absl:bug_fix is both a Task and a Mixture, returning Mixture\n",
            "INFO:absl:Automatically caching small dataset in memory: 'bug_fix:validation'\n",
            "INFO:absl:Padding 'bug_fix' with sequence lengths: {'inputs': 527, 'targets': 547}\n",
            "SimdMeshImpl ignoring devices ['', '', '', '', '', '', '', '']\n",
            "Using default tf glorot_uniform_initializer for variable encoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "INFO:absl:eval/bug_fix/accuracy at step 236250: 12.760\n",
            "INFO:absl:Evaluating checkpoint step: 246250\n",
            "WARNING:absl:bug_fix is both a Task and a Mixture, returning Mixture\n",
            "INFO:absl:Automatically caching small dataset in memory: 'bug_fix:validation'\n",
            "INFO:absl:Padding 'bug_fix' with sequence lengths: {'inputs': 527, 'targets': 547}\n",
            "SimdMeshImpl ignoring devices ['', '', '', '', '', '', '', '']\n",
            "Using default tf glorot_uniform_initializer for variable encoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "INFO:absl:eval/bug_fix/accuracy at step 246250: 12.616\n",
            "INFO:absl:Evaluating checkpoint step: 256250\n",
            "WARNING:absl:bug_fix is both a Task and a Mixture, returning Mixture\n",
            "INFO:absl:Automatically caching small dataset in memory: 'bug_fix:validation'\n",
            "INFO:absl:Padding 'bug_fix' with sequence lengths: {'inputs': 527, 'targets': 547}\n",
            "SimdMeshImpl ignoring devices ['', '', '', '', '', '', '', '']\n",
            "Using default tf glorot_uniform_initializer for variable encoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "INFO:absl:eval/bug_fix/accuracy at step 256250: 11.935\n",
            "INFO:absl:Evaluating checkpoint step: 266250\n",
            "WARNING:absl:bug_fix is both a Task and a Mixture, returning Mixture\n",
            "INFO:absl:Automatically caching small dataset in memory: 'bug_fix:validation'\n",
            "INFO:absl:Padding 'bug_fix' with sequence lengths: {'inputs': 527, 'targets': 547}\n",
            "SimdMeshImpl ignoring devices ['', '', '', '', '', '', '', '']\n",
            "Using default tf glorot_uniform_initializer for variable encoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "INFO:absl:eval/bug_fix/accuracy at step 266250: 12.401\n",
            "INFO:absl:Evaluating checkpoint step: 276250\n",
            "WARNING:absl:bug_fix is both a Task and a Mixture, returning Mixture\n",
            "INFO:absl:Automatically caching small dataset in memory: 'bug_fix:validation'\n",
            "INFO:absl:Padding 'bug_fix' with sequence lengths: {'inputs': 527, 'targets': 547}\n",
            "SimdMeshImpl ignoring devices ['', '', '', '', '', '', '', '']\n",
            "Using default tf glorot_uniform_initializer for variable encoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "INFO:absl:eval/bug_fix/accuracy at step 276250: 12.581\n",
            "INFO:absl:Evaluating checkpoint step: 286250\n",
            "WARNING:absl:bug_fix is both a Task and a Mixture, returning Mixture\n",
            "INFO:absl:Automatically caching small dataset in memory: 'bug_fix:validation'\n",
            "INFO:absl:Padding 'bug_fix' with sequence lengths: {'inputs': 527, 'targets': 547}\n",
            "SimdMeshImpl ignoring devices ['', '', '', '', '', '', '', '']\n",
            "Using default tf glorot_uniform_initializer for variable encoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-7e8c805c4a41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# a list of checkpoints with the following format : [10000, 20000, 30000]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mcheckpoint_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoints\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"validation\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/t5/models/mtf_model.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, mixture_or_task_name, checkpoint_steps, summary_dir, split, eval_with_score, compute_sequence_length)\u001b[0m\n\u001b[1;32m    353\u001b[0m         sequence_length=(None\n\u001b[1;32m    354\u001b[0m                          if compute_sequence_length else self._sequence_length),\n\u001b[0;32m--> 355\u001b[0;31m         batch_size=self._batch_size)\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m   def finetune(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/t5/models/utils.py\u001b[0m in \u001b[0;36mrun_eval\u001b[0;34m(mixture_or_task_name, predict_or_score_fn, checkpoint_steps, dataset_fn, summary_dir, split, sequence_length, batch_size)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0msummary_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m             \"{}_{}_predictions\".format(task.name, step))\n\u001b[0;32m--> 357\u001b[0;31m         \u001b[0mwrite_lines_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/t5/models/utils.py\u001b[0m in \u001b[0;36mwrite_lines_to_file\u001b[0;34m(lines, filename)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moutput_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0moutput_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, unused_type, unused_value, unused_traceback)\u001b[0m\n\u001b[1;32m    195\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munused_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munused_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munused_traceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;34m\"\"\"Make usable with \"with\" statement.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_buf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_writable_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_writable_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_writable_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predictions\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "veXqF0sNuJLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## Set the model checkpoint you want to test\n",
        "best_checkpoint = 196250 #@param {type:\"integer\"}"
      ],
      "metadata": {
        "id": "1ri5ygGiuh8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load test data\n",
        "import pandas as pd\n",
        "\n",
        "#@title ## set the test set path\n",
        "test_set_path = 'gs://literature_review/data/fine-tuning/bug-fix/test.tsv' #@param {type:\"string\"}\n",
        "!gsutil cp {test_set_path} ./test.tsv\n",
        "\n",
        "data = pd.read_csv('./test.tsv', sep='\\t', names=['source', 'target'])\n",
        "source = list(data['source'])\n",
        "target = list(data['target'])\n",
        "\n",
        "f_src = open('./test_source.txt', 'w')\n",
        "f_tgt = open('./test_target.txt', 'w')\n",
        "for i in range(len(data)):\n",
        "    f_src.write(source[i] + '\\n')\n",
        "    f_tgt.write(target[i] + '\\n')\n",
        "f_src.close()\n",
        "f_tgt.close()"
      ],
      "metadata": {
        "id": "-Fd2ir8rutzr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "333ce66e-e039-4395-b7d6-5001f2e5b8c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying gs://literature_review/data/fine-tuning/bug-fix/test.tsv...\n",
            "/ [1 files][  3.5 MiB/  3.5 MiB]                                                \n",
            "Operation completed over 1 objects/3.5 MiB.                                      \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate predictions\n",
        "model.predict(input_file='./test_source.txt', output_file='./output.txt', checkpoint_steps=[best_checkpoint],\n",
        "              beam_size=1, temperature=1.0, keep_top_k=-1, vocabulary=get_default_vocabulary())"
      ],
      "metadata": {
        "id": "bJLk5Gd1uNJD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f2a9c41-08c7-4421-d31e-96d9d3e4d20a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:system_path_file_exists:gs://literature_review/models/fine-tuning/bug_fix/MI_MLM_NSP_RTD/operative_config.gin\n",
            "ERROR:root:Path not found: gs://literature_review/models/fine-tuning/bug_fix/MI_MLM_NSP_RTD/operative_config.gin\n",
            "SimdMeshImpl ignoring devices ['', '', '', '', '', '', '', '']\n",
            "Using default tf glorot_uniform_initializer for variable encoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# eval predictions\n",
        "predictions = [line.strip() for line in open('./output.txt-' + str(best_checkpoint), 'r')]\n",
        "target = [line.strip() for line in open('./test_target.txt', 'r')]\n",
        "\n",
        "print('num predictions:', len(predictions))\n",
        "print('num target:', len(target))\n",
        "\n",
        "n = len(target)\n",
        "correct_predictions = 0\n",
        "for i in range(n):\n",
        "    pred = predictions[i].replace(' ', '')\n",
        "    tgt = target[i].replace(' ', '')\n",
        "    if pred == tgt:\n",
        "        correct_predictions += 1\n",
        "print('correct predictions: ' + str(correct_predictions)+ '/' + str(n))\n",
        "percent = round(correct_predictions * 100 / n, 2)\n",
        "print(str(percent) + '%')"
      ],
      "metadata": {
        "id": "rdgxRXJn_z6-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1903f7f9-9928-4220-8ac2-062bdfa1c7e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num predictions: 2790\n",
            "num target: 2790\n",
            "correct predictions: 224/2790\n",
            "8.03%\n"
          ]
        }
      ]
    }
  ]
}